{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 200,
  "global_step": 8082,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00024746349913387774,
      "grad_norm": 0.01524986233562231,
      "learning_rate": 5e-05,
      "loss": 0.0821,
      "step": 1
    },
    {
      "epoch": 0.0004949269982677555,
      "grad_norm": 0.009311680682003498,
      "learning_rate": 0.0001,
      "loss": 0.0849,
      "step": 2
    },
    {
      "epoch": 0.0007423904974016332,
      "grad_norm": 0.018598733469843864,
      "learning_rate": 0.00015,
      "loss": 0.0995,
      "step": 3
    },
    {
      "epoch": 0.000989853996535511,
      "grad_norm": 0.04788055270910263,
      "learning_rate": 0.0002,
      "loss": 0.192,
      "step": 4
    },
    {
      "epoch": 0.0012373174956693887,
      "grad_norm": 0.021510114893317223,
      "learning_rate": 0.00025,
      "loss": 0.0812,
      "step": 5
    },
    {
      "epoch": 0.0014847809948032665,
      "grad_norm": 0.046755898743867874,
      "learning_rate": 0.0003,
      "loss": 0.2193,
      "step": 6
    },
    {
      "epoch": 0.0017322444939371442,
      "grad_norm": 0.020434845238924026,
      "learning_rate": 0.00035,
      "loss": 0.1102,
      "step": 7
    },
    {
      "epoch": 0.001979707993071022,
      "grad_norm": 0.01422104611992836,
      "learning_rate": 0.0004,
      "loss": 0.0957,
      "step": 8
    },
    {
      "epoch": 0.0022271714922048997,
      "grad_norm": 0.024364689365029335,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.1153,
      "step": 9
    },
    {
      "epoch": 0.0024746349913387774,
      "grad_norm": 0.01867111772298813,
      "learning_rate": 0.0005,
      "loss": 0.0886,
      "step": 10
    },
    {
      "epoch": 0.002722098490472655,
      "grad_norm": 0.022664912045001984,
      "learning_rate": 0.0004999999810657788,
      "loss": 0.0719,
      "step": 11
    },
    {
      "epoch": 0.002969561989606533,
      "grad_norm": 0.01551616657525301,
      "learning_rate": 0.0004999999242631181,
      "loss": 0.0595,
      "step": 12
    },
    {
      "epoch": 0.0032170254887404106,
      "grad_norm": 0.02176433429121971,
      "learning_rate": 0.0004999998295920264,
      "loss": 0.0569,
      "step": 13
    },
    {
      "epoch": 0.0034644889878742884,
      "grad_norm": 0.03794057294726372,
      "learning_rate": 0.0004999996970525182,
      "loss": 0.1921,
      "step": 14
    },
    {
      "epoch": 0.003711952487008166,
      "grad_norm": 0.019957002252340317,
      "learning_rate": 0.0004999995266446134,
      "loss": 0.0924,
      "step": 15
    },
    {
      "epoch": 0.003959415986142044,
      "grad_norm": 0.03097163885831833,
      "learning_rate": 0.000499999318368338,
      "loss": 0.1363,
      "step": 16
    },
    {
      "epoch": 0.004206879485275922,
      "grad_norm": 0.027564220130443573,
      "learning_rate": 0.0004999990722237234,
      "loss": 0.0763,
      "step": 17
    },
    {
      "epoch": 0.004454342984409799,
      "grad_norm": 0.038623709231615067,
      "learning_rate": 0.0004999987882108069,
      "loss": 0.1108,
      "step": 18
    },
    {
      "epoch": 0.0047018064835436775,
      "grad_norm": 0.026812054216861725,
      "learning_rate": 0.0004999984663296317,
      "loss": 0.0685,
      "step": 19
    },
    {
      "epoch": 0.004949269982677555,
      "grad_norm": 0.05195892974734306,
      "learning_rate": 0.0004999981065802462,
      "loss": 0.1359,
      "step": 20
    },
    {
      "epoch": 0.005196733481811433,
      "grad_norm": 0.07385364919900894,
      "learning_rate": 0.0004999977089627052,
      "loss": 0.1024,
      "step": 21
    },
    {
      "epoch": 0.00544419698094531,
      "grad_norm": 0.06791619956493378,
      "learning_rate": 0.0004999972734770689,
      "loss": 0.1079,
      "step": 22
    },
    {
      "epoch": 0.0056916604800791885,
      "grad_norm": 0.06512481719255447,
      "learning_rate": 0.0004999968001234031,
      "loss": 0.1005,
      "step": 23
    },
    {
      "epoch": 0.005939123979213066,
      "grad_norm": 0.05378355458378792,
      "learning_rate": 0.0004999962889017797,
      "loss": 0.1214,
      "step": 24
    },
    {
      "epoch": 0.006186587478346944,
      "grad_norm": 0.054892756044864655,
      "learning_rate": 0.0004999957398122759,
      "loss": 0.1008,
      "step": 25
    },
    {
      "epoch": 0.006434050977480821,
      "grad_norm": 0.05882688984274864,
      "learning_rate": 0.0004999951528549751,
      "loss": 0.085,
      "step": 26
    },
    {
      "epoch": 0.0066815144766146995,
      "grad_norm": 0.032177913933992386,
      "learning_rate": 0.0004999945280299661,
      "loss": 0.0743,
      "step": 27
    },
    {
      "epoch": 0.006928977975748577,
      "grad_norm": 0.04484466463327408,
      "learning_rate": 0.0004999938653373437,
      "loss": 0.0847,
      "step": 28
    },
    {
      "epoch": 0.007176441474882455,
      "grad_norm": 0.02481239102780819,
      "learning_rate": 0.000499993164777208,
      "loss": 0.0447,
      "step": 29
    },
    {
      "epoch": 0.007423904974016332,
      "grad_norm": 0.03540149703621864,
      "learning_rate": 0.0004999924263496652,
      "loss": 0.0759,
      "step": 30
    },
    {
      "epoch": 0.0076713684731502104,
      "grad_norm": 0.03989698365330696,
      "learning_rate": 0.0004999916500548273,
      "loss": 0.0642,
      "step": 31
    },
    {
      "epoch": 0.007918831972284088,
      "grad_norm": 0.04539763554930687,
      "learning_rate": 0.0004999908358928119,
      "loss": 0.0618,
      "step": 32
    },
    {
      "epoch": 0.008166295471417966,
      "grad_norm": 0.04890098050236702,
      "learning_rate": 0.0004999899838637419,
      "loss": 0.064,
      "step": 33
    },
    {
      "epoch": 0.008413758970551844,
      "grad_norm": 0.06431790441274643,
      "learning_rate": 0.0004999890939677469,
      "loss": 0.108,
      "step": 34
    },
    {
      "epoch": 0.00866122246968572,
      "grad_norm": 0.036847177892923355,
      "learning_rate": 0.0004999881662049616,
      "loss": 0.0397,
      "step": 35
    },
    {
      "epoch": 0.008908685968819599,
      "grad_norm": 0.05444732680916786,
      "learning_rate": 0.0004999872005755263,
      "loss": 0.0706,
      "step": 36
    },
    {
      "epoch": 0.009156149467953477,
      "grad_norm": 0.06279376149177551,
      "learning_rate": 0.0004999861970795872,
      "loss": 0.0631,
      "step": 37
    },
    {
      "epoch": 0.009403612967087355,
      "grad_norm": 0.08173958212137222,
      "learning_rate": 0.0004999851557172968,
      "loss": 0.1047,
      "step": 38
    },
    {
      "epoch": 0.009651076466221232,
      "grad_norm": 0.035879191011190414,
      "learning_rate": 0.0004999840764888123,
      "loss": 0.0833,
      "step": 39
    },
    {
      "epoch": 0.00989853996535511,
      "grad_norm": 0.06831823289394379,
      "learning_rate": 0.0004999829593942975,
      "loss": 0.1111,
      "step": 40
    },
    {
      "epoch": 0.010146003464488988,
      "grad_norm": 0.04674462601542473,
      "learning_rate": 0.0004999818044339214,
      "loss": 0.0585,
      "step": 41
    },
    {
      "epoch": 0.010393466963622866,
      "grad_norm": 0.040312252938747406,
      "learning_rate": 0.0004999806116078591,
      "loss": 0.0642,
      "step": 42
    },
    {
      "epoch": 0.010640930462756744,
      "grad_norm": 0.0777064710855484,
      "learning_rate": 0.0004999793809162912,
      "loss": 0.081,
      "step": 43
    },
    {
      "epoch": 0.01088839396189062,
      "grad_norm": 0.035193681716918945,
      "learning_rate": 0.0004999781123594043,
      "loss": 0.05,
      "step": 44
    },
    {
      "epoch": 0.011135857461024499,
      "grad_norm": 0.06901071220636368,
      "learning_rate": 0.0004999768059373902,
      "loss": 0.0854,
      "step": 45
    },
    {
      "epoch": 0.011383320960158377,
      "grad_norm": 0.05225062742829323,
      "learning_rate": 0.0004999754616504472,
      "loss": 0.0907,
      "step": 46
    },
    {
      "epoch": 0.011630784459292255,
      "grad_norm": 0.07809412479400635,
      "learning_rate": 0.0004999740794987786,
      "loss": 0.107,
      "step": 47
    },
    {
      "epoch": 0.011878247958426132,
      "grad_norm": 0.03257609158754349,
      "learning_rate": 0.0004999726594825939,
      "loss": 0.0541,
      "step": 48
    },
    {
      "epoch": 0.01212571145756001,
      "grad_norm": 0.04037296399474144,
      "learning_rate": 0.0004999712016021082,
      "loss": 0.0576,
      "step": 49
    },
    {
      "epoch": 0.012373174956693888,
      "grad_norm": 0.0474371612071991,
      "learning_rate": 0.0004999697058575423,
      "loss": 0.0766,
      "step": 50
    },
    {
      "epoch": 0.012620638455827766,
      "grad_norm": 0.026185153052210808,
      "learning_rate": 0.0004999681722491227,
      "loss": 0.0337,
      "step": 51
    },
    {
      "epoch": 0.012868101954961643,
      "grad_norm": 0.05369655042886734,
      "learning_rate": 0.0004999666007770819,
      "loss": 0.0935,
      "step": 52
    },
    {
      "epoch": 0.01311556545409552,
      "grad_norm": 0.04027699679136276,
      "learning_rate": 0.0004999649914416576,
      "loss": 0.0789,
      "step": 53
    },
    {
      "epoch": 0.013363028953229399,
      "grad_norm": 0.21142268180847168,
      "learning_rate": 0.000499963344243094,
      "loss": 0.1544,
      "step": 54
    },
    {
      "epoch": 0.013610492452363277,
      "grad_norm": 0.0610087588429451,
      "learning_rate": 0.0004999616591816403,
      "loss": 0.1084,
      "step": 55
    },
    {
      "epoch": 0.013857955951497154,
      "grad_norm": 0.0488666407763958,
      "learning_rate": 0.0004999599362575518,
      "loss": 0.0429,
      "step": 56
    },
    {
      "epoch": 0.014105419450631032,
      "grad_norm": 0.038967475295066833,
      "learning_rate": 0.0004999581754710895,
      "loss": 0.0573,
      "step": 57
    },
    {
      "epoch": 0.01435288294976491,
      "grad_norm": 0.05198577791452408,
      "learning_rate": 0.0004999563768225202,
      "loss": 0.0836,
      "step": 58
    },
    {
      "epoch": 0.014600346448898788,
      "grad_norm": 0.07674292474985123,
      "learning_rate": 0.0004999545403121162,
      "loss": 0.0583,
      "step": 59
    },
    {
      "epoch": 0.014847809948032665,
      "grad_norm": 0.07874580472707748,
      "learning_rate": 0.0004999526659401558,
      "loss": 0.0931,
      "step": 60
    },
    {
      "epoch": 0.015095273447166543,
      "grad_norm": 0.047764185816049576,
      "learning_rate": 0.0004999507537069228,
      "loss": 0.0551,
      "step": 61
    },
    {
      "epoch": 0.015342736946300421,
      "grad_norm": 0.11473539471626282,
      "learning_rate": 0.000499948803612707,
      "loss": 0.0984,
      "step": 62
    },
    {
      "epoch": 0.015590200445434299,
      "grad_norm": 0.05025757849216461,
      "learning_rate": 0.0004999468156578037,
      "loss": 0.0447,
      "step": 63
    },
    {
      "epoch": 0.015837663944568176,
      "grad_norm": 0.04719000682234764,
      "learning_rate": 0.000499944789842514,
      "loss": 0.0361,
      "step": 64
    },
    {
      "epoch": 0.016085127443702055,
      "grad_norm": 0.04845096543431282,
      "learning_rate": 0.0004999427261671447,
      "loss": 0.0518,
      "step": 65
    },
    {
      "epoch": 0.016332590942835932,
      "grad_norm": 0.042584192007780075,
      "learning_rate": 0.0004999406246320087,
      "loss": 0.0403,
      "step": 66
    },
    {
      "epoch": 0.01658005444196981,
      "grad_norm": 0.04104214161634445,
      "learning_rate": 0.000499938485237424,
      "loss": 0.0646,
      "step": 67
    },
    {
      "epoch": 0.016827517941103688,
      "grad_norm": 0.037001606076955795,
      "learning_rate": 0.0004999363079837148,
      "loss": 0.0599,
      "step": 68
    },
    {
      "epoch": 0.017074981440237565,
      "grad_norm": 0.05773873254656792,
      "learning_rate": 0.0004999340928712109,
      "loss": 0.0791,
      "step": 69
    },
    {
      "epoch": 0.01732244493937144,
      "grad_norm": 0.058607835322618484,
      "learning_rate": 0.0004999318399002477,
      "loss": 0.0853,
      "step": 70
    },
    {
      "epoch": 0.01756990843850532,
      "grad_norm": 0.04887038841843605,
      "learning_rate": 0.0004999295490711667,
      "loss": 0.065,
      "step": 71
    },
    {
      "epoch": 0.017817371937639197,
      "grad_norm": 0.04569605737924576,
      "learning_rate": 0.0004999272203843146,
      "loss": 0.0439,
      "step": 72
    },
    {
      "epoch": 0.018064835436773077,
      "grad_norm": 0.06462132930755615,
      "learning_rate": 0.0004999248538400444,
      "loss": 0.0829,
      "step": 73
    },
    {
      "epoch": 0.018312298935906954,
      "grad_norm": 0.06311976164579391,
      "learning_rate": 0.0004999224494387144,
      "loss": 0.0514,
      "step": 74
    },
    {
      "epoch": 0.01855976243504083,
      "grad_norm": 0.04283955693244934,
      "learning_rate": 0.0004999200071806889,
      "loss": 0.054,
      "step": 75
    },
    {
      "epoch": 0.01880722593417471,
      "grad_norm": 0.044590577483177185,
      "learning_rate": 0.000499917527066338,
      "loss": 0.0652,
      "step": 76
    },
    {
      "epoch": 0.019054689433308587,
      "grad_norm": 0.06439868360757828,
      "learning_rate": 0.000499915009096037,
      "loss": 0.0876,
      "step": 77
    },
    {
      "epoch": 0.019302152932442463,
      "grad_norm": 0.061903972178697586,
      "learning_rate": 0.0004999124532701675,
      "loss": 0.0551,
      "step": 78
    },
    {
      "epoch": 0.019549616431576343,
      "grad_norm": 0.07355295121669769,
      "learning_rate": 0.0004999098595891168,
      "loss": 0.1018,
      "step": 79
    },
    {
      "epoch": 0.01979707993071022,
      "grad_norm": 0.04191485792398453,
      "learning_rate": 0.0004999072280532775,
      "loss": 0.0461,
      "step": 80
    },
    {
      "epoch": 0.0200445434298441,
      "grad_norm": 0.046176861971616745,
      "learning_rate": 0.0004999045586630482,
      "loss": 0.0882,
      "step": 81
    },
    {
      "epoch": 0.020292006928977976,
      "grad_norm": 0.05288976803421974,
      "learning_rate": 0.0004999018514188335,
      "loss": 0.0519,
      "step": 82
    },
    {
      "epoch": 0.020539470428111852,
      "grad_norm": 0.07790007442235947,
      "learning_rate": 0.0004998991063210434,
      "loss": 0.0664,
      "step": 83
    },
    {
      "epoch": 0.020786933927245732,
      "grad_norm": 0.0551571361720562,
      "learning_rate": 0.0004998963233700935,
      "loss": 0.0907,
      "step": 84
    },
    {
      "epoch": 0.02103439742637961,
      "grad_norm": 0.09092020988464355,
      "learning_rate": 0.0004998935025664055,
      "loss": 0.096,
      "step": 85
    },
    {
      "epoch": 0.02128186092551349,
      "grad_norm": 0.035211022943258286,
      "learning_rate": 0.0004998906439104068,
      "loss": 0.0433,
      "step": 86
    },
    {
      "epoch": 0.021529324424647365,
      "grad_norm": 0.043311458081007004,
      "learning_rate": 0.0004998877474025301,
      "loss": 0.0805,
      "step": 87
    },
    {
      "epoch": 0.02177678792378124,
      "grad_norm": 0.047059133648872375,
      "learning_rate": 0.0004998848130432145,
      "loss": 0.0767,
      "step": 88
    },
    {
      "epoch": 0.02202425142291512,
      "grad_norm": 0.050170790404081345,
      "learning_rate": 0.0004998818408329043,
      "loss": 0.0468,
      "step": 89
    },
    {
      "epoch": 0.022271714922048998,
      "grad_norm": 0.08823265880346298,
      "learning_rate": 0.0004998788307720496,
      "loss": 0.0758,
      "step": 90
    },
    {
      "epoch": 0.022519178421182874,
      "grad_norm": 0.043471597135066986,
      "learning_rate": 0.0004998757828611064,
      "loss": 0.0543,
      "step": 91
    },
    {
      "epoch": 0.022766641920316754,
      "grad_norm": 0.04842253401875496,
      "learning_rate": 0.0004998726971005366,
      "loss": 0.0587,
      "step": 92
    },
    {
      "epoch": 0.02301410541945063,
      "grad_norm": 0.05226214602589607,
      "learning_rate": 0.0004998695734908074,
      "loss": 0.0542,
      "step": 93
    },
    {
      "epoch": 0.02326156891858451,
      "grad_norm": 0.12504355609416962,
      "learning_rate": 0.000499866412032392,
      "loss": 0.0827,
      "step": 94
    },
    {
      "epoch": 0.023509032417718387,
      "grad_norm": 0.07139384746551514,
      "learning_rate": 0.0004998632127257693,
      "loss": 0.0647,
      "step": 95
    },
    {
      "epoch": 0.023756495916852263,
      "grad_norm": 0.034187961369752884,
      "learning_rate": 0.0004998599755714238,
      "loss": 0.037,
      "step": 96
    },
    {
      "epoch": 0.024003959415986143,
      "grad_norm": 0.04591289162635803,
      "learning_rate": 0.0004998567005698461,
      "loss": 0.0377,
      "step": 97
    },
    {
      "epoch": 0.02425142291512002,
      "grad_norm": 0.024773074313998222,
      "learning_rate": 0.000499853387721532,
      "loss": 0.0274,
      "step": 98
    },
    {
      "epoch": 0.024498886414253896,
      "grad_norm": 0.04808671772480011,
      "learning_rate": 0.0004998500370269834,
      "loss": 0.0431,
      "step": 99
    },
    {
      "epoch": 0.024746349913387776,
      "grad_norm": 0.03775831684470177,
      "learning_rate": 0.0004998466484867078,
      "loss": 0.052,
      "step": 100
    },
    {
      "epoch": 0.024993813412521652,
      "grad_norm": 0.04680710285902023,
      "learning_rate": 0.0004998432221012186,
      "loss": 0.038,
      "step": 101
    },
    {
      "epoch": 0.025241276911655532,
      "grad_norm": 0.06005127355456352,
      "learning_rate": 0.0004998397578710348,
      "loss": 0.0763,
      "step": 102
    },
    {
      "epoch": 0.02548874041078941,
      "grad_norm": 0.07929584383964539,
      "learning_rate": 0.0004998362557966811,
      "loss": 0.068,
      "step": 103
    },
    {
      "epoch": 0.025736203909923285,
      "grad_norm": 0.05720546096563339,
      "learning_rate": 0.0004998327158786878,
      "loss": 0.0867,
      "step": 104
    },
    {
      "epoch": 0.025983667409057165,
      "grad_norm": 0.11575187742710114,
      "learning_rate": 0.0004998291381175914,
      "loss": 0.0591,
      "step": 105
    },
    {
      "epoch": 0.02623113090819104,
      "grad_norm": 0.05220927298069,
      "learning_rate": 0.0004998255225139336,
      "loss": 0.0567,
      "step": 106
    },
    {
      "epoch": 0.026478594407324918,
      "grad_norm": 0.05193067714571953,
      "learning_rate": 0.0004998218690682621,
      "loss": 0.0592,
      "step": 107
    },
    {
      "epoch": 0.026726057906458798,
      "grad_norm": 0.049265407025814056,
      "learning_rate": 0.0004998181777811305,
      "loss": 0.0616,
      "step": 108
    },
    {
      "epoch": 0.026973521405592674,
      "grad_norm": 0.07470148801803589,
      "learning_rate": 0.0004998144486530978,
      "loss": 0.0749,
      "step": 109
    },
    {
      "epoch": 0.027220984904726554,
      "grad_norm": 0.05604258179664612,
      "learning_rate": 0.0004998106816847288,
      "loss": 0.0923,
      "step": 110
    },
    {
      "epoch": 0.02746844840386043,
      "grad_norm": 0.09324069321155548,
      "learning_rate": 0.0004998068768765942,
      "loss": 0.1136,
      "step": 111
    },
    {
      "epoch": 0.027715911902994307,
      "grad_norm": 0.0680641457438469,
      "learning_rate": 0.0004998030342292703,
      "loss": 0.05,
      "step": 112
    },
    {
      "epoch": 0.027963375402128187,
      "grad_norm": 0.03860420733690262,
      "learning_rate": 0.0004997991537433391,
      "loss": 0.0603,
      "step": 113
    },
    {
      "epoch": 0.028210838901262063,
      "grad_norm": 0.09228354692459106,
      "learning_rate": 0.0004997952354193884,
      "loss": 0.0666,
      "step": 114
    },
    {
      "epoch": 0.02845830240039594,
      "grad_norm": 0.08773216605186462,
      "learning_rate": 0.000499791279258012,
      "loss": 0.0915,
      "step": 115
    },
    {
      "epoch": 0.02870576589952982,
      "grad_norm": 0.05556907504796982,
      "learning_rate": 0.0004997872852598086,
      "loss": 0.07,
      "step": 116
    },
    {
      "epoch": 0.028953229398663696,
      "grad_norm": 0.04861657693982124,
      "learning_rate": 0.0004997832534253837,
      "loss": 0.0594,
      "step": 117
    },
    {
      "epoch": 0.029200692897797576,
      "grad_norm": 0.03614695742726326,
      "learning_rate": 0.0004997791837553479,
      "loss": 0.0587,
      "step": 118
    },
    {
      "epoch": 0.029448156396931453,
      "grad_norm": 0.04532529041171074,
      "learning_rate": 0.0004997750762503174,
      "loss": 0.0597,
      "step": 119
    },
    {
      "epoch": 0.02969561989606533,
      "grad_norm": 0.053661808371543884,
      "learning_rate": 0.0004997709309109145,
      "loss": 0.0564,
      "step": 120
    },
    {
      "epoch": 0.02994308339519921,
      "grad_norm": 0.04406842216849327,
      "learning_rate": 0.0004997667477377673,
      "loss": 0.0482,
      "step": 121
    },
    {
      "epoch": 0.030190546894333085,
      "grad_norm": 0.05530032888054848,
      "learning_rate": 0.0004997625267315093,
      "loss": 0.0722,
      "step": 122
    },
    {
      "epoch": 0.030438010393466965,
      "grad_norm": 0.1194840744137764,
      "learning_rate": 0.0004997582678927798,
      "loss": 0.0611,
      "step": 123
    },
    {
      "epoch": 0.030685473892600842,
      "grad_norm": 0.04063352942466736,
      "learning_rate": 0.000499753971222224,
      "loss": 0.0537,
      "step": 124
    },
    {
      "epoch": 0.030932937391734718,
      "grad_norm": 0.05991766229271889,
      "learning_rate": 0.0004997496367204928,
      "loss": 0.0726,
      "step": 125
    },
    {
      "epoch": 0.031180400890868598,
      "grad_norm": 0.04366909712553024,
      "learning_rate": 0.0004997452643882425,
      "loss": 0.0806,
      "step": 126
    },
    {
      "epoch": 0.031427864390002475,
      "grad_norm": 0.04334122687578201,
      "learning_rate": 0.0004997408542261357,
      "loss": 0.0511,
      "step": 127
    },
    {
      "epoch": 0.03167532788913635,
      "grad_norm": 0.0364249162375927,
      "learning_rate": 0.0004997364062348402,
      "loss": 0.0511,
      "step": 128
    },
    {
      "epoch": 0.03192279138827023,
      "grad_norm": 0.06532703340053558,
      "learning_rate": 0.0004997319204150299,
      "loss": 0.0865,
      "step": 129
    },
    {
      "epoch": 0.03217025488740411,
      "grad_norm": 0.030968327075242996,
      "learning_rate": 0.0004997273967673842,
      "loss": 0.0272,
      "step": 130
    },
    {
      "epoch": 0.03241771838653799,
      "grad_norm": 0.08367808908224106,
      "learning_rate": 0.0004997228352925884,
      "loss": 0.0845,
      "step": 131
    },
    {
      "epoch": 0.032665181885671864,
      "grad_norm": 0.04808201268315315,
      "learning_rate": 0.0004997182359913334,
      "loss": 0.0439,
      "step": 132
    },
    {
      "epoch": 0.03291264538480574,
      "grad_norm": 0.06503190100193024,
      "learning_rate": 0.0004997135988643158,
      "loss": 0.0582,
      "step": 133
    },
    {
      "epoch": 0.03316010888393962,
      "grad_norm": 0.037932347506284714,
      "learning_rate": 0.000499708923912238,
      "loss": 0.0351,
      "step": 134
    },
    {
      "epoch": 0.0334075723830735,
      "grad_norm": 0.06076967343688011,
      "learning_rate": 0.0004997042111358083,
      "loss": 0.0674,
      "step": 135
    },
    {
      "epoch": 0.033655035882207376,
      "grad_norm": 0.06158439442515373,
      "learning_rate": 0.0004996994605357403,
      "loss": 0.0777,
      "step": 136
    },
    {
      "epoch": 0.03390249938134125,
      "grad_norm": 0.05446784943342209,
      "learning_rate": 0.000499694672112754,
      "loss": 0.0564,
      "step": 137
    },
    {
      "epoch": 0.03414996288047513,
      "grad_norm": 0.07588490098714828,
      "learning_rate": 0.0004996898458675742,
      "loss": 0.0674,
      "step": 138
    },
    {
      "epoch": 0.034397426379609006,
      "grad_norm": 0.04469264671206474,
      "learning_rate": 0.0004996849818009325,
      "loss": 0.0256,
      "step": 139
    },
    {
      "epoch": 0.03464488987874288,
      "grad_norm": 0.04464639350771904,
      "learning_rate": 0.0004996800799135651,
      "loss": 0.0465,
      "step": 140
    },
    {
      "epoch": 0.034892353377876766,
      "grad_norm": 0.06719645112752914,
      "learning_rate": 0.000499675140206215,
      "loss": 0.0468,
      "step": 141
    },
    {
      "epoch": 0.03513981687701064,
      "grad_norm": 0.06647978723049164,
      "learning_rate": 0.0004996701626796302,
      "loss": 0.0699,
      "step": 142
    },
    {
      "epoch": 0.03538728037614452,
      "grad_norm": 0.0647316426038742,
      "learning_rate": 0.0004996651473345647,
      "loss": 0.0872,
      "step": 143
    },
    {
      "epoch": 0.035634743875278395,
      "grad_norm": 0.051856279373168945,
      "learning_rate": 0.0004996600941717782,
      "loss": 0.023,
      "step": 144
    },
    {
      "epoch": 0.03588220737441227,
      "grad_norm": 0.04075290635228157,
      "learning_rate": 0.000499655003192036,
      "loss": 0.0332,
      "step": 145
    },
    {
      "epoch": 0.036129670873546155,
      "grad_norm": 0.03754590451717377,
      "learning_rate": 0.0004996498743961096,
      "loss": 0.036,
      "step": 146
    },
    {
      "epoch": 0.03637713437268003,
      "grad_norm": 0.0525418221950531,
      "learning_rate": 0.0004996447077847755,
      "loss": 0.0459,
      "step": 147
    },
    {
      "epoch": 0.03662459787181391,
      "grad_norm": 0.06515754014253616,
      "learning_rate": 0.0004996395033588164,
      "loss": 0.0493,
      "step": 148
    },
    {
      "epoch": 0.036872061370947784,
      "grad_norm": 0.07083583623170853,
      "learning_rate": 0.0004996342611190208,
      "loss": 0.0749,
      "step": 149
    },
    {
      "epoch": 0.03711952487008166,
      "grad_norm": 0.04338913410902023,
      "learning_rate": 0.0004996289810661826,
      "loss": 0.0633,
      "step": 150
    },
    {
      "epoch": 0.037366988369215544,
      "grad_norm": 0.08967853337526321,
      "learning_rate": 0.0004996236632011016,
      "loss": 0.0585,
      "step": 151
    },
    {
      "epoch": 0.03761445186834942,
      "grad_norm": 0.0375569723546505,
      "learning_rate": 0.0004996183075245834,
      "loss": 0.0483,
      "step": 152
    },
    {
      "epoch": 0.0378619153674833,
      "grad_norm": 0.03724055737257004,
      "learning_rate": 0.0004996129140374392,
      "loss": 0.0397,
      "step": 153
    },
    {
      "epoch": 0.03810937886661717,
      "grad_norm": 0.04944099485874176,
      "learning_rate": 0.000499607482740486,
      "loss": 0.0476,
      "step": 154
    },
    {
      "epoch": 0.03835684236575105,
      "grad_norm": 0.04920181632041931,
      "learning_rate": 0.0004996020136345465,
      "loss": 0.0424,
      "step": 155
    },
    {
      "epoch": 0.038604305864884926,
      "grad_norm": 0.10476681590080261,
      "learning_rate": 0.000499596506720449,
      "loss": 0.1193,
      "step": 156
    },
    {
      "epoch": 0.03885176936401881,
      "grad_norm": 0.046734608709812164,
      "learning_rate": 0.0004995909619990278,
      "loss": 0.0643,
      "step": 157
    },
    {
      "epoch": 0.039099232863152686,
      "grad_norm": 0.07628539204597473,
      "learning_rate": 0.0004995853794711227,
      "loss": 0.0476,
      "step": 158
    },
    {
      "epoch": 0.03934669636228656,
      "grad_norm": 0.04994358867406845,
      "learning_rate": 0.0004995797591375794,
      "loss": 0.0661,
      "step": 159
    },
    {
      "epoch": 0.03959415986142044,
      "grad_norm": 0.08728322386741638,
      "learning_rate": 0.0004995741009992491,
      "loss": 0.0831,
      "step": 160
    },
    {
      "epoch": 0.039841623360554315,
      "grad_norm": 0.06230149790644646,
      "learning_rate": 0.000499568405056989,
      "loss": 0.1092,
      "step": 161
    },
    {
      "epoch": 0.0400890868596882,
      "grad_norm": 0.042108479887247086,
      "learning_rate": 0.0004995626713116618,
      "loss": 0.056,
      "step": 162
    },
    {
      "epoch": 0.040336550358822075,
      "grad_norm": 0.05130412057042122,
      "learning_rate": 0.0004995568997641359,
      "loss": 0.0653,
      "step": 163
    },
    {
      "epoch": 0.04058401385795595,
      "grad_norm": 0.03976710885763168,
      "learning_rate": 0.0004995510904152858,
      "loss": 0.0446,
      "step": 164
    },
    {
      "epoch": 0.04083147735708983,
      "grad_norm": 0.05888865888118744,
      "learning_rate": 0.0004995452432659912,
      "loss": 0.0672,
      "step": 165
    },
    {
      "epoch": 0.041078940856223704,
      "grad_norm": 0.034347306936979294,
      "learning_rate": 0.0004995393583171381,
      "loss": 0.0354,
      "step": 166
    },
    {
      "epoch": 0.04132640435535759,
      "grad_norm": 0.045313917100429535,
      "learning_rate": 0.0004995334355696176,
      "loss": 0.081,
      "step": 167
    },
    {
      "epoch": 0.041573867854491464,
      "grad_norm": 0.05252586305141449,
      "learning_rate": 0.0004995274750243271,
      "loss": 0.0463,
      "step": 168
    },
    {
      "epoch": 0.04182133135362534,
      "grad_norm": 0.08232109993696213,
      "learning_rate": 0.0004995214766821693,
      "loss": 0.1101,
      "step": 169
    },
    {
      "epoch": 0.04206879485275922,
      "grad_norm": 0.07008469104766846,
      "learning_rate": 0.0004995154405440527,
      "loss": 0.0567,
      "step": 170
    },
    {
      "epoch": 0.042316258351893093,
      "grad_norm": 0.06660445034503937,
      "learning_rate": 0.0004995093666108919,
      "loss": 0.0671,
      "step": 171
    },
    {
      "epoch": 0.04256372185102698,
      "grad_norm": 0.051156751811504364,
      "learning_rate": 0.0004995032548836067,
      "loss": 0.0666,
      "step": 172
    },
    {
      "epoch": 0.04281118535016085,
      "grad_norm": 0.05328254774212837,
      "learning_rate": 0.000499497105363123,
      "loss": 0.061,
      "step": 173
    },
    {
      "epoch": 0.04305864884929473,
      "grad_norm": 0.047347292304039,
      "learning_rate": 0.0004994909180503723,
      "loss": 0.0654,
      "step": 174
    },
    {
      "epoch": 0.043306112348428606,
      "grad_norm": 0.034497037529945374,
      "learning_rate": 0.0004994846929462916,
      "loss": 0.061,
      "step": 175
    },
    {
      "epoch": 0.04355357584756248,
      "grad_norm": 0.07947828620672226,
      "learning_rate": 0.0004994784300518242,
      "loss": 0.061,
      "step": 176
    },
    {
      "epoch": 0.04380103934669636,
      "grad_norm": 0.05485032498836517,
      "learning_rate": 0.0004994721293679184,
      "loss": 0.0423,
      "step": 177
    },
    {
      "epoch": 0.04404850284583024,
      "grad_norm": 0.027652651071548462,
      "learning_rate": 0.0004994657908955288,
      "loss": 0.0378,
      "step": 178
    },
    {
      "epoch": 0.04429596634496412,
      "grad_norm": 0.024846533313393593,
      "learning_rate": 0.0004994594146356156,
      "loss": 0.0216,
      "step": 179
    },
    {
      "epoch": 0.044543429844097995,
      "grad_norm": 0.027433080598711967,
      "learning_rate": 0.0004994530005891444,
      "loss": 0.036,
      "step": 180
    },
    {
      "epoch": 0.04479089334323187,
      "grad_norm": 0.04966059327125549,
      "learning_rate": 0.0004994465487570868,
      "loss": 0.0512,
      "step": 181
    },
    {
      "epoch": 0.04503835684236575,
      "grad_norm": 0.04703428968787193,
      "learning_rate": 0.0004994400591404202,
      "loss": 0.0459,
      "step": 182
    },
    {
      "epoch": 0.04528582034149963,
      "grad_norm": 0.03212606906890869,
      "learning_rate": 0.0004994335317401276,
      "loss": 0.0419,
      "step": 183
    },
    {
      "epoch": 0.04553328384063351,
      "grad_norm": 0.05865900218486786,
      "learning_rate": 0.0004994269665571976,
      "loss": 0.0582,
      "step": 184
    },
    {
      "epoch": 0.045780747339767384,
      "grad_norm": 0.0397450253367424,
      "learning_rate": 0.0004994203635926249,
      "loss": 0.0327,
      "step": 185
    },
    {
      "epoch": 0.04602821083890126,
      "grad_norm": 0.049522530287504196,
      "learning_rate": 0.0004994137228474094,
      "loss": 0.0619,
      "step": 186
    },
    {
      "epoch": 0.04627567433803514,
      "grad_norm": 0.034719016402959824,
      "learning_rate": 0.0004994070443225571,
      "loss": 0.0484,
      "step": 187
    },
    {
      "epoch": 0.04652313783716902,
      "grad_norm": 0.06449344009160995,
      "learning_rate": 0.0004994003280190797,
      "loss": 0.0708,
      "step": 188
    },
    {
      "epoch": 0.0467706013363029,
      "grad_norm": 0.06834916025400162,
      "learning_rate": 0.0004993935739379943,
      "loss": 0.1094,
      "step": 189
    },
    {
      "epoch": 0.047018064835436774,
      "grad_norm": 0.061136551201343536,
      "learning_rate": 0.0004993867820803244,
      "loss": 0.0697,
      "step": 190
    },
    {
      "epoch": 0.04726552833457065,
      "grad_norm": 0.06295981258153915,
      "learning_rate": 0.0004993799524470984,
      "loss": 0.0683,
      "step": 191
    },
    {
      "epoch": 0.047512991833704527,
      "grad_norm": 0.04156037047505379,
      "learning_rate": 0.0004993730850393509,
      "loss": 0.0683,
      "step": 192
    },
    {
      "epoch": 0.0477604553328384,
      "grad_norm": 0.029591351747512817,
      "learning_rate": 0.0004993661798581223,
      "loss": 0.0404,
      "step": 193
    },
    {
      "epoch": 0.048007918831972286,
      "grad_norm": 0.025827232748270035,
      "learning_rate": 0.0004993592369044583,
      "loss": 0.0279,
      "step": 194
    },
    {
      "epoch": 0.04825538233110616,
      "grad_norm": 0.057731542736291885,
      "learning_rate": 0.0004993522561794107,
      "loss": 0.0514,
      "step": 195
    },
    {
      "epoch": 0.04850284583024004,
      "grad_norm": 0.04810943827033043,
      "learning_rate": 0.0004993452376840371,
      "loss": 0.0781,
      "step": 196
    },
    {
      "epoch": 0.048750309329373916,
      "grad_norm": 0.044714946299791336,
      "learning_rate": 0.0004993381814194002,
      "loss": 0.035,
      "step": 197
    },
    {
      "epoch": 0.04899777282850779,
      "grad_norm": 0.055250633507966995,
      "learning_rate": 0.0004993310873865691,
      "loss": 0.0736,
      "step": 198
    },
    {
      "epoch": 0.049245236327641675,
      "grad_norm": 0.03957192227244377,
      "learning_rate": 0.0004993239555866184,
      "loss": 0.0651,
      "step": 199
    },
    {
      "epoch": 0.04949269982677555,
      "grad_norm": 0.044200871139764786,
      "learning_rate": 0.0004993167860206282,
      "loss": 0.0323,
      "step": 200
    },
    {
      "epoch": 0.04949269982677555,
      "eval_loss": 0.316805899143219,
      "eval_runtime": 202.9456,
      "eval_samples_per_second": 4.927,
      "eval_steps_per_second": 0.31,
      "step": 200
    },
    {
      "epoch": 0.04974016332590943,
      "grad_norm": 0.06315396726131439,
      "learning_rate": 0.0004993095786896846,
      "loss": 0.05,
      "step": 201
    },
    {
      "epoch": 0.049987626825043305,
      "grad_norm": 0.056796155869960785,
      "learning_rate": 0.0004993023335948794,
      "loss": 0.0766,
      "step": 202
    },
    {
      "epoch": 0.05023509032417718,
      "grad_norm": 0.03691227734088898,
      "learning_rate": 0.0004992950507373099,
      "loss": 0.0501,
      "step": 203
    },
    {
      "epoch": 0.050482553823311065,
      "grad_norm": 0.037509284913539886,
      "learning_rate": 0.0004992877301180793,
      "loss": 0.0307,
      "step": 204
    },
    {
      "epoch": 0.05073001732244494,
      "grad_norm": 0.04192807525396347,
      "learning_rate": 0.0004992803717382967,
      "loss": 0.0502,
      "step": 205
    },
    {
      "epoch": 0.05097748082157882,
      "grad_norm": 0.06591694056987762,
      "learning_rate": 0.0004992729755990763,
      "loss": 0.0789,
      "step": 206
    },
    {
      "epoch": 0.051224944320712694,
      "grad_norm": 0.04950960353016853,
      "learning_rate": 0.0004992655417015387,
      "loss": 0.0534,
      "step": 207
    },
    {
      "epoch": 0.05147240781984657,
      "grad_norm": 0.06156142055988312,
      "learning_rate": 0.0004992580700468099,
      "loss": 0.0493,
      "step": 208
    },
    {
      "epoch": 0.051719871318980454,
      "grad_norm": 0.05447519198060036,
      "learning_rate": 0.0004992505606360216,
      "loss": 0.0635,
      "step": 209
    },
    {
      "epoch": 0.05196733481811433,
      "grad_norm": 0.06598232686519623,
      "learning_rate": 0.0004992430134703113,
      "loss": 0.0567,
      "step": 210
    },
    {
      "epoch": 0.05221479831724821,
      "grad_norm": 0.06615817546844482,
      "learning_rate": 0.0004992354285508221,
      "loss": 0.0801,
      "step": 211
    },
    {
      "epoch": 0.05246226181638208,
      "grad_norm": 0.05518059432506561,
      "learning_rate": 0.0004992278058787032,
      "loss": 0.0497,
      "step": 212
    },
    {
      "epoch": 0.05270972531551596,
      "grad_norm": 0.05742186680436134,
      "learning_rate": 0.000499220145455109,
      "loss": 0.047,
      "step": 213
    },
    {
      "epoch": 0.052957188814649836,
      "grad_norm": 0.05537962540984154,
      "learning_rate": 0.0004992124472811999,
      "loss": 0.057,
      "step": 214
    },
    {
      "epoch": 0.05320465231378372,
      "grad_norm": 0.0419887937605381,
      "learning_rate": 0.0004992047113581419,
      "loss": 0.0529,
      "step": 215
    },
    {
      "epoch": 0.053452115812917596,
      "grad_norm": 0.0416320264339447,
      "learning_rate": 0.0004991969376871069,
      "loss": 0.0628,
      "step": 216
    },
    {
      "epoch": 0.05369957931205147,
      "grad_norm": 0.057974182069301605,
      "learning_rate": 0.0004991891262692725,
      "loss": 0.0648,
      "step": 217
    },
    {
      "epoch": 0.05394704281118535,
      "grad_norm": 0.03899344801902771,
      "learning_rate": 0.0004991812771058217,
      "loss": 0.047,
      "step": 218
    },
    {
      "epoch": 0.054194506310319225,
      "grad_norm": 0.04591182619333267,
      "learning_rate": 0.0004991733901979435,
      "loss": 0.0834,
      "step": 219
    },
    {
      "epoch": 0.05444196980945311,
      "grad_norm": 0.07579828798770905,
      "learning_rate": 0.0004991654655468327,
      "loss": 0.1222,
      "step": 220
    },
    {
      "epoch": 0.054689433308586985,
      "grad_norm": 0.06181161850690842,
      "learning_rate": 0.0004991575031536896,
      "loss": 0.0454,
      "step": 221
    },
    {
      "epoch": 0.05493689680772086,
      "grad_norm": 0.04554423317313194,
      "learning_rate": 0.0004991495030197202,
      "loss": 0.0318,
      "step": 222
    },
    {
      "epoch": 0.05518436030685474,
      "grad_norm": 0.038127653300762177,
      "learning_rate": 0.0004991414651461364,
      "loss": 0.0496,
      "step": 223
    },
    {
      "epoch": 0.055431823805988614,
      "grad_norm": 0.06525009125471115,
      "learning_rate": 0.0004991333895341557,
      "loss": 0.0504,
      "step": 224
    },
    {
      "epoch": 0.0556792873051225,
      "grad_norm": 0.035239335149526596,
      "learning_rate": 0.0004991252761850014,
      "loss": 0.0385,
      "step": 225
    },
    {
      "epoch": 0.055926750804256374,
      "grad_norm": 0.07274030148983002,
      "learning_rate": 0.0004991171250999023,
      "loss": 0.0964,
      "step": 226
    },
    {
      "epoch": 0.05617421430339025,
      "grad_norm": 0.05234386399388313,
      "learning_rate": 0.0004991089362800933,
      "loss": 0.0615,
      "step": 227
    },
    {
      "epoch": 0.05642167780252413,
      "grad_norm": 0.05600458383560181,
      "learning_rate": 0.0004991007097268148,
      "loss": 0.0682,
      "step": 228
    },
    {
      "epoch": 0.056669141301658,
      "grad_norm": 0.05463188514113426,
      "learning_rate": 0.0004990924454413125,
      "loss": 0.0451,
      "step": 229
    },
    {
      "epoch": 0.05691660480079188,
      "grad_norm": 0.09329120069742203,
      "learning_rate": 0.0004990841434248387,
      "loss": 0.0706,
      "step": 230
    },
    {
      "epoch": 0.05716406829992576,
      "grad_norm": 0.028381172567605972,
      "learning_rate": 0.0004990758036786508,
      "loss": 0.0374,
      "step": 231
    },
    {
      "epoch": 0.05741153179905964,
      "grad_norm": 0.06298813968896866,
      "learning_rate": 0.0004990674262040119,
      "loss": 0.0739,
      "step": 232
    },
    {
      "epoch": 0.057658995298193516,
      "grad_norm": 0.05687353014945984,
      "learning_rate": 0.0004990590110021911,
      "loss": 0.0699,
      "step": 233
    },
    {
      "epoch": 0.05790645879732739,
      "grad_norm": 0.05618376284837723,
      "learning_rate": 0.0004990505580744631,
      "loss": 0.0749,
      "step": 234
    },
    {
      "epoch": 0.05815392229646127,
      "grad_norm": 0.08183616399765015,
      "learning_rate": 0.0004990420674221081,
      "loss": 0.0694,
      "step": 235
    },
    {
      "epoch": 0.05840138579559515,
      "grad_norm": 0.032535433769226074,
      "learning_rate": 0.0004990335390464125,
      "loss": 0.0571,
      "step": 236
    },
    {
      "epoch": 0.05864884929472903,
      "grad_norm": 0.047990333288908005,
      "learning_rate": 0.000499024972948668,
      "loss": 0.0578,
      "step": 237
    },
    {
      "epoch": 0.058896312793862905,
      "grad_norm": 0.045592665672302246,
      "learning_rate": 0.000499016369130172,
      "loss": 0.0687,
      "step": 238
    },
    {
      "epoch": 0.05914377629299678,
      "grad_norm": 0.0293724425137043,
      "learning_rate": 0.000499007727592228,
      "loss": 0.0441,
      "step": 239
    },
    {
      "epoch": 0.05939123979213066,
      "grad_norm": 0.07164487987756729,
      "learning_rate": 0.0004989990483361447,
      "loss": 0.0841,
      "step": 240
    },
    {
      "epoch": 0.05963870329126454,
      "grad_norm": 0.04900601878762245,
      "learning_rate": 0.000498990331363237,
      "loss": 0.0493,
      "step": 241
    },
    {
      "epoch": 0.05988616679039842,
      "grad_norm": 0.042926061898469925,
      "learning_rate": 0.0004989815766748253,
      "loss": 0.0331,
      "step": 242
    },
    {
      "epoch": 0.060133630289532294,
      "grad_norm": 0.03502738103270531,
      "learning_rate": 0.0004989727842722356,
      "loss": 0.0683,
      "step": 243
    },
    {
      "epoch": 0.06038109378866617,
      "grad_norm": 0.03774826228618622,
      "learning_rate": 0.0004989639541567996,
      "loss": 0.0505,
      "step": 244
    },
    {
      "epoch": 0.06062855728780005,
      "grad_norm": 0.051254503428936005,
      "learning_rate": 0.0004989550863298551,
      "loss": 0.0532,
      "step": 245
    },
    {
      "epoch": 0.06087602078693393,
      "grad_norm": 0.0774979293346405,
      "learning_rate": 0.0004989461807927451,
      "loss": 0.1167,
      "step": 246
    },
    {
      "epoch": 0.06112348428606781,
      "grad_norm": 0.03462253883481026,
      "learning_rate": 0.0004989372375468189,
      "loss": 0.0448,
      "step": 247
    },
    {
      "epoch": 0.061370947785201684,
      "grad_norm": 0.04372020810842514,
      "learning_rate": 0.0004989282565934307,
      "loss": 0.0412,
      "step": 248
    },
    {
      "epoch": 0.06161841128433556,
      "grad_norm": 0.04376988857984543,
      "learning_rate": 0.0004989192379339412,
      "loss": 0.0808,
      "step": 249
    },
    {
      "epoch": 0.061865874783469436,
      "grad_norm": 0.0407465435564518,
      "learning_rate": 0.0004989101815697164,
      "loss": 0.0433,
      "step": 250
    },
    {
      "epoch": 0.06211333828260331,
      "grad_norm": 0.07273242622613907,
      "learning_rate": 0.0004989010875021281,
      "loss": 0.0805,
      "step": 251
    },
    {
      "epoch": 0.062360801781737196,
      "grad_norm": 0.08894920349121094,
      "learning_rate": 0.0004988919557325539,
      "loss": 0.1393,
      "step": 252
    },
    {
      "epoch": 0.06260826528087107,
      "grad_norm": 0.04462366923689842,
      "learning_rate": 0.0004988827862623768,
      "loss": 0.0747,
      "step": 253
    },
    {
      "epoch": 0.06285572878000495,
      "grad_norm": 0.05982459709048271,
      "learning_rate": 0.000498873579092986,
      "loss": 0.0313,
      "step": 254
    },
    {
      "epoch": 0.06310319227913883,
      "grad_norm": 0.04360247030854225,
      "learning_rate": 0.000498864334225776,
      "loss": 0.025,
      "step": 255
    },
    {
      "epoch": 0.0633506557782727,
      "grad_norm": 0.04022696241736412,
      "learning_rate": 0.0004988550516621471,
      "loss": 0.031,
      "step": 256
    },
    {
      "epoch": 0.06359811927740658,
      "grad_norm": 0.05062346160411835,
      "learning_rate": 0.0004988457314035055,
      "loss": 0.088,
      "step": 257
    },
    {
      "epoch": 0.06384558277654045,
      "grad_norm": 0.06950755417346954,
      "learning_rate": 0.000498836373451263,
      "loss": 0.0768,
      "step": 258
    },
    {
      "epoch": 0.06409304627567433,
      "grad_norm": 0.06680983304977417,
      "learning_rate": 0.0004988269778068369,
      "loss": 0.08,
      "step": 259
    },
    {
      "epoch": 0.06434050977480822,
      "grad_norm": 0.05924048647284508,
      "learning_rate": 0.0004988175444716504,
      "loss": 0.0621,
      "step": 260
    },
    {
      "epoch": 0.0645879732739421,
      "grad_norm": 0.09716147184371948,
      "learning_rate": 0.0004988080734471326,
      "loss": 0.0934,
      "step": 261
    },
    {
      "epoch": 0.06483543677307597,
      "grad_norm": 0.039039310067892075,
      "learning_rate": 0.000498798564734718,
      "loss": 0.0671,
      "step": 262
    },
    {
      "epoch": 0.06508290027220985,
      "grad_norm": 0.05394497886300087,
      "learning_rate": 0.000498789018335847,
      "loss": 0.0758,
      "step": 263
    },
    {
      "epoch": 0.06533036377134373,
      "grad_norm": 0.04004474729299545,
      "learning_rate": 0.0004987794342519655,
      "loss": 0.0515,
      "step": 264
    },
    {
      "epoch": 0.0655778272704776,
      "grad_norm": 0.063453808426857,
      "learning_rate": 0.0004987698124845253,
      "loss": 0.0602,
      "step": 265
    },
    {
      "epoch": 0.06582529076961148,
      "grad_norm": 0.05681321397423744,
      "learning_rate": 0.0004987601530349839,
      "loss": 0.075,
      "step": 266
    },
    {
      "epoch": 0.06607275426874536,
      "grad_norm": 0.03761067986488342,
      "learning_rate": 0.0004987504559048043,
      "loss": 0.0315,
      "step": 267
    },
    {
      "epoch": 0.06632021776787923,
      "grad_norm": 0.06434708088636398,
      "learning_rate": 0.0004987407210954555,
      "loss": 0.0795,
      "step": 268
    },
    {
      "epoch": 0.06656768126701311,
      "grad_norm": 0.09019071608781815,
      "learning_rate": 0.000498730948608412,
      "loss": 0.0847,
      "step": 269
    },
    {
      "epoch": 0.066815144766147,
      "grad_norm": 0.052193477749824524,
      "learning_rate": 0.0004987211384451543,
      "loss": 0.0557,
      "step": 270
    },
    {
      "epoch": 0.06706260826528088,
      "grad_norm": 0.027106495574116707,
      "learning_rate": 0.0004987112906071679,
      "loss": 0.0279,
      "step": 271
    },
    {
      "epoch": 0.06731007176441475,
      "grad_norm": 0.0379156731069088,
      "learning_rate": 0.0004987014050959449,
      "loss": 0.0711,
      "step": 272
    },
    {
      "epoch": 0.06755753526354863,
      "grad_norm": 0.056538015604019165,
      "learning_rate": 0.0004986914819129825,
      "loss": 0.0823,
      "step": 273
    },
    {
      "epoch": 0.0678049987626825,
      "grad_norm": 0.03312583640217781,
      "learning_rate": 0.0004986815210597839,
      "loss": 0.0607,
      "step": 274
    },
    {
      "epoch": 0.06805246226181638,
      "grad_norm": 0.05090184882283211,
      "learning_rate": 0.0004986715225378579,
      "loss": 0.0675,
      "step": 275
    },
    {
      "epoch": 0.06829992576095026,
      "grad_norm": 0.04353254660964012,
      "learning_rate": 0.000498661486348719,
      "loss": 0.0424,
      "step": 276
    },
    {
      "epoch": 0.06854738926008414,
      "grad_norm": 0.06371492892503738,
      "learning_rate": 0.0004986514124938875,
      "loss": 0.064,
      "step": 277
    },
    {
      "epoch": 0.06879485275921801,
      "grad_norm": 0.058336082845926285,
      "learning_rate": 0.000498641300974889,
      "loss": 0.0562,
      "step": 278
    },
    {
      "epoch": 0.06904231625835189,
      "grad_norm": 0.07385548949241638,
      "learning_rate": 0.0004986311517932556,
      "loss": 0.0442,
      "step": 279
    },
    {
      "epoch": 0.06928977975748576,
      "grad_norm": 0.08034983277320862,
      "learning_rate": 0.0004986209649505242,
      "loss": 0.078,
      "step": 280
    },
    {
      "epoch": 0.06953724325661965,
      "grad_norm": 0.06208869442343712,
      "learning_rate": 0.0004986107404482381,
      "loss": 0.037,
      "step": 281
    },
    {
      "epoch": 0.06978470675575353,
      "grad_norm": 0.07020727545022964,
      "learning_rate": 0.0004986004782879461,
      "loss": 0.1071,
      "step": 282
    },
    {
      "epoch": 0.07003217025488741,
      "grad_norm": 0.02487853914499283,
      "learning_rate": 0.0004985901784712024,
      "loss": 0.0346,
      "step": 283
    },
    {
      "epoch": 0.07027963375402128,
      "grad_norm": 0.04302862286567688,
      "learning_rate": 0.0004985798409995674,
      "loss": 0.0337,
      "step": 284
    },
    {
      "epoch": 0.07052709725315516,
      "grad_norm": 0.052986349910497665,
      "learning_rate": 0.0004985694658746067,
      "loss": 0.0674,
      "step": 285
    },
    {
      "epoch": 0.07077456075228904,
      "grad_norm": 0.0406869538128376,
      "learning_rate": 0.0004985590530978921,
      "loss": 0.061,
      "step": 286
    },
    {
      "epoch": 0.07102202425142291,
      "grad_norm": 0.07148699462413788,
      "learning_rate": 0.0004985486026710007,
      "loss": 0.0729,
      "step": 287
    },
    {
      "epoch": 0.07126948775055679,
      "grad_norm": 0.03543456643819809,
      "learning_rate": 0.0004985381145955156,
      "loss": 0.0711,
      "step": 288
    },
    {
      "epoch": 0.07151695124969067,
      "grad_norm": 0.03816112503409386,
      "learning_rate": 0.0004985275888730253,
      "loss": 0.036,
      "step": 289
    },
    {
      "epoch": 0.07176441474882454,
      "grad_norm": 0.022454090416431427,
      "learning_rate": 0.0004985170255051244,
      "loss": 0.0183,
      "step": 290
    },
    {
      "epoch": 0.07201187824795843,
      "grad_norm": 0.039214473217725754,
      "learning_rate": 0.0004985064244934127,
      "loss": 0.0271,
      "step": 291
    },
    {
      "epoch": 0.07225934174709231,
      "grad_norm": 0.024014044553041458,
      "learning_rate": 0.0004984957858394962,
      "loss": 0.0273,
      "step": 292
    },
    {
      "epoch": 0.07250680524622619,
      "grad_norm": 0.043646130710840225,
      "learning_rate": 0.0004984851095449863,
      "loss": 0.0499,
      "step": 293
    },
    {
      "epoch": 0.07275426874536006,
      "grad_norm": 0.038370657712221146,
      "learning_rate": 0.0004984743956115001,
      "loss": 0.0725,
      "step": 294
    },
    {
      "epoch": 0.07300173224449394,
      "grad_norm": 0.024327339604496956,
      "learning_rate": 0.0004984636440406605,
      "loss": 0.0351,
      "step": 295
    },
    {
      "epoch": 0.07324919574362782,
      "grad_norm": 0.061068449169397354,
      "learning_rate": 0.0004984528548340962,
      "loss": 0.082,
      "step": 296
    },
    {
      "epoch": 0.07349665924276169,
      "grad_norm": 0.04048371687531471,
      "learning_rate": 0.0004984420279934414,
      "loss": 0.0666,
      "step": 297
    },
    {
      "epoch": 0.07374412274189557,
      "grad_norm": 0.041666194796562195,
      "learning_rate": 0.0004984311635203362,
      "loss": 0.0485,
      "step": 298
    },
    {
      "epoch": 0.07399158624102944,
      "grad_norm": 0.060905858874320984,
      "learning_rate": 0.000498420261416426,
      "loss": 0.051,
      "step": 299
    },
    {
      "epoch": 0.07423904974016332,
      "grad_norm": 0.05070745199918747,
      "learning_rate": 0.0004984093216833625,
      "loss": 0.0466,
      "step": 300
    },
    {
      "epoch": 0.0744865132392972,
      "grad_norm": 0.07754357904195786,
      "learning_rate": 0.0004983983443228026,
      "loss": 0.101,
      "step": 301
    },
    {
      "epoch": 0.07473397673843109,
      "grad_norm": 0.034004151821136475,
      "learning_rate": 0.0004983873293364091,
      "loss": 0.0325,
      "step": 302
    },
    {
      "epoch": 0.07498144023756496,
      "grad_norm": 0.05533209815621376,
      "learning_rate": 0.0004983762767258506,
      "loss": 0.0755,
      "step": 303
    },
    {
      "epoch": 0.07522890373669884,
      "grad_norm": 0.04469336196780205,
      "learning_rate": 0.0004983651864928011,
      "loss": 0.039,
      "step": 304
    },
    {
      "epoch": 0.07547636723583272,
      "grad_norm": 0.07921802252531052,
      "learning_rate": 0.0004983540586389406,
      "loss": 0.1139,
      "step": 305
    },
    {
      "epoch": 0.0757238307349666,
      "grad_norm": 0.04986313730478287,
      "learning_rate": 0.0004983428931659546,
      "loss": 0.0465,
      "step": 306
    },
    {
      "epoch": 0.07597129423410047,
      "grad_norm": 0.08603225648403168,
      "learning_rate": 0.0004983316900755344,
      "loss": 0.1404,
      "step": 307
    },
    {
      "epoch": 0.07621875773323435,
      "grad_norm": 0.33840715885162354,
      "learning_rate": 0.0004983204493693771,
      "loss": 0.0621,
      "step": 308
    },
    {
      "epoch": 0.07646622123236822,
      "grad_norm": 0.04254188761115074,
      "learning_rate": 0.0004983091710491852,
      "loss": 0.0351,
      "step": 309
    },
    {
      "epoch": 0.0767136847315021,
      "grad_norm": 0.06884626299142838,
      "learning_rate": 0.0004982978551166671,
      "loss": 0.0859,
      "step": 310
    },
    {
      "epoch": 0.07696114823063598,
      "grad_norm": 0.03168559446930885,
      "learning_rate": 0.000498286501573537,
      "loss": 0.0261,
      "step": 311
    },
    {
      "epoch": 0.07720861172976985,
      "grad_norm": 0.02874121256172657,
      "learning_rate": 0.0004982751104215146,
      "loss": 0.0331,
      "step": 312
    },
    {
      "epoch": 0.07745607522890374,
      "grad_norm": 0.02967572584748268,
      "learning_rate": 0.0004982636816623252,
      "loss": 0.0305,
      "step": 313
    },
    {
      "epoch": 0.07770353872803762,
      "grad_norm": 0.05137895420193672,
      "learning_rate": 0.0004982522152977,
      "loss": 0.0441,
      "step": 314
    },
    {
      "epoch": 0.0779510022271715,
      "grad_norm": 0.054943572729825974,
      "learning_rate": 0.0004982407113293761,
      "loss": 0.0666,
      "step": 315
    },
    {
      "epoch": 0.07819846572630537,
      "grad_norm": 0.06437956541776657,
      "learning_rate": 0.000498229169759096,
      "loss": 0.1046,
      "step": 316
    },
    {
      "epoch": 0.07844592922543925,
      "grad_norm": 0.027614112943410873,
      "learning_rate": 0.0004982175905886077,
      "loss": 0.0357,
      "step": 317
    },
    {
      "epoch": 0.07869339272457312,
      "grad_norm": 0.051367808133363724,
      "learning_rate": 0.0004982059738196651,
      "loss": 0.0453,
      "step": 318
    },
    {
      "epoch": 0.078940856223707,
      "grad_norm": 0.05906569957733154,
      "learning_rate": 0.0004981943194540283,
      "loss": 0.0394,
      "step": 319
    },
    {
      "epoch": 0.07918831972284088,
      "grad_norm": 0.06597156822681427,
      "learning_rate": 0.0004981826274934621,
      "loss": 0.0559,
      "step": 320
    },
    {
      "epoch": 0.07943578322197475,
      "grad_norm": 0.037435486912727356,
      "learning_rate": 0.0004981708979397379,
      "loss": 0.0591,
      "step": 321
    },
    {
      "epoch": 0.07968324672110863,
      "grad_norm": 0.08393984287977219,
      "learning_rate": 0.0004981591307946324,
      "loss": 0.0537,
      "step": 322
    },
    {
      "epoch": 0.07993071022024252,
      "grad_norm": 0.04930109903216362,
      "learning_rate": 0.0004981473260599276,
      "loss": 0.0432,
      "step": 323
    },
    {
      "epoch": 0.0801781737193764,
      "grad_norm": 0.06397740542888641,
      "learning_rate": 0.0004981354837374121,
      "loss": 0.0885,
      "step": 324
    },
    {
      "epoch": 0.08042563721851027,
      "grad_norm": 0.03605910390615463,
      "learning_rate": 0.0004981236038288795,
      "loss": 0.053,
      "step": 325
    },
    {
      "epoch": 0.08067310071764415,
      "grad_norm": 0.045964229851961136,
      "learning_rate": 0.0004981116863361294,
      "loss": 0.0664,
      "step": 326
    },
    {
      "epoch": 0.08092056421677803,
      "grad_norm": 0.0362856350839138,
      "learning_rate": 0.0004980997312609667,
      "loss": 0.0494,
      "step": 327
    },
    {
      "epoch": 0.0811680277159119,
      "grad_norm": 0.05246919021010399,
      "learning_rate": 0.0004980877386052025,
      "loss": 0.0678,
      "step": 328
    },
    {
      "epoch": 0.08141549121504578,
      "grad_norm": 0.053091228008270264,
      "learning_rate": 0.0004980757083706534,
      "loss": 0.1004,
      "step": 329
    },
    {
      "epoch": 0.08166295471417966,
      "grad_norm": 0.044510260224342346,
      "learning_rate": 0.0004980636405591416,
      "loss": 0.0462,
      "step": 330
    },
    {
      "epoch": 0.08191041821331353,
      "grad_norm": 0.034619349986314774,
      "learning_rate": 0.0004980515351724951,
      "loss": 0.055,
      "step": 331
    },
    {
      "epoch": 0.08215788171244741,
      "grad_norm": 0.22871878743171692,
      "learning_rate": 0.0004980393922125475,
      "loss": 0.0521,
      "step": 332
    },
    {
      "epoch": 0.08240534521158129,
      "grad_norm": 0.06169320270419121,
      "learning_rate": 0.0004980272116811382,
      "loss": 0.0932,
      "step": 333
    },
    {
      "epoch": 0.08265280871071518,
      "grad_norm": 0.06377341598272324,
      "learning_rate": 0.0004980149935801121,
      "loss": 0.0716,
      "step": 334
    },
    {
      "epoch": 0.08290027220984905,
      "grad_norm": 0.052703212946653366,
      "learning_rate": 0.00049800273791132,
      "loss": 0.048,
      "step": 335
    },
    {
      "epoch": 0.08314773570898293,
      "grad_norm": 0.05379160866141319,
      "learning_rate": 0.0004979904446766184,
      "loss": 0.0811,
      "step": 336
    },
    {
      "epoch": 0.0833951992081168,
      "grad_norm": 0.037822138518095016,
      "learning_rate": 0.0004979781138778692,
      "loss": 0.0423,
      "step": 337
    },
    {
      "epoch": 0.08364266270725068,
      "grad_norm": 0.040007129311561584,
      "learning_rate": 0.0004979657455169405,
      "loss": 0.0667,
      "step": 338
    },
    {
      "epoch": 0.08389012620638456,
      "grad_norm": 0.032499752938747406,
      "learning_rate": 0.0004979533395957055,
      "loss": 0.0417,
      "step": 339
    },
    {
      "epoch": 0.08413758970551843,
      "grad_norm": 0.0388067327439785,
      "learning_rate": 0.0004979408961160433,
      "loss": 0.0346,
      "step": 340
    },
    {
      "epoch": 0.08438505320465231,
      "grad_norm": 0.05502569302916527,
      "learning_rate": 0.0004979284150798391,
      "loss": 0.0329,
      "step": 341
    },
    {
      "epoch": 0.08463251670378619,
      "grad_norm": 0.05944597348570824,
      "learning_rate": 0.0004979158964889832,
      "loss": 0.0809,
      "step": 342
    },
    {
      "epoch": 0.08487998020292006,
      "grad_norm": 0.038635388016700745,
      "learning_rate": 0.0004979033403453721,
      "loss": 0.053,
      "step": 343
    },
    {
      "epoch": 0.08512744370205395,
      "grad_norm": 0.040987901389598846,
      "learning_rate": 0.0004978907466509074,
      "loss": 0.0371,
      "step": 344
    },
    {
      "epoch": 0.08537490720118783,
      "grad_norm": 0.046564217656850815,
      "learning_rate": 0.0004978781154074968,
      "loss": 0.0571,
      "step": 345
    },
    {
      "epoch": 0.0856223707003217,
      "grad_norm": 0.04714827984571457,
      "learning_rate": 0.0004978654466170538,
      "loss": 0.0478,
      "step": 346
    },
    {
      "epoch": 0.08586983419945558,
      "grad_norm": 0.03727757930755615,
      "learning_rate": 0.0004978527402814971,
      "loss": 0.0543,
      "step": 347
    },
    {
      "epoch": 0.08611729769858946,
      "grad_norm": 0.06349363923072815,
      "learning_rate": 0.0004978399964027517,
      "loss": 0.0992,
      "step": 348
    },
    {
      "epoch": 0.08636476119772334,
      "grad_norm": 0.04851001501083374,
      "learning_rate": 0.0004978272149827477,
      "loss": 0.0498,
      "step": 349
    },
    {
      "epoch": 0.08661222469685721,
      "grad_norm": 0.03904957324266434,
      "learning_rate": 0.0004978143960234213,
      "loss": 0.0434,
      "step": 350
    },
    {
      "epoch": 0.08685968819599109,
      "grad_norm": 0.052761804312467575,
      "learning_rate": 0.0004978015395267141,
      "loss": 0.0435,
      "step": 351
    },
    {
      "epoch": 0.08710715169512497,
      "grad_norm": 0.02600914239883423,
      "learning_rate": 0.0004977886454945736,
      "loss": 0.0286,
      "step": 352
    },
    {
      "epoch": 0.08735461519425884,
      "grad_norm": 0.03962700441479683,
      "learning_rate": 0.0004977757139289528,
      "loss": 0.0727,
      "step": 353
    },
    {
      "epoch": 0.08760207869339272,
      "grad_norm": 0.03887918218970299,
      "learning_rate": 0.0004977627448318108,
      "loss": 0.0422,
      "step": 354
    },
    {
      "epoch": 0.08784954219252661,
      "grad_norm": 0.044286493211984634,
      "learning_rate": 0.0004977497382051118,
      "loss": 0.058,
      "step": 355
    },
    {
      "epoch": 0.08809700569166048,
      "grad_norm": 0.04021637141704559,
      "learning_rate": 0.000497736694050826,
      "loss": 0.0463,
      "step": 356
    },
    {
      "epoch": 0.08834446919079436,
      "grad_norm": 0.03174959495663643,
      "learning_rate": 0.0004977236123709293,
      "loss": 0.0584,
      "step": 357
    },
    {
      "epoch": 0.08859193268992824,
      "grad_norm": 0.08380801230669022,
      "learning_rate": 0.0004977104931674032,
      "loss": 0.1496,
      "step": 358
    },
    {
      "epoch": 0.08883939618906211,
      "grad_norm": 0.06039709970355034,
      "learning_rate": 0.000497697336442235,
      "loss": 0.0702,
      "step": 359
    },
    {
      "epoch": 0.08908685968819599,
      "grad_norm": 0.02536865696310997,
      "learning_rate": 0.0004976841421974176,
      "loss": 0.0308,
      "step": 360
    },
    {
      "epoch": 0.08933432318732987,
      "grad_norm": 0.07012012600898743,
      "learning_rate": 0.0004976709104349493,
      "loss": 0.0682,
      "step": 361
    },
    {
      "epoch": 0.08958178668646374,
      "grad_norm": 0.05063146725296974,
      "learning_rate": 0.0004976576411568347,
      "loss": 0.0482,
      "step": 362
    },
    {
      "epoch": 0.08982925018559762,
      "grad_norm": 0.038612399250268936,
      "learning_rate": 0.0004976443343650836,
      "loss": 0.0382,
      "step": 363
    },
    {
      "epoch": 0.0900767136847315,
      "grad_norm": 0.04009849205613136,
      "learning_rate": 0.0004976309900617116,
      "loss": 0.0522,
      "step": 364
    },
    {
      "epoch": 0.09032417718386539,
      "grad_norm": 0.08063577115535736,
      "learning_rate": 0.0004976176082487402,
      "loss": 0.0857,
      "step": 365
    },
    {
      "epoch": 0.09057164068299926,
      "grad_norm": 0.03902021795511246,
      "learning_rate": 0.0004976041889281962,
      "loss": 0.0293,
      "step": 366
    },
    {
      "epoch": 0.09081910418213314,
      "grad_norm": 0.0252707377076149,
      "learning_rate": 0.0004975907321021123,
      "loss": 0.019,
      "step": 367
    },
    {
      "epoch": 0.09106656768126702,
      "grad_norm": 0.03950066864490509,
      "learning_rate": 0.000497577237772527,
      "loss": 0.0438,
      "step": 368
    },
    {
      "epoch": 0.09131403118040089,
      "grad_norm": 0.05282039940357208,
      "learning_rate": 0.0004975637059414841,
      "loss": 0.0543,
      "step": 369
    },
    {
      "epoch": 0.09156149467953477,
      "grad_norm": 0.05668146535754204,
      "learning_rate": 0.0004975501366110335,
      "loss": 0.0678,
      "step": 370
    },
    {
      "epoch": 0.09180895817866865,
      "grad_norm": 0.04087451100349426,
      "learning_rate": 0.0004975365297832306,
      "loss": 0.0733,
      "step": 371
    },
    {
      "epoch": 0.09205642167780252,
      "grad_norm": 0.059879958629608154,
      "learning_rate": 0.0004975228854601363,
      "loss": 0.0419,
      "step": 372
    },
    {
      "epoch": 0.0923038851769364,
      "grad_norm": 0.04818100482225418,
      "learning_rate": 0.0004975092036438176,
      "loss": 0.0606,
      "step": 373
    },
    {
      "epoch": 0.09255134867607027,
      "grad_norm": 0.04348108917474747,
      "learning_rate": 0.0004974954843363468,
      "loss": 0.0605,
      "step": 374
    },
    {
      "epoch": 0.09279881217520415,
      "grad_norm": 0.03976772725582123,
      "learning_rate": 0.0004974817275398019,
      "loss": 0.0674,
      "step": 375
    },
    {
      "epoch": 0.09304627567433804,
      "grad_norm": 0.04573686420917511,
      "learning_rate": 0.0004974679332562669,
      "loss": 0.0658,
      "step": 376
    },
    {
      "epoch": 0.09329373917347192,
      "grad_norm": 0.033449746668338776,
      "learning_rate": 0.0004974541014878312,
      "loss": 0.0319,
      "step": 377
    },
    {
      "epoch": 0.0935412026726058,
      "grad_norm": 0.02911350131034851,
      "learning_rate": 0.00049744023223659,
      "loss": 0.0286,
      "step": 378
    },
    {
      "epoch": 0.09378866617173967,
      "grad_norm": 0.02114226296544075,
      "learning_rate": 0.0004974263255046441,
      "loss": 0.0218,
      "step": 379
    },
    {
      "epoch": 0.09403612967087355,
      "grad_norm": 0.0359526053071022,
      "learning_rate": 0.0004974123812940998,
      "loss": 0.0415,
      "step": 380
    },
    {
      "epoch": 0.09428359317000742,
      "grad_norm": 0.06217186525464058,
      "learning_rate": 0.0004973983996070696,
      "loss": 0.1036,
      "step": 381
    },
    {
      "epoch": 0.0945310566691413,
      "grad_norm": 0.044012945145368576,
      "learning_rate": 0.0004973843804456713,
      "loss": 0.0561,
      "step": 382
    },
    {
      "epoch": 0.09477852016827518,
      "grad_norm": 0.039037395268678665,
      "learning_rate": 0.0004973703238120282,
      "loss": 0.0525,
      "step": 383
    },
    {
      "epoch": 0.09502598366740905,
      "grad_norm": 0.03592503070831299,
      "learning_rate": 0.0004973562297082697,
      "loss": 0.0446,
      "step": 384
    },
    {
      "epoch": 0.09527344716654293,
      "grad_norm": 0.039887841790914536,
      "learning_rate": 0.0004973420981365306,
      "loss": 0.0592,
      "step": 385
    },
    {
      "epoch": 0.0955209106656768,
      "grad_norm": 0.044725071638822556,
      "learning_rate": 0.0004973279290989516,
      "loss": 0.0806,
      "step": 386
    },
    {
      "epoch": 0.0957683741648107,
      "grad_norm": 0.06667358428239822,
      "learning_rate": 0.0004973137225976788,
      "loss": 0.0304,
      "step": 387
    },
    {
      "epoch": 0.09601583766394457,
      "grad_norm": 0.04991642013192177,
      "learning_rate": 0.0004972994786348643,
      "loss": 0.0583,
      "step": 388
    },
    {
      "epoch": 0.09626330116307845,
      "grad_norm": 0.02326681837439537,
      "learning_rate": 0.0004972851972126654,
      "loss": 0.0481,
      "step": 389
    },
    {
      "epoch": 0.09651076466221233,
      "grad_norm": 0.03440213203430176,
      "learning_rate": 0.0004972708783332456,
      "loss": 0.0341,
      "step": 390
    },
    {
      "epoch": 0.0967582281613462,
      "grad_norm": 0.040693484246730804,
      "learning_rate": 0.0004972565219987737,
      "loss": 0.0333,
      "step": 391
    },
    {
      "epoch": 0.09700569166048008,
      "grad_norm": 0.04702768847346306,
      "learning_rate": 0.0004972421282114245,
      "loss": 0.0426,
      "step": 392
    },
    {
      "epoch": 0.09725315515961395,
      "grad_norm": 0.044927481561899185,
      "learning_rate": 0.0004972276969733779,
      "loss": 0.0403,
      "step": 393
    },
    {
      "epoch": 0.09750061865874783,
      "grad_norm": 0.041396379470825195,
      "learning_rate": 0.0004972132282868203,
      "loss": 0.0586,
      "step": 394
    },
    {
      "epoch": 0.09774808215788171,
      "grad_norm": 0.041292525827884674,
      "learning_rate": 0.000497198722153943,
      "loss": 0.0591,
      "step": 395
    },
    {
      "epoch": 0.09799554565701558,
      "grad_norm": 0.03910846263170242,
      "learning_rate": 0.0004971841785769434,
      "loss": 0.0874,
      "step": 396
    },
    {
      "epoch": 0.09824300915614947,
      "grad_norm": 0.028486475348472595,
      "learning_rate": 0.0004971695975580245,
      "loss": 0.0206,
      "step": 397
    },
    {
      "epoch": 0.09849047265528335,
      "grad_norm": 0.061240263283252716,
      "learning_rate": 0.000497154979099395,
      "loss": 0.0451,
      "step": 398
    },
    {
      "epoch": 0.09873793615441723,
      "grad_norm": 0.06716015934944153,
      "learning_rate": 0.000497140323203269,
      "loss": 0.1058,
      "step": 399
    },
    {
      "epoch": 0.0989853996535511,
      "grad_norm": 0.04155829921364784,
      "learning_rate": 0.0004971256298718669,
      "loss": 0.0286,
      "step": 400
    },
    {
      "epoch": 0.0989853996535511,
      "eval_loss": 0.3081038296222687,
      "eval_runtime": 202.7044,
      "eval_samples_per_second": 4.933,
      "eval_steps_per_second": 0.311,
      "step": 400
    },
    {
      "epoch": 0.09923286315268498,
      "grad_norm": 0.045628033578395844,
      "learning_rate": 0.0004971108991074139,
      "loss": 0.0692,
      "step": 401
    },
    {
      "epoch": 0.09948032665181886,
      "grad_norm": 0.062438495457172394,
      "learning_rate": 0.0004970961309121414,
      "loss": 0.0571,
      "step": 402
    },
    {
      "epoch": 0.09972779015095273,
      "grad_norm": 0.04298129305243492,
      "learning_rate": 0.0004970813252882867,
      "loss": 0.0753,
      "step": 403
    },
    {
      "epoch": 0.09997525365008661,
      "grad_norm": 0.030114686116576195,
      "learning_rate": 0.0004970664822380921,
      "loss": 0.0277,
      "step": 404
    },
    {
      "epoch": 0.10022271714922049,
      "grad_norm": 0.050491813570261,
      "learning_rate": 0.0004970516017638063,
      "loss": 0.0649,
      "step": 405
    },
    {
      "epoch": 0.10047018064835436,
      "grad_norm": 0.07498996704816818,
      "learning_rate": 0.0004970366838676828,
      "loss": 0.0653,
      "step": 406
    },
    {
      "epoch": 0.10071764414748824,
      "grad_norm": 0.025580447167158127,
      "learning_rate": 0.0004970217285519818,
      "loss": 0.0165,
      "step": 407
    },
    {
      "epoch": 0.10096510764662213,
      "grad_norm": 0.046041518449783325,
      "learning_rate": 0.0004970067358189684,
      "loss": 0.0509,
      "step": 408
    },
    {
      "epoch": 0.101212571145756,
      "grad_norm": 0.03685804456472397,
      "learning_rate": 0.0004969917056709135,
      "loss": 0.0405,
      "step": 409
    },
    {
      "epoch": 0.10146003464488988,
      "grad_norm": 0.0376676507294178,
      "learning_rate": 0.000496976638110094,
      "loss": 0.0658,
      "step": 410
    },
    {
      "epoch": 0.10170749814402376,
      "grad_norm": 0.047232601791620255,
      "learning_rate": 0.000496961533138792,
      "loss": 0.0711,
      "step": 411
    },
    {
      "epoch": 0.10195496164315763,
      "grad_norm": 0.028807383030653,
      "learning_rate": 0.0004969463907592958,
      "loss": 0.0341,
      "step": 412
    },
    {
      "epoch": 0.10220242514229151,
      "grad_norm": 0.03355107456445694,
      "learning_rate": 0.0004969312109738987,
      "loss": 0.0503,
      "step": 413
    },
    {
      "epoch": 0.10244988864142539,
      "grad_norm": 0.07155407965183258,
      "learning_rate": 0.0004969159937849003,
      "loss": 0.1195,
      "step": 414
    },
    {
      "epoch": 0.10269735214055926,
      "grad_norm": 0.030502185225486755,
      "learning_rate": 0.0004969007391946057,
      "loss": 0.0586,
      "step": 415
    },
    {
      "epoch": 0.10294481563969314,
      "grad_norm": 0.04472537338733673,
      "learning_rate": 0.0004968854472053253,
      "loss": 0.0331,
      "step": 416
    },
    {
      "epoch": 0.10319227913882702,
      "grad_norm": 0.036950912326574326,
      "learning_rate": 0.0004968701178193755,
      "loss": 0.0323,
      "step": 417
    },
    {
      "epoch": 0.10343974263796091,
      "grad_norm": 0.03701246157288551,
      "learning_rate": 0.0004968547510390784,
      "loss": 0.0688,
      "step": 418
    },
    {
      "epoch": 0.10368720613709478,
      "grad_norm": 0.032491471618413925,
      "learning_rate": 0.0004968393468667617,
      "loss": 0.0474,
      "step": 419
    },
    {
      "epoch": 0.10393466963622866,
      "grad_norm": 0.052775464951992035,
      "learning_rate": 0.0004968239053047585,
      "loss": 0.0475,
      "step": 420
    },
    {
      "epoch": 0.10418213313536254,
      "grad_norm": 0.037037819623947144,
      "learning_rate": 0.0004968084263554081,
      "loss": 0.0322,
      "step": 421
    },
    {
      "epoch": 0.10442959663449641,
      "grad_norm": 0.046730201691389084,
      "learning_rate": 0.0004967929100210548,
      "loss": 0.0483,
      "step": 422
    },
    {
      "epoch": 0.10467706013363029,
      "grad_norm": 0.055734168738126755,
      "learning_rate": 0.0004967773563040492,
      "loss": 0.0936,
      "step": 423
    },
    {
      "epoch": 0.10492452363276417,
      "grad_norm": 0.045778121799230576,
      "learning_rate": 0.0004967617652067472,
      "loss": 0.041,
      "step": 424
    },
    {
      "epoch": 0.10517198713189804,
      "grad_norm": 0.07067690044641495,
      "learning_rate": 0.0004967461367315105,
      "loss": 0.0983,
      "step": 425
    },
    {
      "epoch": 0.10541945063103192,
      "grad_norm": 0.05883510783314705,
      "learning_rate": 0.0004967304708807063,
      "loss": 0.0714,
      "step": 426
    },
    {
      "epoch": 0.1056669141301658,
      "grad_norm": 0.08293195068836212,
      "learning_rate": 0.0004967147676567075,
      "loss": 0.0864,
      "step": 427
    },
    {
      "epoch": 0.10591437762929967,
      "grad_norm": 0.025948841124773026,
      "learning_rate": 0.000496699027061893,
      "loss": 0.0317,
      "step": 428
    },
    {
      "epoch": 0.10616184112843356,
      "grad_norm": 0.06778628379106522,
      "learning_rate": 0.0004966832490986468,
      "loss": 0.1262,
      "step": 429
    },
    {
      "epoch": 0.10640930462756744,
      "grad_norm": 0.02854357846081257,
      "learning_rate": 0.000496667433769359,
      "loss": 0.0571,
      "step": 430
    },
    {
      "epoch": 0.10665676812670132,
      "grad_norm": 0.043404869735240936,
      "learning_rate": 0.0004966515810764251,
      "loss": 0.0602,
      "step": 431
    },
    {
      "epoch": 0.10690423162583519,
      "grad_norm": 0.03244992718100548,
      "learning_rate": 0.0004966356910222466,
      "loss": 0.0319,
      "step": 432
    },
    {
      "epoch": 0.10715169512496907,
      "grad_norm": 0.0612046979367733,
      "learning_rate": 0.0004966197636092301,
      "loss": 0.1154,
      "step": 433
    },
    {
      "epoch": 0.10739915862410294,
      "grad_norm": 0.05650736019015312,
      "learning_rate": 0.0004966037988397885,
      "loss": 0.0614,
      "step": 434
    },
    {
      "epoch": 0.10764662212323682,
      "grad_norm": 0.03649060055613518,
      "learning_rate": 0.0004965877967163398,
      "loss": 0.0688,
      "step": 435
    },
    {
      "epoch": 0.1078940856223707,
      "grad_norm": 0.04681425169110298,
      "learning_rate": 0.000496571757241308,
      "loss": 0.0682,
      "step": 436
    },
    {
      "epoch": 0.10814154912150457,
      "grad_norm": 0.048294566571712494,
      "learning_rate": 0.0004965556804171226,
      "loss": 0.0969,
      "step": 437
    },
    {
      "epoch": 0.10838901262063845,
      "grad_norm": 0.08598500490188599,
      "learning_rate": 0.000496539566246219,
      "loss": 0.1189,
      "step": 438
    },
    {
      "epoch": 0.10863647611977233,
      "grad_norm": 0.04354793578386307,
      "learning_rate": 0.0004965234147310378,
      "loss": 0.0436,
      "step": 439
    },
    {
      "epoch": 0.10888393961890622,
      "grad_norm": 0.039826977998018265,
      "learning_rate": 0.0004965072258740258,
      "loss": 0.0451,
      "step": 440
    },
    {
      "epoch": 0.1091314031180401,
      "grad_norm": 0.03028617799282074,
      "learning_rate": 0.000496490999677635,
      "loss": 0.0442,
      "step": 441
    },
    {
      "epoch": 0.10937886661717397,
      "grad_norm": 0.032837893813848495,
      "learning_rate": 0.0004964747361443233,
      "loss": 0.0583,
      "step": 442
    },
    {
      "epoch": 0.10962633011630785,
      "grad_norm": 0.06963181495666504,
      "learning_rate": 0.0004964584352765544,
      "loss": 0.0756,
      "step": 443
    },
    {
      "epoch": 0.10987379361544172,
      "grad_norm": 0.03233153000473976,
      "learning_rate": 0.000496442097076797,
      "loss": 0.0443,
      "step": 444
    },
    {
      "epoch": 0.1101212571145756,
      "grad_norm": 0.05236949026584625,
      "learning_rate": 0.0004964257215475263,
      "loss": 0.0643,
      "step": 445
    },
    {
      "epoch": 0.11036872061370948,
      "grad_norm": 0.06726270169019699,
      "learning_rate": 0.0004964093086912226,
      "loss": 0.0894,
      "step": 446
    },
    {
      "epoch": 0.11061618411284335,
      "grad_norm": 0.03549957275390625,
      "learning_rate": 0.0004963928585103721,
      "loss": 0.0457,
      "step": 447
    },
    {
      "epoch": 0.11086364761197723,
      "grad_norm": 0.08443732559680939,
      "learning_rate": 0.0004963763710074664,
      "loss": 0.0969,
      "step": 448
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 0.09741629660129547,
      "learning_rate": 0.0004963598461850031,
      "loss": 0.0758,
      "step": 449
    },
    {
      "epoch": 0.111358574610245,
      "grad_norm": 0.03026190958917141,
      "learning_rate": 0.0004963432840454853,
      "loss": 0.0173,
      "step": 450
    },
    {
      "epoch": 0.11160603810937887,
      "grad_norm": 0.054645709693431854,
      "learning_rate": 0.0004963266845914216,
      "loss": 0.0473,
      "step": 451
    },
    {
      "epoch": 0.11185350160851275,
      "grad_norm": 0.03325831890106201,
      "learning_rate": 0.0004963100478253264,
      "loss": 0.0356,
      "step": 452
    },
    {
      "epoch": 0.11210096510764662,
      "grad_norm": 0.048348620533943176,
      "learning_rate": 0.0004962933737497197,
      "loss": 0.0371,
      "step": 453
    },
    {
      "epoch": 0.1123484286067805,
      "grad_norm": 0.026138674467802048,
      "learning_rate": 0.0004962766623671274,
      "loss": 0.0319,
      "step": 454
    },
    {
      "epoch": 0.11259589210591438,
      "grad_norm": 0.04754849150776863,
      "learning_rate": 0.0004962599136800806,
      "loss": 0.0783,
      "step": 455
    },
    {
      "epoch": 0.11284335560504825,
      "grad_norm": 0.024087367579340935,
      "learning_rate": 0.0004962431276911164,
      "loss": 0.0182,
      "step": 456
    },
    {
      "epoch": 0.11309081910418213,
      "grad_norm": 0.052204255014657974,
      "learning_rate": 0.0004962263044027775,
      "loss": 0.0742,
      "step": 457
    },
    {
      "epoch": 0.113338282603316,
      "grad_norm": 0.08963224291801453,
      "learning_rate": 0.0004962094438176121,
      "loss": 0.0662,
      "step": 458
    },
    {
      "epoch": 0.11358574610244988,
      "grad_norm": 0.04147188365459442,
      "learning_rate": 0.000496192545938174,
      "loss": 0.0555,
      "step": 459
    },
    {
      "epoch": 0.11383320960158376,
      "grad_norm": 0.05582427233457565,
      "learning_rate": 0.0004961756107670229,
      "loss": 0.062,
      "step": 460
    },
    {
      "epoch": 0.11408067310071765,
      "grad_norm": 0.054540637880563736,
      "learning_rate": 0.0004961586383067241,
      "loss": 0.0667,
      "step": 461
    },
    {
      "epoch": 0.11432813659985153,
      "grad_norm": 0.08835157752037048,
      "learning_rate": 0.0004961416285598486,
      "loss": 0.0385,
      "step": 462
    },
    {
      "epoch": 0.1145756000989854,
      "grad_norm": 0.05295392498373985,
      "learning_rate": 0.0004961245815289726,
      "loss": 0.0659,
      "step": 463
    },
    {
      "epoch": 0.11482306359811928,
      "grad_norm": 0.055231329053640366,
      "learning_rate": 0.0004961074972166785,
      "loss": 0.0794,
      "step": 464
    },
    {
      "epoch": 0.11507052709725316,
      "grad_norm": 0.05598634481430054,
      "learning_rate": 0.000496090375625554,
      "loss": 0.068,
      "step": 465
    },
    {
      "epoch": 0.11531799059638703,
      "grad_norm": 0.053849752992391586,
      "learning_rate": 0.0004960732167581928,
      "loss": 0.067,
      "step": 466
    },
    {
      "epoch": 0.11556545409552091,
      "grad_norm": 0.03230428695678711,
      "learning_rate": 0.0004960560206171936,
      "loss": 0.0651,
      "step": 467
    },
    {
      "epoch": 0.11581291759465479,
      "grad_norm": 0.05339809134602547,
      "learning_rate": 0.0004960387872051617,
      "loss": 0.047,
      "step": 468
    },
    {
      "epoch": 0.11606038109378866,
      "grad_norm": 0.034823399037122726,
      "learning_rate": 0.000496021516524707,
      "loss": 0.0479,
      "step": 469
    },
    {
      "epoch": 0.11630784459292254,
      "grad_norm": 0.03909720852971077,
      "learning_rate": 0.000496004208578446,
      "loss": 0.0469,
      "step": 470
    },
    {
      "epoch": 0.11655530809205643,
      "grad_norm": 0.04677475243806839,
      "learning_rate": 0.0004959868633690001,
      "loss": 0.0684,
      "step": 471
    },
    {
      "epoch": 0.1168027715911903,
      "grad_norm": 0.036356501281261444,
      "learning_rate": 0.0004959694808989967,
      "loss": 0.0479,
      "step": 472
    },
    {
      "epoch": 0.11705023509032418,
      "grad_norm": 0.029638133943080902,
      "learning_rate": 0.000495952061171069,
      "loss": 0.0317,
      "step": 473
    },
    {
      "epoch": 0.11729769858945806,
      "grad_norm": 0.07818958908319473,
      "learning_rate": 0.0004959346041878552,
      "loss": 0.1612,
      "step": 474
    },
    {
      "epoch": 0.11754516208859193,
      "grad_norm": 0.04176788032054901,
      "learning_rate": 0.000495917109952,
      "loss": 0.0575,
      "step": 475
    },
    {
      "epoch": 0.11779262558772581,
      "grad_norm": 0.0726722776889801,
      "learning_rate": 0.0004958995784661531,
      "loss": 0.1255,
      "step": 476
    },
    {
      "epoch": 0.11804008908685969,
      "grad_norm": 0.07581666111946106,
      "learning_rate": 0.0004958820097329702,
      "loss": 0.1014,
      "step": 477
    },
    {
      "epoch": 0.11828755258599356,
      "grad_norm": 0.04844898357987404,
      "learning_rate": 0.0004958644037551122,
      "loss": 0.0453,
      "step": 478
    },
    {
      "epoch": 0.11853501608512744,
      "grad_norm": 0.04533815383911133,
      "learning_rate": 0.0004958467605352463,
      "loss": 0.0596,
      "step": 479
    },
    {
      "epoch": 0.11878247958426132,
      "grad_norm": 0.0411793477833271,
      "learning_rate": 0.0004958290800760448,
      "loss": 0.0532,
      "step": 480
    },
    {
      "epoch": 0.11902994308339519,
      "grad_norm": 0.04274873062968254,
      "learning_rate": 0.0004958113623801858,
      "loss": 0.1079,
      "step": 481
    },
    {
      "epoch": 0.11927740658252908,
      "grad_norm": 0.05320537090301514,
      "learning_rate": 0.0004957936074503533,
      "loss": 0.0426,
      "step": 482
    },
    {
      "epoch": 0.11952487008166296,
      "grad_norm": 0.04139082133769989,
      "learning_rate": 0.0004957758152892365,
      "loss": 0.039,
      "step": 483
    },
    {
      "epoch": 0.11977233358079684,
      "grad_norm": 0.04273628070950508,
      "learning_rate": 0.0004957579858995304,
      "loss": 0.0705,
      "step": 484
    },
    {
      "epoch": 0.12001979707993071,
      "grad_norm": 0.027983857318758965,
      "learning_rate": 0.0004957401192839359,
      "loss": 0.029,
      "step": 485
    },
    {
      "epoch": 0.12026726057906459,
      "grad_norm": 0.046524256467819214,
      "learning_rate": 0.000495722215445159,
      "loss": 0.0774,
      "step": 486
    },
    {
      "epoch": 0.12051472407819847,
      "grad_norm": 0.03908989951014519,
      "learning_rate": 0.0004957042743859119,
      "loss": 0.0752,
      "step": 487
    },
    {
      "epoch": 0.12076218757733234,
      "grad_norm": 0.03540566936135292,
      "learning_rate": 0.0004956862961089123,
      "loss": 0.0469,
      "step": 488
    },
    {
      "epoch": 0.12100965107646622,
      "grad_norm": 0.030172089114785194,
      "learning_rate": 0.0004956682806168831,
      "loss": 0.0483,
      "step": 489
    },
    {
      "epoch": 0.1212571145756001,
      "grad_norm": 0.04484780132770538,
      "learning_rate": 0.0004956502279125535,
      "loss": 0.0846,
      "step": 490
    },
    {
      "epoch": 0.12150457807473397,
      "grad_norm": 0.039612628519535065,
      "learning_rate": 0.0004956321379986578,
      "loss": 0.0558,
      "step": 491
    },
    {
      "epoch": 0.12175204157386786,
      "grad_norm": 0.030857354402542114,
      "learning_rate": 0.0004956140108779363,
      "loss": 0.0294,
      "step": 492
    },
    {
      "epoch": 0.12199950507300174,
      "grad_norm": 0.04857295751571655,
      "learning_rate": 0.0004955958465531347,
      "loss": 0.0518,
      "step": 493
    },
    {
      "epoch": 0.12224696857213561,
      "grad_norm": 0.03773028776049614,
      "learning_rate": 0.0004955776450270043,
      "loss": 0.0436,
      "step": 494
    },
    {
      "epoch": 0.12249443207126949,
      "grad_norm": 0.04557473212480545,
      "learning_rate": 0.0004955594063023025,
      "loss": 0.0685,
      "step": 495
    },
    {
      "epoch": 0.12274189557040337,
      "grad_norm": 0.02739889547228813,
      "learning_rate": 0.0004955411303817916,
      "loss": 0.0293,
      "step": 496
    },
    {
      "epoch": 0.12298935906953724,
      "grad_norm": 0.03827948123216629,
      "learning_rate": 0.0004955228172682401,
      "loss": 0.0428,
      "step": 497
    },
    {
      "epoch": 0.12323682256867112,
      "grad_norm": 0.03418383002281189,
      "learning_rate": 0.000495504466964422,
      "loss": 0.0525,
      "step": 498
    },
    {
      "epoch": 0.123484286067805,
      "grad_norm": 0.03275282308459282,
      "learning_rate": 0.0004954860794731168,
      "loss": 0.0403,
      "step": 499
    },
    {
      "epoch": 0.12373174956693887,
      "grad_norm": 0.05310288816690445,
      "learning_rate": 0.0004954676547971097,
      "loss": 0.0622,
      "step": 500
    },
    {
      "epoch": 0.12397921306607275,
      "grad_norm": 0.02453845553100109,
      "learning_rate": 0.0004954491929391918,
      "loss": 0.0359,
      "step": 501
    },
    {
      "epoch": 0.12422667656520663,
      "grad_norm": 0.03590886667370796,
      "learning_rate": 0.0004954306939021592,
      "loss": 0.0642,
      "step": 502
    },
    {
      "epoch": 0.12447414006434052,
      "grad_norm": 0.039806704968214035,
      "learning_rate": 0.0004954121576888144,
      "loss": 0.0521,
      "step": 503
    },
    {
      "epoch": 0.12472160356347439,
      "grad_norm": 0.039209574460983276,
      "learning_rate": 0.0004953935843019649,
      "loss": 0.0713,
      "step": 504
    },
    {
      "epoch": 0.12496906706260827,
      "grad_norm": 0.041911885142326355,
      "learning_rate": 0.0004953749737444241,
      "loss": 0.0439,
      "step": 505
    },
    {
      "epoch": 0.12521653056174215,
      "grad_norm": 0.04252943769097328,
      "learning_rate": 0.0004953563260190111,
      "loss": 0.0559,
      "step": 506
    },
    {
      "epoch": 0.12546399406087602,
      "grad_norm": 0.03836366534233093,
      "learning_rate": 0.0004953376411285505,
      "loss": 0.0353,
      "step": 507
    },
    {
      "epoch": 0.1257114575600099,
      "grad_norm": 0.03332975134253502,
      "learning_rate": 0.0004953189190758726,
      "loss": 0.0414,
      "step": 508
    },
    {
      "epoch": 0.12595892105914377,
      "grad_norm": 0.046078916639089584,
      "learning_rate": 0.0004953001598638132,
      "loss": 0.048,
      "step": 509
    },
    {
      "epoch": 0.12620638455827765,
      "grad_norm": 0.028974931687116623,
      "learning_rate": 0.000495281363495214,
      "loss": 0.0325,
      "step": 510
    },
    {
      "epoch": 0.12645384805741153,
      "grad_norm": 0.043176501989364624,
      "learning_rate": 0.000495262529972922,
      "loss": 0.0529,
      "step": 511
    },
    {
      "epoch": 0.1267013115565454,
      "grad_norm": 0.07559606432914734,
      "learning_rate": 0.0004952436592997901,
      "loss": 0.0884,
      "step": 512
    },
    {
      "epoch": 0.12694877505567928,
      "grad_norm": 0.03684455156326294,
      "learning_rate": 0.0004952247514786766,
      "loss": 0.0629,
      "step": 513
    },
    {
      "epoch": 0.12719623855481316,
      "grad_norm": 0.038151342421770096,
      "learning_rate": 0.0004952058065124457,
      "loss": 0.0461,
      "step": 514
    },
    {
      "epoch": 0.12744370205394703,
      "grad_norm": 0.05816071107983589,
      "learning_rate": 0.0004951868244039667,
      "loss": 0.0859,
      "step": 515
    },
    {
      "epoch": 0.1276911655530809,
      "grad_norm": 0.032236069440841675,
      "learning_rate": 0.0004951678051561155,
      "loss": 0.0332,
      "step": 516
    },
    {
      "epoch": 0.1279386290522148,
      "grad_norm": 0.04219922795891762,
      "learning_rate": 0.0004951487487717724,
      "loss": 0.0391,
      "step": 517
    },
    {
      "epoch": 0.12818609255134866,
      "grad_norm": 0.07619941979646683,
      "learning_rate": 0.0004951296552538244,
      "loss": 0.0459,
      "step": 518
    },
    {
      "epoch": 0.12843355605048257,
      "grad_norm": 0.057761795818805695,
      "learning_rate": 0.0004951105246051633,
      "loss": 0.0549,
      "step": 519
    },
    {
      "epoch": 0.12868101954961644,
      "grad_norm": 0.03762875869870186,
      "learning_rate": 0.0004950913568286872,
      "loss": 0.0497,
      "step": 520
    },
    {
      "epoch": 0.12892848304875032,
      "grad_norm": 0.04810396581888199,
      "learning_rate": 0.0004950721519272993,
      "loss": 0.0674,
      "step": 521
    },
    {
      "epoch": 0.1291759465478842,
      "grad_norm": 0.03556200861930847,
      "learning_rate": 0.0004950529099039088,
      "loss": 0.119,
      "step": 522
    },
    {
      "epoch": 0.12942341004701807,
      "grad_norm": 0.06370426714420319,
      "learning_rate": 0.0004950336307614304,
      "loss": 0.0593,
      "step": 523
    },
    {
      "epoch": 0.12967087354615195,
      "grad_norm": 0.04938002675771713,
      "learning_rate": 0.0004950143145027841,
      "loss": 0.0558,
      "step": 524
    },
    {
      "epoch": 0.12991833704528583,
      "grad_norm": 0.04724010080099106,
      "learning_rate": 0.000494994961130896,
      "loss": 0.0361,
      "step": 525
    },
    {
      "epoch": 0.1301658005444197,
      "grad_norm": 0.03737473487854004,
      "learning_rate": 0.0004949755706486976,
      "loss": 0.0483,
      "step": 526
    },
    {
      "epoch": 0.13041326404355358,
      "grad_norm": 0.04298103600740433,
      "learning_rate": 0.000494956143059126,
      "loss": 0.0345,
      "step": 527
    },
    {
      "epoch": 0.13066072754268745,
      "grad_norm": 0.028080696240067482,
      "learning_rate": 0.0004949366783651241,
      "loss": 0.0451,
      "step": 528
    },
    {
      "epoch": 0.13090819104182133,
      "grad_norm": 0.05305897071957588,
      "learning_rate": 0.0004949171765696403,
      "loss": 0.0559,
      "step": 529
    },
    {
      "epoch": 0.1311556545409552,
      "grad_norm": 0.03364688903093338,
      "learning_rate": 0.0004948976376756284,
      "loss": 0.0863,
      "step": 530
    },
    {
      "epoch": 0.13140311804008908,
      "grad_norm": 0.030019763857126236,
      "learning_rate": 0.0004948780616860482,
      "loss": 0.045,
      "step": 531
    },
    {
      "epoch": 0.13165058153922296,
      "grad_norm": 0.054104603826999664,
      "learning_rate": 0.0004948584486038649,
      "loss": 0.0843,
      "step": 532
    },
    {
      "epoch": 0.13189804503835684,
      "grad_norm": 0.06483835726976395,
      "learning_rate": 0.0004948387984320495,
      "loss": 0.0948,
      "step": 533
    },
    {
      "epoch": 0.1321455085374907,
      "grad_norm": 0.0722542330622673,
      "learning_rate": 0.0004948191111735782,
      "loss": 0.0939,
      "step": 534
    },
    {
      "epoch": 0.1323929720366246,
      "grad_norm": 0.03043362684547901,
      "learning_rate": 0.0004947993868314333,
      "loss": 0.0362,
      "step": 535
    },
    {
      "epoch": 0.13264043553575847,
      "grad_norm": 0.058980558067560196,
      "learning_rate": 0.0004947796254086026,
      "loss": 0.0779,
      "step": 536
    },
    {
      "epoch": 0.13288789903489234,
      "grad_norm": 0.034014008939266205,
      "learning_rate": 0.0004947598269080791,
      "loss": 0.0524,
      "step": 537
    },
    {
      "epoch": 0.13313536253402622,
      "grad_norm": 0.04661509767174721,
      "learning_rate": 0.000494739991332862,
      "loss": 0.0669,
      "step": 538
    },
    {
      "epoch": 0.1333828260331601,
      "grad_norm": 0.06034354120492935,
      "learning_rate": 0.0004947201186859561,
      "loss": 0.072,
      "step": 539
    },
    {
      "epoch": 0.133630289532294,
      "grad_norm": 0.07072354108095169,
      "learning_rate": 0.000494700208970371,
      "loss": 0.0991,
      "step": 540
    },
    {
      "epoch": 0.13387775303142788,
      "grad_norm": 0.042230796068906784,
      "learning_rate": 0.0004946802621891231,
      "loss": 0.0458,
      "step": 541
    },
    {
      "epoch": 0.13412521653056175,
      "grad_norm": 0.04850029945373535,
      "learning_rate": 0.0004946602783452334,
      "loss": 0.0457,
      "step": 542
    },
    {
      "epoch": 0.13437268002969563,
      "grad_norm": 0.04544730484485626,
      "learning_rate": 0.000494640257441729,
      "loss": 0.0376,
      "step": 543
    },
    {
      "epoch": 0.1346201435288295,
      "grad_norm": 0.04395242780447006,
      "learning_rate": 0.0004946201994816428,
      "loss": 0.0406,
      "step": 544
    },
    {
      "epoch": 0.13486760702796338,
      "grad_norm": 0.212627574801445,
      "learning_rate": 0.0004946001044680128,
      "loss": 0.0709,
      "step": 545
    },
    {
      "epoch": 0.13511507052709726,
      "grad_norm": 0.06666473299264908,
      "learning_rate": 0.0004945799724038829,
      "loss": 0.0964,
      "step": 546
    },
    {
      "epoch": 0.13536253402623113,
      "grad_norm": 0.03736894205212593,
      "learning_rate": 0.0004945598032923026,
      "loss": 0.0708,
      "step": 547
    },
    {
      "epoch": 0.135609997525365,
      "grad_norm": 0.04093598201870918,
      "learning_rate": 0.000494539597136327,
      "loss": 0.0743,
      "step": 548
    },
    {
      "epoch": 0.1358574610244989,
      "grad_norm": 0.08075103163719177,
      "learning_rate": 0.0004945193539390168,
      "loss": 0.111,
      "step": 549
    },
    {
      "epoch": 0.13610492452363276,
      "grad_norm": 0.0531071238219738,
      "learning_rate": 0.0004944990737034383,
      "loss": 0.0644,
      "step": 550
    },
    {
      "epoch": 0.13635238802276664,
      "grad_norm": 0.0533168651163578,
      "learning_rate": 0.0004944787564326636,
      "loss": 0.0581,
      "step": 551
    },
    {
      "epoch": 0.13659985152190052,
      "grad_norm": 0.04090122878551483,
      "learning_rate": 0.0004944584021297699,
      "loss": 0.0403,
      "step": 552
    },
    {
      "epoch": 0.1368473150210344,
      "grad_norm": 0.027788545936346054,
      "learning_rate": 0.0004944380107978406,
      "loss": 0.0421,
      "step": 553
    },
    {
      "epoch": 0.13709477852016827,
      "grad_norm": 0.058748286217451096,
      "learning_rate": 0.0004944175824399643,
      "loss": 0.0497,
      "step": 554
    },
    {
      "epoch": 0.13734224201930215,
      "grad_norm": 0.04596513509750366,
      "learning_rate": 0.0004943971170592356,
      "loss": 0.0602,
      "step": 555
    },
    {
      "epoch": 0.13758970551843602,
      "grad_norm": 0.05186018720269203,
      "learning_rate": 0.0004943766146587542,
      "loss": 0.0966,
      "step": 556
    },
    {
      "epoch": 0.1378371690175699,
      "grad_norm": 0.061715494841337204,
      "learning_rate": 0.0004943560752416257,
      "loss": 0.0526,
      "step": 557
    },
    {
      "epoch": 0.13808463251670378,
      "grad_norm": 0.05825628340244293,
      "learning_rate": 0.0004943354988109615,
      "loss": 0.0804,
      "step": 558
    },
    {
      "epoch": 0.13833209601583765,
      "grad_norm": 0.03243356943130493,
      "learning_rate": 0.0004943148853698781,
      "loss": 0.0654,
      "step": 559
    },
    {
      "epoch": 0.13857955951497153,
      "grad_norm": 0.029287980869412422,
      "learning_rate": 0.0004942942349214982,
      "loss": 0.0428,
      "step": 560
    },
    {
      "epoch": 0.13882702301410543,
      "grad_norm": 0.059217147529125214,
      "learning_rate": 0.0004942735474689496,
      "loss": 0.086,
      "step": 561
    },
    {
      "epoch": 0.1390744865132393,
      "grad_norm": 0.044546399265527725,
      "learning_rate": 0.0004942528230153659,
      "loss": 0.0557,
      "step": 562
    },
    {
      "epoch": 0.13932195001237319,
      "grad_norm": 0.035276759415864944,
      "learning_rate": 0.0004942320615638864,
      "loss": 0.1068,
      "step": 563
    },
    {
      "epoch": 0.13956941351150706,
      "grad_norm": 0.035142865031957626,
      "learning_rate": 0.0004942112631176559,
      "loss": 0.0317,
      "step": 564
    },
    {
      "epoch": 0.13981687701064094,
      "grad_norm": 0.073429174721241,
      "learning_rate": 0.0004941904276798248,
      "loss": 0.1109,
      "step": 565
    },
    {
      "epoch": 0.14006434050977482,
      "grad_norm": 0.08625546842813492,
      "learning_rate": 0.000494169555253549,
      "loss": 0.0932,
      "step": 566
    },
    {
      "epoch": 0.1403118040089087,
      "grad_norm": 0.04261303320527077,
      "learning_rate": 0.0004941486458419904,
      "loss": 0.0513,
      "step": 567
    },
    {
      "epoch": 0.14055926750804257,
      "grad_norm": 0.03943308815360069,
      "learning_rate": 0.000494127699448316,
      "loss": 0.042,
      "step": 568
    },
    {
      "epoch": 0.14080673100717644,
      "grad_norm": 0.04258712753653526,
      "learning_rate": 0.0004941067160756986,
      "loss": 0.0441,
      "step": 569
    },
    {
      "epoch": 0.14105419450631032,
      "grad_norm": 0.03746781498193741,
      "learning_rate": 0.000494085695727317,
      "loss": 0.0415,
      "step": 570
    },
    {
      "epoch": 0.1413016580054442,
      "grad_norm": 0.041303426027297974,
      "learning_rate": 0.0004940646384063547,
      "loss": 0.0602,
      "step": 571
    },
    {
      "epoch": 0.14154912150457807,
      "grad_norm": 0.05217910557985306,
      "learning_rate": 0.0004940435441160018,
      "loss": 0.0828,
      "step": 572
    },
    {
      "epoch": 0.14179658500371195,
      "grad_norm": 0.04533478617668152,
      "learning_rate": 0.0004940224128594532,
      "loss": 0.0595,
      "step": 573
    },
    {
      "epoch": 0.14204404850284583,
      "grad_norm": 0.051800746470689774,
      "learning_rate": 0.0004940012446399099,
      "loss": 0.0662,
      "step": 574
    },
    {
      "epoch": 0.1422915120019797,
      "grad_norm": 0.04538615047931671,
      "learning_rate": 0.0004939800394605783,
      "loss": 0.06,
      "step": 575
    },
    {
      "epoch": 0.14253897550111358,
      "grad_norm": 0.0926145389676094,
      "learning_rate": 0.0004939587973246704,
      "loss": 0.1133,
      "step": 576
    },
    {
      "epoch": 0.14278643900024746,
      "grad_norm": 0.027383124455809593,
      "learning_rate": 0.0004939375182354039,
      "loss": 0.0296,
      "step": 577
    },
    {
      "epoch": 0.14303390249938133,
      "grad_norm": 0.050533391535282135,
      "learning_rate": 0.0004939162021960019,
      "loss": 0.0455,
      "step": 578
    },
    {
      "epoch": 0.1432813659985152,
      "grad_norm": 0.027685819193720818,
      "learning_rate": 0.0004938948492096933,
      "loss": 0.0591,
      "step": 579
    },
    {
      "epoch": 0.14352882949764909,
      "grad_norm": 0.06865669786930084,
      "learning_rate": 0.0004938734592797125,
      "loss": 0.0822,
      "step": 580
    },
    {
      "epoch": 0.14377629299678296,
      "grad_norm": 0.037412870675325394,
      "learning_rate": 0.0004938520324092996,
      "loss": 0.0497,
      "step": 581
    },
    {
      "epoch": 0.14402375649591687,
      "grad_norm": 0.07269494235515594,
      "learning_rate": 0.0004938305686016999,
      "loss": 0.0945,
      "step": 582
    },
    {
      "epoch": 0.14427121999505074,
      "grad_norm": 0.05917244404554367,
      "learning_rate": 0.0004938090678601651,
      "loss": 0.0627,
      "step": 583
    },
    {
      "epoch": 0.14451868349418462,
      "grad_norm": 0.027088269591331482,
      "learning_rate": 0.0004937875301879517,
      "loss": 0.0304,
      "step": 584
    },
    {
      "epoch": 0.1447661469933185,
      "grad_norm": 0.05046345666050911,
      "learning_rate": 0.000493765955588322,
      "loss": 0.058,
      "step": 585
    },
    {
      "epoch": 0.14501361049245237,
      "grad_norm": 0.05362590029835701,
      "learning_rate": 0.0004937443440645441,
      "loss": 0.1125,
      "step": 586
    },
    {
      "epoch": 0.14526107399158625,
      "grad_norm": 0.04086553677916527,
      "learning_rate": 0.0004937226956198916,
      "loss": 0.0437,
      "step": 587
    },
    {
      "epoch": 0.14550853749072012,
      "grad_norm": 0.052320003509521484,
      "learning_rate": 0.0004937010102576437,
      "loss": 0.058,
      "step": 588
    },
    {
      "epoch": 0.145756000989854,
      "grad_norm": 0.033336788415908813,
      "learning_rate": 0.0004936792879810852,
      "loss": 0.0287,
      "step": 589
    },
    {
      "epoch": 0.14600346448898788,
      "grad_norm": 0.033923473209142685,
      "learning_rate": 0.0004936575287935063,
      "loss": 0.0318,
      "step": 590
    },
    {
      "epoch": 0.14625092798812175,
      "grad_norm": 0.03978292644023895,
      "learning_rate": 0.000493635732698203,
      "loss": 0.0537,
      "step": 591
    },
    {
      "epoch": 0.14649839148725563,
      "grad_norm": 0.03304057940840721,
      "learning_rate": 0.0004936138996984769,
      "loss": 0.0383,
      "step": 592
    },
    {
      "epoch": 0.1467458549863895,
      "grad_norm": 0.041392501443624496,
      "learning_rate": 0.0004935920297976351,
      "loss": 0.0858,
      "step": 593
    },
    {
      "epoch": 0.14699331848552338,
      "grad_norm": 0.041924066841602325,
      "learning_rate": 0.0004935701229989901,
      "loss": 0.0582,
      "step": 594
    },
    {
      "epoch": 0.14724078198465726,
      "grad_norm": 0.03205553814768791,
      "learning_rate": 0.0004935481793058608,
      "loss": 0.0298,
      "step": 595
    },
    {
      "epoch": 0.14748824548379114,
      "grad_norm": 0.03368139639496803,
      "learning_rate": 0.0004935261987215704,
      "loss": 0.0411,
      "step": 596
    },
    {
      "epoch": 0.147735708982925,
      "grad_norm": 0.03213706240057945,
      "learning_rate": 0.0004935041812494487,
      "loss": 0.0392,
      "step": 597
    },
    {
      "epoch": 0.1479831724820589,
      "grad_norm": 0.034729763865470886,
      "learning_rate": 0.0004934821268928309,
      "loss": 0.0529,
      "step": 598
    },
    {
      "epoch": 0.14823063598119277,
      "grad_norm": 0.042097046971321106,
      "learning_rate": 0.0004934600356550574,
      "loss": 0.0522,
      "step": 599
    },
    {
      "epoch": 0.14847809948032664,
      "grad_norm": 0.046881671994924545,
      "learning_rate": 0.0004934379075394746,
      "loss": 0.0801,
      "step": 600
    },
    {
      "epoch": 0.14847809948032664,
      "eval_loss": 0.3037438988685608,
      "eval_runtime": 202.754,
      "eval_samples_per_second": 4.932,
      "eval_steps_per_second": 0.311,
      "step": 600
    },
    {
      "epoch": 0.14872556297946052,
      "grad_norm": 0.04575863853096962,
      "learning_rate": 0.0004934157425494344,
      "loss": 0.0284,
      "step": 601
    },
    {
      "epoch": 0.1489730264785944,
      "grad_norm": 0.0436725988984108,
      "learning_rate": 0.0004933935406882939,
      "loss": 0.0526,
      "step": 602
    },
    {
      "epoch": 0.1492204899777283,
      "grad_norm": 0.052480485290288925,
      "learning_rate": 0.0004933713019594164,
      "loss": 0.0727,
      "step": 603
    },
    {
      "epoch": 0.14946795347686218,
      "grad_norm": 0.029077492654323578,
      "learning_rate": 0.0004933490263661703,
      "loss": 0.047,
      "step": 604
    },
    {
      "epoch": 0.14971541697599605,
      "grad_norm": 0.024095749482512474,
      "learning_rate": 0.0004933267139119299,
      "loss": 0.0533,
      "step": 605
    },
    {
      "epoch": 0.14996288047512993,
      "grad_norm": 0.023191023617982864,
      "learning_rate": 0.0004933043646000749,
      "loss": 0.0278,
      "step": 606
    },
    {
      "epoch": 0.1502103439742638,
      "grad_norm": 0.04485022649168968,
      "learning_rate": 0.0004932819784339907,
      "loss": 0.0734,
      "step": 607
    },
    {
      "epoch": 0.15045780747339768,
      "grad_norm": 0.04343772307038307,
      "learning_rate": 0.000493259555417068,
      "loss": 0.0649,
      "step": 608
    },
    {
      "epoch": 0.15070527097253156,
      "grad_norm": 0.03722366318106651,
      "learning_rate": 0.0004932370955527035,
      "loss": 0.0658,
      "step": 609
    },
    {
      "epoch": 0.15095273447166543,
      "grad_norm": 0.05333850905299187,
      "learning_rate": 0.0004932145988442994,
      "loss": 0.1286,
      "step": 610
    },
    {
      "epoch": 0.1512001979707993,
      "grad_norm": 0.05110390484333038,
      "learning_rate": 0.0004931920652952631,
      "loss": 0.0559,
      "step": 611
    },
    {
      "epoch": 0.1514476614699332,
      "grad_norm": 0.05334963649511337,
      "learning_rate": 0.0004931694949090079,
      "loss": 0.0501,
      "step": 612
    },
    {
      "epoch": 0.15169512496906706,
      "grad_norm": 0.034990523010492325,
      "learning_rate": 0.0004931468876889526,
      "loss": 0.0516,
      "step": 613
    },
    {
      "epoch": 0.15194258846820094,
      "grad_norm": 0.0188509002327919,
      "learning_rate": 0.0004931242436385217,
      "loss": 0.032,
      "step": 614
    },
    {
      "epoch": 0.15219005196733482,
      "grad_norm": 0.053263332694768906,
      "learning_rate": 0.0004931015627611451,
      "loss": 0.0593,
      "step": 615
    },
    {
      "epoch": 0.1524375154664687,
      "grad_norm": 0.042042817920446396,
      "learning_rate": 0.0004930788450602586,
      "loss": 0.0713,
      "step": 616
    },
    {
      "epoch": 0.15268497896560257,
      "grad_norm": 0.035906124860048294,
      "learning_rate": 0.0004930560905393029,
      "loss": 0.0623,
      "step": 617
    },
    {
      "epoch": 0.15293244246473645,
      "grad_norm": 0.049339912831783295,
      "learning_rate": 0.0004930332992017251,
      "loss": 0.0867,
      "step": 618
    },
    {
      "epoch": 0.15317990596387032,
      "grad_norm": 0.04630099609494209,
      "learning_rate": 0.0004930104710509772,
      "loss": 0.071,
      "step": 619
    },
    {
      "epoch": 0.1534273694630042,
      "grad_norm": 0.036441560834646225,
      "learning_rate": 0.0004929876060905173,
      "loss": 0.0722,
      "step": 620
    },
    {
      "epoch": 0.15367483296213807,
      "grad_norm": 0.04286733642220497,
      "learning_rate": 0.0004929647043238087,
      "loss": 0.0396,
      "step": 621
    },
    {
      "epoch": 0.15392229646127195,
      "grad_norm": 0.0529995858669281,
      "learning_rate": 0.0004929417657543204,
      "loss": 0.0693,
      "step": 622
    },
    {
      "epoch": 0.15416975996040583,
      "grad_norm": 0.07098276913166046,
      "learning_rate": 0.0004929187903855272,
      "loss": 0.0714,
      "step": 623
    },
    {
      "epoch": 0.1544172234595397,
      "grad_norm": 0.10102023929357529,
      "learning_rate": 0.000492895778220909,
      "loss": 0.0329,
      "step": 624
    },
    {
      "epoch": 0.1546646869586736,
      "grad_norm": 0.07376855611801147,
      "learning_rate": 0.0004928727292639516,
      "loss": 0.1229,
      "step": 625
    },
    {
      "epoch": 0.15491215045780748,
      "grad_norm": 0.054249655455350876,
      "learning_rate": 0.0004928496435181464,
      "loss": 0.111,
      "step": 626
    },
    {
      "epoch": 0.15515961395694136,
      "grad_norm": 0.028564589098095894,
      "learning_rate": 0.0004928265209869903,
      "loss": 0.0317,
      "step": 627
    },
    {
      "epoch": 0.15540707745607524,
      "grad_norm": 0.03638150542974472,
      "learning_rate": 0.0004928033616739858,
      "loss": 0.0429,
      "step": 628
    },
    {
      "epoch": 0.15565454095520911,
      "grad_norm": 0.029575815424323082,
      "learning_rate": 0.0004927801655826408,
      "loss": 0.0505,
      "step": 629
    },
    {
      "epoch": 0.155902004454343,
      "grad_norm": 0.03909832984209061,
      "learning_rate": 0.0004927569327164688,
      "loss": 0.0473,
      "step": 630
    },
    {
      "epoch": 0.15614946795347687,
      "grad_norm": 0.042446691542863846,
      "learning_rate": 0.0004927336630789891,
      "loss": 0.0661,
      "step": 631
    },
    {
      "epoch": 0.15639693145261074,
      "grad_norm": 0.049893029034137726,
      "learning_rate": 0.0004927103566737266,
      "loss": 0.0812,
      "step": 632
    },
    {
      "epoch": 0.15664439495174462,
      "grad_norm": 0.11981875449419022,
      "learning_rate": 0.0004926870135042114,
      "loss": 0.0563,
      "step": 633
    },
    {
      "epoch": 0.1568918584508785,
      "grad_norm": 0.03138330951333046,
      "learning_rate": 0.0004926636335739794,
      "loss": 0.0534,
      "step": 634
    },
    {
      "epoch": 0.15713932195001237,
      "grad_norm": 0.046016398817300797,
      "learning_rate": 0.0004926402168865721,
      "loss": 0.063,
      "step": 635
    },
    {
      "epoch": 0.15738678544914625,
      "grad_norm": 0.07769305258989334,
      "learning_rate": 0.0004926167634455365,
      "loss": 0.1372,
      "step": 636
    },
    {
      "epoch": 0.15763424894828013,
      "grad_norm": 0.05991712957620621,
      "learning_rate": 0.0004925932732544252,
      "loss": 0.0548,
      "step": 637
    },
    {
      "epoch": 0.157881712447414,
      "grad_norm": 0.03796636313199997,
      "learning_rate": 0.0004925697463167963,
      "loss": 0.0379,
      "step": 638
    },
    {
      "epoch": 0.15812917594654788,
      "grad_norm": 0.1154671162366867,
      "learning_rate": 0.0004925461826362135,
      "loss": 0.0698,
      "step": 639
    },
    {
      "epoch": 0.15837663944568176,
      "grad_norm": 0.05629456788301468,
      "learning_rate": 0.0004925225822162461,
      "loss": 0.0991,
      "step": 640
    },
    {
      "epoch": 0.15862410294481563,
      "grad_norm": 0.03482918441295624,
      "learning_rate": 0.0004924989450604691,
      "loss": 0.0175,
      "step": 641
    },
    {
      "epoch": 0.1588715664439495,
      "grad_norm": 0.04615354165434837,
      "learning_rate": 0.0004924752711724626,
      "loss": 0.0511,
      "step": 642
    },
    {
      "epoch": 0.15911902994308338,
      "grad_norm": 0.07311280816793442,
      "learning_rate": 0.0004924515605558129,
      "loss": 0.0578,
      "step": 643
    },
    {
      "epoch": 0.15936649344221726,
      "grad_norm": 0.04534050449728966,
      "learning_rate": 0.0004924278132141112,
      "loss": 0.0651,
      "step": 644
    },
    {
      "epoch": 0.15961395694135114,
      "grad_norm": 0.03573824092745781,
      "learning_rate": 0.0004924040291509549,
      "loss": 0.0381,
      "step": 645
    },
    {
      "epoch": 0.15986142044048504,
      "grad_norm": 0.06346230208873749,
      "learning_rate": 0.0004923802083699466,
      "loss": 0.0796,
      "step": 646
    },
    {
      "epoch": 0.16010888393961892,
      "grad_norm": 0.04521852359175682,
      "learning_rate": 0.0004923563508746943,
      "loss": 0.0674,
      "step": 647
    },
    {
      "epoch": 0.1603563474387528,
      "grad_norm": 0.036573998630046844,
      "learning_rate": 0.000492332456668812,
      "loss": 0.0337,
      "step": 648
    },
    {
      "epoch": 0.16060381093788667,
      "grad_norm": 0.04839996248483658,
      "learning_rate": 0.000492308525755919,
      "loss": 0.1208,
      "step": 649
    },
    {
      "epoch": 0.16085127443702055,
      "grad_norm": 0.03813093155622482,
      "learning_rate": 0.0004922845581396403,
      "loss": 0.0851,
      "step": 650
    },
    {
      "epoch": 0.16109873793615442,
      "grad_norm": 0.02876242995262146,
      "learning_rate": 0.0004922605538236061,
      "loss": 0.0402,
      "step": 651
    },
    {
      "epoch": 0.1613462014352883,
      "grad_norm": 0.04943777620792389,
      "learning_rate": 0.0004922365128114528,
      "loss": 0.0639,
      "step": 652
    },
    {
      "epoch": 0.16159366493442218,
      "grad_norm": 0.04826357215642929,
      "learning_rate": 0.0004922124351068216,
      "loss": 0.0766,
      "step": 653
    },
    {
      "epoch": 0.16184112843355605,
      "grad_norm": 0.04238786920905113,
      "learning_rate": 0.0004921883207133598,
      "loss": 0.0915,
      "step": 654
    },
    {
      "epoch": 0.16208859193268993,
      "grad_norm": 0.055926237255334854,
      "learning_rate": 0.0004921641696347202,
      "loss": 0.0706,
      "step": 655
    },
    {
      "epoch": 0.1623360554318238,
      "grad_norm": 0.10658160597085953,
      "learning_rate": 0.0004921399818745609,
      "loss": 0.0554,
      "step": 656
    },
    {
      "epoch": 0.16258351893095768,
      "grad_norm": 0.04269910976290703,
      "learning_rate": 0.0004921157574365458,
      "loss": 0.0713,
      "step": 657
    },
    {
      "epoch": 0.16283098243009156,
      "grad_norm": 0.0893915444612503,
      "learning_rate": 0.0004920914963243442,
      "loss": 0.1451,
      "step": 658
    },
    {
      "epoch": 0.16307844592922544,
      "grad_norm": 0.05680474266409874,
      "learning_rate": 0.0004920671985416312,
      "loss": 0.0784,
      "step": 659
    },
    {
      "epoch": 0.1633259094283593,
      "grad_norm": 0.043162740767002106,
      "learning_rate": 0.000492042864092087,
      "loss": 0.0583,
      "step": 660
    },
    {
      "epoch": 0.1635733729274932,
      "grad_norm": 0.033613190054893494,
      "learning_rate": 0.0004920184929793979,
      "loss": 0.0504,
      "step": 661
    },
    {
      "epoch": 0.16382083642662706,
      "grad_norm": 0.07868272066116333,
      "learning_rate": 0.0004919940852072553,
      "loss": 0.1146,
      "step": 662
    },
    {
      "epoch": 0.16406829992576094,
      "grad_norm": 0.04163629561662674,
      "learning_rate": 0.0004919696407793563,
      "loss": 0.0463,
      "step": 663
    },
    {
      "epoch": 0.16431576342489482,
      "grad_norm": 0.04523756355047226,
      "learning_rate": 0.0004919451596994038,
      "loss": 0.0364,
      "step": 664
    },
    {
      "epoch": 0.1645632269240287,
      "grad_norm": 0.03929539769887924,
      "learning_rate": 0.0004919206419711059,
      "loss": 0.06,
      "step": 665
    },
    {
      "epoch": 0.16481069042316257,
      "grad_norm": 0.0477372370660305,
      "learning_rate": 0.0004918960875981763,
      "loss": 0.0595,
      "step": 666
    },
    {
      "epoch": 0.16505815392229647,
      "grad_norm": 0.030478499829769135,
      "learning_rate": 0.0004918714965843347,
      "loss": 0.0226,
      "step": 667
    },
    {
      "epoch": 0.16530561742143035,
      "grad_norm": 0.07972521334886551,
      "learning_rate": 0.0004918468689333056,
      "loss": 0.1012,
      "step": 668
    },
    {
      "epoch": 0.16555308092056423,
      "grad_norm": 0.043351877480745316,
      "learning_rate": 0.0004918222046488196,
      "loss": 0.0779,
      "step": 669
    },
    {
      "epoch": 0.1658005444196981,
      "grad_norm": 0.03789237514138222,
      "learning_rate": 0.0004917975037346127,
      "loss": 0.0416,
      "step": 670
    },
    {
      "epoch": 0.16604800791883198,
      "grad_norm": 0.03618689626455307,
      "learning_rate": 0.0004917727661944265,
      "loss": 0.0661,
      "step": 671
    },
    {
      "epoch": 0.16629547141796586,
      "grad_norm": 0.03590978682041168,
      "learning_rate": 0.000491747992032008,
      "loss": 0.0359,
      "step": 672
    },
    {
      "epoch": 0.16654293491709973,
      "grad_norm": 0.038000334054231644,
      "learning_rate": 0.0004917231812511098,
      "loss": 0.0612,
      "step": 673
    },
    {
      "epoch": 0.1667903984162336,
      "grad_norm": 0.03852764144539833,
      "learning_rate": 0.0004916983338554902,
      "loss": 0.0624,
      "step": 674
    },
    {
      "epoch": 0.16703786191536749,
      "grad_norm": 0.0560310073196888,
      "learning_rate": 0.0004916734498489129,
      "loss": 0.076,
      "step": 675
    },
    {
      "epoch": 0.16728532541450136,
      "grad_norm": 0.04036771506071091,
      "learning_rate": 0.000491648529235147,
      "loss": 0.0603,
      "step": 676
    },
    {
      "epoch": 0.16753278891363524,
      "grad_norm": 0.05447333678603172,
      "learning_rate": 0.0004916235720179674,
      "loss": 0.0538,
      "step": 677
    },
    {
      "epoch": 0.16778025241276912,
      "grad_norm": 0.05968823656439781,
      "learning_rate": 0.0004915985782011548,
      "loss": 0.0498,
      "step": 678
    },
    {
      "epoch": 0.168027715911903,
      "grad_norm": 0.04559668153524399,
      "learning_rate": 0.0004915735477884947,
      "loss": 0.0674,
      "step": 679
    },
    {
      "epoch": 0.16827517941103687,
      "grad_norm": 0.03643825277686119,
      "learning_rate": 0.0004915484807837786,
      "loss": 0.0561,
      "step": 680
    },
    {
      "epoch": 0.16852264291017074,
      "grad_norm": 0.06561724841594696,
      "learning_rate": 0.0004915233771908037,
      "loss": 0.0562,
      "step": 681
    },
    {
      "epoch": 0.16877010640930462,
      "grad_norm": 0.036574624478816986,
      "learning_rate": 0.0004914982370133724,
      "loss": 0.0394,
      "step": 682
    },
    {
      "epoch": 0.1690175699084385,
      "grad_norm": 0.06178845092654228,
      "learning_rate": 0.0004914730602552927,
      "loss": 0.062,
      "step": 683
    },
    {
      "epoch": 0.16926503340757237,
      "grad_norm": 0.06558223813772202,
      "learning_rate": 0.0004914478469203783,
      "loss": 0.0893,
      "step": 684
    },
    {
      "epoch": 0.16951249690670625,
      "grad_norm": 0.07469984889030457,
      "learning_rate": 0.0004914225970124484,
      "loss": 0.1791,
      "step": 685
    },
    {
      "epoch": 0.16975996040584013,
      "grad_norm": 0.030517064034938812,
      "learning_rate": 0.0004913973105353278,
      "loss": 0.0365,
      "step": 686
    },
    {
      "epoch": 0.170007423904974,
      "grad_norm": 0.028828773647546768,
      "learning_rate": 0.0004913719874928465,
      "loss": 0.026,
      "step": 687
    },
    {
      "epoch": 0.1702548874041079,
      "grad_norm": 0.07574079185724258,
      "learning_rate": 0.0004913466278888403,
      "loss": 0.0883,
      "step": 688
    },
    {
      "epoch": 0.17050235090324178,
      "grad_norm": 0.05004595220088959,
      "learning_rate": 0.0004913212317271507,
      "loss": 0.0404,
      "step": 689
    },
    {
      "epoch": 0.17074981440237566,
      "grad_norm": 0.05250145494937897,
      "learning_rate": 0.0004912957990116243,
      "loss": 0.0754,
      "step": 690
    },
    {
      "epoch": 0.17099727790150954,
      "grad_norm": 0.03290771692991257,
      "learning_rate": 0.0004912703297461138,
      "loss": 0.0716,
      "step": 691
    },
    {
      "epoch": 0.1712447414006434,
      "grad_norm": 0.04667560011148453,
      "learning_rate": 0.000491244823934477,
      "loss": 0.0682,
      "step": 692
    },
    {
      "epoch": 0.1714922048997773,
      "grad_norm": 0.03294401615858078,
      "learning_rate": 0.0004912192815805772,
      "loss": 0.042,
      "step": 693
    },
    {
      "epoch": 0.17173966839891117,
      "grad_norm": 0.032068975269794464,
      "learning_rate": 0.0004911937026882835,
      "loss": 0.0344,
      "step": 694
    },
    {
      "epoch": 0.17198713189804504,
      "grad_norm": 0.05403727665543556,
      "learning_rate": 0.0004911680872614706,
      "loss": 0.0554,
      "step": 695
    },
    {
      "epoch": 0.17223459539717892,
      "grad_norm": 0.049027130007743835,
      "learning_rate": 0.0004911424353040183,
      "loss": 0.0465,
      "step": 696
    },
    {
      "epoch": 0.1724820588963128,
      "grad_norm": 0.029439054429531097,
      "learning_rate": 0.0004911167468198124,
      "loss": 0.0493,
      "step": 697
    },
    {
      "epoch": 0.17272952239544667,
      "grad_norm": 0.03484126180410385,
      "learning_rate": 0.0004910910218127438,
      "loss": 0.0358,
      "step": 698
    },
    {
      "epoch": 0.17297698589458055,
      "grad_norm": 0.04948902875185013,
      "learning_rate": 0.0004910652602867095,
      "loss": 0.0598,
      "step": 699
    },
    {
      "epoch": 0.17322444939371442,
      "grad_norm": 0.03390145301818848,
      "learning_rate": 0.0004910394622456113,
      "loss": 0.0549,
      "step": 700
    },
    {
      "epoch": 0.1734719128928483,
      "grad_norm": 0.04338081181049347,
      "learning_rate": 0.0004910136276933574,
      "loss": 0.0658,
      "step": 701
    },
    {
      "epoch": 0.17371937639198218,
      "grad_norm": 0.05544893071055412,
      "learning_rate": 0.0004909877566338605,
      "loss": 0.0964,
      "step": 702
    },
    {
      "epoch": 0.17396683989111605,
      "grad_norm": 0.03630978241562843,
      "learning_rate": 0.0004909618490710398,
      "loss": 0.0292,
      "step": 703
    },
    {
      "epoch": 0.17421430339024993,
      "grad_norm": 0.061907634139060974,
      "learning_rate": 0.0004909359050088196,
      "loss": 0.1177,
      "step": 704
    },
    {
      "epoch": 0.1744617668893838,
      "grad_norm": 0.026417970657348633,
      "learning_rate": 0.0004909099244511295,
      "loss": 0.0277,
      "step": 705
    },
    {
      "epoch": 0.17470923038851768,
      "grad_norm": 0.06976223737001419,
      "learning_rate": 0.000490883907401905,
      "loss": 0.0714,
      "step": 706
    },
    {
      "epoch": 0.17495669388765156,
      "grad_norm": 0.033377110958099365,
      "learning_rate": 0.0004908578538650871,
      "loss": 0.0252,
      "step": 707
    },
    {
      "epoch": 0.17520415738678544,
      "grad_norm": 0.029846878722310066,
      "learning_rate": 0.0004908317638446222,
      "loss": 0.0441,
      "step": 708
    },
    {
      "epoch": 0.17545162088591934,
      "grad_norm": 0.05565892159938812,
      "learning_rate": 0.000490805637344462,
      "loss": 0.0767,
      "step": 709
    },
    {
      "epoch": 0.17569908438505322,
      "grad_norm": 0.07343322038650513,
      "learning_rate": 0.0004907794743685643,
      "loss": 0.0903,
      "step": 710
    },
    {
      "epoch": 0.1759465478841871,
      "grad_norm": 0.044735901057720184,
      "learning_rate": 0.000490753274920892,
      "loss": 0.0477,
      "step": 711
    },
    {
      "epoch": 0.17619401138332097,
      "grad_norm": 0.059141211211681366,
      "learning_rate": 0.0004907270390054135,
      "loss": 0.071,
      "step": 712
    },
    {
      "epoch": 0.17644147488245485,
      "grad_norm": 0.027261149138212204,
      "learning_rate": 0.0004907007666261029,
      "loss": 0.0398,
      "step": 713
    },
    {
      "epoch": 0.17668893838158872,
      "grad_norm": 0.03640959784388542,
      "learning_rate": 0.0004906744577869401,
      "loss": 0.0322,
      "step": 714
    },
    {
      "epoch": 0.1769364018807226,
      "grad_norm": 0.023817135021090508,
      "learning_rate": 0.0004906481124919097,
      "loss": 0.0305,
      "step": 715
    },
    {
      "epoch": 0.17718386537985648,
      "grad_norm": 0.04760529100894928,
      "learning_rate": 0.0004906217307450026,
      "loss": 0.046,
      "step": 716
    },
    {
      "epoch": 0.17743132887899035,
      "grad_norm": 0.046869296580553055,
      "learning_rate": 0.0004905953125502149,
      "loss": 0.0982,
      "step": 717
    },
    {
      "epoch": 0.17767879237812423,
      "grad_norm": 0.03179652616381645,
      "learning_rate": 0.0004905688579115484,
      "loss": 0.0525,
      "step": 718
    },
    {
      "epoch": 0.1779262558772581,
      "grad_norm": 0.06530836224555969,
      "learning_rate": 0.00049054236683301,
      "loss": 0.0861,
      "step": 719
    },
    {
      "epoch": 0.17817371937639198,
      "grad_norm": 0.04670589044690132,
      "learning_rate": 0.0004905158393186126,
      "loss": 0.0476,
      "step": 720
    },
    {
      "epoch": 0.17842118287552586,
      "grad_norm": 0.03869559243321419,
      "learning_rate": 0.0004904892753723744,
      "loss": 0.0628,
      "step": 721
    },
    {
      "epoch": 0.17866864637465973,
      "grad_norm": 0.08954145759344101,
      "learning_rate": 0.0004904626749983191,
      "loss": 0.0719,
      "step": 722
    },
    {
      "epoch": 0.1789161098737936,
      "grad_norm": 0.04477320984005928,
      "learning_rate": 0.0004904360382004759,
      "loss": 0.0554,
      "step": 723
    },
    {
      "epoch": 0.1791635733729275,
      "grad_norm": 0.03847390413284302,
      "learning_rate": 0.0004904093649828797,
      "loss": 0.0401,
      "step": 724
    },
    {
      "epoch": 0.17941103687206136,
      "grad_norm": 0.1482953280210495,
      "learning_rate": 0.0004903826553495708,
      "loss": 0.0597,
      "step": 725
    },
    {
      "epoch": 0.17965850037119524,
      "grad_norm": 0.03392883762717247,
      "learning_rate": 0.0004903559093045949,
      "loss": 0.0361,
      "step": 726
    },
    {
      "epoch": 0.17990596387032912,
      "grad_norm": 0.05446712300181389,
      "learning_rate": 0.0004903291268520033,
      "loss": 0.0876,
      "step": 727
    },
    {
      "epoch": 0.180153427369463,
      "grad_norm": 0.06573938578367233,
      "learning_rate": 0.000490302307995853,
      "loss": 0.0887,
      "step": 728
    },
    {
      "epoch": 0.18040089086859687,
      "grad_norm": 0.05982501059770584,
      "learning_rate": 0.0004902754527402061,
      "loss": 0.0606,
      "step": 729
    },
    {
      "epoch": 0.18064835436773077,
      "grad_norm": 0.037180688232183456,
      "learning_rate": 0.0004902485610891309,
      "loss": 0.0313,
      "step": 730
    },
    {
      "epoch": 0.18089581786686465,
      "grad_norm": 0.05982916057109833,
      "learning_rate": 0.0004902216330467003,
      "loss": 0.0704,
      "step": 731
    },
    {
      "epoch": 0.18114328136599853,
      "grad_norm": 0.03064616210758686,
      "learning_rate": 0.0004901946686169935,
      "loss": 0.0313,
      "step": 732
    },
    {
      "epoch": 0.1813907448651324,
      "grad_norm": 0.028419893234968185,
      "learning_rate": 0.0004901676678040948,
      "loss": 0.0349,
      "step": 733
    },
    {
      "epoch": 0.18163820836426628,
      "grad_norm": 0.04961004480719566,
      "learning_rate": 0.0004901406306120941,
      "loss": 0.0655,
      "step": 734
    },
    {
      "epoch": 0.18188567186340016,
      "grad_norm": 0.036423563957214355,
      "learning_rate": 0.0004901135570450868,
      "loss": 0.0463,
      "step": 735
    },
    {
      "epoch": 0.18213313536253403,
      "grad_norm": 0.052473507821559906,
      "learning_rate": 0.000490086447107174,
      "loss": 0.0405,
      "step": 736
    },
    {
      "epoch": 0.1823805988616679,
      "grad_norm": 0.043923087418079376,
      "learning_rate": 0.000490059300802462,
      "loss": 0.0599,
      "step": 737
    },
    {
      "epoch": 0.18262806236080179,
      "grad_norm": 0.02746836468577385,
      "learning_rate": 0.0004900321181350628,
      "loss": 0.0489,
      "step": 738
    },
    {
      "epoch": 0.18287552585993566,
      "grad_norm": 0.06350934505462646,
      "learning_rate": 0.0004900048991090938,
      "loss": 0.1111,
      "step": 739
    },
    {
      "epoch": 0.18312298935906954,
      "grad_norm": 0.038658712059259415,
      "learning_rate": 0.0004899776437286781,
      "loss": 0.0553,
      "step": 740
    },
    {
      "epoch": 0.18337045285820341,
      "grad_norm": 0.07446867227554321,
      "learning_rate": 0.0004899503519979439,
      "loss": 0.0791,
      "step": 741
    },
    {
      "epoch": 0.1836179163573373,
      "grad_norm": 0.029641807079315186,
      "learning_rate": 0.0004899230239210255,
      "loss": 0.0299,
      "step": 742
    },
    {
      "epoch": 0.18386537985647117,
      "grad_norm": 0.024993494153022766,
      "learning_rate": 0.0004898956595020622,
      "loss": 0.019,
      "step": 743
    },
    {
      "epoch": 0.18411284335560504,
      "grad_norm": 0.0563717857003212,
      "learning_rate": 0.0004898682587451991,
      "loss": 0.0462,
      "step": 744
    },
    {
      "epoch": 0.18436030685473892,
      "grad_norm": 0.05686722695827484,
      "learning_rate": 0.0004898408216545866,
      "loss": 0.0487,
      "step": 745
    },
    {
      "epoch": 0.1846077703538728,
      "grad_norm": 0.02584833838045597,
      "learning_rate": 0.0004898133482343808,
      "loss": 0.0469,
      "step": 746
    },
    {
      "epoch": 0.18485523385300667,
      "grad_norm": 0.042746685445308685,
      "learning_rate": 0.000489785838488743,
      "loss": 0.0768,
      "step": 747
    },
    {
      "epoch": 0.18510269735214055,
      "grad_norm": 0.038808852434158325,
      "learning_rate": 0.0004897582924218405,
      "loss": 0.0662,
      "step": 748
    },
    {
      "epoch": 0.18535016085127443,
      "grad_norm": 0.03635166212916374,
      "learning_rate": 0.0004897307100378456,
      "loss": 0.0683,
      "step": 749
    },
    {
      "epoch": 0.1855976243504083,
      "grad_norm": 0.03783692046999931,
      "learning_rate": 0.0004897030913409364,
      "loss": 0.041,
      "step": 750
    },
    {
      "epoch": 0.18584508784954218,
      "grad_norm": 0.036276109516620636,
      "learning_rate": 0.0004896754363352963,
      "loss": 0.0436,
      "step": 751
    },
    {
      "epoch": 0.18609255134867608,
      "grad_norm": 0.031149934977293015,
      "learning_rate": 0.0004896477450251145,
      "loss": 0.0483,
      "step": 752
    },
    {
      "epoch": 0.18634001484780996,
      "grad_norm": 0.055375922471284866,
      "learning_rate": 0.0004896200174145853,
      "loss": 0.0646,
      "step": 753
    },
    {
      "epoch": 0.18658747834694384,
      "grad_norm": 0.07575041800737381,
      "learning_rate": 0.0004895922535079087,
      "loss": 0.0822,
      "step": 754
    },
    {
      "epoch": 0.1868349418460777,
      "grad_norm": 0.028040412813425064,
      "learning_rate": 0.0004895644533092904,
      "loss": 0.0233,
      "step": 755
    },
    {
      "epoch": 0.1870824053452116,
      "grad_norm": 0.048263538628816605,
      "learning_rate": 0.0004895366168229414,
      "loss": 0.0776,
      "step": 756
    },
    {
      "epoch": 0.18732986884434547,
      "grad_norm": 0.04216770827770233,
      "learning_rate": 0.000489508744053078,
      "loss": 0.0643,
      "step": 757
    },
    {
      "epoch": 0.18757733234347934,
      "grad_norm": 0.07050258666276932,
      "learning_rate": 0.0004894808350039222,
      "loss": 0.0467,
      "step": 758
    },
    {
      "epoch": 0.18782479584261322,
      "grad_norm": 0.06647170335054398,
      "learning_rate": 0.0004894528896797017,
      "loss": 0.1176,
      "step": 759
    },
    {
      "epoch": 0.1880722593417471,
      "grad_norm": 0.04269886761903763,
      "learning_rate": 0.0004894249080846493,
      "loss": 0.0635,
      "step": 760
    },
    {
      "epoch": 0.18831972284088097,
      "grad_norm": 0.03988367319107056,
      "learning_rate": 0.0004893968902230035,
      "loss": 0.0491,
      "step": 761
    },
    {
      "epoch": 0.18856718634001485,
      "grad_norm": 0.042189475148916245,
      "learning_rate": 0.0004893688360990084,
      "loss": 0.0699,
      "step": 762
    },
    {
      "epoch": 0.18881464983914872,
      "grad_norm": 0.05725589022040367,
      "learning_rate": 0.0004893407457169134,
      "loss": 0.0585,
      "step": 763
    },
    {
      "epoch": 0.1890621133382826,
      "grad_norm": 0.03242019563913345,
      "learning_rate": 0.0004893126190809733,
      "loss": 0.0527,
      "step": 764
    },
    {
      "epoch": 0.18930957683741648,
      "grad_norm": 0.046426136046648026,
      "learning_rate": 0.0004892844561954487,
      "loss": 0.0851,
      "step": 765
    },
    {
      "epoch": 0.18955704033655035,
      "grad_norm": 0.07442793995141983,
      "learning_rate": 0.0004892562570646056,
      "loss": 0.1004,
      "step": 766
    },
    {
      "epoch": 0.18980450383568423,
      "grad_norm": 0.0392957367002964,
      "learning_rate": 0.0004892280216927153,
      "loss": 0.0648,
      "step": 767
    },
    {
      "epoch": 0.1900519673348181,
      "grad_norm": 0.03391357511281967,
      "learning_rate": 0.0004891997500840547,
      "loss": 0.0408,
      "step": 768
    },
    {
      "epoch": 0.19029943083395198,
      "grad_norm": 0.02882922813296318,
      "learning_rate": 0.0004891714422429063,
      "loss": 0.0281,
      "step": 769
    },
    {
      "epoch": 0.19054689433308586,
      "grad_norm": 0.05375174432992935,
      "learning_rate": 0.0004891430981735581,
      "loss": 0.0461,
      "step": 770
    },
    {
      "epoch": 0.19079435783221974,
      "grad_norm": 0.028254732489585876,
      "learning_rate": 0.0004891147178803032,
      "loss": 0.0441,
      "step": 771
    },
    {
      "epoch": 0.1910418213313536,
      "grad_norm": 0.03862236067652702,
      "learning_rate": 0.0004890863013674407,
      "loss": 0.0559,
      "step": 772
    },
    {
      "epoch": 0.19128928483048752,
      "grad_norm": 0.07240463048219681,
      "learning_rate": 0.0004890578486392748,
      "loss": 0.0692,
      "step": 773
    },
    {
      "epoch": 0.1915367483296214,
      "grad_norm": 0.07964370399713516,
      "learning_rate": 0.0004890293597001154,
      "loss": 0.0911,
      "step": 774
    },
    {
      "epoch": 0.19178421182875527,
      "grad_norm": 0.04593212157487869,
      "learning_rate": 0.0004890008345542779,
      "loss": 0.0475,
      "step": 775
    },
    {
      "epoch": 0.19203167532788915,
      "grad_norm": 0.042124029248952866,
      "learning_rate": 0.000488972273206083,
      "loss": 0.0365,
      "step": 776
    },
    {
      "epoch": 0.19227913882702302,
      "grad_norm": 0.02754293754696846,
      "learning_rate": 0.0004889436756598572,
      "loss": 0.0285,
      "step": 777
    },
    {
      "epoch": 0.1925266023261569,
      "grad_norm": 0.06075615435838699,
      "learning_rate": 0.000488915041919932,
      "loss": 0.0442,
      "step": 778
    },
    {
      "epoch": 0.19277406582529077,
      "grad_norm": 0.06676517426967621,
      "learning_rate": 0.0004888863719906448,
      "loss": 0.0858,
      "step": 779
    },
    {
      "epoch": 0.19302152932442465,
      "grad_norm": 0.026665594428777695,
      "learning_rate": 0.0004888576658763384,
      "loss": 0.04,
      "step": 780
    },
    {
      "epoch": 0.19326899282355853,
      "grad_norm": 0.03261461853981018,
      "learning_rate": 0.0004888289235813609,
      "loss": 0.0473,
      "step": 781
    },
    {
      "epoch": 0.1935164563226924,
      "grad_norm": 0.0797768384218216,
      "learning_rate": 0.000488800145110066,
      "loss": 0.1053,
      "step": 782
    },
    {
      "epoch": 0.19376391982182628,
      "grad_norm": 0.031074775382876396,
      "learning_rate": 0.000488771330466813,
      "loss": 0.0408,
      "step": 783
    },
    {
      "epoch": 0.19401138332096016,
      "grad_norm": 0.032291561365127563,
      "learning_rate": 0.0004887424796559665,
      "loss": 0.032,
      "step": 784
    },
    {
      "epoch": 0.19425884682009403,
      "grad_norm": 0.031065577641129494,
      "learning_rate": 0.0004887135926818966,
      "loss": 0.0257,
      "step": 785
    },
    {
      "epoch": 0.1945063103192279,
      "grad_norm": 0.04990328103303909,
      "learning_rate": 0.0004886846695489791,
      "loss": 0.0553,
      "step": 786
    },
    {
      "epoch": 0.1947537738183618,
      "grad_norm": 0.03069418855011463,
      "learning_rate": 0.0004886557102615948,
      "loss": 0.0331,
      "step": 787
    },
    {
      "epoch": 0.19500123731749566,
      "grad_norm": 0.04027610644698143,
      "learning_rate": 0.0004886267148241305,
      "loss": 0.0822,
      "step": 788
    },
    {
      "epoch": 0.19524870081662954,
      "grad_norm": 0.05884344130754471,
      "learning_rate": 0.0004885976832409782,
      "loss": 0.0547,
      "step": 789
    },
    {
      "epoch": 0.19549616431576342,
      "grad_norm": 0.03542443737387657,
      "learning_rate": 0.0004885686155165354,
      "loss": 0.0458,
      "step": 790
    },
    {
      "epoch": 0.1957436278148973,
      "grad_norm": 0.03098398819565773,
      "learning_rate": 0.000488539511655205,
      "loss": 0.0458,
      "step": 791
    },
    {
      "epoch": 0.19599109131403117,
      "grad_norm": 0.031894005835056305,
      "learning_rate": 0.0004885103716613957,
      "loss": 0.0605,
      "step": 792
    },
    {
      "epoch": 0.19623855481316504,
      "grad_norm": 0.0386369563639164,
      "learning_rate": 0.0004884811955395213,
      "loss": 0.0628,
      "step": 793
    },
    {
      "epoch": 0.19648601831229895,
      "grad_norm": 0.03664960712194443,
      "learning_rate": 0.0004884519832940012,
      "loss": 0.0828,
      "step": 794
    },
    {
      "epoch": 0.19673348181143283,
      "grad_norm": 0.036357056349515915,
      "learning_rate": 0.0004884227349292603,
      "loss": 0.0394,
      "step": 795
    },
    {
      "epoch": 0.1969809453105667,
      "grad_norm": 0.038590651005506516,
      "learning_rate": 0.000488393450449729,
      "loss": 0.0386,
      "step": 796
    },
    {
      "epoch": 0.19722840880970058,
      "grad_norm": 0.028450487181544304,
      "learning_rate": 0.0004883641298598432,
      "loss": 0.0388,
      "step": 797
    },
    {
      "epoch": 0.19747587230883445,
      "grad_norm": 0.050998542457818985,
      "learning_rate": 0.0004883347731640441,
      "loss": 0.0517,
      "step": 798
    },
    {
      "epoch": 0.19772333580796833,
      "grad_norm": 0.06220260635018349,
      "learning_rate": 0.0004883053803667783,
      "loss": 0.0643,
      "step": 799
    },
    {
      "epoch": 0.1979707993071022,
      "grad_norm": 0.02427256666123867,
      "learning_rate": 0.0004882759514724984,
      "loss": 0.0394,
      "step": 800
    },
    {
      "epoch": 0.1979707993071022,
      "eval_loss": 0.3026707172393799,
      "eval_runtime": 202.6706,
      "eval_samples_per_second": 4.934,
      "eval_steps_per_second": 0.311,
      "step": 800
    },
    {
      "epoch": 0.19821826280623608,
      "grad_norm": 0.042694076895713806,
      "learning_rate": 0.00048824648648566183,
      "loss": 0.0416,
      "step": 801
    },
    {
      "epoch": 0.19846572630536996,
      "grad_norm": 0.033351220190525055,
      "learning_rate": 0.0004882169854107319,
      "loss": 0.0481,
      "step": 802
    },
    {
      "epoch": 0.19871318980450384,
      "grad_norm": 0.027353830635547638,
      "learning_rate": 0.00048818744825217725,
      "loss": 0.0388,
      "step": 803
    },
    {
      "epoch": 0.1989606533036377,
      "grad_norm": 0.05424216762185097,
      "learning_rate": 0.00048815787501447187,
      "loss": 0.0498,
      "step": 804
    },
    {
      "epoch": 0.1992081168027716,
      "grad_norm": 0.03046889789402485,
      "learning_rate": 0.00048812826570209547,
      "loss": 0.0333,
      "step": 805
    },
    {
      "epoch": 0.19945558030190547,
      "grad_norm": 0.038103144615888596,
      "learning_rate": 0.000488098620319533,
      "loss": 0.0736,
      "step": 806
    },
    {
      "epoch": 0.19970304380103934,
      "grad_norm": 0.039013199508190155,
      "learning_rate": 0.00048806893887127497,
      "loss": 0.0849,
      "step": 807
    },
    {
      "epoch": 0.19995050730017322,
      "grad_norm": 0.04541062191128731,
      "learning_rate": 0.00048803922136181734,
      "loss": 0.0408,
      "step": 808
    },
    {
      "epoch": 0.2001979707993071,
      "grad_norm": 0.05058150365948677,
      "learning_rate": 0.00048800946779566155,
      "loss": 0.0545,
      "step": 809
    },
    {
      "epoch": 0.20044543429844097,
      "grad_norm": 0.03862108662724495,
      "learning_rate": 0.0004879796781773145,
      "loss": 0.0498,
      "step": 810
    },
    {
      "epoch": 0.20069289779757485,
      "grad_norm": 0.06979622691869736,
      "learning_rate": 0.00048794985251128845,
      "loss": 0.0982,
      "step": 811
    },
    {
      "epoch": 0.20094036129670872,
      "grad_norm": 0.0477583073079586,
      "learning_rate": 0.00048791999080210124,
      "loss": 0.0635,
      "step": 812
    },
    {
      "epoch": 0.2011878247958426,
      "grad_norm": 0.13357670605182648,
      "learning_rate": 0.00048789009305427625,
      "loss": 0.0724,
      "step": 813
    },
    {
      "epoch": 0.20143528829497648,
      "grad_norm": 0.03969431668519974,
      "learning_rate": 0.00048786015927234207,
      "loss": 0.0495,
      "step": 814
    },
    {
      "epoch": 0.20168275179411038,
      "grad_norm": 0.03267897292971611,
      "learning_rate": 0.0004878301894608329,
      "loss": 0.0546,
      "step": 815
    },
    {
      "epoch": 0.20193021529324426,
      "grad_norm": 0.048200879245996475,
      "learning_rate": 0.00048780018362428845,
      "loss": 0.053,
      "step": 816
    },
    {
      "epoch": 0.20217767879237813,
      "grad_norm": 0.026952287182211876,
      "learning_rate": 0.0004877701417672538,
      "loss": 0.0366,
      "step": 817
    },
    {
      "epoch": 0.202425142291512,
      "grad_norm": 0.04111889749765396,
      "learning_rate": 0.0004877400638942795,
      "loss": 0.0615,
      "step": 818
    },
    {
      "epoch": 0.2026726057906459,
      "grad_norm": 0.06344825029373169,
      "learning_rate": 0.00048770995000992144,
      "loss": 0.0577,
      "step": 819
    },
    {
      "epoch": 0.20292006928977976,
      "grad_norm": 0.024907918646931648,
      "learning_rate": 0.00048767980011874126,
      "loss": 0.0379,
      "step": 820
    },
    {
      "epoch": 0.20316753278891364,
      "grad_norm": 0.061526164412498474,
      "learning_rate": 0.00048764961422530576,
      "loss": 0.073,
      "step": 821
    },
    {
      "epoch": 0.20341499628804752,
      "grad_norm": 0.05206383764743805,
      "learning_rate": 0.0004876193923341874,
      "loss": 0.0534,
      "step": 822
    },
    {
      "epoch": 0.2036624597871814,
      "grad_norm": 0.039869274944067,
      "learning_rate": 0.0004875891344499639,
      "loss": 0.061,
      "step": 823
    },
    {
      "epoch": 0.20390992328631527,
      "grad_norm": 0.051215313374996185,
      "learning_rate": 0.0004875588405772186,
      "loss": 0.0623,
      "step": 824
    },
    {
      "epoch": 0.20415738678544915,
      "grad_norm": 0.04978729411959648,
      "learning_rate": 0.0004875285107205403,
      "loss": 0.0769,
      "step": 825
    },
    {
      "epoch": 0.20440485028458302,
      "grad_norm": 0.04256109520792961,
      "learning_rate": 0.00048749814488452294,
      "loss": 0.057,
      "step": 826
    },
    {
      "epoch": 0.2046523137837169,
      "grad_norm": 0.07555444538593292,
      "learning_rate": 0.0004874677430737664,
      "loss": 0.0799,
      "step": 827
    },
    {
      "epoch": 0.20489977728285078,
      "grad_norm": 0.07875104993581772,
      "learning_rate": 0.0004874373052928757,
      "loss": 0.0484,
      "step": 828
    },
    {
      "epoch": 0.20514724078198465,
      "grad_norm": 0.029008546844124794,
      "learning_rate": 0.00048740683154646126,
      "loss": 0.0614,
      "step": 829
    },
    {
      "epoch": 0.20539470428111853,
      "grad_norm": 0.031037846580147743,
      "learning_rate": 0.00048737632183913914,
      "loss": 0.0439,
      "step": 830
    },
    {
      "epoch": 0.2056421677802524,
      "grad_norm": 0.02929086796939373,
      "learning_rate": 0.0004873457761755307,
      "loss": 0.0422,
      "step": 831
    },
    {
      "epoch": 0.20588963127938628,
      "grad_norm": 0.06507056206464767,
      "learning_rate": 0.00048731519456026286,
      "loss": 0.1111,
      "step": 832
    },
    {
      "epoch": 0.20613709477852016,
      "grad_norm": 0.06873393803834915,
      "learning_rate": 0.00048728457699796795,
      "loss": 0.0778,
      "step": 833
    },
    {
      "epoch": 0.20638455827765403,
      "grad_norm": 0.044302623718976974,
      "learning_rate": 0.0004872539234932837,
      "loss": 0.0365,
      "step": 834
    },
    {
      "epoch": 0.2066320217767879,
      "grad_norm": 0.05937018245458603,
      "learning_rate": 0.0004872232340508533,
      "loss": 0.0421,
      "step": 835
    },
    {
      "epoch": 0.20687948527592182,
      "grad_norm": 0.03571175038814545,
      "learning_rate": 0.0004871925086753254,
      "loss": 0.056,
      "step": 836
    },
    {
      "epoch": 0.2071269487750557,
      "grad_norm": 0.04974127933382988,
      "learning_rate": 0.0004871617473713541,
      "loss": 0.0795,
      "step": 837
    },
    {
      "epoch": 0.20737441227418957,
      "grad_norm": 0.03987802192568779,
      "learning_rate": 0.0004871309501435989,
      "loss": 0.0531,
      "step": 838
    },
    {
      "epoch": 0.20762187577332344,
      "grad_norm": 0.05517328158020973,
      "learning_rate": 0.0004871001169967247,
      "loss": 0.0729,
      "step": 839
    },
    {
      "epoch": 0.20786933927245732,
      "grad_norm": 0.09606149047613144,
      "learning_rate": 0.0004870692479354022,
      "loss": 0.134,
      "step": 840
    },
    {
      "epoch": 0.2081168027715912,
      "grad_norm": 0.05455050244927406,
      "learning_rate": 0.00048703834296430693,
      "loss": 0.0576,
      "step": 841
    },
    {
      "epoch": 0.20836426627072507,
      "grad_norm": 0.09368037432432175,
      "learning_rate": 0.00048700740208812036,
      "loss": 0.1209,
      "step": 842
    },
    {
      "epoch": 0.20861172976985895,
      "grad_norm": 0.041321996599435806,
      "learning_rate": 0.0004869764253115292,
      "loss": 0.0456,
      "step": 843
    },
    {
      "epoch": 0.20885919326899283,
      "grad_norm": 0.04886585846543312,
      "learning_rate": 0.00048694541263922554,
      "loss": 0.0526,
      "step": 844
    },
    {
      "epoch": 0.2091066567681267,
      "grad_norm": 0.019954368472099304,
      "learning_rate": 0.00048691436407590703,
      "loss": 0.0253,
      "step": 845
    },
    {
      "epoch": 0.20935412026726058,
      "grad_norm": 0.04757489636540413,
      "learning_rate": 0.00048688327962627674,
      "loss": 0.0794,
      "step": 846
    },
    {
      "epoch": 0.20960158376639446,
      "grad_norm": 0.03552519902586937,
      "learning_rate": 0.0004868521592950432,
      "loss": 0.0388,
      "step": 847
    },
    {
      "epoch": 0.20984904726552833,
      "grad_norm": 0.06856515258550644,
      "learning_rate": 0.0004868210030869202,
      "loss": 0.1114,
      "step": 848
    },
    {
      "epoch": 0.2100965107646622,
      "grad_norm": 0.03944593667984009,
      "learning_rate": 0.00048678981100662715,
      "loss": 0.0469,
      "step": 849
    },
    {
      "epoch": 0.21034397426379609,
      "grad_norm": 0.03200777992606163,
      "learning_rate": 0.0004867585830588888,
      "loss": 0.0632,
      "step": 850
    },
    {
      "epoch": 0.21059143776292996,
      "grad_norm": 0.04888695478439331,
      "learning_rate": 0.0004867273192484354,
      "loss": 0.0492,
      "step": 851
    },
    {
      "epoch": 0.21083890126206384,
      "grad_norm": 0.037073731422424316,
      "learning_rate": 0.00048669601958000266,
      "loss": 0.0313,
      "step": 852
    },
    {
      "epoch": 0.21108636476119771,
      "grad_norm": 0.05133999139070511,
      "learning_rate": 0.0004866646840583315,
      "loss": 0.0807,
      "step": 853
    },
    {
      "epoch": 0.2113338282603316,
      "grad_norm": 0.04287941753864288,
      "learning_rate": 0.0004866333126881686,
      "loss": 0.0581,
      "step": 854
    },
    {
      "epoch": 0.21158129175946547,
      "grad_norm": 0.06950535625219345,
      "learning_rate": 0.00048660190547426574,
      "loss": 0.0608,
      "step": 855
    },
    {
      "epoch": 0.21182875525859934,
      "grad_norm": 0.05492282286286354,
      "learning_rate": 0.00048657046242138035,
      "loss": 0.0947,
      "step": 856
    },
    {
      "epoch": 0.21207621875773325,
      "grad_norm": 0.045209310948848724,
      "learning_rate": 0.0004865389835342753,
      "loss": 0.0683,
      "step": 857
    },
    {
      "epoch": 0.21232368225686712,
      "grad_norm": 0.031087737530469894,
      "learning_rate": 0.00048650746881771876,
      "loss": 0.033,
      "step": 858
    },
    {
      "epoch": 0.212571145756001,
      "grad_norm": 0.06807165592908859,
      "learning_rate": 0.0004864759182764843,
      "loss": 0.0655,
      "step": 859
    },
    {
      "epoch": 0.21281860925513488,
      "grad_norm": 0.05358448252081871,
      "learning_rate": 0.00048644433191535115,
      "loss": 0.0552,
      "step": 860
    },
    {
      "epoch": 0.21306607275426875,
      "grad_norm": 0.0805617943406105,
      "learning_rate": 0.00048641270973910373,
      "loss": 0.11,
      "step": 861
    },
    {
      "epoch": 0.21331353625340263,
      "grad_norm": 0.043006811290979385,
      "learning_rate": 0.000486381051752532,
      "loss": 0.053,
      "step": 862
    },
    {
      "epoch": 0.2135609997525365,
      "grad_norm": 0.05223578214645386,
      "learning_rate": 0.0004863493579604313,
      "loss": 0.0525,
      "step": 863
    },
    {
      "epoch": 0.21380846325167038,
      "grad_norm": 0.053803425282239914,
      "learning_rate": 0.0004863176283676024,
      "loss": 0.0603,
      "step": 864
    },
    {
      "epoch": 0.21405592675080426,
      "grad_norm": 0.041924599558115005,
      "learning_rate": 0.00048628586297885145,
      "loss": 0.0493,
      "step": 865
    },
    {
      "epoch": 0.21430339024993814,
      "grad_norm": 0.09378571808338165,
      "learning_rate": 0.0004862540617989902,
      "loss": 0.1053,
      "step": 866
    },
    {
      "epoch": 0.214550853749072,
      "grad_norm": 0.06579400599002838,
      "learning_rate": 0.0004862222248328357,
      "loss": 0.0311,
      "step": 867
    },
    {
      "epoch": 0.2147983172482059,
      "grad_norm": 0.0323384553194046,
      "learning_rate": 0.0004861903520852102,
      "loss": 0.0346,
      "step": 868
    },
    {
      "epoch": 0.21504578074733977,
      "grad_norm": 0.03776274248957634,
      "learning_rate": 0.00048615844356094187,
      "loss": 0.0398,
      "step": 869
    },
    {
      "epoch": 0.21529324424647364,
      "grad_norm": 0.043966639786958694,
      "learning_rate": 0.00048612649926486375,
      "loss": 0.0514,
      "step": 870
    },
    {
      "epoch": 0.21554070774560752,
      "grad_norm": 0.04308279976248741,
      "learning_rate": 0.00048609451920181475,
      "loss": 0.0589,
      "step": 871
    },
    {
      "epoch": 0.2157881712447414,
      "grad_norm": 0.024920256808400154,
      "learning_rate": 0.0004860625033766389,
      "loss": 0.029,
      "step": 872
    },
    {
      "epoch": 0.21603563474387527,
      "grad_norm": 0.037306539714336395,
      "learning_rate": 0.0004860304517941858,
      "loss": 0.0546,
      "step": 873
    },
    {
      "epoch": 0.21628309824300915,
      "grad_norm": 0.05510052293539047,
      "learning_rate": 0.0004859983644593105,
      "loss": 0.0579,
      "step": 874
    },
    {
      "epoch": 0.21653056174214302,
      "grad_norm": 0.02964887022972107,
      "learning_rate": 0.0004859662413768733,
      "loss": 0.0372,
      "step": 875
    },
    {
      "epoch": 0.2167780252412769,
      "grad_norm": 0.055679358541965485,
      "learning_rate": 0.00048593408255174,
      "loss": 0.081,
      "step": 876
    },
    {
      "epoch": 0.21702548874041078,
      "grad_norm": 0.02217073179781437,
      "learning_rate": 0.0004859018879887818,
      "loss": 0.026,
      "step": 877
    },
    {
      "epoch": 0.21727295223954465,
      "grad_norm": 0.03394835442304611,
      "learning_rate": 0.0004858696576928754,
      "loss": 0.0496,
      "step": 878
    },
    {
      "epoch": 0.21752041573867856,
      "grad_norm": 0.0869629830121994,
      "learning_rate": 0.00048583739166890285,
      "loss": 0.1013,
      "step": 879
    },
    {
      "epoch": 0.21776787923781243,
      "grad_norm": 0.0281862560659647,
      "learning_rate": 0.00048580508992175154,
      "loss": 0.0368,
      "step": 880
    },
    {
      "epoch": 0.2180153427369463,
      "grad_norm": 0.02973346784710884,
      "learning_rate": 0.0004857727524563144,
      "loss": 0.0307,
      "step": 881
    },
    {
      "epoch": 0.2182628062360802,
      "grad_norm": 0.033476151525974274,
      "learning_rate": 0.00048574037927748964,
      "loss": 0.0939,
      "step": 882
    },
    {
      "epoch": 0.21851026973521406,
      "grad_norm": 0.027016237378120422,
      "learning_rate": 0.000485707970390181,
      "loss": 0.0394,
      "step": 883
    },
    {
      "epoch": 0.21875773323434794,
      "grad_norm": 0.024213939905166626,
      "learning_rate": 0.00048567552579929763,
      "loss": 0.0246,
      "step": 884
    },
    {
      "epoch": 0.21900519673348182,
      "grad_norm": 0.07672833651304245,
      "learning_rate": 0.0004856430455097539,
      "loss": 0.0901,
      "step": 885
    },
    {
      "epoch": 0.2192526602326157,
      "grad_norm": 0.03962656483054161,
      "learning_rate": 0.0004856105295264698,
      "loss": 0.0583,
      "step": 886
    },
    {
      "epoch": 0.21950012373174957,
      "grad_norm": 0.09480451047420502,
      "learning_rate": 0.0004855779778543706,
      "loss": 0.0479,
      "step": 887
    },
    {
      "epoch": 0.21974758723088345,
      "grad_norm": 0.03180083632469177,
      "learning_rate": 0.0004855453904983871,
      "loss": 0.0374,
      "step": 888
    },
    {
      "epoch": 0.21999505073001732,
      "grad_norm": 0.0377180352807045,
      "learning_rate": 0.0004855127674634554,
      "loss": 0.0587,
      "step": 889
    },
    {
      "epoch": 0.2202425142291512,
      "grad_norm": 0.03918875381350517,
      "learning_rate": 0.000485480108754517,
      "loss": 0.0741,
      "step": 890
    },
    {
      "epoch": 0.22048997772828507,
      "grad_norm": 0.03944462910294533,
      "learning_rate": 0.00048544741437651885,
      "loss": 0.0509,
      "step": 891
    },
    {
      "epoch": 0.22073744122741895,
      "grad_norm": 0.024290749803185463,
      "learning_rate": 0.00048541468433441334,
      "loss": 0.028,
      "step": 892
    },
    {
      "epoch": 0.22098490472655283,
      "grad_norm": 0.0539763942360878,
      "learning_rate": 0.00048538191863315816,
      "loss": 0.0864,
      "step": 893
    },
    {
      "epoch": 0.2212323682256867,
      "grad_norm": 0.06673513352870941,
      "learning_rate": 0.00048534911727771645,
      "loss": 0.0958,
      "step": 894
    },
    {
      "epoch": 0.22147983172482058,
      "grad_norm": 0.06849216669797897,
      "learning_rate": 0.0004853162802730568,
      "loss": 0.0845,
      "step": 895
    },
    {
      "epoch": 0.22172729522395446,
      "grad_norm": 0.05174758657813072,
      "learning_rate": 0.0004852834076241531,
      "loss": 0.0686,
      "step": 896
    },
    {
      "epoch": 0.22197475872308833,
      "grad_norm": 0.038645170629024506,
      "learning_rate": 0.00048525049933598473,
      "loss": 0.0366,
      "step": 897
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.05174393579363823,
      "learning_rate": 0.00048521755541353653,
      "loss": 0.0574,
      "step": 898
    },
    {
      "epoch": 0.2224696857213561,
      "grad_norm": 0.053914058953523636,
      "learning_rate": 0.00048518457586179845,
      "loss": 0.0583,
      "step": 899
    },
    {
      "epoch": 0.22271714922049,
      "grad_norm": 0.09590145945549011,
      "learning_rate": 0.0004851515606857661,
      "loss": 0.1793,
      "step": 900
    },
    {
      "epoch": 0.22296461271962387,
      "grad_norm": 0.10706201195716858,
      "learning_rate": 0.0004851185098904405,
      "loss": 0.0886,
      "step": 901
    },
    {
      "epoch": 0.22321207621875774,
      "grad_norm": 0.04503258317708969,
      "learning_rate": 0.00048508542348082783,
      "loss": 0.0308,
      "step": 902
    },
    {
      "epoch": 0.22345953971789162,
      "grad_norm": 0.03190971538424492,
      "learning_rate": 0.0004850523014619399,
      "loss": 0.0257,
      "step": 903
    },
    {
      "epoch": 0.2237070032170255,
      "grad_norm": 0.11051756888628006,
      "learning_rate": 0.0004850191438387938,
      "loss": 0.1914,
      "step": 904
    },
    {
      "epoch": 0.22395446671615937,
      "grad_norm": 0.07182160764932632,
      "learning_rate": 0.0004849859506164121,
      "loss": 0.067,
      "step": 905
    },
    {
      "epoch": 0.22420193021529325,
      "grad_norm": 0.07267430424690247,
      "learning_rate": 0.00048495272179982263,
      "loss": 0.1032,
      "step": 906
    },
    {
      "epoch": 0.22444939371442713,
      "grad_norm": 0.02875925414264202,
      "learning_rate": 0.0004849194573940587,
      "loss": 0.0299,
      "step": 907
    },
    {
      "epoch": 0.224696857213561,
      "grad_norm": 0.0289979949593544,
      "learning_rate": 0.0004848861574041591,
      "loss": 0.0452,
      "step": 908
    },
    {
      "epoch": 0.22494432071269488,
      "grad_norm": 0.04033171758055687,
      "learning_rate": 0.00048485282183516776,
      "loss": 0.0529,
      "step": 909
    },
    {
      "epoch": 0.22519178421182875,
      "grad_norm": 0.03356363624334335,
      "learning_rate": 0.00048481945069213416,
      "loss": 0.0501,
      "step": 910
    },
    {
      "epoch": 0.22543924771096263,
      "grad_norm": 0.05163542926311493,
      "learning_rate": 0.00048478604398011324,
      "loss": 0.039,
      "step": 911
    },
    {
      "epoch": 0.2256867112100965,
      "grad_norm": 0.038329172879457474,
      "learning_rate": 0.00048475260170416514,
      "loss": 0.0418,
      "step": 912
    },
    {
      "epoch": 0.22593417470923038,
      "grad_norm": 0.04463730379939079,
      "learning_rate": 0.0004847191238693556,
      "loss": 0.0408,
      "step": 913
    },
    {
      "epoch": 0.22618163820836426,
      "grad_norm": 0.037621285766363144,
      "learning_rate": 0.0004846856104807555,
      "loss": 0.0343,
      "step": 914
    },
    {
      "epoch": 0.22642910170749814,
      "grad_norm": 0.02167593687772751,
      "learning_rate": 0.0004846520615434413,
      "loss": 0.0233,
      "step": 915
    },
    {
      "epoch": 0.226676565206632,
      "grad_norm": 0.04394596815109253,
      "learning_rate": 0.0004846184770624949,
      "loss": 0.0733,
      "step": 916
    },
    {
      "epoch": 0.2269240287057659,
      "grad_norm": 0.06688150018453598,
      "learning_rate": 0.00048458485704300333,
      "loss": 0.1022,
      "step": 917
    },
    {
      "epoch": 0.22717149220489977,
      "grad_norm": 0.03241682052612305,
      "learning_rate": 0.0004845512014900592,
      "loss": 0.0522,
      "step": 918
    },
    {
      "epoch": 0.22741895570403364,
      "grad_norm": 0.05056067928671837,
      "learning_rate": 0.0004845175104087603,
      "loss": 0.0428,
      "step": 919
    },
    {
      "epoch": 0.22766641920316752,
      "grad_norm": 0.033691152930259705,
      "learning_rate": 0.0004844837838042101,
      "loss": 0.0287,
      "step": 920
    },
    {
      "epoch": 0.22791388270230142,
      "grad_norm": 0.036739666014909744,
      "learning_rate": 0.0004844500216815173,
      "loss": 0.0367,
      "step": 921
    },
    {
      "epoch": 0.2281613462014353,
      "grad_norm": 0.031454768031835556,
      "learning_rate": 0.000484416224045796,
      "loss": 0.045,
      "step": 922
    },
    {
      "epoch": 0.22840880970056918,
      "grad_norm": 0.08696888387203217,
      "learning_rate": 0.00048438239090216545,
      "loss": 0.0702,
      "step": 923
    },
    {
      "epoch": 0.22865627319970305,
      "grad_norm": 0.039184194058179855,
      "learning_rate": 0.0004843485222557507,
      "loss": 0.0399,
      "step": 924
    },
    {
      "epoch": 0.22890373669883693,
      "grad_norm": 0.03646860271692276,
      "learning_rate": 0.00048431461811168186,
      "loss": 0.0427,
      "step": 925
    },
    {
      "epoch": 0.2291512001979708,
      "grad_norm": 0.035442840307950974,
      "learning_rate": 0.0004842806784750946,
      "loss": 0.0448,
      "step": 926
    },
    {
      "epoch": 0.22939866369710468,
      "grad_norm": 0.04409988224506378,
      "learning_rate": 0.00048424670335112975,
      "loss": 0.0442,
      "step": 927
    },
    {
      "epoch": 0.22964612719623856,
      "grad_norm": 0.040143124759197235,
      "learning_rate": 0.00048421269274493377,
      "loss": 0.0313,
      "step": 928
    },
    {
      "epoch": 0.22989359069537244,
      "grad_norm": 0.030859926715493202,
      "learning_rate": 0.00048417864666165834,
      "loss": 0.0384,
      "step": 929
    },
    {
      "epoch": 0.2301410541945063,
      "grad_norm": 0.060903400182724,
      "learning_rate": 0.00048414456510646053,
      "loss": 0.0901,
      "step": 930
    },
    {
      "epoch": 0.2303885176936402,
      "grad_norm": 0.08758796006441116,
      "learning_rate": 0.0004841104480845028,
      "loss": 0.1337,
      "step": 931
    },
    {
      "epoch": 0.23063598119277406,
      "grad_norm": 0.07276970893144608,
      "learning_rate": 0.000484076295600953,
      "loss": 0.1212,
      "step": 932
    },
    {
      "epoch": 0.23088344469190794,
      "grad_norm": 0.058751050382852554,
      "learning_rate": 0.0004840421076609844,
      "loss": 0.0903,
      "step": 933
    },
    {
      "epoch": 0.23113090819104182,
      "grad_norm": 0.040327101945877075,
      "learning_rate": 0.0004840078842697754,
      "loss": 0.0865,
      "step": 934
    },
    {
      "epoch": 0.2313783716901757,
      "grad_norm": 0.046607714146375656,
      "learning_rate": 0.00048397362543251015,
      "loss": 0.071,
      "step": 935
    },
    {
      "epoch": 0.23162583518930957,
      "grad_norm": 0.059186503291130066,
      "learning_rate": 0.0004839393311543778,
      "loss": 0.0342,
      "step": 936
    },
    {
      "epoch": 0.23187329868844345,
      "grad_norm": 0.05245248228311539,
      "learning_rate": 0.00048390500144057316,
      "loss": 0.0516,
      "step": 937
    },
    {
      "epoch": 0.23212076218757732,
      "grad_norm": 0.041958924382925034,
      "learning_rate": 0.00048387063629629627,
      "loss": 0.0446,
      "step": 938
    },
    {
      "epoch": 0.2323682256867112,
      "grad_norm": 0.04913463816046715,
      "learning_rate": 0.0004838362357267524,
      "loss": 0.0829,
      "step": 939
    },
    {
      "epoch": 0.23261568918584508,
      "grad_norm": 0.030419986695051193,
      "learning_rate": 0.00048380179973715244,
      "loss": 0.0432,
      "step": 940
    },
    {
      "epoch": 0.23286315268497895,
      "grad_norm": 0.03664207085967064,
      "learning_rate": 0.0004837673283327126,
      "loss": 0.0381,
      "step": 941
    },
    {
      "epoch": 0.23311061618411286,
      "grad_norm": 0.048653144389390945,
      "learning_rate": 0.0004837328215186543,
      "loss": 0.0604,
      "step": 942
    },
    {
      "epoch": 0.23335807968324673,
      "grad_norm": 0.050533439964056015,
      "learning_rate": 0.00048369827930020445,
      "loss": 0.0599,
      "step": 943
    },
    {
      "epoch": 0.2336055431823806,
      "grad_norm": 0.08108516782522202,
      "learning_rate": 0.0004836637016825953,
      "loss": 0.0905,
      "step": 944
    },
    {
      "epoch": 0.23385300668151449,
      "grad_norm": 0.05085189267992973,
      "learning_rate": 0.0004836290886710644,
      "loss": 0.0881,
      "step": 945
    },
    {
      "epoch": 0.23410047018064836,
      "grad_norm": 0.037841543555259705,
      "learning_rate": 0.00048359444027085477,
      "loss": 0.0296,
      "step": 946
    },
    {
      "epoch": 0.23434793367978224,
      "grad_norm": 0.045394353568553925,
      "learning_rate": 0.0004835597564872147,
      "loss": 0.0491,
      "step": 947
    },
    {
      "epoch": 0.23459539717891612,
      "grad_norm": 0.02834157459437847,
      "learning_rate": 0.00048352503732539793,
      "loss": 0.0402,
      "step": 948
    },
    {
      "epoch": 0.23484286067805,
      "grad_norm": 0.057176243513822556,
      "learning_rate": 0.00048349028279066344,
      "loss": 0.0599,
      "step": 949
    },
    {
      "epoch": 0.23509032417718387,
      "grad_norm": 0.04087274894118309,
      "learning_rate": 0.00048345549288827564,
      "loss": 0.0925,
      "step": 950
    },
    {
      "epoch": 0.23533778767631774,
      "grad_norm": 0.03361966833472252,
      "learning_rate": 0.00048342066762350436,
      "loss": 0.0335,
      "step": 951
    },
    {
      "epoch": 0.23558525117545162,
      "grad_norm": 0.045836471021175385,
      "learning_rate": 0.0004833858070016246,
      "loss": 0.0593,
      "step": 952
    },
    {
      "epoch": 0.2358327146745855,
      "grad_norm": 0.041871119290590286,
      "learning_rate": 0.00048335091102791685,
      "loss": 0.0497,
      "step": 953
    },
    {
      "epoch": 0.23608017817371937,
      "grad_norm": 0.03615269809961319,
      "learning_rate": 0.00048331597970766705,
      "loss": 0.0439,
      "step": 954
    },
    {
      "epoch": 0.23632764167285325,
      "grad_norm": 0.08403954654932022,
      "learning_rate": 0.0004832810130461662,
      "loss": 0.0614,
      "step": 955
    },
    {
      "epoch": 0.23657510517198713,
      "grad_norm": 0.034525372087955475,
      "learning_rate": 0.00048324601104871105,
      "loss": 0.0307,
      "step": 956
    },
    {
      "epoch": 0.236822568671121,
      "grad_norm": 0.023898107931017876,
      "learning_rate": 0.0004832109737206033,
      "loss": 0.0243,
      "step": 957
    },
    {
      "epoch": 0.23707003217025488,
      "grad_norm": 0.05807279795408249,
      "learning_rate": 0.0004831759010671503,
      "loss": 0.1223,
      "step": 958
    },
    {
      "epoch": 0.23731749566938876,
      "grad_norm": 0.035683367401361465,
      "learning_rate": 0.0004831407930936645,
      "loss": 0.0403,
      "step": 959
    },
    {
      "epoch": 0.23756495916852263,
      "grad_norm": 0.03892495110630989,
      "learning_rate": 0.00048310564980546394,
      "loss": 0.0442,
      "step": 960
    },
    {
      "epoch": 0.2378124226676565,
      "grad_norm": 0.028238672763109207,
      "learning_rate": 0.000483070471207872,
      "loss": 0.0422,
      "step": 961
    },
    {
      "epoch": 0.23805988616679039,
      "grad_norm": 0.02535196766257286,
      "learning_rate": 0.000483035257306217,
      "loss": 0.0324,
      "step": 962
    },
    {
      "epoch": 0.2383073496659243,
      "grad_norm": 0.051510922610759735,
      "learning_rate": 0.0004830000081058333,
      "loss": 0.0436,
      "step": 963
    },
    {
      "epoch": 0.23855481316505817,
      "grad_norm": 0.026938416063785553,
      "learning_rate": 0.00048296472361206003,
      "loss": 0.0408,
      "step": 964
    },
    {
      "epoch": 0.23880227666419204,
      "grad_norm": 0.03235693648457527,
      "learning_rate": 0.00048292940383024187,
      "loss": 0.0686,
      "step": 965
    },
    {
      "epoch": 0.23904974016332592,
      "grad_norm": 0.03249890357255936,
      "learning_rate": 0.00048289404876572886,
      "loss": 0.072,
      "step": 966
    },
    {
      "epoch": 0.2392972036624598,
      "grad_norm": 0.03966464847326279,
      "learning_rate": 0.0004828586584238763,
      "loss": 0.0502,
      "step": 967
    },
    {
      "epoch": 0.23954466716159367,
      "grad_norm": 0.03535815328359604,
      "learning_rate": 0.00048282323281004507,
      "loss": 0.0526,
      "step": 968
    },
    {
      "epoch": 0.23979213066072755,
      "grad_norm": 0.03864770010113716,
      "learning_rate": 0.0004827877719296011,
      "loss": 0.0708,
      "step": 969
    },
    {
      "epoch": 0.24003959415986142,
      "grad_norm": 0.03745415061712265,
      "learning_rate": 0.0004827522757879158,
      "loss": 0.0466,
      "step": 970
    },
    {
      "epoch": 0.2402870576589953,
      "grad_norm": 0.03697790950536728,
      "learning_rate": 0.0004827167443903658,
      "loss": 0.0535,
      "step": 971
    },
    {
      "epoch": 0.24053452115812918,
      "grad_norm": 0.0690586045384407,
      "learning_rate": 0.00048268117774233343,
      "loss": 0.0903,
      "step": 972
    },
    {
      "epoch": 0.24078198465726305,
      "grad_norm": 0.044332731515169144,
      "learning_rate": 0.00048264557584920586,
      "loss": 0.0535,
      "step": 973
    },
    {
      "epoch": 0.24102944815639693,
      "grad_norm": 0.04009335860610008,
      "learning_rate": 0.00048260993871637606,
      "loss": 0.0677,
      "step": 974
    },
    {
      "epoch": 0.2412769116555308,
      "grad_norm": 0.040701571851968765,
      "learning_rate": 0.00048257426634924186,
      "loss": 0.0607,
      "step": 975
    },
    {
      "epoch": 0.24152437515466468,
      "grad_norm": 0.035257644951343536,
      "learning_rate": 0.00048253855875320686,
      "loss": 0.0435,
      "step": 976
    },
    {
      "epoch": 0.24177183865379856,
      "grad_norm": 0.05450151488184929,
      "learning_rate": 0.00048250281593367983,
      "loss": 0.0402,
      "step": 977
    },
    {
      "epoch": 0.24201930215293244,
      "grad_norm": 0.03134722635149956,
      "learning_rate": 0.00048246703789607483,
      "loss": 0.0344,
      "step": 978
    },
    {
      "epoch": 0.2422667656520663,
      "grad_norm": 0.03042110987007618,
      "learning_rate": 0.00048243122464581125,
      "loss": 0.0297,
      "step": 979
    },
    {
      "epoch": 0.2425142291512002,
      "grad_norm": 0.034737370908260345,
      "learning_rate": 0.00048239537618831384,
      "loss": 0.0443,
      "step": 980
    },
    {
      "epoch": 0.24276169265033407,
      "grad_norm": 0.024141566827893257,
      "learning_rate": 0.0004823594925290129,
      "loss": 0.0418,
      "step": 981
    },
    {
      "epoch": 0.24300915614946794,
      "grad_norm": 0.06894495338201523,
      "learning_rate": 0.0004823235736733437,
      "loss": 0.0863,
      "step": 982
    },
    {
      "epoch": 0.24325661964860182,
      "grad_norm": 0.05118762329220772,
      "learning_rate": 0.00048228761962674696,
      "loss": 0.0436,
      "step": 983
    },
    {
      "epoch": 0.24350408314773572,
      "grad_norm": 0.05358495935797691,
      "learning_rate": 0.0004822516303946689,
      "loss": 0.0469,
      "step": 984
    },
    {
      "epoch": 0.2437515466468696,
      "grad_norm": 0.05061023309826851,
      "learning_rate": 0.0004822156059825609,
      "loss": 0.0516,
      "step": 985
    },
    {
      "epoch": 0.24399901014600348,
      "grad_norm": 0.038858164101839066,
      "learning_rate": 0.00048217954639587967,
      "loss": 0.046,
      "step": 986
    },
    {
      "epoch": 0.24424647364513735,
      "grad_norm": 0.047340575605630875,
      "learning_rate": 0.00048214345164008734,
      "loss": 0.0526,
      "step": 987
    },
    {
      "epoch": 0.24449393714427123,
      "grad_norm": 0.03537347540259361,
      "learning_rate": 0.0004821073217206513,
      "loss": 0.0571,
      "step": 988
    },
    {
      "epoch": 0.2447414006434051,
      "grad_norm": 0.022769063711166382,
      "learning_rate": 0.0004820711566430442,
      "loss": 0.0279,
      "step": 989
    },
    {
      "epoch": 0.24498886414253898,
      "grad_norm": 0.08287306874990463,
      "learning_rate": 0.0004820349564127443,
      "loss": 0.0687,
      "step": 990
    },
    {
      "epoch": 0.24523632764167286,
      "grad_norm": 0.03398718312382698,
      "learning_rate": 0.0004819987210352349,
      "loss": 0.0434,
      "step": 991
    },
    {
      "epoch": 0.24548379114080673,
      "grad_norm": 0.04098208621144295,
      "learning_rate": 0.0004819624505160046,
      "loss": 0.0415,
      "step": 992
    },
    {
      "epoch": 0.2457312546399406,
      "grad_norm": 0.03666171431541443,
      "learning_rate": 0.0004819261448605476,
      "loss": 0.0398,
      "step": 993
    },
    {
      "epoch": 0.2459787181390745,
      "grad_norm": 0.07456330209970474,
      "learning_rate": 0.00048188980407436313,
      "loss": 0.0863,
      "step": 994
    },
    {
      "epoch": 0.24622618163820836,
      "grad_norm": 0.051787104457616806,
      "learning_rate": 0.00048185342816295585,
      "loss": 0.0455,
      "step": 995
    },
    {
      "epoch": 0.24647364513734224,
      "grad_norm": 0.027955856174230576,
      "learning_rate": 0.0004818170171318359,
      "loss": 0.0197,
      "step": 996
    },
    {
      "epoch": 0.24672110863647612,
      "grad_norm": 0.0385175384581089,
      "learning_rate": 0.00048178057098651844,
      "loss": 0.0558,
      "step": 997
    },
    {
      "epoch": 0.24696857213561,
      "grad_norm": 0.0345345064997673,
      "learning_rate": 0.00048174408973252424,
      "loss": 0.0579,
      "step": 998
    },
    {
      "epoch": 0.24721603563474387,
      "grad_norm": 0.04747602343559265,
      "learning_rate": 0.0004817075733753792,
      "loss": 0.1343,
      "step": 999
    },
    {
      "epoch": 0.24746349913387775,
      "grad_norm": 0.03651809319853783,
      "learning_rate": 0.00048167102192061444,
      "loss": 0.0468,
      "step": 1000
    },
    {
      "epoch": 0.24746349913387775,
      "eval_loss": 0.3023914098739624,
      "eval_runtime": 202.6037,
      "eval_samples_per_second": 4.936,
      "eval_steps_per_second": 0.311,
      "step": 1000
    },
    {
      "epoch": 0.24771096263301162,
      "grad_norm": 0.04582571983337402,
      "learning_rate": 0.0004816344353737668,
      "loss": 0.0562,
      "step": 1001
    },
    {
      "epoch": 0.2479584261321455,
      "grad_norm": 0.04458709433674812,
      "learning_rate": 0.00048159781374037804,
      "loss": 0.0449,
      "step": 1002
    },
    {
      "epoch": 0.24820588963127937,
      "grad_norm": 0.04801192134618759,
      "learning_rate": 0.00048156115702599543,
      "loss": 0.1112,
      "step": 1003
    },
    {
      "epoch": 0.24845335313041325,
      "grad_norm": 0.02109953574836254,
      "learning_rate": 0.0004815244652361714,
      "loss": 0.0388,
      "step": 1004
    },
    {
      "epoch": 0.24870081662954716,
      "grad_norm": 0.05079732462763786,
      "learning_rate": 0.000481487738376464,
      "loss": 0.1081,
      "step": 1005
    },
    {
      "epoch": 0.24894828012868103,
      "grad_norm": 0.048538919538259506,
      "learning_rate": 0.0004814509764524361,
      "loss": 0.0757,
      "step": 1006
    },
    {
      "epoch": 0.2491957436278149,
      "grad_norm": 0.034778568893671036,
      "learning_rate": 0.0004814141794696564,
      "loss": 0.0475,
      "step": 1007
    },
    {
      "epoch": 0.24944320712694878,
      "grad_norm": 0.05139268562197685,
      "learning_rate": 0.00048137734743369863,
      "loss": 0.0523,
      "step": 1008
    },
    {
      "epoch": 0.24969067062608266,
      "grad_norm": 0.040153827518224716,
      "learning_rate": 0.0004813404803501418,
      "loss": 0.0472,
      "step": 1009
    },
    {
      "epoch": 0.24993813412521654,
      "grad_norm": 0.03252080827951431,
      "learning_rate": 0.00048130357822457037,
      "loss": 0.0364,
      "step": 1010
    },
    {
      "epoch": 0.2501855976243504,
      "grad_norm": 0.05488062649965286,
      "learning_rate": 0.000481266641062574,
      "loss": 0.0731,
      "step": 1011
    },
    {
      "epoch": 0.2504330611234843,
      "grad_norm": 0.05632670223712921,
      "learning_rate": 0.00048122966886974774,
      "loss": 0.0464,
      "step": 1012
    },
    {
      "epoch": 0.25068052462261814,
      "grad_norm": 0.05879693105816841,
      "learning_rate": 0.0004811926616516919,
      "loss": 0.1002,
      "step": 1013
    },
    {
      "epoch": 0.25092798812175204,
      "grad_norm": 0.10468064993619919,
      "learning_rate": 0.0004811556194140121,
      "loss": 0.0799,
      "step": 1014
    },
    {
      "epoch": 0.2511754516208859,
      "grad_norm": 0.044713281095027924,
      "learning_rate": 0.00048111854216231927,
      "loss": 0.0502,
      "step": 1015
    },
    {
      "epoch": 0.2514229151200198,
      "grad_norm": 0.026405934244394302,
      "learning_rate": 0.0004810814299022296,
      "loss": 0.0363,
      "step": 1016
    },
    {
      "epoch": 0.2516703786191537,
      "grad_norm": 0.022367235273122787,
      "learning_rate": 0.00048104428263936474,
      "loss": 0.0317,
      "step": 1017
    },
    {
      "epoch": 0.25191784211828755,
      "grad_norm": 0.0414404422044754,
      "learning_rate": 0.00048100710037935135,
      "loss": 0.0516,
      "step": 1018
    },
    {
      "epoch": 0.25216530561742145,
      "grad_norm": 0.04042202606797218,
      "learning_rate": 0.0004809698831278217,
      "loss": 0.0757,
      "step": 1019
    },
    {
      "epoch": 0.2524127691165553,
      "grad_norm": 0.03536459058523178,
      "learning_rate": 0.00048093263089041315,
      "loss": 0.0529,
      "step": 1020
    },
    {
      "epoch": 0.2526602326156892,
      "grad_norm": 0.034159302711486816,
      "learning_rate": 0.00048089534367276853,
      "loss": 0.0526,
      "step": 1021
    },
    {
      "epoch": 0.25290769611482306,
      "grad_norm": 0.03409954905509949,
      "learning_rate": 0.00048085802148053585,
      "loss": 0.0559,
      "step": 1022
    },
    {
      "epoch": 0.25315515961395696,
      "grad_norm": 0.050802234560251236,
      "learning_rate": 0.00048082066431936836,
      "loss": 0.0388,
      "step": 1023
    },
    {
      "epoch": 0.2534026231130908,
      "grad_norm": 0.026634614914655685,
      "learning_rate": 0.0004807832721949248,
      "loss": 0.0446,
      "step": 1024
    },
    {
      "epoch": 0.2536500866122247,
      "grad_norm": 0.04647879675030708,
      "learning_rate": 0.000480745845112869,
      "loss": 0.0541,
      "step": 1025
    },
    {
      "epoch": 0.25389755011135856,
      "grad_norm": 0.035777024924755096,
      "learning_rate": 0.0004807083830788702,
      "loss": 0.0323,
      "step": 1026
    },
    {
      "epoch": 0.25414501361049247,
      "grad_norm": 0.07891491800546646,
      "learning_rate": 0.000480670886098603,
      "loss": 0.0629,
      "step": 1027
    },
    {
      "epoch": 0.2543924771096263,
      "grad_norm": 0.08395563066005707,
      "learning_rate": 0.00048063335417774714,
      "loss": 0.0715,
      "step": 1028
    },
    {
      "epoch": 0.2546399406087602,
      "grad_norm": 0.02680489793419838,
      "learning_rate": 0.0004805957873219877,
      "loss": 0.0406,
      "step": 1029
    },
    {
      "epoch": 0.25488740410789407,
      "grad_norm": 0.03213118016719818,
      "learning_rate": 0.0004805581855370151,
      "loss": 0.0602,
      "step": 1030
    },
    {
      "epoch": 0.25513486760702797,
      "grad_norm": 0.07322388142347336,
      "learning_rate": 0.000480520548828525,
      "loss": 0.0636,
      "step": 1031
    },
    {
      "epoch": 0.2553823311061618,
      "grad_norm": 0.03610175848007202,
      "learning_rate": 0.00048048287720221843,
      "loss": 0.0335,
      "step": 1032
    },
    {
      "epoch": 0.2556297946052957,
      "grad_norm": 0.050341278314590454,
      "learning_rate": 0.0004804451706638017,
      "loss": 0.0386,
      "step": 1033
    },
    {
      "epoch": 0.2558772581044296,
      "grad_norm": 0.03517158329486847,
      "learning_rate": 0.00048040742921898617,
      "loss": 0.047,
      "step": 1034
    },
    {
      "epoch": 0.2561247216035635,
      "grad_norm": 0.05345016345381737,
      "learning_rate": 0.00048036965287348885,
      "loss": 0.137,
      "step": 1035
    },
    {
      "epoch": 0.2563721851026973,
      "grad_norm": 0.06973801553249359,
      "learning_rate": 0.0004803318416330318,
      "loss": 0.1229,
      "step": 1036
    },
    {
      "epoch": 0.25661964860183123,
      "grad_norm": 0.030464619398117065,
      "learning_rate": 0.00048029399550334243,
      "loss": 0.0431,
      "step": 1037
    },
    {
      "epoch": 0.25686711210096513,
      "grad_norm": 0.0814904049038887,
      "learning_rate": 0.00048025611449015345,
      "loss": 0.0598,
      "step": 1038
    },
    {
      "epoch": 0.257114575600099,
      "grad_norm": 0.05415097996592522,
      "learning_rate": 0.0004802181985992028,
      "loss": 0.119,
      "step": 1039
    },
    {
      "epoch": 0.2573620390992329,
      "grad_norm": 0.057670872658491135,
      "learning_rate": 0.0004801802478362338,
      "loss": 0.0365,
      "step": 1040
    },
    {
      "epoch": 0.25760950259836674,
      "grad_norm": 0.044103238731622696,
      "learning_rate": 0.0004801422622069951,
      "loss": 0.0464,
      "step": 1041
    },
    {
      "epoch": 0.25785696609750064,
      "grad_norm": 0.041320379823446274,
      "learning_rate": 0.0004801042417172402,
      "loss": 0.058,
      "step": 1042
    },
    {
      "epoch": 0.2581044295966345,
      "grad_norm": 0.04406081140041351,
      "learning_rate": 0.0004800661863727285,
      "loss": 0.1133,
      "step": 1043
    },
    {
      "epoch": 0.2583518930957684,
      "grad_norm": 0.03776593878865242,
      "learning_rate": 0.0004800280961792244,
      "loss": 0.0578,
      "step": 1044
    },
    {
      "epoch": 0.25859935659490224,
      "grad_norm": 0.035992372781038284,
      "learning_rate": 0.0004799899711424973,
      "loss": 0.0877,
      "step": 1045
    },
    {
      "epoch": 0.25884682009403615,
      "grad_norm": 0.05221080780029297,
      "learning_rate": 0.0004799518112683223,
      "loss": 0.1099,
      "step": 1046
    },
    {
      "epoch": 0.25909428359317,
      "grad_norm": 0.051576025784015656,
      "learning_rate": 0.0004799136165624798,
      "loss": 0.0576,
      "step": 1047
    },
    {
      "epoch": 0.2593417470923039,
      "grad_norm": 0.038908157497644424,
      "learning_rate": 0.00047987538703075493,
      "loss": 0.0359,
      "step": 1048
    },
    {
      "epoch": 0.25958921059143775,
      "grad_norm": 0.039488039910793304,
      "learning_rate": 0.0004798371226789388,
      "loss": 0.0943,
      "step": 1049
    },
    {
      "epoch": 0.25983667409057165,
      "grad_norm": 0.054768871515989304,
      "learning_rate": 0.0004797988235128272,
      "loss": 0.0681,
      "step": 1050
    },
    {
      "epoch": 0.2600841375897055,
      "grad_norm": 0.04570041224360466,
      "learning_rate": 0.00047976048953822157,
      "loss": 0.0619,
      "step": 1051
    },
    {
      "epoch": 0.2603316010888394,
      "grad_norm": 0.02072230726480484,
      "learning_rate": 0.00047972212076092846,
      "loss": 0.0271,
      "step": 1052
    },
    {
      "epoch": 0.26057906458797325,
      "grad_norm": 0.023234646767377853,
      "learning_rate": 0.00047968371718675986,
      "loss": 0.0268,
      "step": 1053
    },
    {
      "epoch": 0.26082652808710716,
      "grad_norm": 0.027727356180548668,
      "learning_rate": 0.0004796452788215327,
      "loss": 0.0284,
      "step": 1054
    },
    {
      "epoch": 0.261073991586241,
      "grad_norm": 0.050764452666044235,
      "learning_rate": 0.0004796068056710695,
      "loss": 0.1139,
      "step": 1055
    },
    {
      "epoch": 0.2613214550853749,
      "grad_norm": 0.04677337035536766,
      "learning_rate": 0.000479568297741198,
      "loss": 0.0623,
      "step": 1056
    },
    {
      "epoch": 0.26156891858450876,
      "grad_norm": 0.026835231110453606,
      "learning_rate": 0.000479529755037751,
      "loss": 0.0356,
      "step": 1057
    },
    {
      "epoch": 0.26181638208364266,
      "grad_norm": 0.06356489658355713,
      "learning_rate": 0.0004794911775665668,
      "loss": 0.072,
      "step": 1058
    },
    {
      "epoch": 0.26206384558277657,
      "grad_norm": 0.02208687737584114,
      "learning_rate": 0.0004794525653334888,
      "loss": 0.031,
      "step": 1059
    },
    {
      "epoch": 0.2623113090819104,
      "grad_norm": 0.028635680675506592,
      "learning_rate": 0.0004794139183443658,
      "loss": 0.0537,
      "step": 1060
    },
    {
      "epoch": 0.2625587725810443,
      "grad_norm": 0.04266265407204628,
      "learning_rate": 0.0004793752366050518,
      "loss": 0.0698,
      "step": 1061
    },
    {
      "epoch": 0.26280623608017817,
      "grad_norm": 0.018567776307463646,
      "learning_rate": 0.00047933652012140603,
      "loss": 0.0286,
      "step": 1062
    },
    {
      "epoch": 0.2630536995793121,
      "grad_norm": 0.026997635141015053,
      "learning_rate": 0.0004792977688992931,
      "loss": 0.042,
      "step": 1063
    },
    {
      "epoch": 0.2633011630784459,
      "grad_norm": 0.03507738187909126,
      "learning_rate": 0.0004792589829445828,
      "loss": 0.0782,
      "step": 1064
    },
    {
      "epoch": 0.2635486265775798,
      "grad_norm": 0.048773087561130524,
      "learning_rate": 0.00047922016226315006,
      "loss": 0.0666,
      "step": 1065
    },
    {
      "epoch": 0.2637960900767137,
      "grad_norm": 0.07639976590871811,
      "learning_rate": 0.0004791813068608753,
      "loss": 0.069,
      "step": 1066
    },
    {
      "epoch": 0.2640435535758476,
      "grad_norm": 0.02986234799027443,
      "learning_rate": 0.00047914241674364404,
      "loss": 0.0318,
      "step": 1067
    },
    {
      "epoch": 0.2642910170749814,
      "grad_norm": 0.03474357724189758,
      "learning_rate": 0.0004791034919173472,
      "loss": 0.0343,
      "step": 1068
    },
    {
      "epoch": 0.26453848057411533,
      "grad_norm": 0.04625504091382027,
      "learning_rate": 0.0004790645323878808,
      "loss": 0.0698,
      "step": 1069
    },
    {
      "epoch": 0.2647859440732492,
      "grad_norm": 0.04866829514503479,
      "learning_rate": 0.00047902553816114613,
      "loss": 0.0668,
      "step": 1070
    },
    {
      "epoch": 0.2650334075723831,
      "grad_norm": 0.04216756671667099,
      "learning_rate": 0.00047898650924305,
      "loss": 0.0592,
      "step": 1071
    },
    {
      "epoch": 0.26528087107151693,
      "grad_norm": 0.03401297330856323,
      "learning_rate": 0.000478947445639504,
      "loss": 0.0423,
      "step": 1072
    },
    {
      "epoch": 0.26552833457065084,
      "grad_norm": 0.03729807212948799,
      "learning_rate": 0.0004789083473564254,
      "loss": 0.0452,
      "step": 1073
    },
    {
      "epoch": 0.2657757980697847,
      "grad_norm": 0.038486309349536896,
      "learning_rate": 0.00047886921439973656,
      "loss": 0.0323,
      "step": 1074
    },
    {
      "epoch": 0.2660232615689186,
      "grad_norm": 0.03508364409208298,
      "learning_rate": 0.00047883004677536507,
      "loss": 0.222,
      "step": 1075
    },
    {
      "epoch": 0.26627072506805244,
      "grad_norm": 0.028382135555148125,
      "learning_rate": 0.0004787908444892438,
      "loss": 0.0401,
      "step": 1076
    },
    {
      "epoch": 0.26651818856718634,
      "grad_norm": 0.03542648255825043,
      "learning_rate": 0.0004787516075473108,
      "loss": 0.0548,
      "step": 1077
    },
    {
      "epoch": 0.2667656520663202,
      "grad_norm": 0.02111063152551651,
      "learning_rate": 0.0004787123359555096,
      "loss": 0.0292,
      "step": 1078
    },
    {
      "epoch": 0.2670131155654541,
      "grad_norm": 0.06446294486522675,
      "learning_rate": 0.0004786730297197887,
      "loss": 0.0973,
      "step": 1079
    },
    {
      "epoch": 0.267260579064588,
      "grad_norm": 0.04284663870930672,
      "learning_rate": 0.0004786336888461019,
      "loss": 0.0331,
      "step": 1080
    },
    {
      "epoch": 0.26750804256372185,
      "grad_norm": 0.024453863501548767,
      "learning_rate": 0.0004785943133404085,
      "loss": 0.0387,
      "step": 1081
    },
    {
      "epoch": 0.26775550606285575,
      "grad_norm": 0.036670226603746414,
      "learning_rate": 0.00047855490320867274,
      "loss": 0.0577,
      "step": 1082
    },
    {
      "epoch": 0.2680029695619896,
      "grad_norm": 0.052325859665870667,
      "learning_rate": 0.00047851545845686416,
      "loss": 0.07,
      "step": 1083
    },
    {
      "epoch": 0.2682504330611235,
      "grad_norm": 0.03219934180378914,
      "learning_rate": 0.00047847597909095774,
      "loss": 0.0502,
      "step": 1084
    },
    {
      "epoch": 0.26849789656025735,
      "grad_norm": 0.05277196690440178,
      "learning_rate": 0.0004784364651169335,
      "loss": 0.062,
      "step": 1085
    },
    {
      "epoch": 0.26874536005939126,
      "grad_norm": 0.028168797492980957,
      "learning_rate": 0.0004783969165407768,
      "loss": 0.0341,
      "step": 1086
    },
    {
      "epoch": 0.2689928235585251,
      "grad_norm": 0.06638612598180771,
      "learning_rate": 0.00047835733336847806,
      "loss": 0.0702,
      "step": 1087
    },
    {
      "epoch": 0.269240287057659,
      "grad_norm": 0.02599414996802807,
      "learning_rate": 0.0004783177156060334,
      "loss": 0.0278,
      "step": 1088
    },
    {
      "epoch": 0.26948775055679286,
      "grad_norm": 0.032472655177116394,
      "learning_rate": 0.0004782780632594436,
      "loss": 0.0686,
      "step": 1089
    },
    {
      "epoch": 0.26973521405592676,
      "grad_norm": 0.05339421331882477,
      "learning_rate": 0.000478238376334715,
      "loss": 0.0538,
      "step": 1090
    },
    {
      "epoch": 0.2699826775550606,
      "grad_norm": 0.06694953143596649,
      "learning_rate": 0.0004781986548378592,
      "loss": 0.0791,
      "step": 1091
    },
    {
      "epoch": 0.2702301410541945,
      "grad_norm": 0.07766644656658173,
      "learning_rate": 0.000478158898774893,
      "loss": 0.1263,
      "step": 1092
    },
    {
      "epoch": 0.27047760455332837,
      "grad_norm": 0.035628609359264374,
      "learning_rate": 0.0004781191081518383,
      "loss": 0.0403,
      "step": 1093
    },
    {
      "epoch": 0.27072506805246227,
      "grad_norm": 0.039105821400880814,
      "learning_rate": 0.00047807928297472234,
      "loss": 0.0505,
      "step": 1094
    },
    {
      "epoch": 0.2709725315515961,
      "grad_norm": 0.031764041632413864,
      "learning_rate": 0.0004780394232495777,
      "loss": 0.0367,
      "step": 1095
    },
    {
      "epoch": 0.27121999505073,
      "grad_norm": 0.03069496527314186,
      "learning_rate": 0.00047799952898244194,
      "loss": 0.04,
      "step": 1096
    },
    {
      "epoch": 0.27146745854986387,
      "grad_norm": 0.04998196288943291,
      "learning_rate": 0.000477959600179358,
      "loss": 0.0506,
      "step": 1097
    },
    {
      "epoch": 0.2717149220489978,
      "grad_norm": 0.03517565876245499,
      "learning_rate": 0.0004779196368463742,
      "loss": 0.0626,
      "step": 1098
    },
    {
      "epoch": 0.2719623855481316,
      "grad_norm": 0.05202386528253555,
      "learning_rate": 0.0004778796389895438,
      "loss": 0.0562,
      "step": 1099
    },
    {
      "epoch": 0.27220984904726553,
      "grad_norm": 0.04413190484046936,
      "learning_rate": 0.00047783960661492553,
      "loss": 0.0514,
      "step": 1100
    },
    {
      "epoch": 0.27245731254639943,
      "grad_norm": 0.057668495923280716,
      "learning_rate": 0.00047779953972858317,
      "loss": 0.0489,
      "step": 1101
    },
    {
      "epoch": 0.2727047760455333,
      "grad_norm": 0.05911780521273613,
      "learning_rate": 0.00047775943833658574,
      "loss": 0.099,
      "step": 1102
    },
    {
      "epoch": 0.2729522395446672,
      "grad_norm": 0.04046943411231041,
      "learning_rate": 0.0004777193024450077,
      "loss": 0.048,
      "step": 1103
    },
    {
      "epoch": 0.27319970304380103,
      "grad_norm": 0.04687168076634407,
      "learning_rate": 0.0004776791320599285,
      "loss": 0.0326,
      "step": 1104
    },
    {
      "epoch": 0.27344716654293494,
      "grad_norm": 0.05392950400710106,
      "learning_rate": 0.00047763892718743285,
      "loss": 0.068,
      "step": 1105
    },
    {
      "epoch": 0.2736946300420688,
      "grad_norm": 0.06041574478149414,
      "learning_rate": 0.00047759868783361085,
      "loss": 0.0879,
      "step": 1106
    },
    {
      "epoch": 0.2739420935412027,
      "grad_norm": 0.03383399173617363,
      "learning_rate": 0.0004775584140045576,
      "loss": 0.0268,
      "step": 1107
    },
    {
      "epoch": 0.27418955704033654,
      "grad_norm": 0.04405548796057701,
      "learning_rate": 0.0004775181057063737,
      "loss": 0.0613,
      "step": 1108
    },
    {
      "epoch": 0.27443702053947044,
      "grad_norm": 0.025895575061440468,
      "learning_rate": 0.00047747776294516453,
      "loss": 0.0356,
      "step": 1109
    },
    {
      "epoch": 0.2746844840386043,
      "grad_norm": 0.03854832053184509,
      "learning_rate": 0.00047743738572704117,
      "loss": 0.0347,
      "step": 1110
    },
    {
      "epoch": 0.2749319475377382,
      "grad_norm": 0.06170055270195007,
      "learning_rate": 0.00047739697405811964,
      "loss": 0.0352,
      "step": 1111
    },
    {
      "epoch": 0.27517941103687205,
      "grad_norm": 0.0420292429625988,
      "learning_rate": 0.00047735652794452123,
      "loss": 0.0788,
      "step": 1112
    },
    {
      "epoch": 0.27542687453600595,
      "grad_norm": 0.025893202051520348,
      "learning_rate": 0.00047731604739237256,
      "loss": 0.0353,
      "step": 1113
    },
    {
      "epoch": 0.2756743380351398,
      "grad_norm": 0.04249662905931473,
      "learning_rate": 0.0004772755324078052,
      "loss": 0.0842,
      "step": 1114
    },
    {
      "epoch": 0.2759218015342737,
      "grad_norm": 0.04140980541706085,
      "learning_rate": 0.00047723498299695634,
      "loss": 0.0855,
      "step": 1115
    },
    {
      "epoch": 0.27616926503340755,
      "grad_norm": 0.03615696355700493,
      "learning_rate": 0.000477194399165968,
      "loss": 0.0484,
      "step": 1116
    },
    {
      "epoch": 0.27641672853254146,
      "grad_norm": 0.038465384393930435,
      "learning_rate": 0.0004771537809209875,
      "loss": 0.0399,
      "step": 1117
    },
    {
      "epoch": 0.2766641920316753,
      "grad_norm": 0.060020674020051956,
      "learning_rate": 0.00047711312826816756,
      "loss": 0.0928,
      "step": 1118
    },
    {
      "epoch": 0.2769116555308092,
      "grad_norm": 0.050142910331487656,
      "learning_rate": 0.00047707244121366596,
      "loss": 0.184,
      "step": 1119
    },
    {
      "epoch": 0.27715911902994306,
      "grad_norm": 0.09523840248584747,
      "learning_rate": 0.00047703171976364576,
      "loss": 0.0433,
      "step": 1120
    },
    {
      "epoch": 0.27740658252907696,
      "grad_norm": 0.03890492022037506,
      "learning_rate": 0.0004769909639242751,
      "loss": 0.0538,
      "step": 1121
    },
    {
      "epoch": 0.27765404602821087,
      "grad_norm": 0.03586025908589363,
      "learning_rate": 0.00047695017370172745,
      "loss": 0.0667,
      "step": 1122
    },
    {
      "epoch": 0.2779015095273447,
      "grad_norm": 0.07184664905071259,
      "learning_rate": 0.00047690934910218154,
      "loss": 0.0955,
      "step": 1123
    },
    {
      "epoch": 0.2781489730264786,
      "grad_norm": 0.045964229851961136,
      "learning_rate": 0.00047686849013182117,
      "loss": 0.0633,
      "step": 1124
    },
    {
      "epoch": 0.27839643652561247,
      "grad_norm": 0.07544125616550446,
      "learning_rate": 0.00047682759679683535,
      "loss": 0.0982,
      "step": 1125
    },
    {
      "epoch": 0.27864390002474637,
      "grad_norm": 0.05373738333582878,
      "learning_rate": 0.0004767866691034184,
      "loss": 0.0551,
      "step": 1126
    },
    {
      "epoch": 0.2788913635238802,
      "grad_norm": 0.03893992304801941,
      "learning_rate": 0.0004767457070577698,
      "loss": 0.0526,
      "step": 1127
    },
    {
      "epoch": 0.2791388270230141,
      "grad_norm": 0.0387767069041729,
      "learning_rate": 0.0004767047106660942,
      "loss": 0.0265,
      "step": 1128
    },
    {
      "epoch": 0.279386290522148,
      "grad_norm": 0.04174258932471275,
      "learning_rate": 0.00047666367993460157,
      "loss": 0.0343,
      "step": 1129
    },
    {
      "epoch": 0.2796337540212819,
      "grad_norm": 0.062378786504268646,
      "learning_rate": 0.0004766226148695069,
      "loss": 0.0696,
      "step": 1130
    },
    {
      "epoch": 0.2798812175204157,
      "grad_norm": 0.031458258628845215,
      "learning_rate": 0.0004765815154770303,
      "loss": 0.0642,
      "step": 1131
    },
    {
      "epoch": 0.28012868101954963,
      "grad_norm": 0.03642046079039574,
      "learning_rate": 0.00047654038176339766,
      "loss": 0.04,
      "step": 1132
    },
    {
      "epoch": 0.2803761445186835,
      "grad_norm": 0.02969849668443203,
      "learning_rate": 0.0004764992137348393,
      "loss": 0.042,
      "step": 1133
    },
    {
      "epoch": 0.2806236080178174,
      "grad_norm": 0.028019480407238007,
      "learning_rate": 0.0004764580113975913,
      "loss": 0.0545,
      "step": 1134
    },
    {
      "epoch": 0.28087107151695123,
      "grad_norm": 0.03986557573080063,
      "learning_rate": 0.00047641677475789457,
      "loss": 0.0578,
      "step": 1135
    },
    {
      "epoch": 0.28111853501608514,
      "grad_norm": 0.0409625880420208,
      "learning_rate": 0.00047637550382199554,
      "loss": 0.0585,
      "step": 1136
    },
    {
      "epoch": 0.281365998515219,
      "grad_norm": 0.0588311031460762,
      "learning_rate": 0.0004763341985961456,
      "loss": 0.035,
      "step": 1137
    },
    {
      "epoch": 0.2816134620143529,
      "grad_norm": 0.053839217871427536,
      "learning_rate": 0.00047629285908660135,
      "loss": 0.0679,
      "step": 1138
    },
    {
      "epoch": 0.28186092551348674,
      "grad_norm": 0.4329543709754944,
      "learning_rate": 0.0004762514852996247,
      "loss": 0.0862,
      "step": 1139
    },
    {
      "epoch": 0.28210838901262064,
      "grad_norm": 0.08474726229906082,
      "learning_rate": 0.0004762100772414828,
      "loss": 0.0636,
      "step": 1140
    },
    {
      "epoch": 0.2823558525117545,
      "grad_norm": 0.025577465072274208,
      "learning_rate": 0.00047616863491844764,
      "loss": 0.0407,
      "step": 1141
    },
    {
      "epoch": 0.2826033160108884,
      "grad_norm": 0.03581635653972626,
      "learning_rate": 0.00047612715833679687,
      "loss": 0.0452,
      "step": 1142
    },
    {
      "epoch": 0.2828507795100223,
      "grad_norm": 0.04606328159570694,
      "learning_rate": 0.00047608564750281304,
      "loss": 0.0913,
      "step": 1143
    },
    {
      "epoch": 0.28309824300915615,
      "grad_norm": 0.05808727815747261,
      "learning_rate": 0.0004760441024227839,
      "loss": 0.083,
      "step": 1144
    },
    {
      "epoch": 0.28334570650829005,
      "grad_norm": 0.04526287317276001,
      "learning_rate": 0.00047600252310300254,
      "loss": 0.0383,
      "step": 1145
    },
    {
      "epoch": 0.2835931700074239,
      "grad_norm": 0.02548031136393547,
      "learning_rate": 0.00047596090954976693,
      "loss": 0.0354,
      "step": 1146
    },
    {
      "epoch": 0.2838406335065578,
      "grad_norm": 0.03667669743299484,
      "learning_rate": 0.0004759192617693807,
      "loss": 0.0552,
      "step": 1147
    },
    {
      "epoch": 0.28408809700569165,
      "grad_norm": 0.042319171130657196,
      "learning_rate": 0.00047587757976815214,
      "loss": 0.0457,
      "step": 1148
    },
    {
      "epoch": 0.28433556050482556,
      "grad_norm": 0.0301341749727726,
      "learning_rate": 0.00047583586355239515,
      "loss": 0.0519,
      "step": 1149
    },
    {
      "epoch": 0.2845830240039594,
      "grad_norm": 0.07272607088088989,
      "learning_rate": 0.0004757941131284287,
      "loss": 0.0642,
      "step": 1150
    },
    {
      "epoch": 0.2848304875030933,
      "grad_norm": 0.07635242491960526,
      "learning_rate": 0.00047575232850257667,
      "loss": 0.0518,
      "step": 1151
    },
    {
      "epoch": 0.28507795100222716,
      "grad_norm": 0.028012128546833992,
      "learning_rate": 0.0004757105096811685,
      "loss": 0.0348,
      "step": 1152
    },
    {
      "epoch": 0.28532541450136106,
      "grad_norm": 0.03266964107751846,
      "learning_rate": 0.0004756686566705386,
      "loss": 0.0572,
      "step": 1153
    },
    {
      "epoch": 0.2855728780004949,
      "grad_norm": 0.0404336079955101,
      "learning_rate": 0.0004756267694770265,
      "loss": 0.0479,
      "step": 1154
    },
    {
      "epoch": 0.2858203414996288,
      "grad_norm": 0.047735437750816345,
      "learning_rate": 0.0004755848481069772,
      "loss": 0.0781,
      "step": 1155
    },
    {
      "epoch": 0.28606780499876266,
      "grad_norm": 0.03362136706709862,
      "learning_rate": 0.0004755428925667406,
      "loss": 0.0517,
      "step": 1156
    },
    {
      "epoch": 0.28631526849789657,
      "grad_norm": 0.02999383583664894,
      "learning_rate": 0.00047550090286267184,
      "loss": 0.0442,
      "step": 1157
    },
    {
      "epoch": 0.2865627319970304,
      "grad_norm": 0.05614377185702324,
      "learning_rate": 0.00047545887900113127,
      "loss": 0.0546,
      "step": 1158
    },
    {
      "epoch": 0.2868101954961643,
      "grad_norm": 0.06675083935260773,
      "learning_rate": 0.0004754168209884844,
      "loss": 0.0957,
      "step": 1159
    },
    {
      "epoch": 0.28705765899529817,
      "grad_norm": 0.04617004096508026,
      "learning_rate": 0.00047537472883110195,
      "loss": 0.0505,
      "step": 1160
    },
    {
      "epoch": 0.2873051224944321,
      "grad_norm": 0.04625660553574562,
      "learning_rate": 0.0004753326025353597,
      "loss": 0.0789,
      "step": 1161
    },
    {
      "epoch": 0.2875525859935659,
      "grad_norm": 0.02621576003730297,
      "learning_rate": 0.00047529044210763876,
      "loss": 0.0163,
      "step": 1162
    },
    {
      "epoch": 0.2878000494926998,
      "grad_norm": 0.0896073505282402,
      "learning_rate": 0.00047524824755432535,
      "loss": 0.0771,
      "step": 1163
    },
    {
      "epoch": 0.28804751299183373,
      "grad_norm": 0.04712248593568802,
      "learning_rate": 0.0004752060188818107,
      "loss": 0.0637,
      "step": 1164
    },
    {
      "epoch": 0.2882949764909676,
      "grad_norm": 0.05984478071331978,
      "learning_rate": 0.00047516375609649153,
      "loss": 0.0696,
      "step": 1165
    },
    {
      "epoch": 0.2885424399901015,
      "grad_norm": 0.033254820853471756,
      "learning_rate": 0.0004751214592047693,
      "loss": 0.0334,
      "step": 1166
    },
    {
      "epoch": 0.28878990348923533,
      "grad_norm": 0.043187472969293594,
      "learning_rate": 0.00047507912821305123,
      "loss": 0.0515,
      "step": 1167
    },
    {
      "epoch": 0.28903736698836924,
      "grad_norm": 0.02515437640249729,
      "learning_rate": 0.000475036763127749,
      "loss": 0.0229,
      "step": 1168
    },
    {
      "epoch": 0.2892848304875031,
      "grad_norm": 0.02802363783121109,
      "learning_rate": 0.00047499436395528004,
      "loss": 0.0329,
      "step": 1169
    },
    {
      "epoch": 0.289532293986637,
      "grad_norm": 0.04819729924201965,
      "learning_rate": 0.0004749519307020666,
      "loss": 0.0469,
      "step": 1170
    },
    {
      "epoch": 0.28977975748577084,
      "grad_norm": 0.02919718250632286,
      "learning_rate": 0.0004749094633745362,
      "loss": 0.0379,
      "step": 1171
    },
    {
      "epoch": 0.29002722098490474,
      "grad_norm": 0.050336938351392746,
      "learning_rate": 0.0004748669619791217,
      "loss": 0.0441,
      "step": 1172
    },
    {
      "epoch": 0.2902746844840386,
      "grad_norm": 0.027142632752656937,
      "learning_rate": 0.00047482442652226065,
      "loss": 0.0366,
      "step": 1173
    },
    {
      "epoch": 0.2905221479831725,
      "grad_norm": 0.025431839749217033,
      "learning_rate": 0.0004747818570103963,
      "loss": 0.052,
      "step": 1174
    },
    {
      "epoch": 0.29076961148230634,
      "grad_norm": 0.036300890147686005,
      "learning_rate": 0.00047473925344997684,
      "loss": 0.0275,
      "step": 1175
    },
    {
      "epoch": 0.29101707498144025,
      "grad_norm": 0.030617963522672653,
      "learning_rate": 0.00047469661584745526,
      "loss": 0.0541,
      "step": 1176
    },
    {
      "epoch": 0.2912645384805741,
      "grad_norm": 0.03333535045385361,
      "learning_rate": 0.0004746539442092904,
      "loss": 0.0633,
      "step": 1177
    },
    {
      "epoch": 0.291512001979708,
      "grad_norm": 0.03808271139860153,
      "learning_rate": 0.00047461123854194574,
      "loss": 0.0625,
      "step": 1178
    },
    {
      "epoch": 0.29175946547884185,
      "grad_norm": 0.02965565398335457,
      "learning_rate": 0.00047456849885189005,
      "loss": 0.0377,
      "step": 1179
    },
    {
      "epoch": 0.29200692897797575,
      "grad_norm": 0.025696976110339165,
      "learning_rate": 0.0004745257251455974,
      "loss": 0.0298,
      "step": 1180
    },
    {
      "epoch": 0.2922543924771096,
      "grad_norm": 0.033560458570718765,
      "learning_rate": 0.0004744829174295467,
      "loss": 0.0593,
      "step": 1181
    },
    {
      "epoch": 0.2925018559762435,
      "grad_norm": 0.03518899902701378,
      "learning_rate": 0.0004744400757102223,
      "loss": 0.0459,
      "step": 1182
    },
    {
      "epoch": 0.29274931947537736,
      "grad_norm": 0.03710101544857025,
      "learning_rate": 0.00047439719999411355,
      "loss": 0.0233,
      "step": 1183
    },
    {
      "epoch": 0.29299678297451126,
      "grad_norm": 0.025214089080691338,
      "learning_rate": 0.00047435429028771516,
      "loss": 0.0438,
      "step": 1184
    },
    {
      "epoch": 0.29324424647364516,
      "grad_norm": 0.02795625850558281,
      "learning_rate": 0.0004743113465975266,
      "loss": 0.0288,
      "step": 1185
    },
    {
      "epoch": 0.293491709972779,
      "grad_norm": 0.046799369156360626,
      "learning_rate": 0.0004742683689300528,
      "loss": 0.0645,
      "step": 1186
    },
    {
      "epoch": 0.2937391734719129,
      "grad_norm": 0.03176518529653549,
      "learning_rate": 0.00047422535729180383,
      "loss": 0.0306,
      "step": 1187
    },
    {
      "epoch": 0.29398663697104677,
      "grad_norm": 0.03770153596997261,
      "learning_rate": 0.0004741823116892947,
      "loss": 0.0295,
      "step": 1188
    },
    {
      "epoch": 0.29423410047018067,
      "grad_norm": 0.03482871130108833,
      "learning_rate": 0.00047413923212904574,
      "loss": 0.0543,
      "step": 1189
    },
    {
      "epoch": 0.2944815639693145,
      "grad_norm": 0.05048288404941559,
      "learning_rate": 0.0004740961186175824,
      "loss": 0.0564,
      "step": 1190
    },
    {
      "epoch": 0.2947290274684484,
      "grad_norm": 0.026127733290195465,
      "learning_rate": 0.0004740529711614352,
      "loss": 0.0172,
      "step": 1191
    },
    {
      "epoch": 0.29497649096758227,
      "grad_norm": 0.04524964466691017,
      "learning_rate": 0.0004740097897671398,
      "loss": 0.045,
      "step": 1192
    },
    {
      "epoch": 0.2952239544667162,
      "grad_norm": 0.02819322980940342,
      "learning_rate": 0.00047396657444123723,
      "loss": 0.0514,
      "step": 1193
    },
    {
      "epoch": 0.29547141796585,
      "grad_norm": 0.09085916727781296,
      "learning_rate": 0.0004739233251902733,
      "loss": 0.1619,
      "step": 1194
    },
    {
      "epoch": 0.29571888146498393,
      "grad_norm": 0.021371638402342796,
      "learning_rate": 0.00047388004202079926,
      "loss": 0.0253,
      "step": 1195
    },
    {
      "epoch": 0.2959663449641178,
      "grad_norm": 0.030310388654470444,
      "learning_rate": 0.0004738367249393712,
      "loss": 0.0434,
      "step": 1196
    },
    {
      "epoch": 0.2962138084632517,
      "grad_norm": 0.05366823822259903,
      "learning_rate": 0.00047379337395255086,
      "loss": 0.0652,
      "step": 1197
    },
    {
      "epoch": 0.29646127196238553,
      "grad_norm": 0.03665955737233162,
      "learning_rate": 0.00047374998906690437,
      "loss": 0.0468,
      "step": 1198
    },
    {
      "epoch": 0.29670873546151944,
      "grad_norm": 0.04746470972895622,
      "learning_rate": 0.00047370657028900364,
      "loss": 0.0589,
      "step": 1199
    },
    {
      "epoch": 0.2969561989606533,
      "grad_norm": 0.04572746157646179,
      "learning_rate": 0.00047366311762542544,
      "loss": 0.0506,
      "step": 1200
    },
    {
      "epoch": 0.2969561989606533,
      "eval_loss": 0.2992173135280609,
      "eval_runtime": 202.6752,
      "eval_samples_per_second": 4.934,
      "eval_steps_per_second": 0.311,
      "step": 1200
    },
    {
      "epoch": 0.2972036624597872,
      "grad_norm": 0.053366437554359436,
      "learning_rate": 0.00047361963108275175,
      "loss": 0.0335,
      "step": 1201
    },
    {
      "epoch": 0.29745112595892104,
      "grad_norm": 0.031293079257011414,
      "learning_rate": 0.0004735761106675694,
      "loss": 0.0227,
      "step": 1202
    },
    {
      "epoch": 0.29769858945805494,
      "grad_norm": 0.036418575793504715,
      "learning_rate": 0.00047353255638647095,
      "loss": 0.0437,
      "step": 1203
    },
    {
      "epoch": 0.2979460529571888,
      "grad_norm": 0.05079580470919609,
      "learning_rate": 0.00047348896824605353,
      "loss": 0.0428,
      "step": 1204
    },
    {
      "epoch": 0.2981935164563227,
      "grad_norm": 0.022044586017727852,
      "learning_rate": 0.0004734453462529196,
      "loss": 0.0615,
      "step": 1205
    },
    {
      "epoch": 0.2984409799554566,
      "grad_norm": 0.025019153952598572,
      "learning_rate": 0.0004734016904136768,
      "loss": 0.0364,
      "step": 1206
    },
    {
      "epoch": 0.29868844345459045,
      "grad_norm": 0.03957415372133255,
      "learning_rate": 0.0004733580007349378,
      "loss": 0.0968,
      "step": 1207
    },
    {
      "epoch": 0.29893590695372435,
      "grad_norm": 0.05496500805020332,
      "learning_rate": 0.00047331427722332043,
      "loss": 0.0539,
      "step": 1208
    },
    {
      "epoch": 0.2991833704528582,
      "grad_norm": 0.04469917342066765,
      "learning_rate": 0.00047327051988544776,
      "loss": 0.0534,
      "step": 1209
    },
    {
      "epoch": 0.2994308339519921,
      "grad_norm": 0.03589997440576553,
      "learning_rate": 0.0004732267287279477,
      "loss": 0.0451,
      "step": 1210
    },
    {
      "epoch": 0.29967829745112595,
      "grad_norm": 0.03438909351825714,
      "learning_rate": 0.00047318290375745364,
      "loss": 0.0267,
      "step": 1211
    },
    {
      "epoch": 0.29992576095025986,
      "grad_norm": 0.03439260274171829,
      "learning_rate": 0.0004731390449806039,
      "loss": 0.0308,
      "step": 1212
    },
    {
      "epoch": 0.3001732244493937,
      "grad_norm": 0.04645673185586929,
      "learning_rate": 0.0004730951524040418,
      "loss": 0.0493,
      "step": 1213
    },
    {
      "epoch": 0.3004206879485276,
      "grad_norm": 0.05491247400641441,
      "learning_rate": 0.00047305122603441595,
      "loss": 0.1101,
      "step": 1214
    },
    {
      "epoch": 0.30066815144766146,
      "grad_norm": 0.080465167760849,
      "learning_rate": 0.00047300726587838014,
      "loss": 0.0588,
      "step": 1215
    },
    {
      "epoch": 0.30091561494679536,
      "grad_norm": 0.05459390580654144,
      "learning_rate": 0.00047296327194259303,
      "loss": 0.0795,
      "step": 1216
    },
    {
      "epoch": 0.3011630784459292,
      "grad_norm": 0.0810493752360344,
      "learning_rate": 0.00047291924423371873,
      "loss": 0.0945,
      "step": 1217
    },
    {
      "epoch": 0.3014105419450631,
      "grad_norm": 0.025723949074745178,
      "learning_rate": 0.00047287518275842616,
      "loss": 0.0282,
      "step": 1218
    },
    {
      "epoch": 0.30165800544419696,
      "grad_norm": 0.03669895976781845,
      "learning_rate": 0.0004728310875233895,
      "loss": 0.0346,
      "step": 1219
    },
    {
      "epoch": 0.30190546894333087,
      "grad_norm": 0.04304267466068268,
      "learning_rate": 0.0004727869585352881,
      "loss": 0.0652,
      "step": 1220
    },
    {
      "epoch": 0.3021529324424647,
      "grad_norm": 0.06677433848381042,
      "learning_rate": 0.0004727427958008061,
      "loss": 0.1108,
      "step": 1221
    },
    {
      "epoch": 0.3024003959415986,
      "grad_norm": 0.02726845256984234,
      "learning_rate": 0.00047269859932663337,
      "loss": 0.038,
      "step": 1222
    },
    {
      "epoch": 0.30264785944073247,
      "grad_norm": 0.026860075071454048,
      "learning_rate": 0.00047265436911946425,
      "loss": 0.0309,
      "step": 1223
    },
    {
      "epoch": 0.3028953229398664,
      "grad_norm": 0.03975274786353111,
      "learning_rate": 0.0004726101051859985,
      "loss": 0.0338,
      "step": 1224
    },
    {
      "epoch": 0.3031427864390002,
      "grad_norm": 0.033067259937524796,
      "learning_rate": 0.000472565807532941,
      "loss": 0.0429,
      "step": 1225
    },
    {
      "epoch": 0.3033902499381341,
      "grad_norm": 0.03708872199058533,
      "learning_rate": 0.0004725214761670016,
      "loss": 0.0586,
      "step": 1226
    },
    {
      "epoch": 0.303637713437268,
      "grad_norm": 0.048083655536174774,
      "learning_rate": 0.00047247711109489545,
      "loss": 0.0895,
      "step": 1227
    },
    {
      "epoch": 0.3038851769364019,
      "grad_norm": 0.04270940646529198,
      "learning_rate": 0.0004724327123233426,
      "loss": 0.0589,
      "step": 1228
    },
    {
      "epoch": 0.3041326404355358,
      "grad_norm": 0.04376442730426788,
      "learning_rate": 0.0004723882798590683,
      "loss": 0.093,
      "step": 1229
    },
    {
      "epoch": 0.30438010393466963,
      "grad_norm": 0.07114959508180618,
      "learning_rate": 0.000472343813708803,
      "loss": 0.082,
      "step": 1230
    },
    {
      "epoch": 0.30462756743380354,
      "grad_norm": 0.03279093652963638,
      "learning_rate": 0.000472299313879282,
      "loss": 0.0621,
      "step": 1231
    },
    {
      "epoch": 0.3048750309329374,
      "grad_norm": 0.04949621856212616,
      "learning_rate": 0.00047225478037724614,
      "loss": 0.0554,
      "step": 1232
    },
    {
      "epoch": 0.3051224944320713,
      "grad_norm": 0.025989236310124397,
      "learning_rate": 0.0004722102132094407,
      "loss": 0.046,
      "step": 1233
    },
    {
      "epoch": 0.30536995793120514,
      "grad_norm": 0.02994219772517681,
      "learning_rate": 0.0004721656123826167,
      "loss": 0.0603,
      "step": 1234
    },
    {
      "epoch": 0.30561742143033904,
      "grad_norm": 0.08289500325918198,
      "learning_rate": 0.00047212097790352994,
      "loss": 0.115,
      "step": 1235
    },
    {
      "epoch": 0.3058648849294729,
      "grad_norm": 0.03813760727643967,
      "learning_rate": 0.0004720763097789413,
      "loss": 0.0547,
      "step": 1236
    },
    {
      "epoch": 0.3061123484286068,
      "grad_norm": 0.06734836846590042,
      "learning_rate": 0.00047203160801561684,
      "loss": 0.0637,
      "step": 1237
    },
    {
      "epoch": 0.30635981192774064,
      "grad_norm": 0.024933835491538048,
      "learning_rate": 0.0004719868726203278,
      "loss": 0.0173,
      "step": 1238
    },
    {
      "epoch": 0.30660727542687455,
      "grad_norm": 0.05239887535572052,
      "learning_rate": 0.0004719421035998504,
      "loss": 0.0884,
      "step": 1239
    },
    {
      "epoch": 0.3068547389260084,
      "grad_norm": 0.04322151467204094,
      "learning_rate": 0.0004718973009609659,
      "loss": 0.0612,
      "step": 1240
    },
    {
      "epoch": 0.3071022024251423,
      "grad_norm": 0.04383590444922447,
      "learning_rate": 0.00047185246471046074,
      "loss": 0.0704,
      "step": 1241
    },
    {
      "epoch": 0.30734966592427615,
      "grad_norm": 0.031249068677425385,
      "learning_rate": 0.00047180759485512645,
      "loss": 0.0262,
      "step": 1242
    },
    {
      "epoch": 0.30759712942341005,
      "grad_norm": 0.05861696973443031,
      "learning_rate": 0.00047176269140175977,
      "loss": 0.0792,
      "step": 1243
    },
    {
      "epoch": 0.3078445929225439,
      "grad_norm": 0.05133682116866112,
      "learning_rate": 0.00047171775435716215,
      "loss": 0.1073,
      "step": 1244
    },
    {
      "epoch": 0.3080920564216778,
      "grad_norm": 0.03571920469403267,
      "learning_rate": 0.0004716727837281405,
      "loss": 0.0605,
      "step": 1245
    },
    {
      "epoch": 0.30833951992081166,
      "grad_norm": 0.025584416463971138,
      "learning_rate": 0.0004716277795215067,
      "loss": 0.0444,
      "step": 1246
    },
    {
      "epoch": 0.30858698341994556,
      "grad_norm": 0.03483276814222336,
      "learning_rate": 0.0004715827417440777,
      "loss": 0.051,
      "step": 1247
    },
    {
      "epoch": 0.3088344469190794,
      "grad_norm": 0.03825746476650238,
      "learning_rate": 0.0004715376704026755,
      "loss": 0.048,
      "step": 1248
    },
    {
      "epoch": 0.3090819104182133,
      "grad_norm": 0.04393942654132843,
      "learning_rate": 0.0004714925655041272,
      "loss": 0.0436,
      "step": 1249
    },
    {
      "epoch": 0.3093293739173472,
      "grad_norm": 0.05897970870137215,
      "learning_rate": 0.00047144742705526523,
      "loss": 0.0508,
      "step": 1250
    },
    {
      "epoch": 0.30957683741648107,
      "grad_norm": 0.03725913539528847,
      "learning_rate": 0.00047140225506292656,
      "loss": 0.0547,
      "step": 1251
    },
    {
      "epoch": 0.30982430091561497,
      "grad_norm": 0.026873663067817688,
      "learning_rate": 0.0004713570495339538,
      "loss": 0.0278,
      "step": 1252
    },
    {
      "epoch": 0.3100717644147488,
      "grad_norm": 0.03739580139517784,
      "learning_rate": 0.0004713118104751943,
      "loss": 0.0519,
      "step": 1253
    },
    {
      "epoch": 0.3103192279138827,
      "grad_norm": 0.037932928651571274,
      "learning_rate": 0.00047126653789350065,
      "loss": 0.0735,
      "step": 1254
    },
    {
      "epoch": 0.31056669141301657,
      "grad_norm": 0.045696500688791275,
      "learning_rate": 0.0004712212317957304,
      "loss": 0.064,
      "step": 1255
    },
    {
      "epoch": 0.3108141549121505,
      "grad_norm": 0.03813012316823006,
      "learning_rate": 0.0004711758921887462,
      "loss": 0.0572,
      "step": 1256
    },
    {
      "epoch": 0.3110616184112843,
      "grad_norm": 0.03255404159426689,
      "learning_rate": 0.00047113051907941595,
      "loss": 0.0668,
      "step": 1257
    },
    {
      "epoch": 0.31130908191041823,
      "grad_norm": 0.03602529689669609,
      "learning_rate": 0.0004710851124746124,
      "loss": 0.031,
      "step": 1258
    },
    {
      "epoch": 0.3115565454095521,
      "grad_norm": 0.05514848232269287,
      "learning_rate": 0.00047103967238121334,
      "loss": 0.0438,
      "step": 1259
    },
    {
      "epoch": 0.311804008908686,
      "grad_norm": 0.029981359839439392,
      "learning_rate": 0.00047099419880610203,
      "loss": 0.057,
      "step": 1260
    },
    {
      "epoch": 0.31205147240781983,
      "grad_norm": 0.035039953887462616,
      "learning_rate": 0.0004709486917561663,
      "loss": 0.073,
      "step": 1261
    },
    {
      "epoch": 0.31229893590695373,
      "grad_norm": 0.02577512152493,
      "learning_rate": 0.0004709031512382993,
      "loss": 0.0319,
      "step": 1262
    },
    {
      "epoch": 0.3125463994060876,
      "grad_norm": 0.02836376428604126,
      "learning_rate": 0.0004708575772593994,
      "loss": 0.0411,
      "step": 1263
    },
    {
      "epoch": 0.3127938629052215,
      "grad_norm": 0.04580922797322273,
      "learning_rate": 0.00047081196982636956,
      "loss": 0.0515,
      "step": 1264
    },
    {
      "epoch": 0.31304132640435534,
      "grad_norm": 0.032641854137182236,
      "learning_rate": 0.00047076632894611835,
      "loss": 0.0481,
      "step": 1265
    },
    {
      "epoch": 0.31328878990348924,
      "grad_norm": 0.1263982504606247,
      "learning_rate": 0.00047072065462555916,
      "loss": 0.0443,
      "step": 1266
    },
    {
      "epoch": 0.3135362534026231,
      "grad_norm": 0.022430241107940674,
      "learning_rate": 0.00047067494687161033,
      "loss": 0.0351,
      "step": 1267
    },
    {
      "epoch": 0.313783716901757,
      "grad_norm": 0.02273256704211235,
      "learning_rate": 0.00047062920569119544,
      "loss": 0.0304,
      "step": 1268
    },
    {
      "epoch": 0.31403118040089084,
      "grad_norm": 0.04433431476354599,
      "learning_rate": 0.00047058343109124306,
      "loss": 0.0521,
      "step": 1269
    },
    {
      "epoch": 0.31427864390002475,
      "grad_norm": 0.07522916048765182,
      "learning_rate": 0.00047053762307868694,
      "loss": 0.1027,
      "step": 1270
    },
    {
      "epoch": 0.31452610739915865,
      "grad_norm": 0.022173631936311722,
      "learning_rate": 0.0004704917816604657,
      "loss": 0.0298,
      "step": 1271
    },
    {
      "epoch": 0.3147735708982925,
      "grad_norm": 0.029885081574320793,
      "learning_rate": 0.00047044590684352307,
      "loss": 0.0421,
      "step": 1272
    },
    {
      "epoch": 0.3150210343974264,
      "grad_norm": 0.041811373084783554,
      "learning_rate": 0.0004703999986348079,
      "loss": 0.057,
      "step": 1273
    },
    {
      "epoch": 0.31526849789656025,
      "grad_norm": 0.04039978235960007,
      "learning_rate": 0.0004703540570412742,
      "loss": 0.0536,
      "step": 1274
    },
    {
      "epoch": 0.31551596139569416,
      "grad_norm": 0.0562470369040966,
      "learning_rate": 0.00047030808206988085,
      "loss": 0.0936,
      "step": 1275
    },
    {
      "epoch": 0.315763424894828,
      "grad_norm": 0.03241268917918205,
      "learning_rate": 0.0004702620737275918,
      "loss": 0.0424,
      "step": 1276
    },
    {
      "epoch": 0.3160108883939619,
      "grad_norm": 0.04117843881249428,
      "learning_rate": 0.00047021603202137614,
      "loss": 0.0887,
      "step": 1277
    },
    {
      "epoch": 0.31625835189309576,
      "grad_norm": 0.041564129292964935,
      "learning_rate": 0.000470169956958208,
      "loss": 0.0649,
      "step": 1278
    },
    {
      "epoch": 0.31650581539222966,
      "grad_norm": 0.02876349911093712,
      "learning_rate": 0.00047012384854506654,
      "loss": 0.0804,
      "step": 1279
    },
    {
      "epoch": 0.3167532788913635,
      "grad_norm": 0.031883493065834045,
      "learning_rate": 0.0004700777067889359,
      "loss": 0.0591,
      "step": 1280
    },
    {
      "epoch": 0.3170007423904974,
      "grad_norm": 0.023793496191501617,
      "learning_rate": 0.00047003153169680544,
      "loss": 0.0376,
      "step": 1281
    },
    {
      "epoch": 0.31724820588963126,
      "grad_norm": 0.03866451233625412,
      "learning_rate": 0.0004699853232756695,
      "loss": 0.0475,
      "step": 1282
    },
    {
      "epoch": 0.31749566938876517,
      "grad_norm": 0.04816018417477608,
      "learning_rate": 0.00046993908153252737,
      "loss": 0.0746,
      "step": 1283
    },
    {
      "epoch": 0.317743132887899,
      "grad_norm": 0.027951188385486603,
      "learning_rate": 0.00046989280647438344,
      "loss": 0.04,
      "step": 1284
    },
    {
      "epoch": 0.3179905963870329,
      "grad_norm": 0.04993665963411331,
      "learning_rate": 0.0004698464981082472,
      "loss": 0.0754,
      "step": 1285
    },
    {
      "epoch": 0.31823805988616677,
      "grad_norm": 0.02728545479476452,
      "learning_rate": 0.0004698001564411332,
      "loss": 0.0397,
      "step": 1286
    },
    {
      "epoch": 0.3184855233853007,
      "grad_norm": 0.051399506628513336,
      "learning_rate": 0.000469753781480061,
      "loss": 0.0532,
      "step": 1287
    },
    {
      "epoch": 0.3187329868844345,
      "grad_norm": 0.024331403896212578,
      "learning_rate": 0.00046970737323205505,
      "loss": 0.0341,
      "step": 1288
    },
    {
      "epoch": 0.3189804503835684,
      "grad_norm": 0.05571768432855606,
      "learning_rate": 0.00046966093170414504,
      "loss": 0.0817,
      "step": 1289
    },
    {
      "epoch": 0.3192279138827023,
      "grad_norm": 0.036870382726192474,
      "learning_rate": 0.00046961445690336576,
      "loss": 0.0286,
      "step": 1290
    },
    {
      "epoch": 0.3194753773818362,
      "grad_norm": 0.018775036558508873,
      "learning_rate": 0.0004695679488367568,
      "loss": 0.0177,
      "step": 1291
    },
    {
      "epoch": 0.3197228408809701,
      "grad_norm": 0.03946198895573616,
      "learning_rate": 0.0004695214075113629,
      "loss": 0.0486,
      "step": 1292
    },
    {
      "epoch": 0.31997030438010393,
      "grad_norm": 0.03339904174208641,
      "learning_rate": 0.000469474832934234,
      "loss": 0.0349,
      "step": 1293
    },
    {
      "epoch": 0.32021776787923784,
      "grad_norm": 0.04991045594215393,
      "learning_rate": 0.0004694282251124247,
      "loss": 0.1146,
      "step": 1294
    },
    {
      "epoch": 0.3204652313783717,
      "grad_norm": 0.0449051633477211,
      "learning_rate": 0.0004693815840529951,
      "loss": 0.0654,
      "step": 1295
    },
    {
      "epoch": 0.3207126948775056,
      "grad_norm": 0.05096154659986496,
      "learning_rate": 0.0004693349097630099,
      "loss": 0.0552,
      "step": 1296
    },
    {
      "epoch": 0.32096015837663944,
      "grad_norm": 0.05656445398926735,
      "learning_rate": 0.00046928820224953916,
      "loss": 0.0577,
      "step": 1297
    },
    {
      "epoch": 0.32120762187577334,
      "grad_norm": 0.044200796633958817,
      "learning_rate": 0.00046924146151965785,
      "loss": 0.0612,
      "step": 1298
    },
    {
      "epoch": 0.3214550853749072,
      "grad_norm": 0.03696289658546448,
      "learning_rate": 0.0004691946875804458,
      "loss": 0.0718,
      "step": 1299
    },
    {
      "epoch": 0.3217025488740411,
      "grad_norm": 0.0361425057053566,
      "learning_rate": 0.00046914788043898826,
      "loss": 0.0258,
      "step": 1300
    },
    {
      "epoch": 0.32195001237317494,
      "grad_norm": 0.06412385404109955,
      "learning_rate": 0.00046910104010237507,
      "loss": 0.0496,
      "step": 1301
    },
    {
      "epoch": 0.32219747587230885,
      "grad_norm": 0.02811725065112114,
      "learning_rate": 0.0004690541665777015,
      "loss": 0.0268,
      "step": 1302
    },
    {
      "epoch": 0.3224449393714427,
      "grad_norm": 0.05928882583975792,
      "learning_rate": 0.0004690072598720675,
      "loss": 0.0404,
      "step": 1303
    },
    {
      "epoch": 0.3226924028705766,
      "grad_norm": 0.02976224012672901,
      "learning_rate": 0.0004689603199925783,
      "loss": 0.0276,
      "step": 1304
    },
    {
      "epoch": 0.32293986636971045,
      "grad_norm": 0.02733713388442993,
      "learning_rate": 0.00046891334694634405,
      "loss": 0.0354,
      "step": 1305
    },
    {
      "epoch": 0.32318732986884435,
      "grad_norm": 0.02055690623819828,
      "learning_rate": 0.00046886634074047996,
      "loss": 0.0191,
      "step": 1306
    },
    {
      "epoch": 0.3234347933679782,
      "grad_norm": 0.02644365280866623,
      "learning_rate": 0.00046881930138210616,
      "loss": 0.0379,
      "step": 1307
    },
    {
      "epoch": 0.3236822568671121,
      "grad_norm": 0.04366770386695862,
      "learning_rate": 0.0004687722288783479,
      "loss": 0.0356,
      "step": 1308
    },
    {
      "epoch": 0.32392972036624595,
      "grad_norm": 0.03443072363734245,
      "learning_rate": 0.0004687251232363355,
      "loss": 0.0535,
      "step": 1309
    },
    {
      "epoch": 0.32417718386537986,
      "grad_norm": 0.024747591465711594,
      "learning_rate": 0.0004686779844632041,
      "loss": 0.019,
      "step": 1310
    },
    {
      "epoch": 0.3244246473645137,
      "grad_norm": 0.07689923048019409,
      "learning_rate": 0.0004686308125660942,
      "loss": 0.0718,
      "step": 1311
    },
    {
      "epoch": 0.3246721108636476,
      "grad_norm": 0.04230773076415062,
      "learning_rate": 0.00046858360755215085,
      "loss": 0.0665,
      "step": 1312
    },
    {
      "epoch": 0.3249195743627815,
      "grad_norm": 0.044186487793922424,
      "learning_rate": 0.00046853636942852453,
      "loss": 0.0472,
      "step": 1313
    },
    {
      "epoch": 0.32516703786191536,
      "grad_norm": 0.09533674269914627,
      "learning_rate": 0.0004684890982023705,
      "loss": 0.0311,
      "step": 1314
    },
    {
      "epoch": 0.32541450136104927,
      "grad_norm": 0.02344368025660515,
      "learning_rate": 0.0004684417938808493,
      "loss": 0.0444,
      "step": 1315
    },
    {
      "epoch": 0.3256619648601831,
      "grad_norm": 0.0352356843650341,
      "learning_rate": 0.000468394456471126,
      "loss": 0.0534,
      "step": 1316
    },
    {
      "epoch": 0.325909428359317,
      "grad_norm": 0.03663288801908493,
      "learning_rate": 0.00046834708598037113,
      "loss": 0.0476,
      "step": 1317
    },
    {
      "epoch": 0.32615689185845087,
      "grad_norm": 0.08996547013521194,
      "learning_rate": 0.00046829968241576017,
      "loss": 0.074,
      "step": 1318
    },
    {
      "epoch": 0.3264043553575848,
      "grad_norm": 0.0358872152864933,
      "learning_rate": 0.0004682522457844733,
      "loss": 0.0615,
      "step": 1319
    },
    {
      "epoch": 0.3266518188567186,
      "grad_norm": 0.07823606580495834,
      "learning_rate": 0.00046820477609369604,
      "loss": 0.0746,
      "step": 1320
    },
    {
      "epoch": 0.3268992823558525,
      "grad_norm": 0.037454456090927124,
      "learning_rate": 0.00046815727335061886,
      "loss": 0.0616,
      "step": 1321
    },
    {
      "epoch": 0.3271467458549864,
      "grad_norm": 0.02574821375310421,
      "learning_rate": 0.00046810973756243715,
      "loss": 0.0338,
      "step": 1322
    },
    {
      "epoch": 0.3273942093541203,
      "grad_norm": 0.03625606745481491,
      "learning_rate": 0.0004680621687363512,
      "loss": 0.0513,
      "step": 1323
    },
    {
      "epoch": 0.32764167285325413,
      "grad_norm": 0.04691758379340172,
      "learning_rate": 0.0004680145668795666,
      "loss": 0.0678,
      "step": 1324
    },
    {
      "epoch": 0.32788913635238803,
      "grad_norm": 0.03227858245372772,
      "learning_rate": 0.00046796693199929377,
      "loss": 0.0407,
      "step": 1325
    },
    {
      "epoch": 0.3281365998515219,
      "grad_norm": 0.06854923069477081,
      "learning_rate": 0.00046791926410274804,
      "loss": 0.1284,
      "step": 1326
    },
    {
      "epoch": 0.3283840633506558,
      "grad_norm": 0.08346018195152283,
      "learning_rate": 0.00046787156319714996,
      "loss": 0.0796,
      "step": 1327
    },
    {
      "epoch": 0.32863152684978963,
      "grad_norm": 0.049468591809272766,
      "learning_rate": 0.0004678238292897249,
      "loss": 0.0499,
      "step": 1328
    },
    {
      "epoch": 0.32887899034892354,
      "grad_norm": 0.030318232253193855,
      "learning_rate": 0.00046777606238770334,
      "loss": 0.0511,
      "step": 1329
    },
    {
      "epoch": 0.3291264538480574,
      "grad_norm": 0.04155098646879196,
      "learning_rate": 0.0004677282624983206,
      "loss": 0.052,
      "step": 1330
    },
    {
      "epoch": 0.3293739173471913,
      "grad_norm": 0.03436090424656868,
      "learning_rate": 0.0004676804296288172,
      "loss": 0.0625,
      "step": 1331
    },
    {
      "epoch": 0.32962138084632514,
      "grad_norm": 0.04005814716219902,
      "learning_rate": 0.00046763256378643865,
      "loss": 0.0402,
      "step": 1332
    },
    {
      "epoch": 0.32986884434545904,
      "grad_norm": 0.0359833762049675,
      "learning_rate": 0.0004675846649784352,
      "loss": 0.0653,
      "step": 1333
    },
    {
      "epoch": 0.33011630784459295,
      "grad_norm": 0.08966977894306183,
      "learning_rate": 0.00046753673321206236,
      "loss": 0.0671,
      "step": 1334
    },
    {
      "epoch": 0.3303637713437268,
      "grad_norm": 0.04241899773478508,
      "learning_rate": 0.00046748876849458053,
      "loss": 0.059,
      "step": 1335
    },
    {
      "epoch": 0.3306112348428607,
      "grad_norm": 0.03159608691930771,
      "learning_rate": 0.0004674407708332551,
      "loss": 0.0249,
      "step": 1336
    },
    {
      "epoch": 0.33085869834199455,
      "grad_norm": 0.02955625206232071,
      "learning_rate": 0.0004673927402353564,
      "loss": 0.0695,
      "step": 1337
    },
    {
      "epoch": 0.33110616184112845,
      "grad_norm": 0.03872549906373024,
      "learning_rate": 0.00046734467670815996,
      "loss": 0.0516,
      "step": 1338
    },
    {
      "epoch": 0.3313536253402623,
      "grad_norm": 0.055676307529211044,
      "learning_rate": 0.00046729658025894595,
      "loss": 0.0916,
      "step": 1339
    },
    {
      "epoch": 0.3316010888393962,
      "grad_norm": 0.04611044377088547,
      "learning_rate": 0.0004672484508949998,
      "loss": 0.0618,
      "step": 1340
    },
    {
      "epoch": 0.33184855233853006,
      "grad_norm": 0.05941253527998924,
      "learning_rate": 0.00046720028862361185,
      "loss": 0.0322,
      "step": 1341
    },
    {
      "epoch": 0.33209601583766396,
      "grad_norm": 0.0412849523127079,
      "learning_rate": 0.0004671520934520775,
      "loss": 0.0648,
      "step": 1342
    },
    {
      "epoch": 0.3323434793367978,
      "grad_norm": 0.02912323921918869,
      "learning_rate": 0.0004671038653876969,
      "loss": 0.0287,
      "step": 1343
    },
    {
      "epoch": 0.3325909428359317,
      "grad_norm": 0.03399490565061569,
      "learning_rate": 0.00046705560443777546,
      "loss": 0.0581,
      "step": 1344
    },
    {
      "epoch": 0.33283840633506556,
      "grad_norm": 0.07107515633106232,
      "learning_rate": 0.00046700731060962344,
      "loss": 0.0678,
      "step": 1345
    },
    {
      "epoch": 0.33308586983419947,
      "grad_norm": 0.037299204617738724,
      "learning_rate": 0.0004669589839105559,
      "loss": 0.0498,
      "step": 1346
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.027745919302105904,
      "learning_rate": 0.0004669106243478934,
      "loss": 0.0435,
      "step": 1347
    },
    {
      "epoch": 0.3335807968324672,
      "grad_norm": 0.03714684024453163,
      "learning_rate": 0.00046686223192896086,
      "loss": 0.0631,
      "step": 1348
    },
    {
      "epoch": 0.33382826033160107,
      "grad_norm": 0.026753278449177742,
      "learning_rate": 0.0004668138066610886,
      "loss": 0.0347,
      "step": 1349
    },
    {
      "epoch": 0.33407572383073497,
      "grad_norm": 0.02452712133526802,
      "learning_rate": 0.0004667653485516118,
      "loss": 0.0454,
      "step": 1350
    },
    {
      "epoch": 0.3343231873298688,
      "grad_norm": 0.05509383976459503,
      "learning_rate": 0.0004667168576078704,
      "loss": 0.0301,
      "step": 1351
    },
    {
      "epoch": 0.3345706508290027,
      "grad_norm": 0.1667395830154419,
      "learning_rate": 0.0004666683338372098,
      "loss": 0.0709,
      "step": 1352
    },
    {
      "epoch": 0.3348181143281366,
      "grad_norm": 0.025775404646992683,
      "learning_rate": 0.0004666197772469798,
      "loss": 0.0214,
      "step": 1353
    },
    {
      "epoch": 0.3350655778272705,
      "grad_norm": 0.054619450122117996,
      "learning_rate": 0.00046657118784453565,
      "loss": 0.0687,
      "step": 1354
    },
    {
      "epoch": 0.3353130413264044,
      "grad_norm": 0.0449351891875267,
      "learning_rate": 0.0004665225656372373,
      "loss": 0.0524,
      "step": 1355
    },
    {
      "epoch": 0.33556050482553823,
      "grad_norm": 0.07971183210611343,
      "learning_rate": 0.00046647391063244967,
      "loss": 0.1109,
      "step": 1356
    },
    {
      "epoch": 0.33580796832467213,
      "grad_norm": 0.028900127857923508,
      "learning_rate": 0.0004664252228375428,
      "loss": 0.0602,
      "step": 1357
    },
    {
      "epoch": 0.336055431823806,
      "grad_norm": 0.0692451074719429,
      "learning_rate": 0.00046637650225989156,
      "loss": 0.219,
      "step": 1358
    },
    {
      "epoch": 0.3363028953229399,
      "grad_norm": 0.06894471496343613,
      "learning_rate": 0.0004663277489068759,
      "loss": 0.0528,
      "step": 1359
    },
    {
      "epoch": 0.33655035882207374,
      "grad_norm": 0.05558827146887779,
      "learning_rate": 0.0004662789627858807,
      "loss": 0.0436,
      "step": 1360
    },
    {
      "epoch": 0.33679782232120764,
      "grad_norm": 0.027232397347688675,
      "learning_rate": 0.00046623014390429564,
      "loss": 0.0572,
      "step": 1361
    },
    {
      "epoch": 0.3370452858203415,
      "grad_norm": 0.0641203299164772,
      "learning_rate": 0.0004661812922695157,
      "loss": 0.0974,
      "step": 1362
    },
    {
      "epoch": 0.3372927493194754,
      "grad_norm": 0.025655558332800865,
      "learning_rate": 0.00046613240788894036,
      "loss": 0.0272,
      "step": 1363
    },
    {
      "epoch": 0.33754021281860924,
      "grad_norm": 0.043630778789520264,
      "learning_rate": 0.0004660834907699745,
      "loss": 0.0711,
      "step": 1364
    },
    {
      "epoch": 0.33778767631774315,
      "grad_norm": 0.08245021849870682,
      "learning_rate": 0.00046603454092002784,
      "loss": 0.082,
      "step": 1365
    },
    {
      "epoch": 0.338035139816877,
      "grad_norm": 0.03150539472699165,
      "learning_rate": 0.0004659855583465148,
      "loss": 0.0345,
      "step": 1366
    },
    {
      "epoch": 0.3382826033160109,
      "grad_norm": 0.037373386323451996,
      "learning_rate": 0.0004659365430568551,
      "loss": 0.0735,
      "step": 1367
    },
    {
      "epoch": 0.33853006681514475,
      "grad_norm": 0.03358493372797966,
      "learning_rate": 0.0004658874950584732,
      "loss": 0.0342,
      "step": 1368
    },
    {
      "epoch": 0.33877753031427865,
      "grad_norm": 0.02618616446852684,
      "learning_rate": 0.0004658384143587987,
      "loss": 0.0376,
      "step": 1369
    },
    {
      "epoch": 0.3390249938134125,
      "grad_norm": 0.021202342584729195,
      "learning_rate": 0.0004657893009652658,
      "loss": 0.0302,
      "step": 1370
    },
    {
      "epoch": 0.3392724573125464,
      "grad_norm": 0.031112050637602806,
      "learning_rate": 0.0004657401548853142,
      "loss": 0.0286,
      "step": 1371
    },
    {
      "epoch": 0.33951992081168025,
      "grad_norm": 0.04060618206858635,
      "learning_rate": 0.00046569097612638805,
      "loss": 0.049,
      "step": 1372
    },
    {
      "epoch": 0.33976738431081416,
      "grad_norm": 0.059217263013124466,
      "learning_rate": 0.00046564176469593665,
      "loss": 0.077,
      "step": 1373
    },
    {
      "epoch": 0.340014847809948,
      "grad_norm": 0.03464436158537865,
      "learning_rate": 0.0004655925206014143,
      "loss": 0.0642,
      "step": 1374
    },
    {
      "epoch": 0.3402623113090819,
      "grad_norm": 0.06915514171123505,
      "learning_rate": 0.00046554324385028015,
      "loss": 0.1034,
      "step": 1375
    },
    {
      "epoch": 0.3405097748082158,
      "grad_norm": 0.030862130224704742,
      "learning_rate": 0.0004654939344499983,
      "loss": 0.0457,
      "step": 1376
    },
    {
      "epoch": 0.34075723830734966,
      "grad_norm": 0.05728255957365036,
      "learning_rate": 0.00046544459240803796,
      "loss": 0.0802,
      "step": 1377
    },
    {
      "epoch": 0.34100470180648357,
      "grad_norm": 0.04007703810930252,
      "learning_rate": 0.00046539521773187296,
      "loss": 0.0232,
      "step": 1378
    },
    {
      "epoch": 0.3412521653056174,
      "grad_norm": 0.026257559657096863,
      "learning_rate": 0.0004653458104289825,
      "loss": 0.0341,
      "step": 1379
    },
    {
      "epoch": 0.3414996288047513,
      "grad_norm": 0.0380224846303463,
      "learning_rate": 0.00046529637050685027,
      "loss": 0.0532,
      "step": 1380
    },
    {
      "epoch": 0.34174709230388517,
      "grad_norm": 0.03849608451128006,
      "learning_rate": 0.00046524689797296524,
      "loss": 0.0501,
      "step": 1381
    },
    {
      "epoch": 0.3419945558030191,
      "grad_norm": 0.022179601714015007,
      "learning_rate": 0.0004651973928348212,
      "loss": 0.0339,
      "step": 1382
    },
    {
      "epoch": 0.3422420193021529,
      "grad_norm": 0.060493893921375275,
      "learning_rate": 0.0004651478550999169,
      "loss": 0.089,
      "step": 1383
    },
    {
      "epoch": 0.3424894828012868,
      "grad_norm": 0.033598367124795914,
      "learning_rate": 0.00046509828477575587,
      "loss": 0.0439,
      "step": 1384
    },
    {
      "epoch": 0.3427369463004207,
      "grad_norm": 0.039444949477910995,
      "learning_rate": 0.00046504868186984685,
      "loss": 0.0512,
      "step": 1385
    },
    {
      "epoch": 0.3429844097995546,
      "grad_norm": 0.048672255128622055,
      "learning_rate": 0.00046499904638970334,
      "loss": 0.0823,
      "step": 1386
    },
    {
      "epoch": 0.34323187329868843,
      "grad_norm": 0.03613617643713951,
      "learning_rate": 0.0004649493783428439,
      "loss": 0.052,
      "step": 1387
    },
    {
      "epoch": 0.34347933679782233,
      "grad_norm": 0.03294938802719116,
      "learning_rate": 0.00046489967773679177,
      "loss": 0.0325,
      "step": 1388
    },
    {
      "epoch": 0.3437268002969562,
      "grad_norm": 0.028508076444268227,
      "learning_rate": 0.00046484994457907537,
      "loss": 0.0465,
      "step": 1389
    },
    {
      "epoch": 0.3439742637960901,
      "grad_norm": 0.04262472689151764,
      "learning_rate": 0.000464800178877228,
      "loss": 0.0517,
      "step": 1390
    },
    {
      "epoch": 0.34422172729522393,
      "grad_norm": 0.03184610605239868,
      "learning_rate": 0.00046475038063878784,
      "loss": 0.0421,
      "step": 1391
    },
    {
      "epoch": 0.34446919079435784,
      "grad_norm": 0.05433206632733345,
      "learning_rate": 0.00046470054987129793,
      "loss": 0.0521,
      "step": 1392
    },
    {
      "epoch": 0.3447166542934917,
      "grad_norm": 0.045734044164419174,
      "learning_rate": 0.0004646506865823064,
      "loss": 0.0883,
      "step": 1393
    },
    {
      "epoch": 0.3449641177926256,
      "grad_norm": 0.050216540694236755,
      "learning_rate": 0.00046460079077936627,
      "loss": 0.0473,
      "step": 1394
    },
    {
      "epoch": 0.34521158129175944,
      "grad_norm": 0.08182378113269806,
      "learning_rate": 0.00046455086247003543,
      "loss": 0.0911,
      "step": 1395
    },
    {
      "epoch": 0.34545904479089334,
      "grad_norm": 0.10845150053501129,
      "learning_rate": 0.00046450090166187663,
      "loss": 0.0834,
      "step": 1396
    },
    {
      "epoch": 0.34570650829002725,
      "grad_norm": 0.06624971330165863,
      "learning_rate": 0.0004644509083624577,
      "loss": 0.0751,
      "step": 1397
    },
    {
      "epoch": 0.3459539717891611,
      "grad_norm": 0.04487022012472153,
      "learning_rate": 0.0004644008825793513,
      "loss": 0.0543,
      "step": 1398
    },
    {
      "epoch": 0.346201435288295,
      "grad_norm": 0.0407293327152729,
      "learning_rate": 0.000464350824320135,
      "loss": 0.0459,
      "step": 1399
    },
    {
      "epoch": 0.34644889878742885,
      "grad_norm": 0.04303690791130066,
      "learning_rate": 0.0004643007335923913,
      "loss": 0.0537,
      "step": 1400
    },
    {
      "epoch": 0.34644889878742885,
      "eval_loss": 0.2975853681564331,
      "eval_runtime": 202.5089,
      "eval_samples_per_second": 4.938,
      "eval_steps_per_second": 0.311,
      "step": 1400
    },
    {
      "epoch": 0.34669636228656275,
      "grad_norm": 0.038248542696237564,
      "learning_rate": 0.00046425061040370765,
      "loss": 0.0985,
      "step": 1401
    },
    {
      "epoch": 0.3469438257856966,
      "grad_norm": 0.04471159726381302,
      "learning_rate": 0.00046420045476167646,
      "loss": 0.0694,
      "step": 1402
    },
    {
      "epoch": 0.3471912892848305,
      "grad_norm": 0.03604034706950188,
      "learning_rate": 0.0004641502666738949,
      "loss": 0.0658,
      "step": 1403
    },
    {
      "epoch": 0.34743875278396436,
      "grad_norm": 0.031033487990498543,
      "learning_rate": 0.0004641000461479652,
      "loss": 0.0469,
      "step": 1404
    },
    {
      "epoch": 0.34768621628309826,
      "grad_norm": 0.029713299125432968,
      "learning_rate": 0.00046404979319149443,
      "loss": 0.0625,
      "step": 1405
    },
    {
      "epoch": 0.3479336797822321,
      "grad_norm": 0.03360898047685623,
      "learning_rate": 0.00046399950781209466,
      "loss": 0.0307,
      "step": 1406
    },
    {
      "epoch": 0.348181143281366,
      "grad_norm": 0.07369153201580048,
      "learning_rate": 0.00046394919001738265,
      "loss": 0.076,
      "step": 1407
    },
    {
      "epoch": 0.34842860678049986,
      "grad_norm": 0.02050301991403103,
      "learning_rate": 0.0004638988398149804,
      "loss": 0.0321,
      "step": 1408
    },
    {
      "epoch": 0.34867607027963377,
      "grad_norm": 0.034851692616939545,
      "learning_rate": 0.0004638484572125146,
      "loss": 0.0683,
      "step": 1409
    },
    {
      "epoch": 0.3489235337787676,
      "grad_norm": 0.040595535188913345,
      "learning_rate": 0.0004637980422176168,
      "loss": 0.0555,
      "step": 1410
    },
    {
      "epoch": 0.3491709972779015,
      "grad_norm": 0.02645382657647133,
      "learning_rate": 0.00046374759483792363,
      "loss": 0.0574,
      "step": 1411
    },
    {
      "epoch": 0.34941846077703537,
      "grad_norm": 0.03181375190615654,
      "learning_rate": 0.0004636971150810765,
      "loss": 0.0375,
      "step": 1412
    },
    {
      "epoch": 0.34966592427616927,
      "grad_norm": 0.05203579366207123,
      "learning_rate": 0.0004636466029547218,
      "loss": 0.0536,
      "step": 1413
    },
    {
      "epoch": 0.3499133877753031,
      "grad_norm": 0.01987956091761589,
      "learning_rate": 0.0004635960584665108,
      "loss": 0.0269,
      "step": 1414
    },
    {
      "epoch": 0.350160851274437,
      "grad_norm": 0.07069307565689087,
      "learning_rate": 0.0004635454816240997,
      "loss": 0.137,
      "step": 1415
    },
    {
      "epoch": 0.3504083147735709,
      "grad_norm": 0.054447490721940994,
      "learning_rate": 0.00046349487243514946,
      "loss": 0.0737,
      "step": 1416
    },
    {
      "epoch": 0.3506557782727048,
      "grad_norm": 0.03232378885149956,
      "learning_rate": 0.0004634442309073261,
      "loss": 0.0382,
      "step": 1417
    },
    {
      "epoch": 0.3509032417718387,
      "grad_norm": 0.08941502869129181,
      "learning_rate": 0.00046339355704830054,
      "loss": 0.1041,
      "step": 1418
    },
    {
      "epoch": 0.35115070527097253,
      "grad_norm": 0.029298897832632065,
      "learning_rate": 0.00046334285086574846,
      "loss": 0.0358,
      "step": 1419
    },
    {
      "epoch": 0.35139816877010643,
      "grad_norm": 0.056230392307043076,
      "learning_rate": 0.0004632921123673506,
      "loss": 0.0817,
      "step": 1420
    },
    {
      "epoch": 0.3516456322692403,
      "grad_norm": 0.03190644830465317,
      "learning_rate": 0.0004632413415607924,
      "loss": 0.0278,
      "step": 1421
    },
    {
      "epoch": 0.3518930957683742,
      "grad_norm": 0.03770790621638298,
      "learning_rate": 0.00046319053845376437,
      "loss": 0.0369,
      "step": 1422
    },
    {
      "epoch": 0.35214055926750804,
      "grad_norm": 0.03847837448120117,
      "learning_rate": 0.0004631397030539618,
      "loss": 0.0939,
      "step": 1423
    },
    {
      "epoch": 0.35238802276664194,
      "grad_norm": 0.02666189894080162,
      "learning_rate": 0.00046308883536908495,
      "loss": 0.0424,
      "step": 1424
    },
    {
      "epoch": 0.3526354862657758,
      "grad_norm": 0.04274183511734009,
      "learning_rate": 0.00046303793540683903,
      "loss": 0.0433,
      "step": 1425
    },
    {
      "epoch": 0.3528829497649097,
      "grad_norm": 0.05336254462599754,
      "learning_rate": 0.00046298700317493394,
      "loss": 0.0757,
      "step": 1426
    },
    {
      "epoch": 0.35313041326404354,
      "grad_norm": 0.05294838175177574,
      "learning_rate": 0.0004629360386810846,
      "loss": 0.0724,
      "step": 1427
    },
    {
      "epoch": 0.35337787676317745,
      "grad_norm": 0.02816077508032322,
      "learning_rate": 0.0004628850419330108,
      "loss": 0.0448,
      "step": 1428
    },
    {
      "epoch": 0.3536253402623113,
      "grad_norm": 0.043360091745853424,
      "learning_rate": 0.0004628340129384372,
      "loss": 0.0359,
      "step": 1429
    },
    {
      "epoch": 0.3538728037614452,
      "grad_norm": 0.03679283708333969,
      "learning_rate": 0.00046278295170509333,
      "loss": 0.0331,
      "step": 1430
    },
    {
      "epoch": 0.35412026726057905,
      "grad_norm": 0.032651666551828384,
      "learning_rate": 0.0004627318582407137,
      "loss": 0.0533,
      "step": 1431
    },
    {
      "epoch": 0.35436773075971295,
      "grad_norm": 0.06273818761110306,
      "learning_rate": 0.00046268073255303763,
      "loss": 0.0811,
      "step": 1432
    },
    {
      "epoch": 0.3546151942588468,
      "grad_norm": 0.03435244411230087,
      "learning_rate": 0.0004626295746498093,
      "loss": 0.0506,
      "step": 1433
    },
    {
      "epoch": 0.3548626577579807,
      "grad_norm": 0.05667796730995178,
      "learning_rate": 0.00046257838453877767,
      "loss": 0.0675,
      "step": 1434
    },
    {
      "epoch": 0.35511012125711455,
      "grad_norm": 0.03786773234605789,
      "learning_rate": 0.00046252716222769673,
      "loss": 0.0405,
      "step": 1435
    },
    {
      "epoch": 0.35535758475624846,
      "grad_norm": 0.034184589982032776,
      "learning_rate": 0.00046247590772432556,
      "loss": 0.0766,
      "step": 1436
    },
    {
      "epoch": 0.3556050482553823,
      "grad_norm": 0.03928103670477867,
      "learning_rate": 0.00046242462103642765,
      "loss": 0.043,
      "step": 1437
    },
    {
      "epoch": 0.3558525117545162,
      "grad_norm": 0.0637170746922493,
      "learning_rate": 0.00046237330217177157,
      "loss": 0.0377,
      "step": 1438
    },
    {
      "epoch": 0.3560999752536501,
      "grad_norm": 0.035966817289590836,
      "learning_rate": 0.0004623219511381309,
      "loss": 0.0443,
      "step": 1439
    },
    {
      "epoch": 0.35634743875278396,
      "grad_norm": 0.03209102526307106,
      "learning_rate": 0.0004622705679432839,
      "loss": 0.0472,
      "step": 1440
    },
    {
      "epoch": 0.35659490225191787,
      "grad_norm": 0.021716222167015076,
      "learning_rate": 0.0004622191525950139,
      "loss": 0.0217,
      "step": 1441
    },
    {
      "epoch": 0.3568423657510517,
      "grad_norm": 0.0260007344186306,
      "learning_rate": 0.0004621677051011088,
      "loss": 0.0263,
      "step": 1442
    },
    {
      "epoch": 0.3570898292501856,
      "grad_norm": 0.029217906296253204,
      "learning_rate": 0.0004621162254693616,
      "loss": 0.0374,
      "step": 1443
    },
    {
      "epoch": 0.35733729274931947,
      "grad_norm": 0.03827065974473953,
      "learning_rate": 0.00046206471370757013,
      "loss": 0.0225,
      "step": 1444
    },
    {
      "epoch": 0.3575847562484534,
      "grad_norm": 0.01772734522819519,
      "learning_rate": 0.0004620131698235371,
      "loss": 0.0307,
      "step": 1445
    },
    {
      "epoch": 0.3578322197475872,
      "grad_norm": 0.020462917163968086,
      "learning_rate": 0.0004619615938250701,
      "loss": 0.0162,
      "step": 1446
    },
    {
      "epoch": 0.3580796832467211,
      "grad_norm": 0.057611048221588135,
      "learning_rate": 0.00046190998571998137,
      "loss": 0.1013,
      "step": 1447
    },
    {
      "epoch": 0.358327146745855,
      "grad_norm": 0.05505860224366188,
      "learning_rate": 0.00046185834551608834,
      "loss": 0.0559,
      "step": 1448
    },
    {
      "epoch": 0.3585746102449889,
      "grad_norm": 0.04798067361116409,
      "learning_rate": 0.00046180667322121303,
      "loss": 0.047,
      "step": 1449
    },
    {
      "epoch": 0.3588220737441227,
      "grad_norm": 0.034787438809871674,
      "learning_rate": 0.00046175496884318264,
      "loss": 0.0442,
      "step": 1450
    },
    {
      "epoch": 0.35906953724325663,
      "grad_norm": 0.049238696694374084,
      "learning_rate": 0.0004617032323898288,
      "loss": 0.0448,
      "step": 1451
    },
    {
      "epoch": 0.3593170007423905,
      "grad_norm": 0.04129740968346596,
      "learning_rate": 0.0004616514638689883,
      "loss": 0.0369,
      "step": 1452
    },
    {
      "epoch": 0.3595644642415244,
      "grad_norm": 0.030357925221323967,
      "learning_rate": 0.00046159966328850277,
      "loss": 0.0357,
      "step": 1453
    },
    {
      "epoch": 0.35981192774065823,
      "grad_norm": 0.07869821786880493,
      "learning_rate": 0.0004615478306562186,
      "loss": 0.117,
      "step": 1454
    },
    {
      "epoch": 0.36005939123979214,
      "grad_norm": 0.045680247247219086,
      "learning_rate": 0.000461495965979987,
      "loss": 0.0839,
      "step": 1455
    },
    {
      "epoch": 0.360306854738926,
      "grad_norm": 0.06434957683086395,
      "learning_rate": 0.00046144406926766423,
      "loss": 0.06,
      "step": 1456
    },
    {
      "epoch": 0.3605543182380599,
      "grad_norm": 0.033145226538181305,
      "learning_rate": 0.00046139214052711125,
      "loss": 0.0818,
      "step": 1457
    },
    {
      "epoch": 0.36080178173719374,
      "grad_norm": 0.043621983379125595,
      "learning_rate": 0.0004613401797661939,
      "loss": 0.0457,
      "step": 1458
    },
    {
      "epoch": 0.36104924523632764,
      "grad_norm": 0.03747528791427612,
      "learning_rate": 0.0004612881869927828,
      "loss": 0.0438,
      "step": 1459
    },
    {
      "epoch": 0.36129670873546155,
      "grad_norm": 0.049138233065605164,
      "learning_rate": 0.0004612361622147536,
      "loss": 0.0876,
      "step": 1460
    },
    {
      "epoch": 0.3615441722345954,
      "grad_norm": 0.06390661001205444,
      "learning_rate": 0.00046118410543998656,
      "loss": 0.084,
      "step": 1461
    },
    {
      "epoch": 0.3617916357337293,
      "grad_norm": 0.059443458914756775,
      "learning_rate": 0.00046113201667636705,
      "loss": 0.078,
      "step": 1462
    },
    {
      "epoch": 0.36203909923286315,
      "grad_norm": 0.03237001970410347,
      "learning_rate": 0.00046107989593178507,
      "loss": 0.031,
      "step": 1463
    },
    {
      "epoch": 0.36228656273199705,
      "grad_norm": 0.063001349568367,
      "learning_rate": 0.00046102774321413556,
      "loss": 0.0638,
      "step": 1464
    },
    {
      "epoch": 0.3625340262311309,
      "grad_norm": 0.03722161799669266,
      "learning_rate": 0.0004609755585313183,
      "loss": 0.0587,
      "step": 1465
    },
    {
      "epoch": 0.3627814897302648,
      "grad_norm": 0.025602657347917557,
      "learning_rate": 0.00046092334189123783,
      "loss": 0.0412,
      "step": 1466
    },
    {
      "epoch": 0.36302895322939865,
      "grad_norm": 0.0462835431098938,
      "learning_rate": 0.00046087109330180385,
      "loss": 0.0302,
      "step": 1467
    },
    {
      "epoch": 0.36327641672853256,
      "grad_norm": 0.03326404094696045,
      "learning_rate": 0.0004608188127709303,
      "loss": 0.0579,
      "step": 1468
    },
    {
      "epoch": 0.3635238802276664,
      "grad_norm": 0.06775584071874619,
      "learning_rate": 0.00046076650030653653,
      "loss": 0.0668,
      "step": 1469
    },
    {
      "epoch": 0.3637713437268003,
      "grad_norm": 0.05717561021447182,
      "learning_rate": 0.0004607141559165465,
      "loss": 0.0907,
      "step": 1470
    },
    {
      "epoch": 0.36401880722593416,
      "grad_norm": 0.032914068549871445,
      "learning_rate": 0.0004606617796088889,
      "loss": 0.0365,
      "step": 1471
    },
    {
      "epoch": 0.36426627072506806,
      "grad_norm": 0.02866286039352417,
      "learning_rate": 0.0004606093713914975,
      "loss": 0.0466,
      "step": 1472
    },
    {
      "epoch": 0.3645137342242019,
      "grad_norm": 0.041885338723659515,
      "learning_rate": 0.00046055693127231066,
      "loss": 0.063,
      "step": 1473
    },
    {
      "epoch": 0.3647611977233358,
      "grad_norm": 0.037843577563762665,
      "learning_rate": 0.00046050445925927176,
      "loss": 0.0955,
      "step": 1474
    },
    {
      "epoch": 0.36500866122246967,
      "grad_norm": 0.048823222517967224,
      "learning_rate": 0.00046045195536032884,
      "loss": 0.0742,
      "step": 1475
    },
    {
      "epoch": 0.36525612472160357,
      "grad_norm": 0.03708822652697563,
      "learning_rate": 0.00046039941958343495,
      "loss": 0.019,
      "step": 1476
    },
    {
      "epoch": 0.3655035882207374,
      "grad_norm": 0.05533289909362793,
      "learning_rate": 0.00046034685193654793,
      "loss": 0.0677,
      "step": 1477
    },
    {
      "epoch": 0.3657510517198713,
      "grad_norm": 0.055000875145196915,
      "learning_rate": 0.0004602942524276302,
      "loss": 0.0595,
      "step": 1478
    },
    {
      "epoch": 0.36599851521900517,
      "grad_norm": 0.043779145926237106,
      "learning_rate": 0.0004602416210646494,
      "loss": 0.0708,
      "step": 1479
    },
    {
      "epoch": 0.3662459787181391,
      "grad_norm": 0.034823738038539886,
      "learning_rate": 0.0004601889578555777,
      "loss": 0.0525,
      "step": 1480
    },
    {
      "epoch": 0.366493442217273,
      "grad_norm": 0.03948301821947098,
      "learning_rate": 0.0004601362628083923,
      "loss": 0.0556,
      "step": 1481
    },
    {
      "epoch": 0.36674090571640683,
      "grad_norm": 0.034405868500471115,
      "learning_rate": 0.000460083535931075,
      "loss": 0.0826,
      "step": 1482
    },
    {
      "epoch": 0.36698836921554073,
      "grad_norm": 0.03400416299700737,
      "learning_rate": 0.00046003077723161256,
      "loss": 0.0733,
      "step": 1483
    },
    {
      "epoch": 0.3672358327146746,
      "grad_norm": 0.048847854137420654,
      "learning_rate": 0.00045997798671799657,
      "loss": 0.0598,
      "step": 1484
    },
    {
      "epoch": 0.3674832962138085,
      "grad_norm": 0.045982666313648224,
      "learning_rate": 0.0004599251643982234,
      "loss": 0.0752,
      "step": 1485
    },
    {
      "epoch": 0.36773075971294233,
      "grad_norm": 0.04908423125743866,
      "learning_rate": 0.0004598723102802943,
      "loss": 0.069,
      "step": 1486
    },
    {
      "epoch": 0.36797822321207624,
      "grad_norm": 0.04359033331274986,
      "learning_rate": 0.00045981942437221514,
      "loss": 0.0515,
      "step": 1487
    },
    {
      "epoch": 0.3682256867112101,
      "grad_norm": 0.021228892728686333,
      "learning_rate": 0.0004597665066819969,
      "loss": 0.0367,
      "step": 1488
    },
    {
      "epoch": 0.368473150210344,
      "grad_norm": 0.061880022287368774,
      "learning_rate": 0.00045971355721765515,
      "loss": 0.1522,
      "step": 1489
    },
    {
      "epoch": 0.36872061370947784,
      "grad_norm": 0.03228962421417236,
      "learning_rate": 0.0004596605759872103,
      "loss": 0.0381,
      "step": 1490
    },
    {
      "epoch": 0.36896807720861174,
      "grad_norm": 0.0622982457280159,
      "learning_rate": 0.00045960756299868777,
      "loss": 0.0927,
      "step": 1491
    },
    {
      "epoch": 0.3692155407077456,
      "grad_norm": 0.03642505034804344,
      "learning_rate": 0.0004595545182601175,
      "loss": 0.0338,
      "step": 1492
    },
    {
      "epoch": 0.3694630042068795,
      "grad_norm": 0.032125309109687805,
      "learning_rate": 0.00045950144177953436,
      "loss": 0.0349,
      "step": 1493
    },
    {
      "epoch": 0.36971046770601335,
      "grad_norm": 0.040534935891628265,
      "learning_rate": 0.00045944833356497815,
      "loss": 0.0795,
      "step": 1494
    },
    {
      "epoch": 0.36995793120514725,
      "grad_norm": 0.023873837664723396,
      "learning_rate": 0.0004593951936244933,
      "loss": 0.0325,
      "step": 1495
    },
    {
      "epoch": 0.3702053947042811,
      "grad_norm": 0.04716481640934944,
      "learning_rate": 0.0004593420219661291,
      "loss": 0.0526,
      "step": 1496
    },
    {
      "epoch": 0.370452858203415,
      "grad_norm": 0.033838994801044464,
      "learning_rate": 0.0004592888185979398,
      "loss": 0.0345,
      "step": 1497
    },
    {
      "epoch": 0.37070032170254885,
      "grad_norm": 0.026369353756308556,
      "learning_rate": 0.00045923558352798413,
      "loss": 0.0311,
      "step": 1498
    },
    {
      "epoch": 0.37094778520168276,
      "grad_norm": 0.039893850684165955,
      "learning_rate": 0.0004591823167643259,
      "loss": 0.053,
      "step": 1499
    },
    {
      "epoch": 0.3711952487008166,
      "grad_norm": 0.02312236838042736,
      "learning_rate": 0.00045912901831503364,
      "loss": 0.0365,
      "step": 1500
    },
    {
      "epoch": 0.3714427121999505,
      "grad_norm": 0.07989959418773651,
      "learning_rate": 0.00045907568818818063,
      "loss": 0.0599,
      "step": 1501
    },
    {
      "epoch": 0.37169017569908436,
      "grad_norm": 0.03598722442984581,
      "learning_rate": 0.00045902232639184504,
      "loss": 0.0417,
      "step": 1502
    },
    {
      "epoch": 0.37193763919821826,
      "grad_norm": 0.03212342783808708,
      "learning_rate": 0.00045896893293410966,
      "loss": 0.036,
      "step": 1503
    },
    {
      "epoch": 0.37218510269735217,
      "grad_norm": 0.057937636971473694,
      "learning_rate": 0.00045891550782306236,
      "loss": 0.1049,
      "step": 1504
    },
    {
      "epoch": 0.372432566196486,
      "grad_norm": 0.03039935603737831,
      "learning_rate": 0.0004588620510667956,
      "loss": 0.0427,
      "step": 1505
    },
    {
      "epoch": 0.3726800296956199,
      "grad_norm": 0.038538575172424316,
      "learning_rate": 0.0004588085626734065,
      "loss": 0.0628,
      "step": 1506
    },
    {
      "epoch": 0.37292749319475377,
      "grad_norm": 0.037734176963567734,
      "learning_rate": 0.00045875504265099735,
      "loss": 0.0481,
      "step": 1507
    },
    {
      "epoch": 0.37317495669388767,
      "grad_norm": 0.039172884076833725,
      "learning_rate": 0.000458701491007675,
      "loss": 0.0539,
      "step": 1508
    },
    {
      "epoch": 0.3734224201930215,
      "grad_norm": 0.04482900723814964,
      "learning_rate": 0.0004586479077515511,
      "loss": 0.0729,
      "step": 1509
    },
    {
      "epoch": 0.3736698836921554,
      "grad_norm": 0.04243960976600647,
      "learning_rate": 0.00045859429289074204,
      "loss": 0.0353,
      "step": 1510
    },
    {
      "epoch": 0.3739173471912893,
      "grad_norm": 0.04486459866166115,
      "learning_rate": 0.00045854064643336914,
      "loss": 0.061,
      "step": 1511
    },
    {
      "epoch": 0.3741648106904232,
      "grad_norm": 0.06273699551820755,
      "learning_rate": 0.00045848696838755843,
      "loss": 0.0919,
      "step": 1512
    },
    {
      "epoch": 0.374412274189557,
      "grad_norm": 0.03508767485618591,
      "learning_rate": 0.0004584332587614407,
      "loss": 0.0556,
      "step": 1513
    },
    {
      "epoch": 0.37465973768869093,
      "grad_norm": 0.02919071726500988,
      "learning_rate": 0.00045837951756315154,
      "loss": 0.0428,
      "step": 1514
    },
    {
      "epoch": 0.3749072011878248,
      "grad_norm": 0.03303908556699753,
      "learning_rate": 0.00045832574480083135,
      "loss": 0.0483,
      "step": 1515
    },
    {
      "epoch": 0.3751546646869587,
      "grad_norm": 0.0574757382273674,
      "learning_rate": 0.0004582719404826253,
      "loss": 0.0961,
      "step": 1516
    },
    {
      "epoch": 0.37540212818609253,
      "grad_norm": 0.03632793202996254,
      "learning_rate": 0.00045821810461668335,
      "loss": 0.075,
      "step": 1517
    },
    {
      "epoch": 0.37564959168522644,
      "grad_norm": 0.038866907358169556,
      "learning_rate": 0.0004581642372111601,
      "loss": 0.0405,
      "step": 1518
    },
    {
      "epoch": 0.3758970551843603,
      "grad_norm": 0.0390121191740036,
      "learning_rate": 0.00045811033827421524,
      "loss": 0.0599,
      "step": 1519
    },
    {
      "epoch": 0.3761445186834942,
      "grad_norm": 0.028600303456187248,
      "learning_rate": 0.0004580564078140129,
      "loss": 0.0457,
      "step": 1520
    },
    {
      "epoch": 0.37639198218262804,
      "grad_norm": 0.03461066260933876,
      "learning_rate": 0.0004580024458387223,
      "loss": 0.0491,
      "step": 1521
    },
    {
      "epoch": 0.37663944568176194,
      "grad_norm": 0.02875789999961853,
      "learning_rate": 0.000457948452356517,
      "loss": 0.0356,
      "step": 1522
    },
    {
      "epoch": 0.3768869091808958,
      "grad_norm": 0.05591733753681183,
      "learning_rate": 0.00045789442737557577,
      "loss": 0.0369,
      "step": 1523
    },
    {
      "epoch": 0.3771343726800297,
      "grad_norm": 0.03859303891658783,
      "learning_rate": 0.00045784037090408196,
      "loss": 0.036,
      "step": 1524
    },
    {
      "epoch": 0.3773818361791636,
      "grad_norm": 0.02534044161438942,
      "learning_rate": 0.00045778628295022375,
      "loss": 0.0567,
      "step": 1525
    },
    {
      "epoch": 0.37762929967829745,
      "grad_norm": 0.03150267153978348,
      "learning_rate": 0.0004577321635221939,
      "loss": 0.0397,
      "step": 1526
    },
    {
      "epoch": 0.37787676317743135,
      "grad_norm": 0.03317630663514137,
      "learning_rate": 0.00045767801262819026,
      "loss": 0.0355,
      "step": 1527
    },
    {
      "epoch": 0.3781242266765652,
      "grad_norm": 0.03594323247671127,
      "learning_rate": 0.0004576238302764152,
      "loss": 0.043,
      "step": 1528
    },
    {
      "epoch": 0.3783716901756991,
      "grad_norm": 0.03247012943029404,
      "learning_rate": 0.0004575696164750758,
      "loss": 0.0425,
      "step": 1529
    },
    {
      "epoch": 0.37861915367483295,
      "grad_norm": 0.05016377568244934,
      "learning_rate": 0.00045751537123238416,
      "loss": 0.0267,
      "step": 1530
    },
    {
      "epoch": 0.37886661717396686,
      "grad_norm": 0.027687950059771538,
      "learning_rate": 0.000457461094556557,
      "loss": 0.0427,
      "step": 1531
    },
    {
      "epoch": 0.3791140806731007,
      "grad_norm": 0.03875179588794708,
      "learning_rate": 0.0004574067864558159,
      "loss": 0.0506,
      "step": 1532
    },
    {
      "epoch": 0.3793615441722346,
      "grad_norm": 0.057172033935785294,
      "learning_rate": 0.00045735244693838697,
      "loss": 0.0498,
      "step": 1533
    },
    {
      "epoch": 0.37960900767136846,
      "grad_norm": 0.054222434759140015,
      "learning_rate": 0.0004572980760125013,
      "loss": 0.0452,
      "step": 1534
    },
    {
      "epoch": 0.37985647117050236,
      "grad_norm": 0.054989416152238846,
      "learning_rate": 0.0004572436736863945,
      "loss": 0.072,
      "step": 1535
    },
    {
      "epoch": 0.3801039346696362,
      "grad_norm": 0.06873750686645508,
      "learning_rate": 0.0004571892399683073,
      "loss": 0.0434,
      "step": 1536
    },
    {
      "epoch": 0.3803513981687701,
      "grad_norm": 0.04942247271537781,
      "learning_rate": 0.00045713477486648493,
      "loss": 0.053,
      "step": 1537
    },
    {
      "epoch": 0.38059886166790396,
      "grad_norm": 0.03872327879071236,
      "learning_rate": 0.0004570802783891774,
      "loss": 0.0449,
      "step": 1538
    },
    {
      "epoch": 0.38084632516703787,
      "grad_norm": 0.04259702190756798,
      "learning_rate": 0.00045702575054463947,
      "loss": 0.0618,
      "step": 1539
    },
    {
      "epoch": 0.3810937886661717,
      "grad_norm": 0.04818853735923767,
      "learning_rate": 0.00045697119134113063,
      "loss": 0.0356,
      "step": 1540
    },
    {
      "epoch": 0.3813412521653056,
      "grad_norm": 0.038487084209918976,
      "learning_rate": 0.0004569166007869153,
      "loss": 0.0467,
      "step": 1541
    },
    {
      "epoch": 0.38158871566443947,
      "grad_norm": 0.024600399658083916,
      "learning_rate": 0.0004568619788902625,
      "loss": 0.0421,
      "step": 1542
    },
    {
      "epoch": 0.3818361791635734,
      "grad_norm": 0.04939143359661102,
      "learning_rate": 0.00045680732565944596,
      "loss": 0.0493,
      "step": 1543
    },
    {
      "epoch": 0.3820836426627072,
      "grad_norm": 0.042271699756383896,
      "learning_rate": 0.0004567526411027442,
      "loss": 0.0702,
      "step": 1544
    },
    {
      "epoch": 0.3823311061618411,
      "grad_norm": 0.06662287563085556,
      "learning_rate": 0.0004566979252284405,
      "loss": 0.1085,
      "step": 1545
    },
    {
      "epoch": 0.38257856966097503,
      "grad_norm": 0.04134234040975571,
      "learning_rate": 0.00045664317804482293,
      "loss": 0.0726,
      "step": 1546
    },
    {
      "epoch": 0.3828260331601089,
      "grad_norm": 0.05654194951057434,
      "learning_rate": 0.00045658839956018416,
      "loss": 0.0633,
      "step": 1547
    },
    {
      "epoch": 0.3830734966592428,
      "grad_norm": 0.053499411791563034,
      "learning_rate": 0.0004565335897828218,
      "loss": 0.0782,
      "step": 1548
    },
    {
      "epoch": 0.38332096015837663,
      "grad_norm": 0.05184087157249451,
      "learning_rate": 0.0004564787487210381,
      "loss": 0.0659,
      "step": 1549
    },
    {
      "epoch": 0.38356842365751054,
      "grad_norm": 0.024071751162409782,
      "learning_rate": 0.0004564238763831399,
      "loss": 0.0438,
      "step": 1550
    },
    {
      "epoch": 0.3838158871566444,
      "grad_norm": 0.07546935230493546,
      "learning_rate": 0.0004563689727774391,
      "loss": 0.0544,
      "step": 1551
    },
    {
      "epoch": 0.3840633506557783,
      "grad_norm": 0.060296714305877686,
      "learning_rate": 0.000456314037912252,
      "loss": 0.0727,
      "step": 1552
    },
    {
      "epoch": 0.38431081415491214,
      "grad_norm": 0.04366768151521683,
      "learning_rate": 0.0004562590717958999,
      "loss": 0.049,
      "step": 1553
    },
    {
      "epoch": 0.38455827765404604,
      "grad_norm": 0.03223446011543274,
      "learning_rate": 0.0004562040744367086,
      "loss": 0.0335,
      "step": 1554
    },
    {
      "epoch": 0.3848057411531799,
      "grad_norm": 0.07496137917041779,
      "learning_rate": 0.0004561490458430089,
      "loss": 0.092,
      "step": 1555
    },
    {
      "epoch": 0.3850532046523138,
      "grad_norm": 0.033860113471746445,
      "learning_rate": 0.00045609398602313616,
      "loss": 0.0778,
      "step": 1556
    },
    {
      "epoch": 0.38530066815144765,
      "grad_norm": 0.050653889775276184,
      "learning_rate": 0.00045603889498543047,
      "loss": 0.0622,
      "step": 1557
    },
    {
      "epoch": 0.38554813165058155,
      "grad_norm": 0.05984831973910332,
      "learning_rate": 0.0004559837727382366,
      "loss": 0.0824,
      "step": 1558
    },
    {
      "epoch": 0.3857955951497154,
      "grad_norm": 0.04015592485666275,
      "learning_rate": 0.00045592861928990427,
      "loss": 0.0626,
      "step": 1559
    },
    {
      "epoch": 0.3860430586488493,
      "grad_norm": 0.054983191192150116,
      "learning_rate": 0.0004558734346487876,
      "loss": 0.0603,
      "step": 1560
    },
    {
      "epoch": 0.38629052214798315,
      "grad_norm": 0.06597530096769333,
      "learning_rate": 0.0004558182188232458,
      "loss": 0.0586,
      "step": 1561
    },
    {
      "epoch": 0.38653798564711705,
      "grad_norm": 0.03578454256057739,
      "learning_rate": 0.0004557629718216426,
      "loss": 0.0728,
      "step": 1562
    },
    {
      "epoch": 0.3867854491462509,
      "grad_norm": 0.03526166453957558,
      "learning_rate": 0.0004557076936523463,
      "loss": 0.0293,
      "step": 1563
    },
    {
      "epoch": 0.3870329126453848,
      "grad_norm": 0.03640303015708923,
      "learning_rate": 0.00045565238432373036,
      "loss": 0.0466,
      "step": 1564
    },
    {
      "epoch": 0.38728037614451866,
      "grad_norm": 0.019909000024199486,
      "learning_rate": 0.00045559704384417246,
      "loss": 0.0317,
      "step": 1565
    },
    {
      "epoch": 0.38752783964365256,
      "grad_norm": 0.03982921317219734,
      "learning_rate": 0.0004555416722220552,
      "loss": 0.0655,
      "step": 1566
    },
    {
      "epoch": 0.38777530314278646,
      "grad_norm": 0.022080356255173683,
      "learning_rate": 0.0004554862694657662,
      "loss": 0.0302,
      "step": 1567
    },
    {
      "epoch": 0.3880227666419203,
      "grad_norm": 0.03714044392108917,
      "learning_rate": 0.0004554308355836973,
      "loss": 0.0395,
      "step": 1568
    },
    {
      "epoch": 0.3882702301410542,
      "grad_norm": 0.016739727929234505,
      "learning_rate": 0.00045537537058424536,
      "loss": 0.0368,
      "step": 1569
    },
    {
      "epoch": 0.38851769364018807,
      "grad_norm": 0.043264999985694885,
      "learning_rate": 0.0004553198744758118,
      "loss": 0.0436,
      "step": 1570
    },
    {
      "epoch": 0.38876515713932197,
      "grad_norm": 0.052197426557540894,
      "learning_rate": 0.00045526434726680297,
      "loss": 0.067,
      "step": 1571
    },
    {
      "epoch": 0.3890126206384558,
      "grad_norm": 0.03329094499349594,
      "learning_rate": 0.00045520878896562967,
      "loss": 0.0536,
      "step": 1572
    },
    {
      "epoch": 0.3892600841375897,
      "grad_norm": 0.05103154852986336,
      "learning_rate": 0.0004551531995807075,
      "loss": 0.0701,
      "step": 1573
    },
    {
      "epoch": 0.3895075476367236,
      "grad_norm": 0.05815420672297478,
      "learning_rate": 0.0004550975791204569,
      "loss": 0.07,
      "step": 1574
    },
    {
      "epoch": 0.3897550111358575,
      "grad_norm": 0.021298181265592575,
      "learning_rate": 0.00045504192759330284,
      "loss": 0.0336,
      "step": 1575
    },
    {
      "epoch": 0.3900024746349913,
      "grad_norm": 0.03151661902666092,
      "learning_rate": 0.00045498624500767506,
      "loss": 0.0405,
      "step": 1576
    },
    {
      "epoch": 0.39024993813412523,
      "grad_norm": 0.059094201773405075,
      "learning_rate": 0.000454930531372008,
      "loss": 0.0328,
      "step": 1577
    },
    {
      "epoch": 0.3904974016332591,
      "grad_norm": 0.04076641425490379,
      "learning_rate": 0.0004548747866947409,
      "loss": 0.0531,
      "step": 1578
    },
    {
      "epoch": 0.390744865132393,
      "grad_norm": 0.02553986757993698,
      "learning_rate": 0.0004548190109843176,
      "loss": 0.0386,
      "step": 1579
    },
    {
      "epoch": 0.39099232863152683,
      "grad_norm": 0.03754463791847229,
      "learning_rate": 0.0004547632042491866,
      "loss": 0.0602,
      "step": 1580
    },
    {
      "epoch": 0.39123979213066074,
      "grad_norm": 0.07608073204755783,
      "learning_rate": 0.0004547073664978012,
      "loss": 0.1174,
      "step": 1581
    },
    {
      "epoch": 0.3914872556297946,
      "grad_norm": 0.031147465109825134,
      "learning_rate": 0.00045465149773861924,
      "loss": 0.0487,
      "step": 1582
    },
    {
      "epoch": 0.3917347191289285,
      "grad_norm": 0.063947394490242,
      "learning_rate": 0.00045459559798010355,
      "loss": 0.0899,
      "step": 1583
    },
    {
      "epoch": 0.39198218262806234,
      "grad_norm": 0.04135735705494881,
      "learning_rate": 0.00045453966723072136,
      "loss": 0.1047,
      "step": 1584
    },
    {
      "epoch": 0.39222964612719624,
      "grad_norm": 0.041877225041389465,
      "learning_rate": 0.00045448370549894474,
      "loss": 0.0517,
      "step": 1585
    },
    {
      "epoch": 0.3924771096263301,
      "grad_norm": 0.03843871131539345,
      "learning_rate": 0.00045442771279325045,
      "loss": 0.0408,
      "step": 1586
    },
    {
      "epoch": 0.392724573125464,
      "grad_norm": 0.02766934409737587,
      "learning_rate": 0.0004543716891221199,
      "loss": 0.0402,
      "step": 1587
    },
    {
      "epoch": 0.3929720366245979,
      "grad_norm": 0.06916366517543793,
      "learning_rate": 0.0004543156344940392,
      "loss": 0.0273,
      "step": 1588
    },
    {
      "epoch": 0.39321950012373175,
      "grad_norm": 0.032452233135700226,
      "learning_rate": 0.0004542595489174991,
      "loss": 0.0661,
      "step": 1589
    },
    {
      "epoch": 0.39346696362286565,
      "grad_norm": 0.02838195115327835,
      "learning_rate": 0.0004542034324009952,
      "loss": 0.0408,
      "step": 1590
    },
    {
      "epoch": 0.3937144271219995,
      "grad_norm": 0.06483595818281174,
      "learning_rate": 0.0004541472849530276,
      "loss": 0.0731,
      "step": 1591
    },
    {
      "epoch": 0.3939618906211334,
      "grad_norm": 0.039882026612758636,
      "learning_rate": 0.0004540911065821012,
      "loss": 0.058,
      "step": 1592
    },
    {
      "epoch": 0.39420935412026725,
      "grad_norm": 0.038807936012744904,
      "learning_rate": 0.0004540348972967256,
      "loss": 0.0546,
      "step": 1593
    },
    {
      "epoch": 0.39445681761940116,
      "grad_norm": 0.038990139961242676,
      "learning_rate": 0.00045397865710541496,
      "loss": 0.052,
      "step": 1594
    },
    {
      "epoch": 0.394704281118535,
      "grad_norm": 0.03395192697644234,
      "learning_rate": 0.0004539223860166882,
      "loss": 0.0381,
      "step": 1595
    },
    {
      "epoch": 0.3949517446176689,
      "grad_norm": 0.031050659716129303,
      "learning_rate": 0.0004538660840390689,
      "loss": 0.0267,
      "step": 1596
    },
    {
      "epoch": 0.39519920811680276,
      "grad_norm": 0.043691787868738174,
      "learning_rate": 0.0004538097511810854,
      "loss": 0.0715,
      "step": 1597
    },
    {
      "epoch": 0.39544667161593666,
      "grad_norm": 0.04355253279209137,
      "learning_rate": 0.0004537533874512706,
      "loss": 0.0815,
      "step": 1598
    },
    {
      "epoch": 0.3956941351150705,
      "grad_norm": 0.08850701153278351,
      "learning_rate": 0.0004536969928581621,
      "loss": 0.106,
      "step": 1599
    },
    {
      "epoch": 0.3959415986142044,
      "grad_norm": 0.04762542247772217,
      "learning_rate": 0.00045364056741030223,
      "loss": 0.0478,
      "step": 1600
    },
    {
      "epoch": 0.3959415986142044,
      "eval_loss": 0.2979142665863037,
      "eval_runtime": 202.6134,
      "eval_samples_per_second": 4.936,
      "eval_steps_per_second": 0.311,
      "step": 1600
    },
    {
      "epoch": 0.39618906211333826,
      "grad_norm": 0.054913606494665146,
      "learning_rate": 0.000453584111116238,
      "loss": 0.0557,
      "step": 1601
    },
    {
      "epoch": 0.39643652561247217,
      "grad_norm": 0.025884361937642097,
      "learning_rate": 0.000453527623984521,
      "loss": 0.0184,
      "step": 1602
    },
    {
      "epoch": 0.396683989111606,
      "grad_norm": 0.045158784836530685,
      "learning_rate": 0.00045347110602370766,
      "loss": 0.0601,
      "step": 1603
    },
    {
      "epoch": 0.3969314526107399,
      "grad_norm": 0.023785900324583054,
      "learning_rate": 0.00045341455724235883,
      "loss": 0.0525,
      "step": 1604
    },
    {
      "epoch": 0.39717891610987377,
      "grad_norm": 0.025935031473636627,
      "learning_rate": 0.00045335797764904016,
      "loss": 0.042,
      "step": 1605
    },
    {
      "epoch": 0.3974263796090077,
      "grad_norm": 0.03951292857527733,
      "learning_rate": 0.00045330136725232217,
      "loss": 0.0474,
      "step": 1606
    },
    {
      "epoch": 0.3976738431081415,
      "grad_norm": 0.03809816762804985,
      "learning_rate": 0.00045324472606077964,
      "loss": 0.0314,
      "step": 1607
    },
    {
      "epoch": 0.3979213066072754,
      "grad_norm": 0.03227398544549942,
      "learning_rate": 0.00045318805408299236,
      "loss": 0.0347,
      "step": 1608
    },
    {
      "epoch": 0.39816877010640933,
      "grad_norm": 0.027904480695724487,
      "learning_rate": 0.0004531313513275446,
      "loss": 0.0481,
      "step": 1609
    },
    {
      "epoch": 0.3984162336055432,
      "grad_norm": 0.030743954703211784,
      "learning_rate": 0.00045307461780302524,
      "loss": 0.0551,
      "step": 1610
    },
    {
      "epoch": 0.3986636971046771,
      "grad_norm": 0.03498222306370735,
      "learning_rate": 0.00045301785351802807,
      "loss": 0.036,
      "step": 1611
    },
    {
      "epoch": 0.39891116060381093,
      "grad_norm": 0.05574759840965271,
      "learning_rate": 0.00045296105848115133,
      "loss": 0.0815,
      "step": 1612
    },
    {
      "epoch": 0.39915862410294484,
      "grad_norm": 0.02399527095258236,
      "learning_rate": 0.00045290423270099797,
      "loss": 0.0327,
      "step": 1613
    },
    {
      "epoch": 0.3994060876020787,
      "grad_norm": 0.0378793329000473,
      "learning_rate": 0.00045284737618617556,
      "loss": 0.0373,
      "step": 1614
    },
    {
      "epoch": 0.3996535511012126,
      "grad_norm": 0.03689330443739891,
      "learning_rate": 0.00045279048894529653,
      "loss": 0.0325,
      "step": 1615
    },
    {
      "epoch": 0.39990101460034644,
      "grad_norm": 0.04220909625291824,
      "learning_rate": 0.0004527335709869777,
      "loss": 0.0398,
      "step": 1616
    },
    {
      "epoch": 0.40014847809948034,
      "grad_norm": 0.028799090534448624,
      "learning_rate": 0.0004526766223198405,
      "loss": 0.0764,
      "step": 1617
    },
    {
      "epoch": 0.4003959415986142,
      "grad_norm": 0.03117304854094982,
      "learning_rate": 0.00045261964295251146,
      "loss": 0.0505,
      "step": 1618
    },
    {
      "epoch": 0.4006434050977481,
      "grad_norm": 0.03606468439102173,
      "learning_rate": 0.0004525626328936213,
      "loss": 0.0273,
      "step": 1619
    },
    {
      "epoch": 0.40089086859688194,
      "grad_norm": 0.03178463876247406,
      "learning_rate": 0.00045250559215180553,
      "loss": 0.0473,
      "step": 1620
    },
    {
      "epoch": 0.40113833209601585,
      "grad_norm": 0.03786403685808182,
      "learning_rate": 0.00045244852073570433,
      "loss": 0.0531,
      "step": 1621
    },
    {
      "epoch": 0.4013857955951497,
      "grad_norm": 0.07242023199796677,
      "learning_rate": 0.00045239141865396253,
      "loss": 0.0832,
      "step": 1622
    },
    {
      "epoch": 0.4016332590942836,
      "grad_norm": 0.060668934136629105,
      "learning_rate": 0.0004523342859152297,
      "loss": 0.0717,
      "step": 1623
    },
    {
      "epoch": 0.40188072259341745,
      "grad_norm": 0.05211395025253296,
      "learning_rate": 0.00045227712252815977,
      "loss": 0.0941,
      "step": 1624
    },
    {
      "epoch": 0.40212818609255135,
      "grad_norm": 0.03916749730706215,
      "learning_rate": 0.0004522199285014117,
      "loss": 0.072,
      "step": 1625
    },
    {
      "epoch": 0.4023756495916852,
      "grad_norm": 0.03419079631567001,
      "learning_rate": 0.00045216270384364866,
      "loss": 0.0355,
      "step": 1626
    },
    {
      "epoch": 0.4026231130908191,
      "grad_norm": 0.05202687159180641,
      "learning_rate": 0.00045210544856353886,
      "loss": 0.0485,
      "step": 1627
    },
    {
      "epoch": 0.40287057658995296,
      "grad_norm": 0.028712227940559387,
      "learning_rate": 0.00045204816266975487,
      "loss": 0.0503,
      "step": 1628
    },
    {
      "epoch": 0.40311804008908686,
      "grad_norm": 0.024667244404554367,
      "learning_rate": 0.0004519908461709741,
      "loss": 0.0384,
      "step": 1629
    },
    {
      "epoch": 0.40336550358822076,
      "grad_norm": 0.02151973359286785,
      "learning_rate": 0.00045193349907587835,
      "loss": 0.0219,
      "step": 1630
    },
    {
      "epoch": 0.4036129670873546,
      "grad_norm": 0.040565282106399536,
      "learning_rate": 0.00045187612139315435,
      "loss": 0.1032,
      "step": 1631
    },
    {
      "epoch": 0.4038604305864885,
      "grad_norm": 0.02122357115149498,
      "learning_rate": 0.00045181871313149325,
      "loss": 0.0393,
      "step": 1632
    },
    {
      "epoch": 0.40410789408562237,
      "grad_norm": 0.034852392971515656,
      "learning_rate": 0.00045176127429959087,
      "loss": 0.0458,
      "step": 1633
    },
    {
      "epoch": 0.40435535758475627,
      "grad_norm": 0.03879976272583008,
      "learning_rate": 0.0004517038049061477,
      "loss": 0.0401,
      "step": 1634
    },
    {
      "epoch": 0.4046028210838901,
      "grad_norm": 0.056937672197818756,
      "learning_rate": 0.00045164630495986893,
      "loss": 0.1384,
      "step": 1635
    },
    {
      "epoch": 0.404850284583024,
      "grad_norm": 0.03275345265865326,
      "learning_rate": 0.0004515887744694641,
      "loss": 0.0683,
      "step": 1636
    },
    {
      "epoch": 0.40509774808215787,
      "grad_norm": 0.027456238865852356,
      "learning_rate": 0.0004515312134436478,
      "loss": 0.0499,
      "step": 1637
    },
    {
      "epoch": 0.4053452115812918,
      "grad_norm": 0.033895343542099,
      "learning_rate": 0.00045147362189113893,
      "loss": 0.0225,
      "step": 1638
    },
    {
      "epoch": 0.4055926750804256,
      "grad_norm": 0.06764096766710281,
      "learning_rate": 0.00045141599982066104,
      "loss": 0.061,
      "step": 1639
    },
    {
      "epoch": 0.40584013857955953,
      "grad_norm": 0.028294632211327553,
      "learning_rate": 0.0004513583472409424,
      "loss": 0.051,
      "step": 1640
    },
    {
      "epoch": 0.4060876020786934,
      "grad_norm": 0.06568778306245804,
      "learning_rate": 0.00045130066416071586,
      "loss": 0.0541,
      "step": 1641
    },
    {
      "epoch": 0.4063350655778273,
      "grad_norm": 0.03377172350883484,
      "learning_rate": 0.0004512429505887189,
      "loss": 0.0585,
      "step": 1642
    },
    {
      "epoch": 0.40658252907696113,
      "grad_norm": 0.031768959015607834,
      "learning_rate": 0.00045118520653369363,
      "loss": 0.0789,
      "step": 1643
    },
    {
      "epoch": 0.40682999257609503,
      "grad_norm": 0.05486054718494415,
      "learning_rate": 0.00045112743200438664,
      "loss": 0.0579,
      "step": 1644
    },
    {
      "epoch": 0.4070774560752289,
      "grad_norm": 0.03935960680246353,
      "learning_rate": 0.00045106962700954944,
      "loss": 0.053,
      "step": 1645
    },
    {
      "epoch": 0.4073249195743628,
      "grad_norm": 0.03433839976787567,
      "learning_rate": 0.0004510117915579379,
      "loss": 0.0451,
      "step": 1646
    },
    {
      "epoch": 0.40757238307349664,
      "grad_norm": 0.032561514526605606,
      "learning_rate": 0.0004509539256583126,
      "loss": 0.0437,
      "step": 1647
    },
    {
      "epoch": 0.40781984657263054,
      "grad_norm": 0.0756072998046875,
      "learning_rate": 0.0004508960293194386,
      "loss": 0.1369,
      "step": 1648
    },
    {
      "epoch": 0.4080673100717644,
      "grad_norm": 0.02835634909570217,
      "learning_rate": 0.00045083810255008573,
      "loss": 0.0318,
      "step": 1649
    },
    {
      "epoch": 0.4083147735708983,
      "grad_norm": 0.06311678141355515,
      "learning_rate": 0.00045078014535902843,
      "loss": 0.052,
      "step": 1650
    },
    {
      "epoch": 0.4085622370700322,
      "grad_norm": 0.0627971887588501,
      "learning_rate": 0.0004507221577550456,
      "loss": 0.056,
      "step": 1651
    },
    {
      "epoch": 0.40880970056916605,
      "grad_norm": 0.0599307157099247,
      "learning_rate": 0.00045066413974692094,
      "loss": 0.0786,
      "step": 1652
    },
    {
      "epoch": 0.40905716406829995,
      "grad_norm": 0.04749372974038124,
      "learning_rate": 0.0004506060913434426,
      "loss": 0.0784,
      "step": 1653
    },
    {
      "epoch": 0.4093046275674338,
      "grad_norm": 0.059111252427101135,
      "learning_rate": 0.0004505480125534034,
      "loss": 0.0613,
      "step": 1654
    },
    {
      "epoch": 0.4095520910665677,
      "grad_norm": 0.022552084177732468,
      "learning_rate": 0.00045048990338560074,
      "loss": 0.0477,
      "step": 1655
    },
    {
      "epoch": 0.40979955456570155,
      "grad_norm": 0.035413555800914764,
      "learning_rate": 0.00045043176384883666,
      "loss": 0.0598,
      "step": 1656
    },
    {
      "epoch": 0.41004701806483546,
      "grad_norm": 0.026731127873063087,
      "learning_rate": 0.0004503735939519178,
      "loss": 0.032,
      "step": 1657
    },
    {
      "epoch": 0.4102944815639693,
      "grad_norm": 0.0733117014169693,
      "learning_rate": 0.00045031539370365525,
      "loss": 0.0719,
      "step": 1658
    },
    {
      "epoch": 0.4105419450631032,
      "grad_norm": 0.03821014240384102,
      "learning_rate": 0.00045025716311286503,
      "loss": 0.053,
      "step": 1659
    },
    {
      "epoch": 0.41078940856223706,
      "grad_norm": 0.06094880774617195,
      "learning_rate": 0.0004501989021883673,
      "loss": 0.1198,
      "step": 1660
    },
    {
      "epoch": 0.41103687206137096,
      "grad_norm": 0.03938421979546547,
      "learning_rate": 0.0004501406109389873,
      "loss": 0.0794,
      "step": 1661
    },
    {
      "epoch": 0.4112843355605048,
      "grad_norm": 0.051496561616659164,
      "learning_rate": 0.00045008228937355445,
      "loss": 0.0513,
      "step": 1662
    },
    {
      "epoch": 0.4115317990596387,
      "grad_norm": 0.08175363391637802,
      "learning_rate": 0.000450023937500903,
      "loss": 0.1065,
      "step": 1663
    },
    {
      "epoch": 0.41177926255877256,
      "grad_norm": 0.05106697604060173,
      "learning_rate": 0.0004499655553298717,
      "loss": 0.1073,
      "step": 1664
    },
    {
      "epoch": 0.41202672605790647,
      "grad_norm": 0.09619839489459991,
      "learning_rate": 0.000449907142869304,
      "loss": 0.1026,
      "step": 1665
    },
    {
      "epoch": 0.4122741895570403,
      "grad_norm": 0.041861895471811295,
      "learning_rate": 0.0004498487001280478,
      "loss": 0.0265,
      "step": 1666
    },
    {
      "epoch": 0.4125216530561742,
      "grad_norm": 0.04617034271359444,
      "learning_rate": 0.0004497902271149556,
      "loss": 0.0254,
      "step": 1667
    },
    {
      "epoch": 0.41276911655530807,
      "grad_norm": 0.046506647020578384,
      "learning_rate": 0.0004497317238388846,
      "loss": 0.0644,
      "step": 1668
    },
    {
      "epoch": 0.413016580054442,
      "grad_norm": 0.052655816078186035,
      "learning_rate": 0.00044967319030869647,
      "loss": 0.0836,
      "step": 1669
    },
    {
      "epoch": 0.4132640435535758,
      "grad_norm": 0.03189739212393761,
      "learning_rate": 0.0004496146265332575,
      "loss": 0.0435,
      "step": 1670
    },
    {
      "epoch": 0.4135115070527097,
      "grad_norm": 0.05654710903763771,
      "learning_rate": 0.0004495560325214386,
      "loss": 0.0623,
      "step": 1671
    },
    {
      "epoch": 0.41375897055184363,
      "grad_norm": 0.057956546545028687,
      "learning_rate": 0.0004494974082821153,
      "loss": 0.0501,
      "step": 1672
    },
    {
      "epoch": 0.4140064340509775,
      "grad_norm": 0.04144100472331047,
      "learning_rate": 0.00044943875382416745,
      "loss": 0.0597,
      "step": 1673
    },
    {
      "epoch": 0.4142538975501114,
      "grad_norm": 0.040950894355773926,
      "learning_rate": 0.0004493800691564798,
      "loss": 0.0632,
      "step": 1674
    },
    {
      "epoch": 0.41450136104924523,
      "grad_norm": 0.0485118068754673,
      "learning_rate": 0.0004493213542879414,
      "loss": 0.06,
      "step": 1675
    },
    {
      "epoch": 0.41474882454837914,
      "grad_norm": 0.06107670068740845,
      "learning_rate": 0.0004492626092274462,
      "loss": 0.1077,
      "step": 1676
    },
    {
      "epoch": 0.414996288047513,
      "grad_norm": 0.03856450319290161,
      "learning_rate": 0.00044920383398389243,
      "loss": 0.0589,
      "step": 1677
    },
    {
      "epoch": 0.4152437515466469,
      "grad_norm": 0.032733287662267685,
      "learning_rate": 0.00044914502856618297,
      "loss": 0.0453,
      "step": 1678
    },
    {
      "epoch": 0.41549121504578074,
      "grad_norm": 0.05882817506790161,
      "learning_rate": 0.00044908619298322534,
      "loss": 0.0485,
      "step": 1679
    },
    {
      "epoch": 0.41573867854491464,
      "grad_norm": 0.027494225651025772,
      "learning_rate": 0.00044902732724393156,
      "loss": 0.0368,
      "step": 1680
    },
    {
      "epoch": 0.4159861420440485,
      "grad_norm": 0.03250013664364815,
      "learning_rate": 0.00044896843135721834,
      "loss": 0.0364,
      "step": 1681
    },
    {
      "epoch": 0.4162336055431824,
      "grad_norm": 0.08410104364156723,
      "learning_rate": 0.00044890950533200676,
      "loss": 0.0932,
      "step": 1682
    },
    {
      "epoch": 0.41648106904231624,
      "grad_norm": 0.02342613786458969,
      "learning_rate": 0.00044885054917722256,
      "loss": 0.0442,
      "step": 1683
    },
    {
      "epoch": 0.41672853254145015,
      "grad_norm": 0.05534800887107849,
      "learning_rate": 0.0004487915629017961,
      "loss": 0.0518,
      "step": 1684
    },
    {
      "epoch": 0.416975996040584,
      "grad_norm": 0.04474809020757675,
      "learning_rate": 0.00044873254651466233,
      "loss": 0.0788,
      "step": 1685
    },
    {
      "epoch": 0.4172234595397179,
      "grad_norm": 0.04043223336338997,
      "learning_rate": 0.00044867350002476055,
      "loss": 0.0734,
      "step": 1686
    },
    {
      "epoch": 0.41747092303885175,
      "grad_norm": 0.03764128312468529,
      "learning_rate": 0.0004486144234410348,
      "loss": 0.0333,
      "step": 1687
    },
    {
      "epoch": 0.41771838653798565,
      "grad_norm": 0.04142044484615326,
      "learning_rate": 0.00044855531677243365,
      "loss": 0.0415,
      "step": 1688
    },
    {
      "epoch": 0.4179658500371195,
      "grad_norm": 0.03449353948235512,
      "learning_rate": 0.00044849618002791016,
      "loss": 0.0541,
      "step": 1689
    },
    {
      "epoch": 0.4182133135362534,
      "grad_norm": 0.07694303244352341,
      "learning_rate": 0.00044843701321642203,
      "loss": 0.0771,
      "step": 1690
    },
    {
      "epoch": 0.41846077703538725,
      "grad_norm": 0.031626950949430466,
      "learning_rate": 0.00044837781634693153,
      "loss": 0.0517,
      "step": 1691
    },
    {
      "epoch": 0.41870824053452116,
      "grad_norm": 0.07747171819210052,
      "learning_rate": 0.00044831858942840535,
      "loss": 0.1527,
      "step": 1692
    },
    {
      "epoch": 0.41895570403365506,
      "grad_norm": 0.025796521455049515,
      "learning_rate": 0.00044825933246981487,
      "loss": 0.0473,
      "step": 1693
    },
    {
      "epoch": 0.4192031675327889,
      "grad_norm": 0.043883997946977615,
      "learning_rate": 0.00044820004548013593,
      "loss": 0.0757,
      "step": 1694
    },
    {
      "epoch": 0.4194506310319228,
      "grad_norm": 0.06566605716943741,
      "learning_rate": 0.000448140728468349,
      "loss": 0.0664,
      "step": 1695
    },
    {
      "epoch": 0.41969809453105666,
      "grad_norm": 0.048939451575279236,
      "learning_rate": 0.000448081381443439,
      "loss": 0.0635,
      "step": 1696
    },
    {
      "epoch": 0.41994555803019057,
      "grad_norm": 0.019297849386930466,
      "learning_rate": 0.0004480220044143954,
      "loss": 0.0327,
      "step": 1697
    },
    {
      "epoch": 0.4201930215293244,
      "grad_norm": 0.03767958655953407,
      "learning_rate": 0.0004479625973902124,
      "loss": 0.0482,
      "step": 1698
    },
    {
      "epoch": 0.4204404850284583,
      "grad_norm": 0.0667043998837471,
      "learning_rate": 0.00044790316037988855,
      "loss": 0.0927,
      "step": 1699
    },
    {
      "epoch": 0.42068794852759217,
      "grad_norm": 0.03886428847908974,
      "learning_rate": 0.00044784369339242693,
      "loss": 0.0331,
      "step": 1700
    },
    {
      "epoch": 0.4209354120267261,
      "grad_norm": 0.032022830098867416,
      "learning_rate": 0.00044778419643683534,
      "loss": 0.0509,
      "step": 1701
    },
    {
      "epoch": 0.4211828755258599,
      "grad_norm": 0.04510028660297394,
      "learning_rate": 0.0004477246695221259,
      "loss": 0.035,
      "step": 1702
    },
    {
      "epoch": 0.4214303390249938,
      "grad_norm": 0.04960332065820694,
      "learning_rate": 0.0004476651126573154,
      "loss": 0.0507,
      "step": 1703
    },
    {
      "epoch": 0.4216778025241277,
      "grad_norm": 0.02529340423643589,
      "learning_rate": 0.00044760552585142524,
      "loss": 0.0189,
      "step": 1704
    },
    {
      "epoch": 0.4219252660232616,
      "grad_norm": 0.024601612240076065,
      "learning_rate": 0.00044754590911348115,
      "loss": 0.0269,
      "step": 1705
    },
    {
      "epoch": 0.42217272952239543,
      "grad_norm": 0.030673695728182793,
      "learning_rate": 0.0004474862624525136,
      "loss": 0.0589,
      "step": 1706
    },
    {
      "epoch": 0.42242019302152933,
      "grad_norm": 0.04216478765010834,
      "learning_rate": 0.0004474265858775573,
      "loss": 0.0503,
      "step": 1707
    },
    {
      "epoch": 0.4226676565206632,
      "grad_norm": 0.03637482970952988,
      "learning_rate": 0.00044736687939765185,
      "loss": 0.0719,
      "step": 1708
    },
    {
      "epoch": 0.4229151200197971,
      "grad_norm": 0.034484293311834335,
      "learning_rate": 0.00044730714302184126,
      "loss": 0.0549,
      "step": 1709
    },
    {
      "epoch": 0.42316258351893093,
      "grad_norm": 0.03833688795566559,
      "learning_rate": 0.00044724737675917394,
      "loss": 0.0618,
      "step": 1710
    },
    {
      "epoch": 0.42341004701806484,
      "grad_norm": 0.0246671624481678,
      "learning_rate": 0.0004471875806187029,
      "loss": 0.0137,
      "step": 1711
    },
    {
      "epoch": 0.4236575105171987,
      "grad_norm": 0.02875029481947422,
      "learning_rate": 0.00044712775460948575,
      "loss": 0.0211,
      "step": 1712
    },
    {
      "epoch": 0.4239049740163326,
      "grad_norm": 0.027439862489700317,
      "learning_rate": 0.0004470678987405844,
      "loss": 0.032,
      "step": 1713
    },
    {
      "epoch": 0.4241524375154665,
      "grad_norm": 0.034563321620225906,
      "learning_rate": 0.0004470080130210657,
      "loss": 0.0597,
      "step": 1714
    },
    {
      "epoch": 0.42439990101460034,
      "grad_norm": 0.031152527779340744,
      "learning_rate": 0.00044694809746000054,
      "loss": 0.045,
      "step": 1715
    },
    {
      "epoch": 0.42464736451373425,
      "grad_norm": 0.028217852115631104,
      "learning_rate": 0.0004468881520664647,
      "loss": 0.0293,
      "step": 1716
    },
    {
      "epoch": 0.4248948280128681,
      "grad_norm": 0.12735724449157715,
      "learning_rate": 0.00044682817684953824,
      "loss": 0.0983,
      "step": 1717
    },
    {
      "epoch": 0.425142291512002,
      "grad_norm": 0.029177095741033554,
      "learning_rate": 0.00044676817181830584,
      "loss": 0.0304,
      "step": 1718
    },
    {
      "epoch": 0.42538975501113585,
      "grad_norm": 0.052439089864492416,
      "learning_rate": 0.00044670813698185677,
      "loss": 0.0411,
      "step": 1719
    },
    {
      "epoch": 0.42563721851026975,
      "grad_norm": 0.03835904970765114,
      "learning_rate": 0.00044664807234928467,
      "loss": 0.0401,
      "step": 1720
    },
    {
      "epoch": 0.4258846820094036,
      "grad_norm": 0.03157978132367134,
      "learning_rate": 0.0004465879779296878,
      "loss": 0.0412,
      "step": 1721
    },
    {
      "epoch": 0.4261321455085375,
      "grad_norm": 0.05375409498810768,
      "learning_rate": 0.00044652785373216873,
      "loss": 0.0663,
      "step": 1722
    },
    {
      "epoch": 0.42637960900767136,
      "grad_norm": 0.03381757810711861,
      "learning_rate": 0.000446467699765835,
      "loss": 0.0545,
      "step": 1723
    },
    {
      "epoch": 0.42662707250680526,
      "grad_norm": 0.0231303907930851,
      "learning_rate": 0.00044640751603979814,
      "loss": 0.033,
      "step": 1724
    },
    {
      "epoch": 0.4268745360059391,
      "grad_norm": 0.06356119364500046,
      "learning_rate": 0.00044634730256317437,
      "loss": 0.0714,
      "step": 1725
    },
    {
      "epoch": 0.427121999505073,
      "grad_norm": 0.039043180644512177,
      "learning_rate": 0.0004462870593450846,
      "loss": 0.0569,
      "step": 1726
    },
    {
      "epoch": 0.42736946300420686,
      "grad_norm": 0.05653219297528267,
      "learning_rate": 0.00044622678639465396,
      "loss": 0.0746,
      "step": 1727
    },
    {
      "epoch": 0.42761692650334077,
      "grad_norm": 0.029692895710468292,
      "learning_rate": 0.0004461664837210123,
      "loss": 0.0493,
      "step": 1728
    },
    {
      "epoch": 0.4278643900024746,
      "grad_norm": 0.03306496888399124,
      "learning_rate": 0.0004461061513332939,
      "loss": 0.0448,
      "step": 1729
    },
    {
      "epoch": 0.4281118535016085,
      "grad_norm": 0.050757210701704025,
      "learning_rate": 0.0004460457892406376,
      "loss": 0.063,
      "step": 1730
    },
    {
      "epoch": 0.42835931700074237,
      "grad_norm": 0.050802890211343765,
      "learning_rate": 0.00044598539745218654,
      "loss": 0.0878,
      "step": 1731
    },
    {
      "epoch": 0.42860678049987627,
      "grad_norm": 0.02421065792441368,
      "learning_rate": 0.0004459249759770885,
      "loss": 0.0394,
      "step": 1732
    },
    {
      "epoch": 0.4288542439990101,
      "grad_norm": 0.04632658138871193,
      "learning_rate": 0.0004458645248244958,
      "loss": 0.0488,
      "step": 1733
    },
    {
      "epoch": 0.429101707498144,
      "grad_norm": 0.09965261071920395,
      "learning_rate": 0.0004458040440035652,
      "loss": 0.1318,
      "step": 1734
    },
    {
      "epoch": 0.42934917099727793,
      "grad_norm": 0.03745294734835625,
      "learning_rate": 0.00044574353352345796,
      "loss": 0.0595,
      "step": 1735
    },
    {
      "epoch": 0.4295966344964118,
      "grad_norm": 0.03613964468240738,
      "learning_rate": 0.00044568299339333984,
      "loss": 0.0659,
      "step": 1736
    },
    {
      "epoch": 0.4298440979955457,
      "grad_norm": 0.024291785433888435,
      "learning_rate": 0.0004456224236223811,
      "loss": 0.0402,
      "step": 1737
    },
    {
      "epoch": 0.43009156149467953,
      "grad_norm": 0.052680667489767075,
      "learning_rate": 0.00044556182421975633,
      "loss": 0.075,
      "step": 1738
    },
    {
      "epoch": 0.43033902499381343,
      "grad_norm": 0.03366902843117714,
      "learning_rate": 0.00044550119519464496,
      "loss": 0.0243,
      "step": 1739
    },
    {
      "epoch": 0.4305864884929473,
      "grad_norm": 0.07329244166612625,
      "learning_rate": 0.0004454405365562305,
      "loss": 0.0852,
      "step": 1740
    },
    {
      "epoch": 0.4308339519920812,
      "grad_norm": 0.03994995355606079,
      "learning_rate": 0.0004453798483137014,
      "loss": 0.0685,
      "step": 1741
    },
    {
      "epoch": 0.43108141549121504,
      "grad_norm": 0.01970359869301319,
      "learning_rate": 0.00044531913047625,
      "loss": 0.0259,
      "step": 1742
    },
    {
      "epoch": 0.43132887899034894,
      "grad_norm": 0.03822902962565422,
      "learning_rate": 0.0004452583830530737,
      "loss": 0.0459,
      "step": 1743
    },
    {
      "epoch": 0.4315763424894828,
      "grad_norm": 0.038634516298770905,
      "learning_rate": 0.0004451976060533741,
      "loss": 0.0435,
      "step": 1744
    },
    {
      "epoch": 0.4318238059886167,
      "grad_norm": 0.052942294627428055,
      "learning_rate": 0.0004451367994863572,
      "loss": 0.0292,
      "step": 1745
    },
    {
      "epoch": 0.43207126948775054,
      "grad_norm": 0.03080669976770878,
      "learning_rate": 0.0004450759633612338,
      "loss": 0.0394,
      "step": 1746
    },
    {
      "epoch": 0.43231873298688445,
      "grad_norm": 0.02996358461678028,
      "learning_rate": 0.0004450150976872188,
      "loss": 0.0502,
      "step": 1747
    },
    {
      "epoch": 0.4325661964860183,
      "grad_norm": 0.040638815611600876,
      "learning_rate": 0.00044495420247353193,
      "loss": 0.0507,
      "step": 1748
    },
    {
      "epoch": 0.4328136599851522,
      "grad_norm": 0.05630100145936012,
      "learning_rate": 0.000444893277729397,
      "loss": 0.0733,
      "step": 1749
    },
    {
      "epoch": 0.43306112348428605,
      "grad_norm": 0.07813364267349243,
      "learning_rate": 0.0004448323234640427,
      "loss": 0.0318,
      "step": 1750
    },
    {
      "epoch": 0.43330858698341995,
      "grad_norm": 0.08150067925453186,
      "learning_rate": 0.0004447713396867019,
      "loss": 0.0752,
      "step": 1751
    },
    {
      "epoch": 0.4335560504825538,
      "grad_norm": 0.0588349774479866,
      "learning_rate": 0.00044471032640661204,
      "loss": 0.0639,
      "step": 1752
    },
    {
      "epoch": 0.4338035139816877,
      "grad_norm": 0.09163033217191696,
      "learning_rate": 0.0004446492836330151,
      "loss": 0.1514,
      "step": 1753
    },
    {
      "epoch": 0.43405097748082155,
      "grad_norm": 0.03938716650009155,
      "learning_rate": 0.0004445882113751574,
      "loss": 0.0925,
      "step": 1754
    },
    {
      "epoch": 0.43429844097995546,
      "grad_norm": 0.036595579236745834,
      "learning_rate": 0.00044452710964228983,
      "loss": 0.0557,
      "step": 1755
    },
    {
      "epoch": 0.4345459044790893,
      "grad_norm": 0.051712967455387115,
      "learning_rate": 0.0004444659784436677,
      "loss": 0.0891,
      "step": 1756
    },
    {
      "epoch": 0.4347933679782232,
      "grad_norm": 0.029173726215958595,
      "learning_rate": 0.0004444048177885506,
      "loss": 0.0747,
      "step": 1757
    },
    {
      "epoch": 0.4350408314773571,
      "grad_norm": 0.030281497165560722,
      "learning_rate": 0.0004443436276862031,
      "loss": 0.0481,
      "step": 1758
    },
    {
      "epoch": 0.43528829497649096,
      "grad_norm": 0.04343949630856514,
      "learning_rate": 0.0004442824081458936,
      "loss": 0.0637,
      "step": 1759
    },
    {
      "epoch": 0.43553575847562487,
      "grad_norm": 0.04384608566761017,
      "learning_rate": 0.0004442211591768954,
      "loss": 0.0376,
      "step": 1760
    },
    {
      "epoch": 0.4357832219747587,
      "grad_norm": 0.05215676501393318,
      "learning_rate": 0.0004441598807884861,
      "loss": 0.0585,
      "step": 1761
    },
    {
      "epoch": 0.4360306854738926,
      "grad_norm": 0.05664438754320145,
      "learning_rate": 0.00044409857298994784,
      "loss": 0.113,
      "step": 1762
    },
    {
      "epoch": 0.43627814897302647,
      "grad_norm": 0.04009774699807167,
      "learning_rate": 0.00044403723579056697,
      "loss": 0.0473,
      "step": 1763
    },
    {
      "epoch": 0.4365256124721604,
      "grad_norm": 0.03912701457738876,
      "learning_rate": 0.00044397586919963457,
      "loss": 0.0517,
      "step": 1764
    },
    {
      "epoch": 0.4367730759712942,
      "grad_norm": 0.026889115571975708,
      "learning_rate": 0.00044391447322644605,
      "loss": 0.0727,
      "step": 1765
    },
    {
      "epoch": 0.4370205394704281,
      "grad_norm": 0.036013901233673096,
      "learning_rate": 0.00044385304788030125,
      "loss": 0.053,
      "step": 1766
    },
    {
      "epoch": 0.437268002969562,
      "grad_norm": 0.02739388309419155,
      "learning_rate": 0.0004437915931705046,
      "loss": 0.0387,
      "step": 1767
    },
    {
      "epoch": 0.4375154664686959,
      "grad_norm": 0.04765429347753525,
      "learning_rate": 0.0004437301091063648,
      "loss": 0.0287,
      "step": 1768
    },
    {
      "epoch": 0.43776292996782973,
      "grad_norm": 0.03413860872387886,
      "learning_rate": 0.0004436685956971951,
      "loss": 0.052,
      "step": 1769
    },
    {
      "epoch": 0.43801039346696363,
      "grad_norm": 0.03392384946346283,
      "learning_rate": 0.0004436070529523131,
      "loss": 0.0388,
      "step": 1770
    },
    {
      "epoch": 0.4382578569660975,
      "grad_norm": 0.05569560453295708,
      "learning_rate": 0.0004435454808810411,
      "loss": 0.0599,
      "step": 1771
    },
    {
      "epoch": 0.4385053204652314,
      "grad_norm": 0.04156000539660454,
      "learning_rate": 0.0004434838794927054,
      "loss": 0.0793,
      "step": 1772
    },
    {
      "epoch": 0.43875278396436523,
      "grad_norm": 0.02709355764091015,
      "learning_rate": 0.00044342224879663717,
      "loss": 0.0415,
      "step": 1773
    },
    {
      "epoch": 0.43900024746349914,
      "grad_norm": 0.022103700786828995,
      "learning_rate": 0.0004433605888021718,
      "loss": 0.0301,
      "step": 1774
    },
    {
      "epoch": 0.439247710962633,
      "grad_norm": 0.03298310190439224,
      "learning_rate": 0.0004432988995186491,
      "loss": 0.0365,
      "step": 1775
    },
    {
      "epoch": 0.4394951744617669,
      "grad_norm": 0.03221868723630905,
      "learning_rate": 0.00044323718095541344,
      "loss": 0.0376,
      "step": 1776
    },
    {
      "epoch": 0.43974263796090074,
      "grad_norm": 0.02791130170226097,
      "learning_rate": 0.00044317543312181356,
      "loss": 0.0436,
      "step": 1777
    },
    {
      "epoch": 0.43999010146003464,
      "grad_norm": 0.0472104549407959,
      "learning_rate": 0.00044311365602720264,
      "loss": 0.0698,
      "step": 1778
    },
    {
      "epoch": 0.44023756495916855,
      "grad_norm": 0.0250865388661623,
      "learning_rate": 0.0004430518496809383,
      "loss": 0.0336,
      "step": 1779
    },
    {
      "epoch": 0.4404850284583024,
      "grad_norm": 0.03544117882847786,
      "learning_rate": 0.0004429900140923826,
      "loss": 0.0747,
      "step": 1780
    },
    {
      "epoch": 0.4407324919574363,
      "grad_norm": 0.03908156603574753,
      "learning_rate": 0.0004429281492709019,
      "loss": 0.0736,
      "step": 1781
    },
    {
      "epoch": 0.44097995545657015,
      "grad_norm": 0.05530889704823494,
      "learning_rate": 0.0004428662552258672,
      "loss": 0.0524,
      "step": 1782
    },
    {
      "epoch": 0.44122741895570405,
      "grad_norm": 0.020430266857147217,
      "learning_rate": 0.0004428043319666538,
      "loss": 0.0293,
      "step": 1783
    },
    {
      "epoch": 0.4414748824548379,
      "grad_norm": 0.03275357931852341,
      "learning_rate": 0.0004427423795026414,
      "loss": 0.0814,
      "step": 1784
    },
    {
      "epoch": 0.4417223459539718,
      "grad_norm": 0.052867524325847626,
      "learning_rate": 0.00044268039784321425,
      "loss": 0.0618,
      "step": 1785
    },
    {
      "epoch": 0.44196980945310566,
      "grad_norm": 0.019431639462709427,
      "learning_rate": 0.0004426183869977609,
      "loss": 0.0195,
      "step": 1786
    },
    {
      "epoch": 0.44221727295223956,
      "grad_norm": 0.036421943455934525,
      "learning_rate": 0.0004425563469756744,
      "loss": 0.0503,
      "step": 1787
    },
    {
      "epoch": 0.4424647364513734,
      "grad_norm": 0.03602555766701698,
      "learning_rate": 0.00044249427778635217,
      "loss": 0.043,
      "step": 1788
    },
    {
      "epoch": 0.4427121999505073,
      "grad_norm": 0.0258667953312397,
      "learning_rate": 0.000442432179439196,
      "loss": 0.0349,
      "step": 1789
    },
    {
      "epoch": 0.44295966344964116,
      "grad_norm": 0.040985070168972015,
      "learning_rate": 0.0004423700519436123,
      "loss": 0.0812,
      "step": 1790
    },
    {
      "epoch": 0.44320712694877507,
      "grad_norm": 0.02707340195775032,
      "learning_rate": 0.0004423078953090116,
      "loss": 0.0163,
      "step": 1791
    },
    {
      "epoch": 0.4434545904479089,
      "grad_norm": 0.0475032739341259,
      "learning_rate": 0.00044224570954480916,
      "loss": 0.0311,
      "step": 1792
    },
    {
      "epoch": 0.4437020539470428,
      "grad_norm": 0.041897207498550415,
      "learning_rate": 0.00044218349466042435,
      "loss": 0.0553,
      "step": 1793
    },
    {
      "epoch": 0.44394951744617667,
      "grad_norm": 0.04201328381896019,
      "learning_rate": 0.00044212125066528114,
      "loss": 0.0279,
      "step": 1794
    },
    {
      "epoch": 0.44419698094531057,
      "grad_norm": 0.042588479816913605,
      "learning_rate": 0.00044205897756880786,
      "loss": 0.0547,
      "step": 1795
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.04271409660577774,
      "learning_rate": 0.0004419966753804373,
      "loss": 0.079,
      "step": 1796
    },
    {
      "epoch": 0.4446919079435783,
      "grad_norm": 0.04780026525259018,
      "learning_rate": 0.00044193434410960664,
      "loss": 0.079,
      "step": 1797
    },
    {
      "epoch": 0.4449393714427122,
      "grad_norm": 0.05676724761724472,
      "learning_rate": 0.0004418719837657572,
      "loss": 0.0472,
      "step": 1798
    },
    {
      "epoch": 0.4451868349418461,
      "grad_norm": 0.026332879438996315,
      "learning_rate": 0.0004418095943583352,
      "loss": 0.0501,
      "step": 1799
    },
    {
      "epoch": 0.44543429844098,
      "grad_norm": 0.052098773419857025,
      "learning_rate": 0.00044174717589679093,
      "loss": 0.0556,
      "step": 1800
    },
    {
      "epoch": 0.44543429844098,
      "eval_loss": 0.29727429151535034,
      "eval_runtime": 202.8187,
      "eval_samples_per_second": 4.931,
      "eval_steps_per_second": 0.311,
      "step": 1800
    },
    {
      "epoch": 0.44568176194011383,
      "grad_norm": 0.04028702154755592,
      "learning_rate": 0.00044168472839057905,
      "loss": 0.036,
      "step": 1801
    },
    {
      "epoch": 0.44592922543924773,
      "grad_norm": 0.0353110171854496,
      "learning_rate": 0.00044162225184915885,
      "loss": 0.048,
      "step": 1802
    },
    {
      "epoch": 0.4461766889383816,
      "grad_norm": 0.02203804813325405,
      "learning_rate": 0.0004415597462819938,
      "loss": 0.034,
      "step": 1803
    },
    {
      "epoch": 0.4464241524375155,
      "grad_norm": 0.025240963324904442,
      "learning_rate": 0.0004414972116985518,
      "loss": 0.0399,
      "step": 1804
    },
    {
      "epoch": 0.44667161593664934,
      "grad_norm": 0.05284411087632179,
      "learning_rate": 0.0004414346481083054,
      "loss": 0.1132,
      "step": 1805
    },
    {
      "epoch": 0.44691907943578324,
      "grad_norm": 0.045286789536476135,
      "learning_rate": 0.0004413720555207312,
      "loss": 0.0868,
      "step": 1806
    },
    {
      "epoch": 0.4471665429349171,
      "grad_norm": 0.05398479849100113,
      "learning_rate": 0.0004413094339453103,
      "loss": 0.0323,
      "step": 1807
    },
    {
      "epoch": 0.447414006434051,
      "grad_norm": 0.04198014736175537,
      "learning_rate": 0.0004412467833915283,
      "loss": 0.0488,
      "step": 1808
    },
    {
      "epoch": 0.44766146993318484,
      "grad_norm": 0.022805744782090187,
      "learning_rate": 0.0004411841038688751,
      "loss": 0.0236,
      "step": 1809
    },
    {
      "epoch": 0.44790893343231875,
      "grad_norm": 0.02284054458141327,
      "learning_rate": 0.00044112139538684506,
      "loss": 0.0547,
      "step": 1810
    },
    {
      "epoch": 0.4481563969314526,
      "grad_norm": 0.04908619076013565,
      "learning_rate": 0.0004410586579549367,
      "loss": 0.0252,
      "step": 1811
    },
    {
      "epoch": 0.4484038604305865,
      "grad_norm": 0.05346113070845604,
      "learning_rate": 0.00044099589158265326,
      "loss": 0.0891,
      "step": 1812
    },
    {
      "epoch": 0.44865132392972035,
      "grad_norm": 0.034021615982055664,
      "learning_rate": 0.0004409330962795021,
      "loss": 0.0442,
      "step": 1813
    },
    {
      "epoch": 0.44889878742885425,
      "grad_norm": 0.036081958562135696,
      "learning_rate": 0.0004408702720549952,
      "loss": 0.0903,
      "step": 1814
    },
    {
      "epoch": 0.4491462509279881,
      "grad_norm": 0.031682662665843964,
      "learning_rate": 0.0004408074189186486,
      "loss": 0.0534,
      "step": 1815
    },
    {
      "epoch": 0.449393714427122,
      "grad_norm": 0.037463679909706116,
      "learning_rate": 0.00044074453687998304,
      "loss": 0.0506,
      "step": 1816
    },
    {
      "epoch": 0.44964117792625585,
      "grad_norm": 0.04640612751245499,
      "learning_rate": 0.00044068162594852343,
      "loss": 0.1125,
      "step": 1817
    },
    {
      "epoch": 0.44988864142538976,
      "grad_norm": 0.033552903681993484,
      "learning_rate": 0.0004406186861337992,
      "loss": 0.0399,
      "step": 1818
    },
    {
      "epoch": 0.4501361049245236,
      "grad_norm": 0.04526421055197716,
      "learning_rate": 0.00044055571744534394,
      "loss": 0.0434,
      "step": 1819
    },
    {
      "epoch": 0.4503835684236575,
      "grad_norm": 0.028077688068151474,
      "learning_rate": 0.0004404927198926959,
      "loss": 0.0264,
      "step": 1820
    },
    {
      "epoch": 0.4506310319227914,
      "grad_norm": 0.031887222081422806,
      "learning_rate": 0.00044042969348539746,
      "loss": 0.0449,
      "step": 1821
    },
    {
      "epoch": 0.45087849542192526,
      "grad_norm": 0.04342690110206604,
      "learning_rate": 0.00044036663823299557,
      "loss": 0.0505,
      "step": 1822
    },
    {
      "epoch": 0.45112595892105917,
      "grad_norm": 0.0422578826546669,
      "learning_rate": 0.0004403035541450413,
      "loss": 0.0404,
      "step": 1823
    },
    {
      "epoch": 0.451373422420193,
      "grad_norm": 0.05116693302989006,
      "learning_rate": 0.00044024044123109035,
      "loss": 0.0439,
      "step": 1824
    },
    {
      "epoch": 0.4516208859193269,
      "grad_norm": 0.05221521854400635,
      "learning_rate": 0.0004401772995007026,
      "loss": 0.1046,
      "step": 1825
    },
    {
      "epoch": 0.45186834941846077,
      "grad_norm": 0.029760386794805527,
      "learning_rate": 0.0004401141289634425,
      "loss": 0.0376,
      "step": 1826
    },
    {
      "epoch": 0.4521158129175947,
      "grad_norm": 0.12094790488481522,
      "learning_rate": 0.0004400509296288786,
      "loss": 0.0789,
      "step": 1827
    },
    {
      "epoch": 0.4523632764167285,
      "grad_norm": 0.034252431243658066,
      "learning_rate": 0.00043998770150658397,
      "loss": 0.0498,
      "step": 1828
    },
    {
      "epoch": 0.4526107399158624,
      "grad_norm": 0.03272657096385956,
      "learning_rate": 0.000439924444606136,
      "loss": 0.0558,
      "step": 1829
    },
    {
      "epoch": 0.4528582034149963,
      "grad_norm": 0.03483806550502777,
      "learning_rate": 0.00043986115893711645,
      "loss": 0.0382,
      "step": 1830
    },
    {
      "epoch": 0.4531056669141302,
      "grad_norm": 0.03671817481517792,
      "learning_rate": 0.0004397978445091115,
      "loss": 0.0789,
      "step": 1831
    },
    {
      "epoch": 0.453353130413264,
      "grad_norm": 0.029345015063881874,
      "learning_rate": 0.00043973450133171155,
      "loss": 0.032,
      "step": 1832
    },
    {
      "epoch": 0.45360059391239793,
      "grad_norm": 0.030229439958930016,
      "learning_rate": 0.00043967112941451143,
      "loss": 0.0322,
      "step": 1833
    },
    {
      "epoch": 0.4538480574115318,
      "grad_norm": 0.04178820922970772,
      "learning_rate": 0.0004396077287671104,
      "loss": 0.0848,
      "step": 1834
    },
    {
      "epoch": 0.4540955209106657,
      "grad_norm": 0.0497957244515419,
      "learning_rate": 0.00043954429939911194,
      "loss": 0.0683,
      "step": 1835
    },
    {
      "epoch": 0.45434298440979953,
      "grad_norm": 0.04119003191590309,
      "learning_rate": 0.00043948084132012393,
      "loss": 0.0466,
      "step": 1836
    },
    {
      "epoch": 0.45459044790893344,
      "grad_norm": 0.0788239985704422,
      "learning_rate": 0.00043941735453975866,
      "loss": 0.0968,
      "step": 1837
    },
    {
      "epoch": 0.4548379114080673,
      "grad_norm": 0.02987748570740223,
      "learning_rate": 0.00043935383906763267,
      "loss": 0.0431,
      "step": 1838
    },
    {
      "epoch": 0.4550853749072012,
      "grad_norm": 0.04652520269155502,
      "learning_rate": 0.00043929029491336684,
      "loss": 0.0715,
      "step": 1839
    },
    {
      "epoch": 0.45533283840633504,
      "grad_norm": 0.027321984991431236,
      "learning_rate": 0.0004392267220865865,
      "loss": 0.0189,
      "step": 1840
    },
    {
      "epoch": 0.45558030190546894,
      "grad_norm": 0.09913316369056702,
      "learning_rate": 0.0004391631205969213,
      "loss": 0.0822,
      "step": 1841
    },
    {
      "epoch": 0.45582776540460285,
      "grad_norm": 0.04655221104621887,
      "learning_rate": 0.0004390994904540051,
      "loss": 0.0681,
      "step": 1842
    },
    {
      "epoch": 0.4560752289037367,
      "grad_norm": 0.030512988567352295,
      "learning_rate": 0.0004390358316674763,
      "loss": 0.0661,
      "step": 1843
    },
    {
      "epoch": 0.4563226924028706,
      "grad_norm": 0.02890121564269066,
      "learning_rate": 0.0004389721442469775,
      "loss": 0.0495,
      "step": 1844
    },
    {
      "epoch": 0.45657015590200445,
      "grad_norm": 0.026162242516875267,
      "learning_rate": 0.00043890842820215565,
      "loss": 0.0739,
      "step": 1845
    },
    {
      "epoch": 0.45681761940113835,
      "grad_norm": 0.030761227011680603,
      "learning_rate": 0.00043884468354266206,
      "loss": 0.042,
      "step": 1846
    },
    {
      "epoch": 0.4570650829002722,
      "grad_norm": 0.024185162037611008,
      "learning_rate": 0.0004387809102781524,
      "loss": 0.0339,
      "step": 1847
    },
    {
      "epoch": 0.4573125463994061,
      "grad_norm": 0.025060715153813362,
      "learning_rate": 0.0004387171084182866,
      "loss": 0.0502,
      "step": 1848
    },
    {
      "epoch": 0.45756000989853995,
      "grad_norm": 0.05590987205505371,
      "learning_rate": 0.00043865327797272904,
      "loss": 0.0539,
      "step": 1849
    },
    {
      "epoch": 0.45780747339767386,
      "grad_norm": 0.11669156700372696,
      "learning_rate": 0.00043858941895114827,
      "loss": 0.0626,
      "step": 1850
    },
    {
      "epoch": 0.4580549368968077,
      "grad_norm": 0.039642445743083954,
      "learning_rate": 0.00043852553136321736,
      "loss": 0.0605,
      "step": 1851
    },
    {
      "epoch": 0.4583024003959416,
      "grad_norm": 0.02469957433640957,
      "learning_rate": 0.0004384616152186135,
      "loss": 0.0482,
      "step": 1852
    },
    {
      "epoch": 0.45854986389507546,
      "grad_norm": 0.05987097695469856,
      "learning_rate": 0.0004383976705270184,
      "loss": 0.0658,
      "step": 1853
    },
    {
      "epoch": 0.45879732739420936,
      "grad_norm": 0.05271567031741142,
      "learning_rate": 0.000438333697298118,
      "loss": 0.057,
      "step": 1854
    },
    {
      "epoch": 0.4590447908933432,
      "grad_norm": 0.046456433832645416,
      "learning_rate": 0.0004382696955416025,
      "loss": 0.0685,
      "step": 1855
    },
    {
      "epoch": 0.4592922543924771,
      "grad_norm": 0.05640863627195358,
      "learning_rate": 0.00043820566526716644,
      "loss": 0.1062,
      "step": 1856
    },
    {
      "epoch": 0.45953971789161097,
      "grad_norm": 0.03430640324950218,
      "learning_rate": 0.00043814160648450885,
      "loss": 0.0562,
      "step": 1857
    },
    {
      "epoch": 0.45978718139074487,
      "grad_norm": 0.03963060677051544,
      "learning_rate": 0.00043807751920333293,
      "loss": 0.0573,
      "step": 1858
    },
    {
      "epoch": 0.4600346448898787,
      "grad_norm": 0.04782447591423988,
      "learning_rate": 0.00043801340343334615,
      "loss": 0.0432,
      "step": 1859
    },
    {
      "epoch": 0.4602821083890126,
      "grad_norm": 0.03505531698465347,
      "learning_rate": 0.0004379492591842605,
      "loss": 0.038,
      "step": 1860
    },
    {
      "epoch": 0.46052957188814647,
      "grad_norm": 0.022167259827256203,
      "learning_rate": 0.00043788508646579205,
      "loss": 0.0306,
      "step": 1861
    },
    {
      "epoch": 0.4607770353872804,
      "grad_norm": 0.034814201295375824,
      "learning_rate": 0.0004378208852876612,
      "loss": 0.0672,
      "step": 1862
    },
    {
      "epoch": 0.4610244988864143,
      "grad_norm": 0.0441519059240818,
      "learning_rate": 0.000437756655659593,
      "loss": 0.0676,
      "step": 1863
    },
    {
      "epoch": 0.46127196238554813,
      "grad_norm": 0.04259529337286949,
      "learning_rate": 0.0004376923975913163,
      "loss": 0.0274,
      "step": 1864
    },
    {
      "epoch": 0.46151942588468203,
      "grad_norm": 0.024820514023303986,
      "learning_rate": 0.00043762811109256464,
      "loss": 0.0192,
      "step": 1865
    },
    {
      "epoch": 0.4617668893838159,
      "grad_norm": 0.038424648344516754,
      "learning_rate": 0.00043756379617307574,
      "loss": 0.0583,
      "step": 1866
    },
    {
      "epoch": 0.4620143528829498,
      "grad_norm": 0.08098088949918747,
      "learning_rate": 0.00043749945284259154,
      "loss": 0.1021,
      "step": 1867
    },
    {
      "epoch": 0.46226181638208363,
      "grad_norm": 0.028169387951493263,
      "learning_rate": 0.00043743508111085844,
      "loss": 0.0315,
      "step": 1868
    },
    {
      "epoch": 0.46250927988121754,
      "grad_norm": 0.05234260857105255,
      "learning_rate": 0.00043737068098762704,
      "loss": 0.0439,
      "step": 1869
    },
    {
      "epoch": 0.4627567433803514,
      "grad_norm": 0.03273935988545418,
      "learning_rate": 0.00043730625248265234,
      "loss": 0.0656,
      "step": 1870
    },
    {
      "epoch": 0.4630042068794853,
      "grad_norm": 0.04405048117041588,
      "learning_rate": 0.0004372417956056934,
      "loss": 0.0668,
      "step": 1871
    },
    {
      "epoch": 0.46325167037861914,
      "grad_norm": 0.05541033670306206,
      "learning_rate": 0.000437177310366514,
      "loss": 0.0686,
      "step": 1872
    },
    {
      "epoch": 0.46349913387775304,
      "grad_norm": 0.025536850094795227,
      "learning_rate": 0.00043711279677488167,
      "loss": 0.0235,
      "step": 1873
    },
    {
      "epoch": 0.4637465973768869,
      "grad_norm": 0.03800509124994278,
      "learning_rate": 0.0004370482548405688,
      "loss": 0.0438,
      "step": 1874
    },
    {
      "epoch": 0.4639940608760208,
      "grad_norm": 0.10512746125459671,
      "learning_rate": 0.00043698368457335157,
      "loss": 0.0781,
      "step": 1875
    },
    {
      "epoch": 0.46424152437515465,
      "grad_norm": 0.03510453179478645,
      "learning_rate": 0.0004369190859830109,
      "loss": 0.0535,
      "step": 1876
    },
    {
      "epoch": 0.46448898787428855,
      "grad_norm": 0.030376924201846123,
      "learning_rate": 0.00043685445907933154,
      "loss": 0.031,
      "step": 1877
    },
    {
      "epoch": 0.4647364513734224,
      "grad_norm": 0.020474081858992577,
      "learning_rate": 0.000436789803872103,
      "loss": 0.0253,
      "step": 1878
    },
    {
      "epoch": 0.4649839148725563,
      "grad_norm": 0.03184250369668007,
      "learning_rate": 0.0004367251203711187,
      "loss": 0.0509,
      "step": 1879
    },
    {
      "epoch": 0.46523137837169015,
      "grad_norm": 0.04101037234067917,
      "learning_rate": 0.0004366604085861765,
      "loss": 0.0956,
      "step": 1880
    },
    {
      "epoch": 0.46547884187082406,
      "grad_norm": 0.026808222755789757,
      "learning_rate": 0.00043659566852707866,
      "loss": 0.0392,
      "step": 1881
    },
    {
      "epoch": 0.4657263053699579,
      "grad_norm": 0.021590491756796837,
      "learning_rate": 0.0004365309002036314,
      "loss": 0.035,
      "step": 1882
    },
    {
      "epoch": 0.4659737688690918,
      "grad_norm": 0.05882774293422699,
      "learning_rate": 0.00043646610362564565,
      "loss": 0.0892,
      "step": 1883
    },
    {
      "epoch": 0.4662212323682257,
      "grad_norm": 0.05293688923120499,
      "learning_rate": 0.00043640127880293625,
      "loss": 0.077,
      "step": 1884
    },
    {
      "epoch": 0.46646869586735956,
      "grad_norm": 0.039577070623636246,
      "learning_rate": 0.0004363364257453225,
      "loss": 0.0394,
      "step": 1885
    },
    {
      "epoch": 0.46671615936649347,
      "grad_norm": 0.03784395754337311,
      "learning_rate": 0.00043627154446262793,
      "loss": 0.0367,
      "step": 1886
    },
    {
      "epoch": 0.4669636228656273,
      "grad_norm": 0.057166505604982376,
      "learning_rate": 0.00043620663496468035,
      "loss": 0.045,
      "step": 1887
    },
    {
      "epoch": 0.4672110863647612,
      "grad_norm": 0.03589174523949623,
      "learning_rate": 0.0004361416972613118,
      "loss": 0.0547,
      "step": 1888
    },
    {
      "epoch": 0.46745854986389507,
      "grad_norm": 0.06866813451051712,
      "learning_rate": 0.0004360767313623588,
      "loss": 0.0807,
      "step": 1889
    },
    {
      "epoch": 0.46770601336302897,
      "grad_norm": 0.0630306601524353,
      "learning_rate": 0.00043601173727766176,
      "loss": 0.1085,
      "step": 1890
    },
    {
      "epoch": 0.4679534768621628,
      "grad_norm": 0.02514522336423397,
      "learning_rate": 0.00043594671501706573,
      "loss": 0.0337,
      "step": 1891
    },
    {
      "epoch": 0.4682009403612967,
      "grad_norm": 0.06194467097520828,
      "learning_rate": 0.00043588166459041986,
      "loss": 0.0898,
      "step": 1892
    },
    {
      "epoch": 0.4684484038604306,
      "grad_norm": 0.022219382226467133,
      "learning_rate": 0.00043581658600757743,
      "loss": 0.0294,
      "step": 1893
    },
    {
      "epoch": 0.4686958673595645,
      "grad_norm": 0.048030462116003036,
      "learning_rate": 0.00043575147927839644,
      "loss": 0.0776,
      "step": 1894
    },
    {
      "epoch": 0.4689433308586983,
      "grad_norm": 0.031679436564445496,
      "learning_rate": 0.0004356863444127386,
      "loss": 0.04,
      "step": 1895
    },
    {
      "epoch": 0.46919079435783223,
      "grad_norm": 0.06883105635643005,
      "learning_rate": 0.00043562118142047014,
      "loss": 0.0663,
      "step": 1896
    },
    {
      "epoch": 0.4694382578569661,
      "grad_norm": 0.07143951952457428,
      "learning_rate": 0.00043555599031146175,
      "loss": 0.0893,
      "step": 1897
    },
    {
      "epoch": 0.4696857213561,
      "grad_norm": 0.06698822975158691,
      "learning_rate": 0.00043549077109558795,
      "loss": 0.1044,
      "step": 1898
    },
    {
      "epoch": 0.46993318485523383,
      "grad_norm": 0.043853651732206345,
      "learning_rate": 0.00043542552378272783,
      "loss": 0.0506,
      "step": 1899
    },
    {
      "epoch": 0.47018064835436774,
      "grad_norm": 0.05749006196856499,
      "learning_rate": 0.0004353602483827647,
      "loss": 0.0635,
      "step": 1900
    },
    {
      "epoch": 0.4704281118535016,
      "grad_norm": 0.032344501465559006,
      "learning_rate": 0.00043529494490558593,
      "loss": 0.0241,
      "step": 1901
    },
    {
      "epoch": 0.4706755753526355,
      "grad_norm": 0.04414898529648781,
      "learning_rate": 0.00043522961336108336,
      "loss": 0.0628,
      "step": 1902
    },
    {
      "epoch": 0.47092303885176934,
      "grad_norm": 0.029069790616631508,
      "learning_rate": 0.00043516425375915316,
      "loss": 0.0282,
      "step": 1903
    },
    {
      "epoch": 0.47117050235090324,
      "grad_norm": 0.04320302978157997,
      "learning_rate": 0.0004350988661096953,
      "loss": 0.0351,
      "step": 1904
    },
    {
      "epoch": 0.47141796585003715,
      "grad_norm": 0.05136839672923088,
      "learning_rate": 0.0004350334504226145,
      "loss": 0.059,
      "step": 1905
    },
    {
      "epoch": 0.471665429349171,
      "grad_norm": 0.048042457550764084,
      "learning_rate": 0.00043496800670781945,
      "loss": 0.0736,
      "step": 1906
    },
    {
      "epoch": 0.4719128928483049,
      "grad_norm": 0.041476793587207794,
      "learning_rate": 0.00043490253497522317,
      "loss": 0.0509,
      "step": 1907
    },
    {
      "epoch": 0.47216035634743875,
      "grad_norm": 0.0325327068567276,
      "learning_rate": 0.00043483703523474295,
      "loss": 0.0419,
      "step": 1908
    },
    {
      "epoch": 0.47240781984657265,
      "grad_norm": 0.02637159638106823,
      "learning_rate": 0.0004347715074963002,
      "loss": 0.0337,
      "step": 1909
    },
    {
      "epoch": 0.4726552833457065,
      "grad_norm": 0.034769456833601,
      "learning_rate": 0.00043470595176982075,
      "loss": 0.0385,
      "step": 1910
    },
    {
      "epoch": 0.4729027468448404,
      "grad_norm": 0.03704119473695755,
      "learning_rate": 0.00043464036806523446,
      "loss": 0.0533,
      "step": 1911
    },
    {
      "epoch": 0.47315021034397425,
      "grad_norm": 0.03710845112800598,
      "learning_rate": 0.0004345747563924756,
      "loss": 0.0471,
      "step": 1912
    },
    {
      "epoch": 0.47339767384310816,
      "grad_norm": 0.02372235804796219,
      "learning_rate": 0.0004345091167614826,
      "loss": 0.0443,
      "step": 1913
    },
    {
      "epoch": 0.473645137342242,
      "grad_norm": 0.04552659019827843,
      "learning_rate": 0.00043444344918219823,
      "loss": 0.0646,
      "step": 1914
    },
    {
      "epoch": 0.4738926008413759,
      "grad_norm": 0.03859998285770416,
      "learning_rate": 0.00043437775366456934,
      "loss": 0.0304,
      "step": 1915
    },
    {
      "epoch": 0.47414006434050976,
      "grad_norm": 0.052298132330179214,
      "learning_rate": 0.000434312030218547,
      "loss": 0.0953,
      "step": 1916
    },
    {
      "epoch": 0.47438752783964366,
      "grad_norm": 0.03750566393136978,
      "learning_rate": 0.00043424627885408674,
      "loss": 0.0263,
      "step": 1917
    },
    {
      "epoch": 0.4746349913387775,
      "grad_norm": 0.050698477774858475,
      "learning_rate": 0.00043418049958114804,
      "loss": 0.0732,
      "step": 1918
    },
    {
      "epoch": 0.4748824548379114,
      "grad_norm": 0.04411539062857628,
      "learning_rate": 0.0004341146924096948,
      "loss": 0.0839,
      "step": 1919
    },
    {
      "epoch": 0.47512991833704527,
      "grad_norm": 0.03532950580120087,
      "learning_rate": 0.000434048857349695,
      "loss": 0.0321,
      "step": 1920
    },
    {
      "epoch": 0.47537738183617917,
      "grad_norm": 0.03226211294531822,
      "learning_rate": 0.0004339829944111211,
      "loss": 0.0414,
      "step": 1921
    },
    {
      "epoch": 0.475624845335313,
      "grad_norm": 0.0432763546705246,
      "learning_rate": 0.00043391710360394945,
      "loss": 0.0422,
      "step": 1922
    },
    {
      "epoch": 0.4758723088344469,
      "grad_norm": 0.05999051779508591,
      "learning_rate": 0.0004338511849381608,
      "loss": 0.0599,
      "step": 1923
    },
    {
      "epoch": 0.47611977233358077,
      "grad_norm": 0.0686209499835968,
      "learning_rate": 0.00043378523842374007,
      "loss": 0.0635,
      "step": 1924
    },
    {
      "epoch": 0.4763672358327147,
      "grad_norm": 0.04511126130819321,
      "learning_rate": 0.00043371926407067657,
      "loss": 0.0636,
      "step": 1925
    },
    {
      "epoch": 0.4766146993318486,
      "grad_norm": 0.03541918098926544,
      "learning_rate": 0.0004336532618889636,
      "loss": 0.0224,
      "step": 1926
    },
    {
      "epoch": 0.47686216283098243,
      "grad_norm": 0.04499519243836403,
      "learning_rate": 0.00043358723188859874,
      "loss": 0.1167,
      "step": 1927
    },
    {
      "epoch": 0.47710962633011633,
      "grad_norm": 0.02658928371965885,
      "learning_rate": 0.0004335211740795838,
      "loss": 0.0518,
      "step": 1928
    },
    {
      "epoch": 0.4773570898292502,
      "grad_norm": 0.041359297931194305,
      "learning_rate": 0.0004334550884719248,
      "loss": 0.0521,
      "step": 1929
    },
    {
      "epoch": 0.4776045533283841,
      "grad_norm": 0.032536476850509644,
      "learning_rate": 0.00043338897507563204,
      "loss": 0.0522,
      "step": 1930
    },
    {
      "epoch": 0.47785201682751793,
      "grad_norm": 0.04518884792923927,
      "learning_rate": 0.00043332283390071983,
      "loss": 0.058,
      "step": 1931
    },
    {
      "epoch": 0.47809948032665184,
      "grad_norm": 0.030746877193450928,
      "learning_rate": 0.00043325666495720705,
      "loss": 0.0544,
      "step": 1932
    },
    {
      "epoch": 0.4783469438257857,
      "grad_norm": 0.021981172263622284,
      "learning_rate": 0.0004331904682551164,
      "loss": 0.0212,
      "step": 1933
    },
    {
      "epoch": 0.4785944073249196,
      "grad_norm": 0.03880218416452408,
      "learning_rate": 0.0004331242438044749,
      "loss": 0.0544,
      "step": 1934
    },
    {
      "epoch": 0.47884187082405344,
      "grad_norm": 0.20646949112415314,
      "learning_rate": 0.0004330579916153139,
      "loss": 0.0716,
      "step": 1935
    },
    {
      "epoch": 0.47908933432318734,
      "grad_norm": 0.051277466118335724,
      "learning_rate": 0.0004329917116976689,
      "loss": 0.058,
      "step": 1936
    },
    {
      "epoch": 0.4793367978223212,
      "grad_norm": 0.03187979385256767,
      "learning_rate": 0.0004329254040615795,
      "loss": 0.0546,
      "step": 1937
    },
    {
      "epoch": 0.4795842613214551,
      "grad_norm": 0.028805265203118324,
      "learning_rate": 0.0004328590687170896,
      "loss": 0.0372,
      "step": 1938
    },
    {
      "epoch": 0.47983172482058895,
      "grad_norm": 0.08097603917121887,
      "learning_rate": 0.0004327927056742472,
      "loss": 0.1017,
      "step": 1939
    },
    {
      "epoch": 0.48007918831972285,
      "grad_norm": 0.029619773849844933,
      "learning_rate": 0.0004327263149431048,
      "loss": 0.0377,
      "step": 1940
    },
    {
      "epoch": 0.4803266518188567,
      "grad_norm": 0.03696002811193466,
      "learning_rate": 0.0004326598965337185,
      "loss": 0.0446,
      "step": 1941
    },
    {
      "epoch": 0.4805741153179906,
      "grad_norm": 0.08072972297668457,
      "learning_rate": 0.00043259345045614916,
      "loss": 0.045,
      "step": 1942
    },
    {
      "epoch": 0.48082157881712445,
      "grad_norm": 0.06646095961332321,
      "learning_rate": 0.0004325269767204616,
      "loss": 0.08,
      "step": 1943
    },
    {
      "epoch": 0.48106904231625836,
      "grad_norm": 0.033837299793958664,
      "learning_rate": 0.0004324604753367248,
      "loss": 0.0567,
      "step": 1944
    },
    {
      "epoch": 0.4813165058153922,
      "grad_norm": 0.03995367884635925,
      "learning_rate": 0.00043239394631501197,
      "loss": 0.0794,
      "step": 1945
    },
    {
      "epoch": 0.4815639693145261,
      "grad_norm": 0.06961845606565475,
      "learning_rate": 0.00043232738966540057,
      "loss": 0.0728,
      "step": 1946
    },
    {
      "epoch": 0.48181143281366,
      "grad_norm": 0.04100879654288292,
      "learning_rate": 0.0004322608053979722,
      "loss": 0.0805,
      "step": 1947
    },
    {
      "epoch": 0.48205889631279386,
      "grad_norm": 0.03242552652955055,
      "learning_rate": 0.0004321941935228125,
      "loss": 0.0295,
      "step": 1948
    },
    {
      "epoch": 0.48230635981192777,
      "grad_norm": 0.11211804300546646,
      "learning_rate": 0.0004321275540500116,
      "loss": 0.1135,
      "step": 1949
    },
    {
      "epoch": 0.4825538233110616,
      "grad_norm": 0.022962329909205437,
      "learning_rate": 0.0004320608869896635,
      "loss": 0.0329,
      "step": 1950
    },
    {
      "epoch": 0.4828012868101955,
      "grad_norm": 0.045678433030843735,
      "learning_rate": 0.00043199419235186653,
      "loss": 0.052,
      "step": 1951
    },
    {
      "epoch": 0.48304875030932937,
      "grad_norm": 0.06689903140068054,
      "learning_rate": 0.0004319274701467232,
      "loss": 0.0905,
      "step": 1952
    },
    {
      "epoch": 0.48329621380846327,
      "grad_norm": 0.03190278261899948,
      "learning_rate": 0.00043186072038434024,
      "loss": 0.0468,
      "step": 1953
    },
    {
      "epoch": 0.4835436773075971,
      "grad_norm": 0.0661281943321228,
      "learning_rate": 0.00043179394307482833,
      "loss": 0.0932,
      "step": 1954
    },
    {
      "epoch": 0.483791140806731,
      "grad_norm": 0.03259381279349327,
      "learning_rate": 0.0004317271382283027,
      "loss": 0.0648,
      "step": 1955
    },
    {
      "epoch": 0.4840386043058649,
      "grad_norm": 0.16375546157360077,
      "learning_rate": 0.00043166030585488227,
      "loss": 0.055,
      "step": 1956
    },
    {
      "epoch": 0.4842860678049988,
      "grad_norm": 0.030645128339529037,
      "learning_rate": 0.0004315934459646906,
      "loss": 0.0431,
      "step": 1957
    },
    {
      "epoch": 0.4845335313041326,
      "grad_norm": 0.03342941775918007,
      "learning_rate": 0.00043152655856785506,
      "loss": 0.038,
      "step": 1958
    },
    {
      "epoch": 0.48478099480326653,
      "grad_norm": 0.04051235690712929,
      "learning_rate": 0.0004314596436745075,
      "loss": 0.0425,
      "step": 1959
    },
    {
      "epoch": 0.4850284583024004,
      "grad_norm": 0.02644144371151924,
      "learning_rate": 0.0004313927012947837,
      "loss": 0.0348,
      "step": 1960
    },
    {
      "epoch": 0.4852759218015343,
      "grad_norm": 0.029089853167533875,
      "learning_rate": 0.0004313257314388236,
      "loss": 0.031,
      "step": 1961
    },
    {
      "epoch": 0.48552338530066813,
      "grad_norm": 0.03406789153814316,
      "learning_rate": 0.0004312587341167714,
      "loss": 0.0434,
      "step": 1962
    },
    {
      "epoch": 0.48577084879980204,
      "grad_norm": 0.028894437476992607,
      "learning_rate": 0.00043119170933877553,
      "loss": 0.0385,
      "step": 1963
    },
    {
      "epoch": 0.4860183122989359,
      "grad_norm": 0.031356628984212875,
      "learning_rate": 0.0004311246571149884,
      "loss": 0.0319,
      "step": 1964
    },
    {
      "epoch": 0.4862657757980698,
      "grad_norm": 0.041442450135946274,
      "learning_rate": 0.00043105757745556675,
      "loss": 0.0493,
      "step": 1965
    },
    {
      "epoch": 0.48651323929720364,
      "grad_norm": 0.05006828531622887,
      "learning_rate": 0.0004309904703706713,
      "loss": 0.0434,
      "step": 1966
    },
    {
      "epoch": 0.48676070279633754,
      "grad_norm": 0.032692890614271164,
      "learning_rate": 0.00043092333587046707,
      "loss": 0.0514,
      "step": 1967
    },
    {
      "epoch": 0.48700816629547145,
      "grad_norm": 0.03905882686376572,
      "learning_rate": 0.00043085617396512304,
      "loss": 0.0424,
      "step": 1968
    },
    {
      "epoch": 0.4872556297946053,
      "grad_norm": 0.06193501874804497,
      "learning_rate": 0.0004307889846648127,
      "loss": 0.0582,
      "step": 1969
    },
    {
      "epoch": 0.4875030932937392,
      "grad_norm": 0.025699814781546593,
      "learning_rate": 0.0004307217679797133,
      "loss": 0.0441,
      "step": 1970
    },
    {
      "epoch": 0.48775055679287305,
      "grad_norm": 0.04348290339112282,
      "learning_rate": 0.0004306545239200065,
      "loss": 0.0782,
      "step": 1971
    },
    {
      "epoch": 0.48799802029200695,
      "grad_norm": 0.05099758505821228,
      "learning_rate": 0.00043058725249587795,
      "loss": 0.0597,
      "step": 1972
    },
    {
      "epoch": 0.4882454837911408,
      "grad_norm": 0.09700702130794525,
      "learning_rate": 0.0004305199537175175,
      "loss": 0.1662,
      "step": 1973
    },
    {
      "epoch": 0.4884929472902747,
      "grad_norm": 0.03704627603292465,
      "learning_rate": 0.00043045262759511916,
      "loss": 0.0488,
      "step": 1974
    },
    {
      "epoch": 0.48874041078940855,
      "grad_norm": 0.02092326618731022,
      "learning_rate": 0.00043038527413888106,
      "loss": 0.0339,
      "step": 1975
    },
    {
      "epoch": 0.48898787428854246,
      "grad_norm": 0.04495030641555786,
      "learning_rate": 0.00043031789335900563,
      "loss": 0.054,
      "step": 1976
    },
    {
      "epoch": 0.4892353377876763,
      "grad_norm": 0.03221126273274422,
      "learning_rate": 0.00043025048526569907,
      "loss": 0.0781,
      "step": 1977
    },
    {
      "epoch": 0.4894828012868102,
      "grad_norm": 0.0180544164031744,
      "learning_rate": 0.000430183049869172,
      "loss": 0.0237,
      "step": 1978
    },
    {
      "epoch": 0.48973026478594406,
      "grad_norm": 0.042931657284498215,
      "learning_rate": 0.0004301155871796393,
      "loss": 0.05,
      "step": 1979
    },
    {
      "epoch": 0.48997772828507796,
      "grad_norm": 0.062952920794487,
      "learning_rate": 0.00043004809720731955,
      "loss": 0.0766,
      "step": 1980
    },
    {
      "epoch": 0.4902251917842118,
      "grad_norm": 0.03591568022966385,
      "learning_rate": 0.00042998057996243577,
      "loss": 0.0626,
      "step": 1981
    },
    {
      "epoch": 0.4904726552833457,
      "grad_norm": 0.03863470256328583,
      "learning_rate": 0.0004299130354552151,
      "loss": 0.0569,
      "step": 1982
    },
    {
      "epoch": 0.49072011878247956,
      "grad_norm": 0.04550394415855408,
      "learning_rate": 0.00042984546369588886,
      "loss": 0.0626,
      "step": 1983
    },
    {
      "epoch": 0.49096758228161347,
      "grad_norm": 0.03101322241127491,
      "learning_rate": 0.00042977786469469215,
      "loss": 0.0524,
      "step": 1984
    },
    {
      "epoch": 0.4912150457807473,
      "grad_norm": 0.03840227425098419,
      "learning_rate": 0.00042971023846186465,
      "loss": 0.0409,
      "step": 1985
    },
    {
      "epoch": 0.4914625092798812,
      "grad_norm": 0.039018139243125916,
      "learning_rate": 0.0004296425850076499,
      "loss": 0.0304,
      "step": 1986
    },
    {
      "epoch": 0.49170997277901507,
      "grad_norm": 0.04186372458934784,
      "learning_rate": 0.0004295749043422956,
      "loss": 0.0468,
      "step": 1987
    },
    {
      "epoch": 0.491957436278149,
      "grad_norm": 0.031853605061769485,
      "learning_rate": 0.0004295071964760536,
      "loss": 0.0475,
      "step": 1988
    },
    {
      "epoch": 0.4922048997772829,
      "grad_norm": 0.05530830845236778,
      "learning_rate": 0.00042943946141918,
      "loss": 0.0694,
      "step": 1989
    },
    {
      "epoch": 0.4924523632764167,
      "grad_norm": 0.035078149288892746,
      "learning_rate": 0.00042937169918193463,
      "loss": 0.0532,
      "step": 1990
    },
    {
      "epoch": 0.49269982677555063,
      "grad_norm": 0.04848477616906166,
      "learning_rate": 0.0004293039097745819,
      "loss": 0.0498,
      "step": 1991
    },
    {
      "epoch": 0.4929472902746845,
      "grad_norm": 0.0431692861020565,
      "learning_rate": 0.00042923609320739,
      "loss": 0.0664,
      "step": 1992
    },
    {
      "epoch": 0.4931947537738184,
      "grad_norm": 0.07406497001647949,
      "learning_rate": 0.00042916824949063145,
      "loss": 0.0916,
      "step": 1993
    },
    {
      "epoch": 0.49344221727295223,
      "grad_norm": 0.05743015930056572,
      "learning_rate": 0.00042910037863458275,
      "loss": 0.0745,
      "step": 1994
    },
    {
      "epoch": 0.49368968077208614,
      "grad_norm": 0.0581195205450058,
      "learning_rate": 0.00042903248064952456,
      "loss": 0.1113,
      "step": 1995
    },
    {
      "epoch": 0.49393714427122,
      "grad_norm": 0.03297729045152664,
      "learning_rate": 0.0004289645555457417,
      "loss": 0.0385,
      "step": 1996
    },
    {
      "epoch": 0.4941846077703539,
      "grad_norm": 0.03807204216718674,
      "learning_rate": 0.00042889660333352285,
      "loss": 0.0234,
      "step": 1997
    },
    {
      "epoch": 0.49443207126948774,
      "grad_norm": 0.058400366455316544,
      "learning_rate": 0.00042882862402316124,
      "loss": 0.0819,
      "step": 1998
    },
    {
      "epoch": 0.49467953476862164,
      "grad_norm": 0.04390169307589531,
      "learning_rate": 0.00042876061762495377,
      "loss": 0.0426,
      "step": 1999
    },
    {
      "epoch": 0.4949269982677555,
      "grad_norm": 0.02880876697599888,
      "learning_rate": 0.00042869258414920177,
      "loss": 0.0348,
      "step": 2000
    },
    {
      "epoch": 0.4949269982677555,
      "eval_loss": 0.29654350876808167,
      "eval_runtime": 202.6632,
      "eval_samples_per_second": 4.934,
      "eval_steps_per_second": 0.311,
      "step": 2000
    },
    {
      "epoch": 0.4951744617668894,
      "grad_norm": 0.04924335703253746,
      "learning_rate": 0.0004286245236062104,
      "loss": 0.0558,
      "step": 2001
    },
    {
      "epoch": 0.49542192526602324,
      "grad_norm": 0.07999809086322784,
      "learning_rate": 0.00042855643600628915,
      "loss": 0.1237,
      "step": 2002
    },
    {
      "epoch": 0.49566938876515715,
      "grad_norm": 0.09201643615961075,
      "learning_rate": 0.0004284883213597514,
      "loss": 0.0596,
      "step": 2003
    },
    {
      "epoch": 0.495916852264291,
      "grad_norm": 0.05262383446097374,
      "learning_rate": 0.00042842017967691483,
      "loss": 0.074,
      "step": 2004
    },
    {
      "epoch": 0.4961643157634249,
      "grad_norm": 0.07675360143184662,
      "learning_rate": 0.000428352010968101,
      "loss": 0.0794,
      "step": 2005
    },
    {
      "epoch": 0.49641177926255875,
      "grad_norm": 0.023350846022367477,
      "learning_rate": 0.0004282838152436358,
      "loss": 0.0332,
      "step": 2006
    },
    {
      "epoch": 0.49665924276169265,
      "grad_norm": 0.048120565712451935,
      "learning_rate": 0.00042821559251384904,
      "loss": 0.0543,
      "step": 2007
    },
    {
      "epoch": 0.4969067062608265,
      "grad_norm": 0.04598887637257576,
      "learning_rate": 0.00042814734278907466,
      "loss": 0.0591,
      "step": 2008
    },
    {
      "epoch": 0.4971541697599604,
      "grad_norm": 0.04613400250673294,
      "learning_rate": 0.00042807906607965075,
      "loss": 0.1219,
      "step": 2009
    },
    {
      "epoch": 0.4974016332590943,
      "grad_norm": 0.03690715134143829,
      "learning_rate": 0.0004280107623959194,
      "loss": 0.0777,
      "step": 2010
    },
    {
      "epoch": 0.49764909675822816,
      "grad_norm": 0.026338238269090652,
      "learning_rate": 0.0004279424317482269,
      "loss": 0.047,
      "step": 2011
    },
    {
      "epoch": 0.49789656025736206,
      "grad_norm": 0.04947105050086975,
      "learning_rate": 0.0004278740741469235,
      "loss": 0.0393,
      "step": 2012
    },
    {
      "epoch": 0.4981440237564959,
      "grad_norm": 0.04034990444779396,
      "learning_rate": 0.0004278056896023635,
      "loss": 0.0362,
      "step": 2013
    },
    {
      "epoch": 0.4983914872556298,
      "grad_norm": 0.04815942794084549,
      "learning_rate": 0.00042773727812490546,
      "loss": 0.0912,
      "step": 2014
    },
    {
      "epoch": 0.49863895075476367,
      "grad_norm": 0.032573115080595016,
      "learning_rate": 0.00042766883972491186,
      "loss": 0.0442,
      "step": 2015
    },
    {
      "epoch": 0.49888641425389757,
      "grad_norm": 0.04315030574798584,
      "learning_rate": 0.00042760037441274943,
      "loss": 0.0636,
      "step": 2016
    },
    {
      "epoch": 0.4991338777530314,
      "grad_norm": 0.03523997217416763,
      "learning_rate": 0.0004275318821987888,
      "loss": 0.0491,
      "step": 2017
    },
    {
      "epoch": 0.4993813412521653,
      "grad_norm": 0.029532920569181442,
      "learning_rate": 0.00042746336309340484,
      "loss": 0.0261,
      "step": 2018
    },
    {
      "epoch": 0.49962880475129917,
      "grad_norm": 0.05887888744473457,
      "learning_rate": 0.0004273948171069762,
      "loss": 0.0813,
      "step": 2019
    },
    {
      "epoch": 0.4998762682504331,
      "grad_norm": 0.06327466666698456,
      "learning_rate": 0.00042732624424988584,
      "loss": 0.0992,
      "step": 2020
    },
    {
      "epoch": 0.5001237317495669,
      "grad_norm": 0.029874857515096664,
      "learning_rate": 0.00042725764453252095,
      "loss": 0.033,
      "step": 2021
    },
    {
      "epoch": 0.5003711952487008,
      "grad_norm": 0.06031407415866852,
      "learning_rate": 0.0004271890179652724,
      "loss": 0.0509,
      "step": 2022
    },
    {
      "epoch": 0.5006186587478347,
      "grad_norm": 0.07028036564588547,
      "learning_rate": 0.00042712036455853545,
      "loss": 0.0487,
      "step": 2023
    },
    {
      "epoch": 0.5008661222469686,
      "grad_norm": 0.03503914549946785,
      "learning_rate": 0.0004270516843227091,
      "loss": 0.0322,
      "step": 2024
    },
    {
      "epoch": 0.5011135857461024,
      "grad_norm": 0.031203635036945343,
      "learning_rate": 0.00042698297726819666,
      "loss": 0.0352,
      "step": 2025
    },
    {
      "epoch": 0.5013610492452363,
      "grad_norm": 0.03208143264055252,
      "learning_rate": 0.0004269142434054056,
      "loss": 0.0705,
      "step": 2026
    },
    {
      "epoch": 0.5016085127443702,
      "grad_norm": 0.043569862842559814,
      "learning_rate": 0.0004268454827447472,
      "loss": 0.0447,
      "step": 2027
    },
    {
      "epoch": 0.5018559762435041,
      "grad_norm": 0.028097888454794884,
      "learning_rate": 0.00042677669529663686,
      "loss": 0.0695,
      "step": 2028
    },
    {
      "epoch": 0.5021034397426379,
      "grad_norm": 0.02430758811533451,
      "learning_rate": 0.00042670788107149416,
      "loss": 0.0408,
      "step": 2029
    },
    {
      "epoch": 0.5023509032417718,
      "grad_norm": 0.03779076412320137,
      "learning_rate": 0.00042663904007974256,
      "loss": 0.0489,
      "step": 2030
    },
    {
      "epoch": 0.5025983667409057,
      "grad_norm": 0.048707265406847,
      "learning_rate": 0.00042657017233180966,
      "loss": 0.0523,
      "step": 2031
    },
    {
      "epoch": 0.5028458302400396,
      "grad_norm": 0.0542927049100399,
      "learning_rate": 0.0004265012778381272,
      "loss": 0.0462,
      "step": 2032
    },
    {
      "epoch": 0.5030932937391734,
      "grad_norm": 0.03813694044947624,
      "learning_rate": 0.00042643235660913087,
      "loss": 0.0472,
      "step": 2033
    },
    {
      "epoch": 0.5033407572383074,
      "grad_norm": 0.02322550304234028,
      "learning_rate": 0.0004263634086552604,
      "loss": 0.0385,
      "step": 2034
    },
    {
      "epoch": 0.5035882207374413,
      "grad_norm": 0.05143430829048157,
      "learning_rate": 0.0004262944339869596,
      "loss": 0.1037,
      "step": 2035
    },
    {
      "epoch": 0.5038356842365751,
      "grad_norm": 0.048380620777606964,
      "learning_rate": 0.00042622543261467625,
      "loss": 0.0752,
      "step": 2036
    },
    {
      "epoch": 0.504083147735709,
      "grad_norm": 0.029696602374315262,
      "learning_rate": 0.0004261564045488624,
      "loss": 0.0195,
      "step": 2037
    },
    {
      "epoch": 0.5043306112348429,
      "grad_norm": 0.05486954003572464,
      "learning_rate": 0.00042608734979997394,
      "loss": 0.0844,
      "step": 2038
    },
    {
      "epoch": 0.5045780747339768,
      "grad_norm": 0.04671801999211311,
      "learning_rate": 0.00042601826837847077,
      "loss": 0.0505,
      "step": 2039
    },
    {
      "epoch": 0.5048255382331106,
      "grad_norm": 0.1538688689470291,
      "learning_rate": 0.000425949160294817,
      "loss": 0.0534,
      "step": 2040
    },
    {
      "epoch": 0.5050730017322445,
      "grad_norm": 0.057803213596343994,
      "learning_rate": 0.00042588002555948065,
      "loss": 0.0976,
      "step": 2041
    },
    {
      "epoch": 0.5053204652313784,
      "grad_norm": 0.09620039165019989,
      "learning_rate": 0.00042581086418293386,
      "loss": 0.1325,
      "step": 2042
    },
    {
      "epoch": 0.5055679287305123,
      "grad_norm": 0.09115910530090332,
      "learning_rate": 0.00042574167617565273,
      "loss": 0.1043,
      "step": 2043
    },
    {
      "epoch": 0.5058153922296461,
      "grad_norm": 0.08356709033250809,
      "learning_rate": 0.0004256724615481174,
      "loss": 0.0938,
      "step": 2044
    },
    {
      "epoch": 0.50606285572878,
      "grad_norm": 0.059884972870349884,
      "learning_rate": 0.0004256032203108121,
      "loss": 0.0772,
      "step": 2045
    },
    {
      "epoch": 0.5063103192279139,
      "grad_norm": 0.03991572931408882,
      "learning_rate": 0.0004255339524742251,
      "loss": 0.0738,
      "step": 2046
    },
    {
      "epoch": 0.5065577827270478,
      "grad_norm": 0.09353336691856384,
      "learning_rate": 0.0004254646580488486,
      "loss": 0.1144,
      "step": 2047
    },
    {
      "epoch": 0.5068052462261816,
      "grad_norm": 0.03494586423039436,
      "learning_rate": 0.00042539533704517895,
      "loss": 0.0671,
      "step": 2048
    },
    {
      "epoch": 0.5070527097253155,
      "grad_norm": 0.053562186658382416,
      "learning_rate": 0.00042532598947371645,
      "loss": 0.0881,
      "step": 2049
    },
    {
      "epoch": 0.5073001732244494,
      "grad_norm": 0.043293341994285583,
      "learning_rate": 0.00042525661534496535,
      "loss": 0.0423,
      "step": 2050
    },
    {
      "epoch": 0.5075476367235833,
      "grad_norm": 0.09421222656965256,
      "learning_rate": 0.00042518721466943413,
      "loss": 0.0622,
      "step": 2051
    },
    {
      "epoch": 0.5077951002227171,
      "grad_norm": 0.0557854063808918,
      "learning_rate": 0.0004251177874576351,
      "loss": 0.0462,
      "step": 2052
    },
    {
      "epoch": 0.5080425637218511,
      "grad_norm": 0.044404029846191406,
      "learning_rate": 0.0004250483337200847,
      "loss": 0.0603,
      "step": 2053
    },
    {
      "epoch": 0.5082900272209849,
      "grad_norm": 0.08699671179056168,
      "learning_rate": 0.00042497885346730324,
      "loss": 0.0638,
      "step": 2054
    },
    {
      "epoch": 0.5085374907201188,
      "grad_norm": 0.03272894024848938,
      "learning_rate": 0.0004249093467098153,
      "loss": 0.0552,
      "step": 2055
    },
    {
      "epoch": 0.5087849542192526,
      "grad_norm": 0.07694078236818314,
      "learning_rate": 0.0004248398134581493,
      "loss": 0.0544,
      "step": 2056
    },
    {
      "epoch": 0.5090324177183866,
      "grad_norm": 0.04787776619195938,
      "learning_rate": 0.0004247702537228377,
      "loss": 0.0587,
      "step": 2057
    },
    {
      "epoch": 0.5092798812175204,
      "grad_norm": 0.03039013035595417,
      "learning_rate": 0.0004247006675144169,
      "loss": 0.0458,
      "step": 2058
    },
    {
      "epoch": 0.5095273447166543,
      "grad_norm": 0.08073869347572327,
      "learning_rate": 0.0004246310548434275,
      "loss": 0.0741,
      "step": 2059
    },
    {
      "epoch": 0.5097748082157881,
      "grad_norm": 0.028497155755758286,
      "learning_rate": 0.0004245614157204139,
      "loss": 0.0457,
      "step": 2060
    },
    {
      "epoch": 0.5100222717149221,
      "grad_norm": 0.04800852760672569,
      "learning_rate": 0.00042449175015592467,
      "loss": 0.0612,
      "step": 2061
    },
    {
      "epoch": 0.5102697352140559,
      "grad_norm": 0.03613083064556122,
      "learning_rate": 0.0004244220581605122,
      "loss": 0.0581,
      "step": 2062
    },
    {
      "epoch": 0.5105171987131898,
      "grad_norm": 0.04052082821726799,
      "learning_rate": 0.0004243523397447332,
      "loss": 0.0568,
      "step": 2063
    },
    {
      "epoch": 0.5107646622123236,
      "grad_norm": 0.03215260058641434,
      "learning_rate": 0.00042428259491914796,
      "loss": 0.0336,
      "step": 2064
    },
    {
      "epoch": 0.5110121257114576,
      "grad_norm": 0.03770957887172699,
      "learning_rate": 0.0004242128236943211,
      "loss": 0.0481,
      "step": 2065
    },
    {
      "epoch": 0.5112595892105914,
      "grad_norm": 0.032048292458057404,
      "learning_rate": 0.00042414302608082113,
      "loss": 0.0426,
      "step": 2066
    },
    {
      "epoch": 0.5115070527097253,
      "grad_norm": 0.047128826379776,
      "learning_rate": 0.00042407320208922065,
      "loss": 0.0619,
      "step": 2067
    },
    {
      "epoch": 0.5117545162088591,
      "grad_norm": 0.03803494945168495,
      "learning_rate": 0.00042400335173009595,
      "loss": 0.0449,
      "step": 2068
    },
    {
      "epoch": 0.5120019797079931,
      "grad_norm": 0.06039426103234291,
      "learning_rate": 0.00042393347501402766,
      "loss": 0.0633,
      "step": 2069
    },
    {
      "epoch": 0.512249443207127,
      "grad_norm": 0.04983055964112282,
      "learning_rate": 0.0004238635719516003,
      "loss": 0.0367,
      "step": 2070
    },
    {
      "epoch": 0.5124969067062608,
      "grad_norm": 0.04657178372144699,
      "learning_rate": 0.00042379364255340225,
      "loss": 0.0739,
      "step": 2071
    },
    {
      "epoch": 0.5127443702053947,
      "grad_norm": 0.03040728159248829,
      "learning_rate": 0.00042372368683002605,
      "loss": 0.0338,
      "step": 2072
    },
    {
      "epoch": 0.5129918337045286,
      "grad_norm": 0.05280138924717903,
      "learning_rate": 0.00042365370479206815,
      "loss": 0.0962,
      "step": 2073
    },
    {
      "epoch": 0.5132392972036625,
      "grad_norm": 0.028972486034035683,
      "learning_rate": 0.00042358369645012896,
      "loss": 0.0408,
      "step": 2074
    },
    {
      "epoch": 0.5134867607027963,
      "grad_norm": 0.047142527997493744,
      "learning_rate": 0.00042351366181481295,
      "loss": 0.0375,
      "step": 2075
    },
    {
      "epoch": 0.5137342242019303,
      "grad_norm": 0.029269646853208542,
      "learning_rate": 0.0004234436008967285,
      "loss": 0.0283,
      "step": 2076
    },
    {
      "epoch": 0.5139816877010641,
      "grad_norm": 0.06391249597072601,
      "learning_rate": 0.00042337351370648805,
      "loss": 0.048,
      "step": 2077
    },
    {
      "epoch": 0.514229151200198,
      "grad_norm": 0.02205808088183403,
      "learning_rate": 0.0004233034002547079,
      "loss": 0.0433,
      "step": 2078
    },
    {
      "epoch": 0.5144766146993318,
      "grad_norm": 0.0376046746969223,
      "learning_rate": 0.00042323326055200837,
      "loss": 0.0447,
      "step": 2079
    },
    {
      "epoch": 0.5147240781984658,
      "grad_norm": 0.022257938981056213,
      "learning_rate": 0.00042316309460901393,
      "loss": 0.0252,
      "step": 2080
    },
    {
      "epoch": 0.5149715416975996,
      "grad_norm": 0.04152969643473625,
      "learning_rate": 0.0004230929024363527,
      "loss": 0.0326,
      "step": 2081
    },
    {
      "epoch": 0.5152190051967335,
      "grad_norm": 0.050309646874666214,
      "learning_rate": 0.0004230226840446572,
      "loss": 0.0923,
      "step": 2082
    },
    {
      "epoch": 0.5154664686958673,
      "grad_norm": 0.06975411623716354,
      "learning_rate": 0.0004229524394445634,
      "loss": 0.0803,
      "step": 2083
    },
    {
      "epoch": 0.5157139321950013,
      "grad_norm": 0.04550269618630409,
      "learning_rate": 0.0004228821686467117,
      "loss": 0.0668,
      "step": 2084
    },
    {
      "epoch": 0.5159613956941351,
      "grad_norm": 0.029516728594899178,
      "learning_rate": 0.0004228118716617462,
      "loss": 0.0304,
      "step": 2085
    },
    {
      "epoch": 0.516208859193269,
      "grad_norm": 0.036028362810611725,
      "learning_rate": 0.00042274154850031496,
      "loss": 0.0361,
      "step": 2086
    },
    {
      "epoch": 0.5164563226924028,
      "grad_norm": 0.02501736953854561,
      "learning_rate": 0.00042267119917307035,
      "loss": 0.0303,
      "step": 2087
    },
    {
      "epoch": 0.5167037861915368,
      "grad_norm": 0.029011664912104607,
      "learning_rate": 0.00042260082369066827,
      "loss": 0.0419,
      "step": 2088
    },
    {
      "epoch": 0.5169512496906706,
      "grad_norm": 0.04292236268520355,
      "learning_rate": 0.0004225304220637688,
      "loss": 0.0524,
      "step": 2089
    },
    {
      "epoch": 0.5171987131898045,
      "grad_norm": 0.04761254042387009,
      "learning_rate": 0.0004224599943030358,
      "loss": 0.0488,
      "step": 2090
    },
    {
      "epoch": 0.5174461766889383,
      "grad_norm": 0.028578566387295723,
      "learning_rate": 0.0004223895404191375,
      "loss": 0.0318,
      "step": 2091
    },
    {
      "epoch": 0.5176936401880723,
      "grad_norm": 0.03161413595080376,
      "learning_rate": 0.0004223190604227457,
      "loss": 0.038,
      "step": 2092
    },
    {
      "epoch": 0.5179411036872061,
      "grad_norm": 0.03970111906528473,
      "learning_rate": 0.0004222485543245361,
      "loss": 0.0688,
      "step": 2093
    },
    {
      "epoch": 0.51818856718634,
      "grad_norm": 0.02593538910150528,
      "learning_rate": 0.00042217802213518886,
      "loss": 0.057,
      "step": 2094
    },
    {
      "epoch": 0.518436030685474,
      "grad_norm": 0.06583493947982788,
      "learning_rate": 0.0004221074638653874,
      "loss": 0.0468,
      "step": 2095
    },
    {
      "epoch": 0.5186834941846078,
      "grad_norm": 0.036445919424295425,
      "learning_rate": 0.0004220368795258197,
      "loss": 0.075,
      "step": 2096
    },
    {
      "epoch": 0.5189309576837416,
      "grad_norm": 0.029298564419150352,
      "learning_rate": 0.00042196626912717727,
      "loss": 0.0572,
      "step": 2097
    },
    {
      "epoch": 0.5191784211828755,
      "grad_norm": 0.07753781974315643,
      "learning_rate": 0.0004218956326801559,
      "loss": 0.0452,
      "step": 2098
    },
    {
      "epoch": 0.5194258846820095,
      "grad_norm": 0.03894508630037308,
      "learning_rate": 0.00042182497019545507,
      "loss": 0.0537,
      "step": 2099
    },
    {
      "epoch": 0.5196733481811433,
      "grad_norm": 0.1230725422501564,
      "learning_rate": 0.00042175428168377823,
      "loss": 0.0777,
      "step": 2100
    },
    {
      "epoch": 0.5199208116802772,
      "grad_norm": 0.035810112953186035,
      "learning_rate": 0.0004216835671558329,
      "loss": 0.0427,
      "step": 2101
    },
    {
      "epoch": 0.520168275179411,
      "grad_norm": 0.060898974537849426,
      "learning_rate": 0.00042161282662233056,
      "loss": 0.0736,
      "step": 2102
    },
    {
      "epoch": 0.520415738678545,
      "grad_norm": 0.035662055015563965,
      "learning_rate": 0.0004215420600939864,
      "loss": 0.0721,
      "step": 2103
    },
    {
      "epoch": 0.5206632021776788,
      "grad_norm": 0.038319092243909836,
      "learning_rate": 0.00042147126758151974,
      "loss": 0.0576,
      "step": 2104
    },
    {
      "epoch": 0.5209106656768127,
      "grad_norm": 0.05648032948374748,
      "learning_rate": 0.00042140044909565384,
      "loss": 0.0731,
      "step": 2105
    },
    {
      "epoch": 0.5211581291759465,
      "grad_norm": 0.028852393850684166,
      "learning_rate": 0.00042132960464711576,
      "loss": 0.0271,
      "step": 2106
    },
    {
      "epoch": 0.5214055926750805,
      "grad_norm": 0.026437392458319664,
      "learning_rate": 0.0004212587342466366,
      "loss": 0.0469,
      "step": 2107
    },
    {
      "epoch": 0.5216530561742143,
      "grad_norm": 0.021607473492622375,
      "learning_rate": 0.00042118783790495154,
      "loss": 0.0413,
      "step": 2108
    },
    {
      "epoch": 0.5219005196733482,
      "grad_norm": 0.02181350067257881,
      "learning_rate": 0.00042111691563279926,
      "loss": 0.0314,
      "step": 2109
    },
    {
      "epoch": 0.522147983172482,
      "grad_norm": 0.026796117424964905,
      "learning_rate": 0.0004210459674409227,
      "loss": 0.0384,
      "step": 2110
    },
    {
      "epoch": 0.522395446671616,
      "grad_norm": 0.02754005417227745,
      "learning_rate": 0.00042097499334006874,
      "loss": 0.0278,
      "step": 2111
    },
    {
      "epoch": 0.5226429101707498,
      "grad_norm": 0.08813437074422836,
      "learning_rate": 0.00042090399334098795,
      "loss": 0.1074,
      "step": 2112
    },
    {
      "epoch": 0.5228903736698837,
      "grad_norm": 0.04488370195031166,
      "learning_rate": 0.00042083296745443514,
      "loss": 0.0783,
      "step": 2113
    },
    {
      "epoch": 0.5231378371690175,
      "grad_norm": 0.03860880807042122,
      "learning_rate": 0.00042076191569116874,
      "loss": 0.0431,
      "step": 2114
    },
    {
      "epoch": 0.5233853006681515,
      "grad_norm": 0.03249470144510269,
      "learning_rate": 0.0004206908380619513,
      "loss": 0.068,
      "step": 2115
    },
    {
      "epoch": 0.5236327641672853,
      "grad_norm": 0.04055037721991539,
      "learning_rate": 0.0004206197345775491,
      "loss": 0.0343,
      "step": 2116
    },
    {
      "epoch": 0.5238802276664192,
      "grad_norm": 0.02929944545030594,
      "learning_rate": 0.0004205486052487326,
      "loss": 0.0409,
      "step": 2117
    },
    {
      "epoch": 0.5241276911655531,
      "grad_norm": 0.04871073737740517,
      "learning_rate": 0.00042047745008627597,
      "loss": 0.0544,
      "step": 2118
    },
    {
      "epoch": 0.524375154664687,
      "grad_norm": 0.026780545711517334,
      "learning_rate": 0.00042040626910095735,
      "loss": 0.0562,
      "step": 2119
    },
    {
      "epoch": 0.5246226181638208,
      "grad_norm": 0.029982982203364372,
      "learning_rate": 0.0004203350623035587,
      "loss": 0.041,
      "step": 2120
    },
    {
      "epoch": 0.5248700816629547,
      "grad_norm": 0.03527175635099411,
      "learning_rate": 0.0004202638297048662,
      "loss": 0.0781,
      "step": 2121
    },
    {
      "epoch": 0.5251175451620886,
      "grad_norm": 0.057670120149850845,
      "learning_rate": 0.00042019257131566953,
      "loss": 0.0617,
      "step": 2122
    },
    {
      "epoch": 0.5253650086612225,
      "grad_norm": 0.047914206981658936,
      "learning_rate": 0.0004201212871467625,
      "loss": 0.0533,
      "step": 2123
    },
    {
      "epoch": 0.5256124721603563,
      "grad_norm": 0.0547662079334259,
      "learning_rate": 0.00042004997720894276,
      "loss": 0.1095,
      "step": 2124
    },
    {
      "epoch": 0.5258599356594902,
      "grad_norm": 0.04889855906367302,
      "learning_rate": 0.00041997864151301214,
      "loss": 0.0816,
      "step": 2125
    },
    {
      "epoch": 0.5261073991586241,
      "grad_norm": 0.05997829884290695,
      "learning_rate": 0.0004199072800697758,
      "loss": 0.0633,
      "step": 2126
    },
    {
      "epoch": 0.526354862657758,
      "grad_norm": 0.033535636961460114,
      "learning_rate": 0.00041983589289004334,
      "loss": 0.0312,
      "step": 2127
    },
    {
      "epoch": 0.5266023261568918,
      "grad_norm": 0.04079769179224968,
      "learning_rate": 0.0004197644799846279,
      "loss": 0.0827,
      "step": 2128
    },
    {
      "epoch": 0.5268497896560257,
      "grad_norm": 0.03733178973197937,
      "learning_rate": 0.00041969304136434674,
      "loss": 0.0448,
      "step": 2129
    },
    {
      "epoch": 0.5270972531551597,
      "grad_norm": 0.05701339244842529,
      "learning_rate": 0.000419621577040021,
      "loss": 0.0864,
      "step": 2130
    },
    {
      "epoch": 0.5273447166542935,
      "grad_norm": 0.027942849323153496,
      "learning_rate": 0.00041955008702247547,
      "loss": 0.0599,
      "step": 2131
    },
    {
      "epoch": 0.5275921801534273,
      "grad_norm": 0.05195154622197151,
      "learning_rate": 0.0004194785713225392,
      "loss": 0.1095,
      "step": 2132
    },
    {
      "epoch": 0.5278396436525612,
      "grad_norm": 0.017273638397455215,
      "learning_rate": 0.0004194070299510449,
      "loss": 0.028,
      "step": 2133
    },
    {
      "epoch": 0.5280871071516952,
      "grad_norm": 0.035019487142562866,
      "learning_rate": 0.0004193354629188291,
      "loss": 0.033,
      "step": 2134
    },
    {
      "epoch": 0.528334570650829,
      "grad_norm": 0.026437239721417427,
      "learning_rate": 0.0004192638702367324,
      "loss": 0.0358,
      "step": 2135
    },
    {
      "epoch": 0.5285820341499629,
      "grad_norm": 0.0346866138279438,
      "learning_rate": 0.0004191922519155993,
      "loss": 0.0463,
      "step": 2136
    },
    {
      "epoch": 0.5288294976490968,
      "grad_norm": 0.15185755491256714,
      "learning_rate": 0.000419120607966278,
      "loss": 0.0727,
      "step": 2137
    },
    {
      "epoch": 0.5290769611482307,
      "grad_norm": 0.03132113441824913,
      "learning_rate": 0.00041904893839962064,
      "loss": 0.0451,
      "step": 2138
    },
    {
      "epoch": 0.5293244246473645,
      "grad_norm": 0.03565362095832825,
      "learning_rate": 0.00041897724322648334,
      "loss": 0.0634,
      "step": 2139
    },
    {
      "epoch": 0.5295718881464984,
      "grad_norm": 0.054191458970308304,
      "learning_rate": 0.00041890552245772613,
      "loss": 0.0642,
      "step": 2140
    },
    {
      "epoch": 0.5298193516456323,
      "grad_norm": 0.019622456282377243,
      "learning_rate": 0.00041883377610421265,
      "loss": 0.0279,
      "step": 2141
    },
    {
      "epoch": 0.5300668151447662,
      "grad_norm": 0.06567484885454178,
      "learning_rate": 0.00041876200417681063,
      "loss": 0.0816,
      "step": 2142
    },
    {
      "epoch": 0.5303142786439,
      "grad_norm": 0.06261660903692245,
      "learning_rate": 0.0004186902066863918,
      "loss": 0.0783,
      "step": 2143
    },
    {
      "epoch": 0.5305617421430339,
      "grad_norm": 0.02946585789322853,
      "learning_rate": 0.0004186183836438313,
      "loss": 0.0366,
      "step": 2144
    },
    {
      "epoch": 0.5308092056421678,
      "grad_norm": 0.03610353171825409,
      "learning_rate": 0.0004185465350600087,
      "loss": 0.0649,
      "step": 2145
    },
    {
      "epoch": 0.5310566691413017,
      "grad_norm": 0.03964684158563614,
      "learning_rate": 0.0004184746609458071,
      "loss": 0.0667,
      "step": 2146
    },
    {
      "epoch": 0.5313041326404355,
      "grad_norm": 0.03566884249448776,
      "learning_rate": 0.0004184027613121135,
      "loss": 0.0735,
      "step": 2147
    },
    {
      "epoch": 0.5315515961395694,
      "grad_norm": 0.03277105093002319,
      "learning_rate": 0.00041833083616981895,
      "loss": 0.0363,
      "step": 2148
    },
    {
      "epoch": 0.5317990596387033,
      "grad_norm": 0.021137326955795288,
      "learning_rate": 0.00041825888552981795,
      "loss": 0.0185,
      "step": 2149
    },
    {
      "epoch": 0.5320465231378372,
      "grad_norm": 0.036506813019514084,
      "learning_rate": 0.0004181869094030093,
      "loss": 0.054,
      "step": 2150
    },
    {
      "epoch": 0.532293986636971,
      "grad_norm": 0.03320416063070297,
      "learning_rate": 0.0004181149078002955,
      "loss": 0.0395,
      "step": 2151
    },
    {
      "epoch": 0.5325414501361049,
      "grad_norm": 0.02923930250108242,
      "learning_rate": 0.00041804288073258303,
      "loss": 0.0504,
      "step": 2152
    },
    {
      "epoch": 0.5327889136352388,
      "grad_norm": 0.034094423055648804,
      "learning_rate": 0.0004179708282107818,
      "loss": 0.0735,
      "step": 2153
    },
    {
      "epoch": 0.5330363771343727,
      "grad_norm": 0.051028721034526825,
      "learning_rate": 0.00041789875024580606,
      "loss": 0.1307,
      "step": 2154
    },
    {
      "epoch": 0.5332838406335065,
      "grad_norm": 0.025664573535323143,
      "learning_rate": 0.0004178266468485737,
      "loss": 0.0302,
      "step": 2155
    },
    {
      "epoch": 0.5335313041326404,
      "grad_norm": 0.02856416255235672,
      "learning_rate": 0.00041775451803000664,
      "loss": 0.0387,
      "step": 2156
    },
    {
      "epoch": 0.5337787676317743,
      "grad_norm": 0.0247476976364851,
      "learning_rate": 0.0004176823638010302,
      "loss": 0.0396,
      "step": 2157
    },
    {
      "epoch": 0.5340262311309082,
      "grad_norm": 0.038234908133745193,
      "learning_rate": 0.0004176101841725741,
      "loss": 0.0823,
      "step": 2158
    },
    {
      "epoch": 0.534273694630042,
      "grad_norm": 0.04177512973546982,
      "learning_rate": 0.00041753797915557156,
      "loss": 0.0383,
      "step": 2159
    },
    {
      "epoch": 0.534521158129176,
      "grad_norm": 0.027696840465068817,
      "learning_rate": 0.00041746574876095976,
      "loss": 0.0315,
      "step": 2160
    },
    {
      "epoch": 0.5347686216283098,
      "grad_norm": 0.05508433282375336,
      "learning_rate": 0.00041739349299967977,
      "loss": 0.0715,
      "step": 2161
    },
    {
      "epoch": 0.5350160851274437,
      "grad_norm": 0.03946639969944954,
      "learning_rate": 0.00041732121188267635,
      "loss": 0.0674,
      "step": 2162
    },
    {
      "epoch": 0.5352635486265775,
      "grad_norm": 0.0735144317150116,
      "learning_rate": 0.00041724890542089825,
      "loss": 0.0981,
      "step": 2163
    },
    {
      "epoch": 0.5355110121257115,
      "grad_norm": 0.051203276962041855,
      "learning_rate": 0.000417176573625298,
      "loss": 0.06,
      "step": 2164
    },
    {
      "epoch": 0.5357584756248454,
      "grad_norm": 0.049347344785928726,
      "learning_rate": 0.0004171042165068319,
      "loss": 0.0614,
      "step": 2165
    },
    {
      "epoch": 0.5360059391239792,
      "grad_norm": 0.0604855902493,
      "learning_rate": 0.0004170318340764603,
      "loss": 0.0612,
      "step": 2166
    },
    {
      "epoch": 0.536253402623113,
      "grad_norm": 0.030795138329267502,
      "learning_rate": 0.00041695942634514707,
      "loss": 0.0577,
      "step": 2167
    },
    {
      "epoch": 0.536500866122247,
      "grad_norm": 0.07136397063732147,
      "learning_rate": 0.00041688699332386017,
      "loss": 0.0894,
      "step": 2168
    },
    {
      "epoch": 0.5367483296213809,
      "grad_norm": 0.033711645752191544,
      "learning_rate": 0.00041681453502357123,
      "loss": 0.0251,
      "step": 2169
    },
    {
      "epoch": 0.5369957931205147,
      "grad_norm": 0.053769487887620926,
      "learning_rate": 0.000416742051455256,
      "loss": 0.0736,
      "step": 2170
    },
    {
      "epoch": 0.5372432566196486,
      "grad_norm": 0.04544432833790779,
      "learning_rate": 0.00041666954262989365,
      "loss": 0.0497,
      "step": 2171
    },
    {
      "epoch": 0.5374907201187825,
      "grad_norm": 0.025467833504080772,
      "learning_rate": 0.0004165970085584674,
      "loss": 0.0176,
      "step": 2172
    },
    {
      "epoch": 0.5377381836179164,
      "grad_norm": 0.046419497579336166,
      "learning_rate": 0.0004165244492519642,
      "loss": 0.0479,
      "step": 2173
    },
    {
      "epoch": 0.5379856471170502,
      "grad_norm": 0.13767290115356445,
      "learning_rate": 0.00041645186472137494,
      "loss": 0.0837,
      "step": 2174
    },
    {
      "epoch": 0.5382331106161841,
      "grad_norm": 0.029407372698187828,
      "learning_rate": 0.0004163792549776943,
      "loss": 0.0533,
      "step": 2175
    },
    {
      "epoch": 0.538480574115318,
      "grad_norm": 0.05296444520354271,
      "learning_rate": 0.0004163066200319208,
      "loss": 0.0573,
      "step": 2176
    },
    {
      "epoch": 0.5387280376144519,
      "grad_norm": 0.040570978075265884,
      "learning_rate": 0.0004162339598950565,
      "loss": 0.0585,
      "step": 2177
    },
    {
      "epoch": 0.5389755011135857,
      "grad_norm": 0.026778532192111015,
      "learning_rate": 0.0004161612745781078,
      "loss": 0.056,
      "step": 2178
    },
    {
      "epoch": 0.5392229646127196,
      "grad_norm": 0.032872967422008514,
      "learning_rate": 0.0004160885640920844,
      "loss": 0.043,
      "step": 2179
    },
    {
      "epoch": 0.5394704281118535,
      "grad_norm": 0.03749370574951172,
      "learning_rate": 0.00041601582844800014,
      "loss": 0.0385,
      "step": 2180
    },
    {
      "epoch": 0.5397178916109874,
      "grad_norm": 0.049459896981716156,
      "learning_rate": 0.0004159430676568725,
      "loss": 0.0763,
      "step": 2181
    },
    {
      "epoch": 0.5399653551101212,
      "grad_norm": 0.03280077502131462,
      "learning_rate": 0.0004158702817297229,
      "loss": 0.06,
      "step": 2182
    },
    {
      "epoch": 0.5402128186092552,
      "grad_norm": 0.05128015577793121,
      "learning_rate": 0.0004157974706775764,
      "loss": 0.1236,
      "step": 2183
    },
    {
      "epoch": 0.540460282108389,
      "grad_norm": 0.02404257468879223,
      "learning_rate": 0.00041572463451146216,
      "loss": 0.0297,
      "step": 2184
    },
    {
      "epoch": 0.5407077456075229,
      "grad_norm": 0.03454495593905449,
      "learning_rate": 0.00041565177324241276,
      "loss": 0.0205,
      "step": 2185
    },
    {
      "epoch": 0.5409552091066567,
      "grad_norm": 0.05687350034713745,
      "learning_rate": 0.0004155788868814648,
      "loss": 0.0554,
      "step": 2186
    },
    {
      "epoch": 0.5412026726057907,
      "grad_norm": 0.04382043704390526,
      "learning_rate": 0.00041550597543965867,
      "loss": 0.1108,
      "step": 2187
    },
    {
      "epoch": 0.5414501361049245,
      "grad_norm": 0.03815821558237076,
      "learning_rate": 0.0004154330389280385,
      "loss": 0.0361,
      "step": 2188
    },
    {
      "epoch": 0.5416975996040584,
      "grad_norm": 0.03005027398467064,
      "learning_rate": 0.0004153600773576525,
      "loss": 0.0477,
      "step": 2189
    },
    {
      "epoch": 0.5419450631031922,
      "grad_norm": 0.04598933830857277,
      "learning_rate": 0.00041528709073955207,
      "loss": 0.0561,
      "step": 2190
    },
    {
      "epoch": 0.5421925266023262,
      "grad_norm": 0.04236438125371933,
      "learning_rate": 0.00041521407908479303,
      "loss": 0.0525,
      "step": 2191
    },
    {
      "epoch": 0.54243999010146,
      "grad_norm": 0.05438577011227608,
      "learning_rate": 0.00041514104240443457,
      "loss": 0.0621,
      "step": 2192
    },
    {
      "epoch": 0.5426874536005939,
      "grad_norm": 0.04812747240066528,
      "learning_rate": 0.00041506798070953987,
      "loss": 0.0714,
      "step": 2193
    },
    {
      "epoch": 0.5429349170997277,
      "grad_norm": 0.03031991794705391,
      "learning_rate": 0.000414994894011176,
      "loss": 0.048,
      "step": 2194
    },
    {
      "epoch": 0.5431823805988617,
      "grad_norm": 0.029583454132080078,
      "learning_rate": 0.00041492178232041354,
      "loss": 0.0284,
      "step": 2195
    },
    {
      "epoch": 0.5434298440979956,
      "grad_norm": 0.03962520509958267,
      "learning_rate": 0.000414848645648327,
      "loss": 0.0586,
      "step": 2196
    },
    {
      "epoch": 0.5436773075971294,
      "grad_norm": 0.02335510030388832,
      "learning_rate": 0.0004147754840059947,
      "loss": 0.0223,
      "step": 2197
    },
    {
      "epoch": 0.5439247710962632,
      "grad_norm": 0.027083273977041245,
      "learning_rate": 0.00041470229740449873,
      "loss": 0.0421,
      "step": 2198
    },
    {
      "epoch": 0.5441722345953972,
      "grad_norm": 0.02633693441748619,
      "learning_rate": 0.00041462908585492486,
      "loss": 0.0433,
      "step": 2199
    },
    {
      "epoch": 0.5444196980945311,
      "grad_norm": 0.037337981164455414,
      "learning_rate": 0.00041455584936836274,
      "loss": 0.0424,
      "step": 2200
    },
    {
      "epoch": 0.5444196980945311,
      "eval_loss": 0.2955283224582672,
      "eval_runtime": 202.6185,
      "eval_samples_per_second": 4.935,
      "eval_steps_per_second": 0.311,
      "step": 2200
    },
    {
      "epoch": 0.5446671615936649,
      "grad_norm": 0.023179933428764343,
      "learning_rate": 0.00041448258795590595,
      "loss": 0.0306,
      "step": 2201
    },
    {
      "epoch": 0.5449146250927989,
      "grad_norm": 0.02845335192978382,
      "learning_rate": 0.00041440930162865144,
      "loss": 0.0373,
      "step": 2202
    },
    {
      "epoch": 0.5451620885919327,
      "grad_norm": 0.02565586194396019,
      "learning_rate": 0.0004143359903977002,
      "loss": 0.0414,
      "step": 2203
    },
    {
      "epoch": 0.5454095520910666,
      "grad_norm": 0.01717563346028328,
      "learning_rate": 0.0004142626542741571,
      "loss": 0.0324,
      "step": 2204
    },
    {
      "epoch": 0.5456570155902004,
      "grad_norm": 0.062299348413944244,
      "learning_rate": 0.0004141892932691306,
      "loss": 0.0343,
      "step": 2205
    },
    {
      "epoch": 0.5459044790893344,
      "grad_norm": 0.034558653831481934,
      "learning_rate": 0.00041411590739373285,
      "loss": 0.0343,
      "step": 2206
    },
    {
      "epoch": 0.5461519425884682,
      "grad_norm": 0.03657153248786926,
      "learning_rate": 0.0004140424966590799,
      "loss": 0.0315,
      "step": 2207
    },
    {
      "epoch": 0.5463994060876021,
      "grad_norm": 0.05070623755455017,
      "learning_rate": 0.0004139690610762917,
      "loss": 0.081,
      "step": 2208
    },
    {
      "epoch": 0.5466468695867359,
      "grad_norm": 0.06348392367362976,
      "learning_rate": 0.0004138956006564917,
      "loss": 0.0729,
      "step": 2209
    },
    {
      "epoch": 0.5468943330858699,
      "grad_norm": 0.03594634309411049,
      "learning_rate": 0.00041382211541080716,
      "loss": 0.0596,
      "step": 2210
    },
    {
      "epoch": 0.5471417965850037,
      "grad_norm": 0.08259117603302002,
      "learning_rate": 0.0004137486053503694,
      "loss": 0.0802,
      "step": 2211
    },
    {
      "epoch": 0.5473892600841376,
      "grad_norm": 0.04847073182463646,
      "learning_rate": 0.00041367507048631303,
      "loss": 0.0342,
      "step": 2212
    },
    {
      "epoch": 0.5476367235832714,
      "grad_norm": 0.026975363492965698,
      "learning_rate": 0.00041360151082977674,
      "loss": 0.0498,
      "step": 2213
    },
    {
      "epoch": 0.5478841870824054,
      "grad_norm": 0.02266295626759529,
      "learning_rate": 0.0004135279263919029,
      "loss": 0.0252,
      "step": 2214
    },
    {
      "epoch": 0.5481316505815392,
      "grad_norm": 0.021119918674230576,
      "learning_rate": 0.00041345431718383763,
      "loss": 0.0205,
      "step": 2215
    },
    {
      "epoch": 0.5483791140806731,
      "grad_norm": 0.05411291494965553,
      "learning_rate": 0.00041338068321673083,
      "loss": 0.0393,
      "step": 2216
    },
    {
      "epoch": 0.5486265775798069,
      "grad_norm": 0.041849229484796524,
      "learning_rate": 0.00041330702450173605,
      "loss": 0.0813,
      "step": 2217
    },
    {
      "epoch": 0.5488740410789409,
      "grad_norm": 0.025676270946860313,
      "learning_rate": 0.00041323334105001065,
      "loss": 0.0337,
      "step": 2218
    },
    {
      "epoch": 0.5491215045780747,
      "grad_norm": 0.03027157112956047,
      "learning_rate": 0.00041315963287271573,
      "loss": 0.0367,
      "step": 2219
    },
    {
      "epoch": 0.5493689680772086,
      "grad_norm": 0.040904249995946884,
      "learning_rate": 0.0004130858999810162,
      "loss": 0.0504,
      "step": 2220
    },
    {
      "epoch": 0.5496164315763424,
      "grad_norm": 0.03652922436594963,
      "learning_rate": 0.00041301214238608066,
      "loss": 0.051,
      "step": 2221
    },
    {
      "epoch": 0.5498638950754764,
      "grad_norm": 0.0429573617875576,
      "learning_rate": 0.00041293836009908137,
      "loss": 0.0595,
      "step": 2222
    },
    {
      "epoch": 0.5501113585746102,
      "grad_norm": 0.040237631648778915,
      "learning_rate": 0.0004128645531311944,
      "loss": 0.0559,
      "step": 2223
    },
    {
      "epoch": 0.5503588220737441,
      "grad_norm": 0.042159464210271835,
      "learning_rate": 0.00041279072149359974,
      "loss": 0.0419,
      "step": 2224
    },
    {
      "epoch": 0.550606285572878,
      "grad_norm": 0.03481181338429451,
      "learning_rate": 0.00041271686519748086,
      "loss": 0.0681,
      "step": 2225
    },
    {
      "epoch": 0.5508537490720119,
      "grad_norm": 0.033425528556108475,
      "learning_rate": 0.00041264298425402503,
      "loss": 0.06,
      "step": 2226
    },
    {
      "epoch": 0.5511012125711457,
      "grad_norm": 0.026946239173412323,
      "learning_rate": 0.00041256907867442315,
      "loss": 0.0352,
      "step": 2227
    },
    {
      "epoch": 0.5513486760702796,
      "grad_norm": 0.04352369159460068,
      "learning_rate": 0.0004124951484698702,
      "loss": 0.0282,
      "step": 2228
    },
    {
      "epoch": 0.5515961395694136,
      "grad_norm": 0.050389692187309265,
      "learning_rate": 0.0004124211936515646,
      "loss": 0.0636,
      "step": 2229
    },
    {
      "epoch": 0.5518436030685474,
      "grad_norm": 0.04466354101896286,
      "learning_rate": 0.00041234721423070856,
      "loss": 0.1166,
      "step": 2230
    },
    {
      "epoch": 0.5520910665676813,
      "grad_norm": 0.06152413785457611,
      "learning_rate": 0.0004122732102185079,
      "loss": 0.0692,
      "step": 2231
    },
    {
      "epoch": 0.5523385300668151,
      "grad_norm": 0.04984872043132782,
      "learning_rate": 0.0004121991816261724,
      "loss": 0.0645,
      "step": 2232
    },
    {
      "epoch": 0.5525859935659491,
      "grad_norm": 0.047129131853580475,
      "learning_rate": 0.00041212512846491547,
      "loss": 0.0512,
      "step": 2233
    },
    {
      "epoch": 0.5528334570650829,
      "grad_norm": 0.0386979840695858,
      "learning_rate": 0.0004120510507459542,
      "loss": 0.0682,
      "step": 2234
    },
    {
      "epoch": 0.5530809205642168,
      "grad_norm": 0.04997248575091362,
      "learning_rate": 0.0004119769484805094,
      "loss": 0.1061,
      "step": 2235
    },
    {
      "epoch": 0.5533283840633506,
      "grad_norm": 0.03375387191772461,
      "learning_rate": 0.00041190282167980554,
      "loss": 0.0472,
      "step": 2236
    },
    {
      "epoch": 0.5535758475624846,
      "grad_norm": 0.03966446965932846,
      "learning_rate": 0.00041182867035507103,
      "loss": 0.0594,
      "step": 2237
    },
    {
      "epoch": 0.5538233110616184,
      "grad_norm": 0.031283650547266006,
      "learning_rate": 0.0004117544945175378,
      "loss": 0.0409,
      "step": 2238
    },
    {
      "epoch": 0.5540707745607523,
      "grad_norm": 0.030188556760549545,
      "learning_rate": 0.0004116802941784415,
      "loss": 0.0493,
      "step": 2239
    },
    {
      "epoch": 0.5543182380598861,
      "grad_norm": 0.04893747344613075,
      "learning_rate": 0.00041160606934902166,
      "loss": 0.0523,
      "step": 2240
    },
    {
      "epoch": 0.5545657015590201,
      "grad_norm": 0.03248528018593788,
      "learning_rate": 0.00041153182004052125,
      "loss": 0.0543,
      "step": 2241
    },
    {
      "epoch": 0.5548131650581539,
      "grad_norm": 0.03469166159629822,
      "learning_rate": 0.0004114575462641872,
      "loss": 0.0684,
      "step": 2242
    },
    {
      "epoch": 0.5550606285572878,
      "grad_norm": 0.04342471808195114,
      "learning_rate": 0.0004113832480312699,
      "loss": 0.0353,
      "step": 2243
    },
    {
      "epoch": 0.5553080920564217,
      "grad_norm": 0.04127434641122818,
      "learning_rate": 0.00041130892535302374,
      "loss": 0.055,
      "step": 2244
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.029518738389015198,
      "learning_rate": 0.0004112345782407065,
      "loss": 0.0456,
      "step": 2245
    },
    {
      "epoch": 0.5558030190546894,
      "grad_norm": 0.06510791182518005,
      "learning_rate": 0.00041116020670558,
      "loss": 0.0397,
      "step": 2246
    },
    {
      "epoch": 0.5560504825538233,
      "grad_norm": 0.05175792798399925,
      "learning_rate": 0.0004110858107589095,
      "loss": 0.0572,
      "step": 2247
    },
    {
      "epoch": 0.5562979460529572,
      "grad_norm": 0.033855244517326355,
      "learning_rate": 0.0004110113904119639,
      "loss": 0.0542,
      "step": 2248
    },
    {
      "epoch": 0.5565454095520911,
      "grad_norm": 0.054128821939229965,
      "learning_rate": 0.00041093694567601616,
      "loss": 0.0671,
      "step": 2249
    },
    {
      "epoch": 0.5567928730512249,
      "grad_norm": 0.03503995016217232,
      "learning_rate": 0.00041086247656234255,
      "loss": 0.0604,
      "step": 2250
    },
    {
      "epoch": 0.5570403365503588,
      "grad_norm": 0.048055995255708694,
      "learning_rate": 0.0004107879830822232,
      "loss": 0.1049,
      "step": 2251
    },
    {
      "epoch": 0.5572878000494927,
      "grad_norm": 0.030873171985149384,
      "learning_rate": 0.000410713465246942,
      "loss": 0.0514,
      "step": 2252
    },
    {
      "epoch": 0.5575352635486266,
      "grad_norm": 0.03148223087191582,
      "learning_rate": 0.00041063892306778633,
      "loss": 0.0373,
      "step": 2253
    },
    {
      "epoch": 0.5577827270477604,
      "grad_norm": 0.03934487700462341,
      "learning_rate": 0.0004105643565560475,
      "loss": 0.0437,
      "step": 2254
    },
    {
      "epoch": 0.5580301905468943,
      "grad_norm": 0.0232922974973917,
      "learning_rate": 0.0004104897657230203,
      "loss": 0.033,
      "step": 2255
    },
    {
      "epoch": 0.5582776540460282,
      "grad_norm": 0.03897840902209282,
      "learning_rate": 0.00041041515058000335,
      "loss": 0.0675,
      "step": 2256
    },
    {
      "epoch": 0.5585251175451621,
      "grad_norm": 0.05558694899082184,
      "learning_rate": 0.0004103405111382987,
      "loss": 0.0362,
      "step": 2257
    },
    {
      "epoch": 0.558772581044296,
      "grad_norm": 0.061989594250917435,
      "learning_rate": 0.0004102658474092125,
      "loss": 0.1087,
      "step": 2258
    },
    {
      "epoch": 0.5590200445434298,
      "grad_norm": 0.04385032504796982,
      "learning_rate": 0.00041019115940405426,
      "loss": 0.0886,
      "step": 2259
    },
    {
      "epoch": 0.5592675080425638,
      "grad_norm": 0.03865209221839905,
      "learning_rate": 0.0004101164471341373,
      "loss": 0.0424,
      "step": 2260
    },
    {
      "epoch": 0.5595149715416976,
      "grad_norm": 0.03312920778989792,
      "learning_rate": 0.00041004171061077843,
      "loss": 0.0462,
      "step": 2261
    },
    {
      "epoch": 0.5597624350408315,
      "grad_norm": 0.02856004424393177,
      "learning_rate": 0.0004099669498452984,
      "loss": 0.0436,
      "step": 2262
    },
    {
      "epoch": 0.5600098985399653,
      "grad_norm": 0.025421904399991035,
      "learning_rate": 0.0004098921648490215,
      "loss": 0.0532,
      "step": 2263
    },
    {
      "epoch": 0.5602573620390993,
      "grad_norm": 0.027652382850646973,
      "learning_rate": 0.0004098173556332756,
      "loss": 0.043,
      "step": 2264
    },
    {
      "epoch": 0.5605048255382331,
      "grad_norm": 0.04127126187086105,
      "learning_rate": 0.0004097425222093925,
      "loss": 0.0623,
      "step": 2265
    },
    {
      "epoch": 0.560752289037367,
      "grad_norm": 0.08016080409288406,
      "learning_rate": 0.00040966766458870726,
      "loss": 0.1136,
      "step": 2266
    },
    {
      "epoch": 0.5609997525365009,
      "grad_norm": 0.05368814989924431,
      "learning_rate": 0.000409592782782559,
      "loss": 0.0903,
      "step": 2267
    },
    {
      "epoch": 0.5612472160356348,
      "grad_norm": 0.10425440222024918,
      "learning_rate": 0.0004095178768022904,
      "loss": 0.0999,
      "step": 2268
    },
    {
      "epoch": 0.5614946795347686,
      "grad_norm": 0.0334041528403759,
      "learning_rate": 0.0004094429466592476,
      "loss": 0.0485,
      "step": 2269
    },
    {
      "epoch": 0.5617421430339025,
      "grad_norm": 0.05937133729457855,
      "learning_rate": 0.00040936799236478074,
      "loss": 0.1243,
      "step": 2270
    },
    {
      "epoch": 0.5619896065330364,
      "grad_norm": 0.05722911283373833,
      "learning_rate": 0.0004092930139302433,
      "loss": 0.0785,
      "step": 2271
    },
    {
      "epoch": 0.5622370700321703,
      "grad_norm": 0.07606383413076401,
      "learning_rate": 0.00040921801136699256,
      "loss": 0.0738,
      "step": 2272
    },
    {
      "epoch": 0.5624845335313041,
      "grad_norm": 0.06873588263988495,
      "learning_rate": 0.00040914298468638943,
      "loss": 0.0716,
      "step": 2273
    },
    {
      "epoch": 0.562731997030438,
      "grad_norm": 0.044239167124032974,
      "learning_rate": 0.00040906793389979847,
      "loss": 0.0764,
      "step": 2274
    },
    {
      "epoch": 0.5629794605295719,
      "grad_norm": 0.039785053580999374,
      "learning_rate": 0.00040899285901858804,
      "loss": 0.0532,
      "step": 2275
    },
    {
      "epoch": 0.5632269240287058,
      "grad_norm": 0.032985419034957886,
      "learning_rate": 0.00040891776005412984,
      "loss": 0.0371,
      "step": 2276
    },
    {
      "epoch": 0.5634743875278396,
      "grad_norm": 0.041192181408405304,
      "learning_rate": 0.00040884263701779943,
      "loss": 0.0606,
      "step": 2277
    },
    {
      "epoch": 0.5637218510269735,
      "grad_norm": 0.04350513592362404,
      "learning_rate": 0.0004087674899209761,
      "loss": 0.0635,
      "step": 2278
    },
    {
      "epoch": 0.5639693145261074,
      "grad_norm": 0.01810315251350403,
      "learning_rate": 0.0004086923187750426,
      "loss": 0.0298,
      "step": 2279
    },
    {
      "epoch": 0.5642167780252413,
      "grad_norm": 0.04526553303003311,
      "learning_rate": 0.00040861712359138534,
      "loss": 0.0385,
      "step": 2280
    },
    {
      "epoch": 0.5644642415243751,
      "grad_norm": 0.06329447776079178,
      "learning_rate": 0.0004085419043813945,
      "loss": 0.0899,
      "step": 2281
    },
    {
      "epoch": 0.564711705023509,
      "grad_norm": 0.03246193379163742,
      "learning_rate": 0.00040846666115646363,
      "loss": 0.06,
      "step": 2282
    },
    {
      "epoch": 0.5649591685226429,
      "grad_norm": 0.026862798258662224,
      "learning_rate": 0.0004083913939279903,
      "loss": 0.0287,
      "step": 2283
    },
    {
      "epoch": 0.5652066320217768,
      "grad_norm": 0.029711442068219185,
      "learning_rate": 0.0004083161027073755,
      "loss": 0.0541,
      "step": 2284
    },
    {
      "epoch": 0.5654540955209106,
      "grad_norm": 0.0389983132481575,
      "learning_rate": 0.00040824078750602386,
      "loss": 0.0674,
      "step": 2285
    },
    {
      "epoch": 0.5657015590200446,
      "grad_norm": 0.038422755897045135,
      "learning_rate": 0.0004081654483353435,
      "loss": 0.0565,
      "step": 2286
    },
    {
      "epoch": 0.5659490225191784,
      "grad_norm": 0.03075818158686161,
      "learning_rate": 0.00040809008520674656,
      "loss": 0.0452,
      "step": 2287
    },
    {
      "epoch": 0.5661964860183123,
      "grad_norm": 0.03428490832448006,
      "learning_rate": 0.00040801469813164847,
      "loss": 0.0642,
      "step": 2288
    },
    {
      "epoch": 0.5664439495174461,
      "grad_norm": 0.03453268110752106,
      "learning_rate": 0.00040793928712146843,
      "loss": 0.0328,
      "step": 2289
    },
    {
      "epoch": 0.5666914130165801,
      "grad_norm": 0.04226177930831909,
      "learning_rate": 0.0004078638521876292,
      "loss": 0.045,
      "step": 2290
    },
    {
      "epoch": 0.566938876515714,
      "grad_norm": 0.018033700063824654,
      "learning_rate": 0.00040778839334155715,
      "loss": 0.0214,
      "step": 2291
    },
    {
      "epoch": 0.5671863400148478,
      "grad_norm": 0.03843710944056511,
      "learning_rate": 0.00040771291059468236,
      "loss": 0.0312,
      "step": 2292
    },
    {
      "epoch": 0.5674338035139816,
      "grad_norm": 0.03845524415373802,
      "learning_rate": 0.0004076374039584385,
      "loss": 0.0492,
      "step": 2293
    },
    {
      "epoch": 0.5676812670131156,
      "grad_norm": 0.05820302665233612,
      "learning_rate": 0.00040756187344426285,
      "loss": 0.0764,
      "step": 2294
    },
    {
      "epoch": 0.5679287305122495,
      "grad_norm": 0.039400022476911545,
      "learning_rate": 0.00040748631906359627,
      "loss": 0.0602,
      "step": 2295
    },
    {
      "epoch": 0.5681761940113833,
      "grad_norm": 0.036644041538238525,
      "learning_rate": 0.00040741074082788334,
      "loss": 0.0458,
      "step": 2296
    },
    {
      "epoch": 0.5684236575105172,
      "grad_norm": 0.02066202461719513,
      "learning_rate": 0.00040733513874857203,
      "loss": 0.0276,
      "step": 2297
    },
    {
      "epoch": 0.5686711210096511,
      "grad_norm": 0.03748071938753128,
      "learning_rate": 0.00040725951283711423,
      "loss": 0.0432,
      "step": 2298
    },
    {
      "epoch": 0.568918584508785,
      "grad_norm": 0.03448832780122757,
      "learning_rate": 0.0004071838631049652,
      "loss": 0.0319,
      "step": 2299
    },
    {
      "epoch": 0.5691660480079188,
      "grad_norm": 0.04024257883429527,
      "learning_rate": 0.00040710818956358384,
      "loss": 0.0705,
      "step": 2300
    },
    {
      "epoch": 0.5694135115070527,
      "grad_norm": 0.032704077661037445,
      "learning_rate": 0.0004070324922244329,
      "loss": 0.0456,
      "step": 2301
    },
    {
      "epoch": 0.5696609750061866,
      "grad_norm": 0.035435859113931656,
      "learning_rate": 0.00040695677109897833,
      "loss": 0.068,
      "step": 2302
    },
    {
      "epoch": 0.5699084385053205,
      "grad_norm": 0.02493130974471569,
      "learning_rate": 0.0004068810261986899,
      "loss": 0.0462,
      "step": 2303
    },
    {
      "epoch": 0.5701559020044543,
      "grad_norm": 0.03743153065443039,
      "learning_rate": 0.0004068052575350413,
      "loss": 0.0776,
      "step": 2304
    },
    {
      "epoch": 0.5704033655035882,
      "grad_norm": 0.03533630073070526,
      "learning_rate": 0.00040672946511950903,
      "loss": 0.0362,
      "step": 2305
    },
    {
      "epoch": 0.5706508290027221,
      "grad_norm": 0.054167699068784714,
      "learning_rate": 0.00040665364896357394,
      "loss": 0.0595,
      "step": 2306
    },
    {
      "epoch": 0.570898292501856,
      "grad_norm": 0.027841154485940933,
      "learning_rate": 0.0004065778090787201,
      "loss": 0.0401,
      "step": 2307
    },
    {
      "epoch": 0.5711457560009898,
      "grad_norm": 0.060453981161117554,
      "learning_rate": 0.00040650194547643535,
      "loss": 0.1263,
      "step": 2308
    },
    {
      "epoch": 0.5713932195001238,
      "grad_norm": 0.02839033491909504,
      "learning_rate": 0.00040642605816821096,
      "loss": 0.0452,
      "step": 2309
    },
    {
      "epoch": 0.5716406829992576,
      "grad_norm": 0.021274125203490257,
      "learning_rate": 0.0004063501471655418,
      "loss": 0.043,
      "step": 2310
    },
    {
      "epoch": 0.5718881464983915,
      "grad_norm": 0.06258910149335861,
      "learning_rate": 0.00040627421247992654,
      "loss": 0.1675,
      "step": 2311
    },
    {
      "epoch": 0.5721356099975253,
      "grad_norm": 0.03496381640434265,
      "learning_rate": 0.00040619825412286715,
      "loss": 0.0489,
      "step": 2312
    },
    {
      "epoch": 0.5723830734966593,
      "grad_norm": 0.027939891442656517,
      "learning_rate": 0.0004061222721058695,
      "loss": 0.0512,
      "step": 2313
    },
    {
      "epoch": 0.5726305369957931,
      "grad_norm": 0.0361335352063179,
      "learning_rate": 0.00040604626644044276,
      "loss": 0.0364,
      "step": 2314
    },
    {
      "epoch": 0.572878000494927,
      "grad_norm": 0.026818089187145233,
      "learning_rate": 0.00040597023713809976,
      "loss": 0.0451,
      "step": 2315
    },
    {
      "epoch": 0.5731254639940608,
      "grad_norm": 0.03921572118997574,
      "learning_rate": 0.00040589418421035694,
      "loss": 0.0309,
      "step": 2316
    },
    {
      "epoch": 0.5733729274931948,
      "grad_norm": 0.054115671664476395,
      "learning_rate": 0.0004058181076687345,
      "loss": 0.0503,
      "step": 2317
    },
    {
      "epoch": 0.5736203909923286,
      "grad_norm": 0.03892465680837631,
      "learning_rate": 0.0004057420075247559,
      "loss": 0.0235,
      "step": 2318
    },
    {
      "epoch": 0.5738678544914625,
      "grad_norm": 0.07692689448595047,
      "learning_rate": 0.0004056658837899483,
      "loss": 0.1083,
      "step": 2319
    },
    {
      "epoch": 0.5741153179905963,
      "grad_norm": 0.06853199750185013,
      "learning_rate": 0.00040558973647584253,
      "loss": 0.0985,
      "step": 2320
    },
    {
      "epoch": 0.5743627814897303,
      "grad_norm": 0.05365889146924019,
      "learning_rate": 0.0004055135655939728,
      "loss": 0.0812,
      "step": 2321
    },
    {
      "epoch": 0.5746102449888641,
      "grad_norm": 0.043659795075654984,
      "learning_rate": 0.00040543737115587717,
      "loss": 0.0366,
      "step": 2322
    },
    {
      "epoch": 0.574857708487998,
      "grad_norm": 0.029483841732144356,
      "learning_rate": 0.00040536115317309684,
      "loss": 0.0337,
      "step": 2323
    },
    {
      "epoch": 0.5751051719871318,
      "grad_norm": 0.0513547882437706,
      "learning_rate": 0.0004052849116571771,
      "loss": 0.0749,
      "step": 2324
    },
    {
      "epoch": 0.5753526354862658,
      "grad_norm": 0.036836639046669006,
      "learning_rate": 0.00040520864661966645,
      "loss": 0.0596,
      "step": 2325
    },
    {
      "epoch": 0.5756000989853997,
      "grad_norm": 0.023509489372372627,
      "learning_rate": 0.00040513235807211704,
      "loss": 0.0308,
      "step": 2326
    },
    {
      "epoch": 0.5758475624845335,
      "grad_norm": 0.028129929676651955,
      "learning_rate": 0.0004050560460260844,
      "loss": 0.0471,
      "step": 2327
    },
    {
      "epoch": 0.5760950259836675,
      "grad_norm": 0.033774036914110184,
      "learning_rate": 0.0004049797104931281,
      "loss": 0.0389,
      "step": 2328
    },
    {
      "epoch": 0.5763424894828013,
      "grad_norm": 0.041229236871004105,
      "learning_rate": 0.00040490335148481084,
      "loss": 0.0502,
      "step": 2329
    },
    {
      "epoch": 0.5765899529819352,
      "grad_norm": 0.02836286835372448,
      "learning_rate": 0.000404826969012699,
      "loss": 0.0566,
      "step": 2330
    },
    {
      "epoch": 0.576837416481069,
      "grad_norm": 0.031601130962371826,
      "learning_rate": 0.0004047505630883625,
      "loss": 0.0421,
      "step": 2331
    },
    {
      "epoch": 0.577084879980203,
      "grad_norm": 0.0524163544178009,
      "learning_rate": 0.0004046741337233749,
      "loss": 0.062,
      "step": 2332
    },
    {
      "epoch": 0.5773323434793368,
      "grad_norm": 0.03737339377403259,
      "learning_rate": 0.0004045976809293132,
      "loss": 0.0509,
      "step": 2333
    },
    {
      "epoch": 0.5775798069784707,
      "grad_norm": 0.0575442835688591,
      "learning_rate": 0.000404521204717758,
      "loss": 0.0466,
      "step": 2334
    },
    {
      "epoch": 0.5778272704776045,
      "grad_norm": 0.03330601006746292,
      "learning_rate": 0.0004044447051002934,
      "loss": 0.0321,
      "step": 2335
    },
    {
      "epoch": 0.5780747339767385,
      "grad_norm": 0.029391366988420486,
      "learning_rate": 0.0004043681820885072,
      "loss": 0.0424,
      "step": 2336
    },
    {
      "epoch": 0.5783221974758723,
      "grad_norm": 0.059474483132362366,
      "learning_rate": 0.0004042916356939905,
      "loss": 0.1145,
      "step": 2337
    },
    {
      "epoch": 0.5785696609750062,
      "grad_norm": 0.07909602671861649,
      "learning_rate": 0.0004042150659283381,
      "loss": 0.0496,
      "step": 2338
    },
    {
      "epoch": 0.57881712447414,
      "grad_norm": 0.0392911434173584,
      "learning_rate": 0.0004041384728031484,
      "loss": 0.0412,
      "step": 2339
    },
    {
      "epoch": 0.579064587973274,
      "grad_norm": 0.041971415281295776,
      "learning_rate": 0.0004040618563300231,
      "loss": 0.0364,
      "step": 2340
    },
    {
      "epoch": 0.5793120514724078,
      "grad_norm": 0.06031025946140289,
      "learning_rate": 0.00040398521652056774,
      "loss": 0.0439,
      "step": 2341
    },
    {
      "epoch": 0.5795595149715417,
      "grad_norm": 0.03821174427866936,
      "learning_rate": 0.0004039085533863912,
      "loss": 0.0271,
      "step": 2342
    },
    {
      "epoch": 0.5798069784706755,
      "grad_norm": 0.10408319532871246,
      "learning_rate": 0.00040383186693910585,
      "loss": 0.0583,
      "step": 2343
    },
    {
      "epoch": 0.5800544419698095,
      "grad_norm": 0.03337447717785835,
      "learning_rate": 0.00040375515719032776,
      "loss": 0.0404,
      "step": 2344
    },
    {
      "epoch": 0.5803019054689433,
      "grad_norm": 0.026551328599452972,
      "learning_rate": 0.00040367842415167643,
      "loss": 0.0259,
      "step": 2345
    },
    {
      "epoch": 0.5805493689680772,
      "grad_norm": 0.034283481538295746,
      "learning_rate": 0.0004036016678347748,
      "loss": 0.0458,
      "step": 2346
    },
    {
      "epoch": 0.580796832467211,
      "grad_norm": 0.021874481812119484,
      "learning_rate": 0.0004035248882512496,
      "loss": 0.0261,
      "step": 2347
    },
    {
      "epoch": 0.581044295966345,
      "grad_norm": 0.04362424090504646,
      "learning_rate": 0.0004034480854127308,
      "loss": 0.0674,
      "step": 2348
    },
    {
      "epoch": 0.5812917594654788,
      "grad_norm": 0.027492262423038483,
      "learning_rate": 0.0004033712593308522,
      "loss": 0.0818,
      "step": 2349
    },
    {
      "epoch": 0.5815392229646127,
      "grad_norm": 0.04321012273430824,
      "learning_rate": 0.0004032944100172506,
      "loss": 0.0587,
      "step": 2350
    },
    {
      "epoch": 0.5817866864637466,
      "grad_norm": 0.03427555412054062,
      "learning_rate": 0.00040321753748356697,
      "loss": 0.0474,
      "step": 2351
    },
    {
      "epoch": 0.5820341499628805,
      "grad_norm": 0.045284342020750046,
      "learning_rate": 0.0004031406417414454,
      "loss": 0.0518,
      "step": 2352
    },
    {
      "epoch": 0.5822816134620143,
      "grad_norm": 0.06607288867235184,
      "learning_rate": 0.00040306372280253354,
      "loss": 0.0535,
      "step": 2353
    },
    {
      "epoch": 0.5825290769611482,
      "grad_norm": 0.05405256152153015,
      "learning_rate": 0.0004029867806784826,
      "loss": 0.0769,
      "step": 2354
    },
    {
      "epoch": 0.5827765404602822,
      "grad_norm": 0.047994039952754974,
      "learning_rate": 0.00040290981538094724,
      "loss": 0.0437,
      "step": 2355
    },
    {
      "epoch": 0.583024003959416,
      "grad_norm": 0.04222314804792404,
      "learning_rate": 0.0004028328269215857,
      "loss": 0.0482,
      "step": 2356
    },
    {
      "epoch": 0.5832714674585499,
      "grad_norm": 0.034430813044309616,
      "learning_rate": 0.00040275581531205987,
      "loss": 0.0312,
      "step": 2357
    },
    {
      "epoch": 0.5835189309576837,
      "grad_norm": 0.049420204013586044,
      "learning_rate": 0.0004026787805640348,
      "loss": 0.0563,
      "step": 2358
    },
    {
      "epoch": 0.5837663944568177,
      "grad_norm": 0.02834944613277912,
      "learning_rate": 0.00040260172268917927,
      "loss": 0.0276,
      "step": 2359
    },
    {
      "epoch": 0.5840138579559515,
      "grad_norm": 0.05341031029820442,
      "learning_rate": 0.00040252464169916556,
      "loss": 0.0778,
      "step": 2360
    },
    {
      "epoch": 0.5842613214550854,
      "grad_norm": 0.06712166219949722,
      "learning_rate": 0.0004024475376056695,
      "loss": 0.069,
      "step": 2361
    },
    {
      "epoch": 0.5845087849542192,
      "grad_norm": 0.03412538766860962,
      "learning_rate": 0.0004023704104203702,
      "loss": 0.0519,
      "step": 2362
    },
    {
      "epoch": 0.5847562484533532,
      "grad_norm": 0.06353581696748734,
      "learning_rate": 0.0004022932601549505,
      "loss": 0.0676,
      "step": 2363
    },
    {
      "epoch": 0.585003711952487,
      "grad_norm": 0.0813622698187828,
      "learning_rate": 0.00040221608682109654,
      "loss": 0.1085,
      "step": 2364
    },
    {
      "epoch": 0.5852511754516209,
      "grad_norm": 0.053340520709753036,
      "learning_rate": 0.0004021388904304981,
      "loss": 0.0692,
      "step": 2365
    },
    {
      "epoch": 0.5854986389507547,
      "grad_norm": 0.028403092175722122,
      "learning_rate": 0.0004020616709948485,
      "loss": 0.0392,
      "step": 2366
    },
    {
      "epoch": 0.5857461024498887,
      "grad_norm": 0.042161725461483,
      "learning_rate": 0.00040198442852584435,
      "loss": 0.0662,
      "step": 2367
    },
    {
      "epoch": 0.5859935659490225,
      "grad_norm": 0.027052709832787514,
      "learning_rate": 0.0004019071630351859,
      "loss": 0.0506,
      "step": 2368
    },
    {
      "epoch": 0.5862410294481564,
      "grad_norm": 0.022807110100984573,
      "learning_rate": 0.0004018298745345768,
      "loss": 0.0349,
      "step": 2369
    },
    {
      "epoch": 0.5864884929472903,
      "grad_norm": 0.02456696331501007,
      "learning_rate": 0.0004017525630357243,
      "loss": 0.0506,
      "step": 2370
    },
    {
      "epoch": 0.5867359564464242,
      "grad_norm": 0.0863976776599884,
      "learning_rate": 0.000401675228550339,
      "loss": 0.1168,
      "step": 2371
    },
    {
      "epoch": 0.586983419945558,
      "grad_norm": 0.06047504395246506,
      "learning_rate": 0.00040159787109013515,
      "loss": 0.046,
      "step": 2372
    },
    {
      "epoch": 0.5872308834446919,
      "grad_norm": 0.028336701914668083,
      "learning_rate": 0.00040152049066683024,
      "loss": 0.0363,
      "step": 2373
    },
    {
      "epoch": 0.5874783469438258,
      "grad_norm": 0.027880143374204636,
      "learning_rate": 0.00040144308729214543,
      "loss": 0.0504,
      "step": 2374
    },
    {
      "epoch": 0.5877258104429597,
      "grad_norm": 0.040577251464128494,
      "learning_rate": 0.00040136566097780536,
      "loss": 0.0279,
      "step": 2375
    },
    {
      "epoch": 0.5879732739420935,
      "grad_norm": 0.027279354631900787,
      "learning_rate": 0.000401288211735538,
      "loss": 0.0269,
      "step": 2376
    },
    {
      "epoch": 0.5882207374412274,
      "grad_norm": 0.07527606189250946,
      "learning_rate": 0.0004012107395770749,
      "loss": 0.0842,
      "step": 2377
    },
    {
      "epoch": 0.5884682009403613,
      "grad_norm": 0.06486837565898895,
      "learning_rate": 0.00040113324451415114,
      "loss": 0.069,
      "step": 2378
    },
    {
      "epoch": 0.5887156644394952,
      "grad_norm": 0.02538003958761692,
      "learning_rate": 0.0004010557265585051,
      "loss": 0.0446,
      "step": 2379
    },
    {
      "epoch": 0.588963127938629,
      "grad_norm": 0.04229089245200157,
      "learning_rate": 0.0004009781857218787,
      "loss": 0.1042,
      "step": 2380
    },
    {
      "epoch": 0.5892105914377629,
      "grad_norm": 0.017905544489622116,
      "learning_rate": 0.0004009006220160174,
      "loss": 0.0179,
      "step": 2381
    },
    {
      "epoch": 0.5894580549368968,
      "grad_norm": 0.03146664425730705,
      "learning_rate": 0.00040082303545267006,
      "loss": 0.0452,
      "step": 2382
    },
    {
      "epoch": 0.5897055184360307,
      "grad_norm": 0.04458273947238922,
      "learning_rate": 0.000400745426043589,
      "loss": 0.0811,
      "step": 2383
    },
    {
      "epoch": 0.5899529819351645,
      "grad_norm": 0.027248134836554527,
      "learning_rate": 0.00040066779380053,
      "loss": 0.0508,
      "step": 2384
    },
    {
      "epoch": 0.5902004454342984,
      "grad_norm": 0.06957832723855972,
      "learning_rate": 0.0004005901387352523,
      "loss": 0.0609,
      "step": 2385
    },
    {
      "epoch": 0.5904479089334324,
      "grad_norm": 0.041066426783800125,
      "learning_rate": 0.00040051246085951863,
      "loss": 0.0331,
      "step": 2386
    },
    {
      "epoch": 0.5906953724325662,
      "grad_norm": 0.05017128959298134,
      "learning_rate": 0.0004004347601850952,
      "loss": 0.0691,
      "step": 2387
    },
    {
      "epoch": 0.5909428359317,
      "grad_norm": 0.026008334010839462,
      "learning_rate": 0.00040035703672375143,
      "loss": 0.0416,
      "step": 2388
    },
    {
      "epoch": 0.5911902994308339,
      "grad_norm": 0.051405128091573715,
      "learning_rate": 0.0004002792904872606,
      "loss": 0.0603,
      "step": 2389
    },
    {
      "epoch": 0.5914377629299679,
      "grad_norm": 0.03590462729334831,
      "learning_rate": 0.0004002015214873992,
      "loss": 0.0261,
      "step": 2390
    },
    {
      "epoch": 0.5916852264291017,
      "grad_norm": 0.029030373319983482,
      "learning_rate": 0.00040012372973594713,
      "loss": 0.0588,
      "step": 2391
    },
    {
      "epoch": 0.5919326899282356,
      "grad_norm": 0.06525750458240509,
      "learning_rate": 0.00040004591524468773,
      "loss": 0.1288,
      "step": 2392
    },
    {
      "epoch": 0.5921801534273695,
      "grad_norm": 0.05794509872794151,
      "learning_rate": 0.000399968078025408,
      "loss": 0.0664,
      "step": 2393
    },
    {
      "epoch": 0.5924276169265034,
      "grad_norm": 0.0503111332654953,
      "learning_rate": 0.0003998902180898982,
      "loss": 0.0407,
      "step": 2394
    },
    {
      "epoch": 0.5926750804256372,
      "grad_norm": 0.06858950108289719,
      "learning_rate": 0.000399812335449952,
      "loss": 0.0981,
      "step": 2395
    },
    {
      "epoch": 0.5929225439247711,
      "grad_norm": 0.037779007107019424,
      "learning_rate": 0.0003997344301173666,
      "loss": 0.0447,
      "step": 2396
    },
    {
      "epoch": 0.593170007423905,
      "grad_norm": 0.041768405586481094,
      "learning_rate": 0.00039965650210394265,
      "loss": 0.0741,
      "step": 2397
    },
    {
      "epoch": 0.5934174709230389,
      "grad_norm": 0.03415493667125702,
      "learning_rate": 0.0003995785514214842,
      "loss": 0.084,
      "step": 2398
    },
    {
      "epoch": 0.5936649344221727,
      "grad_norm": 0.02728714793920517,
      "learning_rate": 0.00039950057808179875,
      "loss": 0.0238,
      "step": 2399
    },
    {
      "epoch": 0.5939123979213066,
      "grad_norm": 0.05085387080907822,
      "learning_rate": 0.00039942258209669704,
      "loss": 0.0852,
      "step": 2400
    },
    {
      "epoch": 0.5939123979213066,
      "eval_loss": 0.29409271478652954,
      "eval_runtime": 202.6393,
      "eval_samples_per_second": 4.935,
      "eval_steps_per_second": 0.311,
      "step": 2400
    },
    {
      "epoch": 0.5941598614204405,
      "grad_norm": 0.027388421818614006,
      "learning_rate": 0.00039934456347799364,
      "loss": 0.0259,
      "step": 2401
    },
    {
      "epoch": 0.5944073249195744,
      "grad_norm": 0.04403229057788849,
      "learning_rate": 0.00039926652223750623,
      "loss": 0.0628,
      "step": 2402
    },
    {
      "epoch": 0.5946547884187082,
      "grad_norm": 0.024276381358504295,
      "learning_rate": 0.0003991884583870561,
      "loss": 0.0384,
      "step": 2403
    },
    {
      "epoch": 0.5949022519178421,
      "grad_norm": 0.06432923674583435,
      "learning_rate": 0.00039911037193846776,
      "loss": 0.0681,
      "step": 2404
    },
    {
      "epoch": 0.595149715416976,
      "grad_norm": 0.029695019125938416,
      "learning_rate": 0.00039903226290356926,
      "loss": 0.0593,
      "step": 2405
    },
    {
      "epoch": 0.5953971789161099,
      "grad_norm": 0.04066429287195206,
      "learning_rate": 0.00039895413129419213,
      "loss": 0.0597,
      "step": 2406
    },
    {
      "epoch": 0.5956446424152437,
      "grad_norm": 0.04242471605539322,
      "learning_rate": 0.0003988759771221713,
      "loss": 0.0428,
      "step": 2407
    },
    {
      "epoch": 0.5958921059143776,
      "grad_norm": 0.04050295427441597,
      "learning_rate": 0.0003987978003993449,
      "loss": 0.0392,
      "step": 2408
    },
    {
      "epoch": 0.5961395694135115,
      "grad_norm": 0.07185628265142441,
      "learning_rate": 0.00039871960113755484,
      "loss": 0.0901,
      "step": 2409
    },
    {
      "epoch": 0.5963870329126454,
      "grad_norm": 0.04996565356850624,
      "learning_rate": 0.00039864137934864606,
      "loss": 0.0606,
      "step": 2410
    },
    {
      "epoch": 0.5966344964117792,
      "grad_norm": 0.041266340762376785,
      "learning_rate": 0.0003985631350444674,
      "loss": 0.0542,
      "step": 2411
    },
    {
      "epoch": 0.5968819599109132,
      "grad_norm": 0.03321639075875282,
      "learning_rate": 0.0003984848682368706,
      "loss": 0.0204,
      "step": 2412
    },
    {
      "epoch": 0.597129423410047,
      "grad_norm": 0.02025153487920761,
      "learning_rate": 0.00039840657893771105,
      "loss": 0.026,
      "step": 2413
    },
    {
      "epoch": 0.5973768869091809,
      "grad_norm": 0.11609479784965515,
      "learning_rate": 0.0003983282671588476,
      "loss": 0.1094,
      "step": 2414
    },
    {
      "epoch": 0.5976243504083147,
      "grad_norm": 0.047265324741601944,
      "learning_rate": 0.0003982499329121423,
      "loss": 0.0565,
      "step": 2415
    },
    {
      "epoch": 0.5978718139074487,
      "grad_norm": 0.0412510521709919,
      "learning_rate": 0.00039817157620946085,
      "loss": 0.0378,
      "step": 2416
    },
    {
      "epoch": 0.5981192774065826,
      "grad_norm": 0.029754111543297768,
      "learning_rate": 0.00039809319706267225,
      "loss": 0.0405,
      "step": 2417
    },
    {
      "epoch": 0.5983667409057164,
      "grad_norm": 0.029561029747128487,
      "learning_rate": 0.0003980147954836488,
      "loss": 0.0437,
      "step": 2418
    },
    {
      "epoch": 0.5986142044048502,
      "grad_norm": 0.04245210811495781,
      "learning_rate": 0.0003979363714842664,
      "loss": 0.0555,
      "step": 2419
    },
    {
      "epoch": 0.5988616679039842,
      "grad_norm": 0.044176794588565826,
      "learning_rate": 0.00039785792507640404,
      "loss": 0.0563,
      "step": 2420
    },
    {
      "epoch": 0.5991091314031181,
      "grad_norm": 0.04704852029681206,
      "learning_rate": 0.0003977794562719445,
      "loss": 0.0535,
      "step": 2421
    },
    {
      "epoch": 0.5993565949022519,
      "grad_norm": 0.049609724432229996,
      "learning_rate": 0.00039770096508277364,
      "loss": 0.0622,
      "step": 2422
    },
    {
      "epoch": 0.5996040584013858,
      "grad_norm": 0.04087637737393379,
      "learning_rate": 0.0003976224515207808,
      "loss": 0.0529,
      "step": 2423
    },
    {
      "epoch": 0.5998515219005197,
      "grad_norm": 0.029187887907028198,
      "learning_rate": 0.00039754391559785875,
      "loss": 0.0265,
      "step": 2424
    },
    {
      "epoch": 0.6000989853996536,
      "grad_norm": 0.0673825591802597,
      "learning_rate": 0.0003974653573259036,
      "loss": 0.0771,
      "step": 2425
    },
    {
      "epoch": 0.6003464488987874,
      "grad_norm": 0.05208047479391098,
      "learning_rate": 0.0003973867767168149,
      "loss": 0.07,
      "step": 2426
    },
    {
      "epoch": 0.6005939123979213,
      "grad_norm": 0.03711555153131485,
      "learning_rate": 0.0003973081737824955,
      "loss": 0.0491,
      "step": 2427
    },
    {
      "epoch": 0.6008413758970552,
      "grad_norm": 0.05317717418074608,
      "learning_rate": 0.00039722954853485185,
      "loss": 0.0749,
      "step": 2428
    },
    {
      "epoch": 0.6010888393961891,
      "grad_norm": 0.10340224206447601,
      "learning_rate": 0.00039715090098579344,
      "loss": 0.0323,
      "step": 2429
    },
    {
      "epoch": 0.6013363028953229,
      "grad_norm": 0.025671839714050293,
      "learning_rate": 0.0003970722311472333,
      "loss": 0.0366,
      "step": 2430
    },
    {
      "epoch": 0.6015837663944568,
      "grad_norm": 0.030803553760051727,
      "learning_rate": 0.00039699353903108794,
      "loss": 0.0371,
      "step": 2431
    },
    {
      "epoch": 0.6018312298935907,
      "grad_norm": 0.03544777259230614,
      "learning_rate": 0.0003969148246492771,
      "loss": 0.029,
      "step": 2432
    },
    {
      "epoch": 0.6020786933927246,
      "grad_norm": 0.03512374684214592,
      "learning_rate": 0.000396836088013724,
      "loss": 0.041,
      "step": 2433
    },
    {
      "epoch": 0.6023261568918584,
      "grad_norm": 0.02854877896606922,
      "learning_rate": 0.00039675732913635513,
      "loss": 0.0387,
      "step": 2434
    },
    {
      "epoch": 0.6025736203909924,
      "grad_norm": 0.028369681909680367,
      "learning_rate": 0.00039667854802910037,
      "loss": 0.0399,
      "step": 2435
    },
    {
      "epoch": 0.6028210838901262,
      "grad_norm": 0.0317218042910099,
      "learning_rate": 0.00039659974470389304,
      "loss": 0.0626,
      "step": 2436
    },
    {
      "epoch": 0.6030685473892601,
      "grad_norm": 0.0490204282104969,
      "learning_rate": 0.00039652091917266976,
      "loss": 0.0492,
      "step": 2437
    },
    {
      "epoch": 0.6033160108883939,
      "grad_norm": 0.03887002915143967,
      "learning_rate": 0.00039644207144737053,
      "loss": 0.0334,
      "step": 2438
    },
    {
      "epoch": 0.6035634743875279,
      "grad_norm": 0.0514717660844326,
      "learning_rate": 0.00039636320153993865,
      "loss": 0.0426,
      "step": 2439
    },
    {
      "epoch": 0.6038109378866617,
      "grad_norm": 0.02877543494105339,
      "learning_rate": 0.00039628430946232095,
      "loss": 0.0361,
      "step": 2440
    },
    {
      "epoch": 0.6040584013857956,
      "grad_norm": 0.02165713720023632,
      "learning_rate": 0.00039620539522646755,
      "loss": 0.0439,
      "step": 2441
    },
    {
      "epoch": 0.6043058648849294,
      "grad_norm": 0.04429779574275017,
      "learning_rate": 0.00039612645884433173,
      "loss": 0.0629,
      "step": 2442
    },
    {
      "epoch": 0.6045533283840634,
      "grad_norm": 0.027143267914652824,
      "learning_rate": 0.00039604750032787035,
      "loss": 0.0393,
      "step": 2443
    },
    {
      "epoch": 0.6048007918831972,
      "grad_norm": 0.04173004627227783,
      "learning_rate": 0.0003959685196890435,
      "loss": 0.0675,
      "step": 2444
    },
    {
      "epoch": 0.6050482553823311,
      "grad_norm": 0.03628522902727127,
      "learning_rate": 0.0003958895169398148,
      "loss": 0.0434,
      "step": 2445
    },
    {
      "epoch": 0.6052957188814649,
      "grad_norm": 0.041465722024440765,
      "learning_rate": 0.000395810492092151,
      "loss": 0.0768,
      "step": 2446
    },
    {
      "epoch": 0.6055431823805989,
      "grad_norm": 0.04492852836847305,
      "learning_rate": 0.0003957314451580224,
      "loss": 0.0516,
      "step": 2447
    },
    {
      "epoch": 0.6057906458797327,
      "grad_norm": 0.03853745013475418,
      "learning_rate": 0.00039565237614940234,
      "loss": 0.0692,
      "step": 2448
    },
    {
      "epoch": 0.6060381093788666,
      "grad_norm": 0.03508022055029869,
      "learning_rate": 0.0003955732850782678,
      "loss": 0.047,
      "step": 2449
    },
    {
      "epoch": 0.6062855728780004,
      "grad_norm": 0.05750560387969017,
      "learning_rate": 0.0003954941719565991,
      "loss": 0.0277,
      "step": 2450
    },
    {
      "epoch": 0.6065330363771344,
      "grad_norm": 0.048313457518815994,
      "learning_rate": 0.00039541503679637967,
      "loss": 0.0969,
      "step": 2451
    },
    {
      "epoch": 0.6067804998762683,
      "grad_norm": 0.0466320738196373,
      "learning_rate": 0.00039533587960959655,
      "loss": 0.0634,
      "step": 2452
    },
    {
      "epoch": 0.6070279633754021,
      "grad_norm": 0.03321519121527672,
      "learning_rate": 0.0003952567004082398,
      "loss": 0.0195,
      "step": 2453
    },
    {
      "epoch": 0.607275426874536,
      "grad_norm": 0.019710250198841095,
      "learning_rate": 0.00039517749920430314,
      "loss": 0.0298,
      "step": 2454
    },
    {
      "epoch": 0.6075228903736699,
      "grad_norm": 0.0722159743309021,
      "learning_rate": 0.0003950982760097833,
      "loss": 0.1212,
      "step": 2455
    },
    {
      "epoch": 0.6077703538728038,
      "grad_norm": 0.028508948162198067,
      "learning_rate": 0.0003950190308366808,
      "loss": 0.035,
      "step": 2456
    },
    {
      "epoch": 0.6080178173719376,
      "grad_norm": 0.03503034636378288,
      "learning_rate": 0.0003949397636969989,
      "loss": 0.0443,
      "step": 2457
    },
    {
      "epoch": 0.6082652808710716,
      "grad_norm": 0.038432665169239044,
      "learning_rate": 0.00039486047460274466,
      "loss": 0.0527,
      "step": 2458
    },
    {
      "epoch": 0.6085127443702054,
      "grad_norm": 0.04586663097143173,
      "learning_rate": 0.0003947811635659283,
      "loss": 0.0766,
      "step": 2459
    },
    {
      "epoch": 0.6087602078693393,
      "grad_norm": 0.02704078145325184,
      "learning_rate": 0.0003947018305985633,
      "loss": 0.0718,
      "step": 2460
    },
    {
      "epoch": 0.6090076713684731,
      "grad_norm": 0.06679808348417282,
      "learning_rate": 0.0003946224757126666,
      "loss": 0.0858,
      "step": 2461
    },
    {
      "epoch": 0.6092551348676071,
      "grad_norm": 0.06497064977884293,
      "learning_rate": 0.0003945430989202583,
      "loss": 0.0964,
      "step": 2462
    },
    {
      "epoch": 0.6095025983667409,
      "grad_norm": 0.027947336435317993,
      "learning_rate": 0.0003944637002333619,
      "loss": 0.0262,
      "step": 2463
    },
    {
      "epoch": 0.6097500618658748,
      "grad_norm": 0.032709166407585144,
      "learning_rate": 0.00039438427966400426,
      "loss": 0.0296,
      "step": 2464
    },
    {
      "epoch": 0.6099975253650086,
      "grad_norm": 0.04240916669368744,
      "learning_rate": 0.00039430483722421557,
      "loss": 0.0783,
      "step": 2465
    },
    {
      "epoch": 0.6102449888641426,
      "grad_norm": 0.062204230576753616,
      "learning_rate": 0.00039422537292602925,
      "loss": 0.0544,
      "step": 2466
    },
    {
      "epoch": 0.6104924523632764,
      "grad_norm": 0.05323605239391327,
      "learning_rate": 0.0003941458867814819,
      "loss": 0.0861,
      "step": 2467
    },
    {
      "epoch": 0.6107399158624103,
      "grad_norm": 0.0501982606947422,
      "learning_rate": 0.0003940663788026138,
      "loss": 0.0332,
      "step": 2468
    },
    {
      "epoch": 0.6109873793615441,
      "grad_norm": 0.03152434155344963,
      "learning_rate": 0.0003939868490014681,
      "loss": 0.0415,
      "step": 2469
    },
    {
      "epoch": 0.6112348428606781,
      "grad_norm": 0.06734625995159149,
      "learning_rate": 0.0003939072973900918,
      "loss": 0.0824,
      "step": 2470
    },
    {
      "epoch": 0.6114823063598119,
      "grad_norm": 0.040687158703804016,
      "learning_rate": 0.00039382772398053456,
      "loss": 0.0393,
      "step": 2471
    },
    {
      "epoch": 0.6117297698589458,
      "grad_norm": 0.0384451188147068,
      "learning_rate": 0.0003937481287848499,
      "loss": 0.0357,
      "step": 2472
    },
    {
      "epoch": 0.6119772333580796,
      "grad_norm": 0.030944854021072388,
      "learning_rate": 0.0003936685118150942,
      "loss": 0.0421,
      "step": 2473
    },
    {
      "epoch": 0.6122246968572136,
      "grad_norm": 0.05324310064315796,
      "learning_rate": 0.0003935888730833275,
      "loss": 0.0744,
      "step": 2474
    },
    {
      "epoch": 0.6124721603563474,
      "grad_norm": 0.04491696506738663,
      "learning_rate": 0.00039350921260161286,
      "loss": 0.0482,
      "step": 2475
    },
    {
      "epoch": 0.6127196238554813,
      "grad_norm": 0.02687741070985794,
      "learning_rate": 0.0003934295303820169,
      "loss": 0.0554,
      "step": 2476
    },
    {
      "epoch": 0.6129670873546152,
      "grad_norm": 0.0423281230032444,
      "learning_rate": 0.00039334982643660923,
      "loss": 0.0722,
      "step": 2477
    },
    {
      "epoch": 0.6132145508537491,
      "grad_norm": 0.038940057158470154,
      "learning_rate": 0.00039327010077746307,
      "loss": 0.0526,
      "step": 2478
    },
    {
      "epoch": 0.613462014352883,
      "grad_norm": 0.03988133743405342,
      "learning_rate": 0.0003931903534166545,
      "loss": 0.0714,
      "step": 2479
    },
    {
      "epoch": 0.6137094778520168,
      "grad_norm": 0.0407182052731514,
      "learning_rate": 0.0003931105843662635,
      "loss": 0.0676,
      "step": 2480
    },
    {
      "epoch": 0.6139569413511508,
      "grad_norm": 0.05882604047656059,
      "learning_rate": 0.0003930307936383728,
      "loss": 0.077,
      "step": 2481
    },
    {
      "epoch": 0.6142044048502846,
      "grad_norm": 0.05344481021165848,
      "learning_rate": 0.0003929509812450686,
      "loss": 0.0518,
      "step": 2482
    },
    {
      "epoch": 0.6144518683494185,
      "grad_norm": 0.06775770336389542,
      "learning_rate": 0.0003928711471984404,
      "loss": 0.1104,
      "step": 2483
    },
    {
      "epoch": 0.6146993318485523,
      "grad_norm": 0.02493775449693203,
      "learning_rate": 0.0003927912915105809,
      "loss": 0.0343,
      "step": 2484
    },
    {
      "epoch": 0.6149467953476863,
      "grad_norm": 0.027890071272850037,
      "learning_rate": 0.00039271141419358624,
      "loss": 0.0469,
      "step": 2485
    },
    {
      "epoch": 0.6151942588468201,
      "grad_norm": 0.03337955102324486,
      "learning_rate": 0.0003926315152595558,
      "loss": 0.0435,
      "step": 2486
    },
    {
      "epoch": 0.615441722345954,
      "grad_norm": 0.046449825167655945,
      "learning_rate": 0.000392551594720592,
      "loss": 0.0652,
      "step": 2487
    },
    {
      "epoch": 0.6156891858450878,
      "grad_norm": 0.03403312340378761,
      "learning_rate": 0.0003924716525888008,
      "loss": 0.0379,
      "step": 2488
    },
    {
      "epoch": 0.6159366493442218,
      "grad_norm": 0.06408990919589996,
      "learning_rate": 0.00039239168887629126,
      "loss": 0.059,
      "step": 2489
    },
    {
      "epoch": 0.6161841128433556,
      "grad_norm": 0.04503772407770157,
      "learning_rate": 0.00039231170359517594,
      "loss": 0.0322,
      "step": 2490
    },
    {
      "epoch": 0.6164315763424895,
      "grad_norm": 0.029081642627716064,
      "learning_rate": 0.0003922316967575704,
      "loss": 0.0333,
      "step": 2491
    },
    {
      "epoch": 0.6166790398416233,
      "grad_norm": 0.0691630095243454,
      "learning_rate": 0.00039215166837559357,
      "loss": 0.0877,
      "step": 2492
    },
    {
      "epoch": 0.6169265033407573,
      "grad_norm": 0.055231668055057526,
      "learning_rate": 0.00039207161846136775,
      "loss": 0.0376,
      "step": 2493
    },
    {
      "epoch": 0.6171739668398911,
      "grad_norm": 0.042859386652708054,
      "learning_rate": 0.0003919915470270182,
      "loss": 0.0808,
      "step": 2494
    },
    {
      "epoch": 0.617421430339025,
      "grad_norm": 0.061791643500328064,
      "learning_rate": 0.0003919114540846739,
      "loss": 0.0814,
      "step": 2495
    },
    {
      "epoch": 0.6176688938381588,
      "grad_norm": 0.06067376211285591,
      "learning_rate": 0.0003918313396464666,
      "loss": 0.1065,
      "step": 2496
    },
    {
      "epoch": 0.6179163573372928,
      "grad_norm": 0.4876338839530945,
      "learning_rate": 0.00039175120372453173,
      "loss": 0.0752,
      "step": 2497
    },
    {
      "epoch": 0.6181638208364266,
      "grad_norm": 0.033421073108911514,
      "learning_rate": 0.0003916710463310076,
      "loss": 0.0379,
      "step": 2498
    },
    {
      "epoch": 0.6184112843355605,
      "grad_norm": 0.06885392218828201,
      "learning_rate": 0.00039159086747803606,
      "loss": 0.0706,
      "step": 2499
    },
    {
      "epoch": 0.6186587478346944,
      "grad_norm": 0.049180977046489716,
      "learning_rate": 0.0003915106671777621,
      "loss": 0.0605,
      "step": 2500
    },
    {
      "epoch": 0.6189062113338283,
      "grad_norm": 0.03480446711182594,
      "learning_rate": 0.00039143044544233396,
      "loss": 0.0545,
      "step": 2501
    },
    {
      "epoch": 0.6191536748329621,
      "grad_norm": 0.09332600980997086,
      "learning_rate": 0.00039135020228390317,
      "loss": 0.1089,
      "step": 2502
    },
    {
      "epoch": 0.619401138332096,
      "grad_norm": 0.021497804671525955,
      "learning_rate": 0.0003912699377146243,
      "loss": 0.0328,
      "step": 2503
    },
    {
      "epoch": 0.6196486018312299,
      "grad_norm": 0.028951505199074745,
      "learning_rate": 0.00039118965174665546,
      "loss": 0.0456,
      "step": 2504
    },
    {
      "epoch": 0.6198960653303638,
      "grad_norm": 0.039016298949718475,
      "learning_rate": 0.00039110934439215786,
      "loss": 0.0634,
      "step": 2505
    },
    {
      "epoch": 0.6201435288294976,
      "grad_norm": 0.04103640466928482,
      "learning_rate": 0.00039102901566329595,
      "loss": 0.0709,
      "step": 2506
    },
    {
      "epoch": 0.6203909923286315,
      "grad_norm": 0.034820783883333206,
      "learning_rate": 0.0003909486655722374,
      "loss": 0.0828,
      "step": 2507
    },
    {
      "epoch": 0.6206384558277654,
      "grad_norm": 0.027500921860337257,
      "learning_rate": 0.0003908682941311531,
      "loss": 0.0336,
      "step": 2508
    },
    {
      "epoch": 0.6208859193268993,
      "grad_norm": 0.022886546328663826,
      "learning_rate": 0.0003907879013522174,
      "loss": 0.0226,
      "step": 2509
    },
    {
      "epoch": 0.6211333828260331,
      "grad_norm": 0.04687150940299034,
      "learning_rate": 0.0003907074872476075,
      "loss": 0.0523,
      "step": 2510
    },
    {
      "epoch": 0.621380846325167,
      "grad_norm": 0.0448993481695652,
      "learning_rate": 0.00039062705182950407,
      "loss": 0.0728,
      "step": 2511
    },
    {
      "epoch": 0.621628309824301,
      "grad_norm": 0.038333479315042496,
      "learning_rate": 0.00039054659511009106,
      "loss": 0.0615,
      "step": 2512
    },
    {
      "epoch": 0.6218757733234348,
      "grad_norm": 0.04213063791394234,
      "learning_rate": 0.00039046611710155546,
      "loss": 0.0443,
      "step": 2513
    },
    {
      "epoch": 0.6221232368225686,
      "grad_norm": 0.07897498458623886,
      "learning_rate": 0.0003903856178160875,
      "loss": 0.0786,
      "step": 2514
    },
    {
      "epoch": 0.6223707003217025,
      "grad_norm": 0.05379630997776985,
      "learning_rate": 0.0003903050972658809,
      "loss": 0.0629,
      "step": 2515
    },
    {
      "epoch": 0.6226181638208365,
      "grad_norm": 0.03436089679598808,
      "learning_rate": 0.0003902245554631324,
      "loss": 0.0448,
      "step": 2516
    },
    {
      "epoch": 0.6228656273199703,
      "grad_norm": 0.04036248102784157,
      "learning_rate": 0.00039014399242004184,
      "loss": 0.0745,
      "step": 2517
    },
    {
      "epoch": 0.6231130908191042,
      "grad_norm": 0.030554743483662605,
      "learning_rate": 0.00039006340814881234,
      "loss": 0.0252,
      "step": 2518
    },
    {
      "epoch": 0.6233605543182381,
      "grad_norm": 0.028074702247977257,
      "learning_rate": 0.0003899828026616506,
      "loss": 0.0504,
      "step": 2519
    },
    {
      "epoch": 0.623608017817372,
      "grad_norm": 0.0378921777009964,
      "learning_rate": 0.000389902175970766,
      "loss": 0.0706,
      "step": 2520
    },
    {
      "epoch": 0.6238554813165058,
      "grad_norm": 0.04359983280301094,
      "learning_rate": 0.0003898215280883714,
      "loss": 0.0792,
      "step": 2521
    },
    {
      "epoch": 0.6241029448156397,
      "grad_norm": 0.0494966134428978,
      "learning_rate": 0.00038974085902668287,
      "loss": 0.0801,
      "step": 2522
    },
    {
      "epoch": 0.6243504083147736,
      "grad_norm": 0.0365089513361454,
      "learning_rate": 0.00038966016879791966,
      "loss": 0.0507,
      "step": 2523
    },
    {
      "epoch": 0.6245978718139075,
      "grad_norm": 0.056625571101903915,
      "learning_rate": 0.0003895794574143043,
      "loss": 0.1149,
      "step": 2524
    },
    {
      "epoch": 0.6248453353130413,
      "grad_norm": 0.03128757327795029,
      "learning_rate": 0.0003894987248880623,
      "loss": 0.0348,
      "step": 2525
    },
    {
      "epoch": 0.6250927988121752,
      "grad_norm": 0.04049360007047653,
      "learning_rate": 0.00038941797123142257,
      "loss": 0.0754,
      "step": 2526
    },
    {
      "epoch": 0.6253402623113091,
      "grad_norm": 0.021445252001285553,
      "learning_rate": 0.00038933719645661727,
      "loss": 0.0263,
      "step": 2527
    },
    {
      "epoch": 0.625587725810443,
      "grad_norm": 0.031737253069877625,
      "learning_rate": 0.0003892564005758815,
      "loss": 0.0371,
      "step": 2528
    },
    {
      "epoch": 0.6258351893095768,
      "grad_norm": 0.03895033895969391,
      "learning_rate": 0.0003891755836014538,
      "loss": 0.0652,
      "step": 2529
    },
    {
      "epoch": 0.6260826528087107,
      "grad_norm": 0.029222968965768814,
      "learning_rate": 0.00038909474554557584,
      "loss": 0.0526,
      "step": 2530
    },
    {
      "epoch": 0.6263301163078446,
      "grad_norm": 0.045508820563554764,
      "learning_rate": 0.0003890138864204924,
      "loss": 0.1194,
      "step": 2531
    },
    {
      "epoch": 0.6265775798069785,
      "grad_norm": 0.04448195919394493,
      "learning_rate": 0.00038893300623845165,
      "loss": 0.0448,
      "step": 2532
    },
    {
      "epoch": 0.6268250433061123,
      "grad_norm": 0.04529577121138573,
      "learning_rate": 0.00038885210501170465,
      "loss": 0.0802,
      "step": 2533
    },
    {
      "epoch": 0.6270725068052462,
      "grad_norm": 0.03464208543300629,
      "learning_rate": 0.0003887711827525059,
      "loss": 0.0291,
      "step": 2534
    },
    {
      "epoch": 0.6273199703043801,
      "grad_norm": 0.06572777777910233,
      "learning_rate": 0.000388690239473113,
      "loss": 0.0319,
      "step": 2535
    },
    {
      "epoch": 0.627567433803514,
      "grad_norm": 0.08906888216733932,
      "learning_rate": 0.0003886092751857867,
      "loss": 0.0578,
      "step": 2536
    },
    {
      "epoch": 0.6278148973026478,
      "grad_norm": 0.03396929055452347,
      "learning_rate": 0.00038852828990279097,
      "loss": 0.0464,
      "step": 2537
    },
    {
      "epoch": 0.6280623608017817,
      "grad_norm": 0.030139191076159477,
      "learning_rate": 0.000388447283636393,
      "loss": 0.0523,
      "step": 2538
    },
    {
      "epoch": 0.6283098243009156,
      "grad_norm": 0.02996337041258812,
      "learning_rate": 0.00038836625639886315,
      "loss": 0.045,
      "step": 2539
    },
    {
      "epoch": 0.6285572878000495,
      "grad_norm": 0.04101959988474846,
      "learning_rate": 0.00038828520820247483,
      "loss": 0.061,
      "step": 2540
    },
    {
      "epoch": 0.6288047512991833,
      "grad_norm": 0.0403660386800766,
      "learning_rate": 0.0003882041390595047,
      "loss": 0.071,
      "step": 2541
    },
    {
      "epoch": 0.6290522147983173,
      "grad_norm": 0.03187215328216553,
      "learning_rate": 0.00038812304898223264,
      "loss": 0.0496,
      "step": 2542
    },
    {
      "epoch": 0.6292996782974511,
      "grad_norm": 0.05639217048883438,
      "learning_rate": 0.0003880419379829417,
      "loss": 0.0945,
      "step": 2543
    },
    {
      "epoch": 0.629547141796585,
      "grad_norm": 0.031100338324904442,
      "learning_rate": 0.00038796080607391803,
      "loss": 0.0267,
      "step": 2544
    },
    {
      "epoch": 0.6297946052957188,
      "grad_norm": 0.031200462952256203,
      "learning_rate": 0.00038787965326745114,
      "loss": 0.0482,
      "step": 2545
    },
    {
      "epoch": 0.6300420687948528,
      "grad_norm": 0.0299238208681345,
      "learning_rate": 0.00038779847957583327,
      "loss": 0.047,
      "step": 2546
    },
    {
      "epoch": 0.6302895322939867,
      "grad_norm": 0.04354991018772125,
      "learning_rate": 0.0003877172850113603,
      "loss": 0.0483,
      "step": 2547
    },
    {
      "epoch": 0.6305369957931205,
      "grad_norm": 0.027434194460511208,
      "learning_rate": 0.000387636069586331,
      "loss": 0.0332,
      "step": 2548
    },
    {
      "epoch": 0.6307844592922544,
      "grad_norm": 0.02933821640908718,
      "learning_rate": 0.0003875548333130475,
      "loss": 0.0465,
      "step": 2549
    },
    {
      "epoch": 0.6310319227913883,
      "grad_norm": 0.05199956148862839,
      "learning_rate": 0.00038747357620381485,
      "loss": 0.0408,
      "step": 2550
    },
    {
      "epoch": 0.6312793862905222,
      "grad_norm": 0.02128303237259388,
      "learning_rate": 0.00038739229827094133,
      "loss": 0.0362,
      "step": 2551
    },
    {
      "epoch": 0.631526849789656,
      "grad_norm": 0.038253575563430786,
      "learning_rate": 0.00038731099952673853,
      "loss": 0.0673,
      "step": 2552
    },
    {
      "epoch": 0.6317743132887899,
      "grad_norm": 0.03572575002908707,
      "learning_rate": 0.000387229679983521,
      "loss": 0.0699,
      "step": 2553
    },
    {
      "epoch": 0.6320217767879238,
      "grad_norm": 0.04041744023561478,
      "learning_rate": 0.00038714833965360654,
      "loss": 0.0633,
      "step": 2554
    },
    {
      "epoch": 0.6322692402870577,
      "grad_norm": 0.029355136677622795,
      "learning_rate": 0.00038706697854931605,
      "loss": 0.0349,
      "step": 2555
    },
    {
      "epoch": 0.6325167037861915,
      "grad_norm": 0.030923426151275635,
      "learning_rate": 0.0003869855966829737,
      "loss": 0.0516,
      "step": 2556
    },
    {
      "epoch": 0.6327641672853254,
      "grad_norm": 0.04160814359784126,
      "learning_rate": 0.0003869041940669066,
      "loss": 0.0529,
      "step": 2557
    },
    {
      "epoch": 0.6330116307844593,
      "grad_norm": 0.05100400745868683,
      "learning_rate": 0.0003868227707134452,
      "loss": 0.0583,
      "step": 2558
    },
    {
      "epoch": 0.6332590942835932,
      "grad_norm": 0.04628176614642143,
      "learning_rate": 0.0003867413266349229,
      "loss": 0.0553,
      "step": 2559
    },
    {
      "epoch": 0.633506557782727,
      "grad_norm": 0.0370248407125473,
      "learning_rate": 0.0003866598618436764,
      "loss": 0.0499,
      "step": 2560
    },
    {
      "epoch": 0.633754021281861,
      "grad_norm": 0.05161171033978462,
      "learning_rate": 0.0003865783763520455,
      "loss": 0.048,
      "step": 2561
    },
    {
      "epoch": 0.6340014847809948,
      "grad_norm": 0.04658148065209389,
      "learning_rate": 0.00038649687017237313,
      "loss": 0.0509,
      "step": 2562
    },
    {
      "epoch": 0.6342489482801287,
      "grad_norm": 0.03336861729621887,
      "learning_rate": 0.0003864153433170051,
      "loss": 0.0478,
      "step": 2563
    },
    {
      "epoch": 0.6344964117792625,
      "grad_norm": 0.03936314210295677,
      "learning_rate": 0.00038633379579829097,
      "loss": 0.0646,
      "step": 2564
    },
    {
      "epoch": 0.6347438752783965,
      "grad_norm": 0.06769702583551407,
      "learning_rate": 0.0003862522276285829,
      "loss": 0.0693,
      "step": 2565
    },
    {
      "epoch": 0.6349913387775303,
      "grad_norm": 0.04564501345157623,
      "learning_rate": 0.00038617063882023615,
      "loss": 0.0346,
      "step": 2566
    },
    {
      "epoch": 0.6352388022766642,
      "grad_norm": 0.04318002238869667,
      "learning_rate": 0.0003860890293856095,
      "loss": 0.0605,
      "step": 2567
    },
    {
      "epoch": 0.635486265775798,
      "grad_norm": 0.026292841881513596,
      "learning_rate": 0.00038600739933706456,
      "loss": 0.0398,
      "step": 2568
    },
    {
      "epoch": 0.635733729274932,
      "grad_norm": 0.037025086581707,
      "learning_rate": 0.00038592574868696617,
      "loss": 0.0609,
      "step": 2569
    },
    {
      "epoch": 0.6359811927740658,
      "grad_norm": 0.046903081238269806,
      "learning_rate": 0.00038584407744768226,
      "loss": 0.0512,
      "step": 2570
    },
    {
      "epoch": 0.6362286562731997,
      "grad_norm": 0.09430857747793198,
      "learning_rate": 0.00038576238563158385,
      "loss": 0.0588,
      "step": 2571
    },
    {
      "epoch": 0.6364761197723335,
      "grad_norm": 0.08235207200050354,
      "learning_rate": 0.00038568067325104504,
      "loss": 0.0931,
      "step": 2572
    },
    {
      "epoch": 0.6367235832714675,
      "grad_norm": 0.04248296841979027,
      "learning_rate": 0.00038559894031844335,
      "loss": 0.0963,
      "step": 2573
    },
    {
      "epoch": 0.6369710467706013,
      "grad_norm": 0.045296501368284225,
      "learning_rate": 0.00038551718684615887,
      "loss": 0.0814,
      "step": 2574
    },
    {
      "epoch": 0.6372185102697352,
      "grad_norm": 0.049016162753105164,
      "learning_rate": 0.00038543541284657534,
      "loss": 0.0803,
      "step": 2575
    },
    {
      "epoch": 0.637465973768869,
      "grad_norm": 0.030804665759205818,
      "learning_rate": 0.0003853536183320793,
      "loss": 0.0723,
      "step": 2576
    },
    {
      "epoch": 0.637713437268003,
      "grad_norm": 0.045354463160037994,
      "learning_rate": 0.0003852718033150603,
      "loss": 0.0434,
      "step": 2577
    },
    {
      "epoch": 0.6379609007671369,
      "grad_norm": 0.04185367003083229,
      "learning_rate": 0.0003851899678079115,
      "loss": 0.0648,
      "step": 2578
    },
    {
      "epoch": 0.6382083642662707,
      "grad_norm": 0.06062805652618408,
      "learning_rate": 0.00038510811182302865,
      "loss": 0.0594,
      "step": 2579
    },
    {
      "epoch": 0.6384558277654045,
      "grad_norm": 0.03106009215116501,
      "learning_rate": 0.00038502623537281077,
      "loss": 0.0615,
      "step": 2580
    },
    {
      "epoch": 0.6387032912645385,
      "grad_norm": 0.03159298747777939,
      "learning_rate": 0.00038494433846966003,
      "loss": 0.0391,
      "step": 2581
    },
    {
      "epoch": 0.6389507547636724,
      "grad_norm": 0.030511319637298584,
      "learning_rate": 0.0003848624211259816,
      "loss": 0.0229,
      "step": 2582
    },
    {
      "epoch": 0.6391982182628062,
      "grad_norm": 0.05334583669900894,
      "learning_rate": 0.0003847804833541839,
      "loss": 0.0665,
      "step": 2583
    },
    {
      "epoch": 0.6394456817619402,
      "grad_norm": 0.054535459727048874,
      "learning_rate": 0.00038469852516667837,
      "loss": 0.1102,
      "step": 2584
    },
    {
      "epoch": 0.639693145261074,
      "grad_norm": 0.038320913910865784,
      "learning_rate": 0.0003846165465758794,
      "loss": 0.044,
      "step": 2585
    },
    {
      "epoch": 0.6399406087602079,
      "grad_norm": 0.07621227949857712,
      "learning_rate": 0.0003845345475942048,
      "loss": 0.1162,
      "step": 2586
    },
    {
      "epoch": 0.6401880722593417,
      "grad_norm": 0.04491780325770378,
      "learning_rate": 0.00038445252823407494,
      "loss": 0.0978,
      "step": 2587
    },
    {
      "epoch": 0.6404355357584757,
      "grad_norm": 0.061931513249874115,
      "learning_rate": 0.000384370488507914,
      "loss": 0.1097,
      "step": 2588
    },
    {
      "epoch": 0.6406829992576095,
      "grad_norm": 0.02487846650183201,
      "learning_rate": 0.00038428842842814847,
      "loss": 0.0447,
      "step": 2589
    },
    {
      "epoch": 0.6409304627567434,
      "grad_norm": 0.07615659385919571,
      "learning_rate": 0.0003842063480072086,
      "loss": 0.0668,
      "step": 2590
    },
    {
      "epoch": 0.6411779262558772,
      "grad_norm": 0.03802910074591637,
      "learning_rate": 0.00038412424725752723,
      "loss": 0.05,
      "step": 2591
    },
    {
      "epoch": 0.6414253897550112,
      "grad_norm": 0.0523655004799366,
      "learning_rate": 0.00038404212619154053,
      "loss": 0.0595,
      "step": 2592
    },
    {
      "epoch": 0.641672853254145,
      "grad_norm": 0.0382222905755043,
      "learning_rate": 0.0003839599848216877,
      "loss": 0.0839,
      "step": 2593
    },
    {
      "epoch": 0.6419203167532789,
      "grad_norm": 0.04177934303879738,
      "learning_rate": 0.000383877823160411,
      "loss": 0.0518,
      "step": 2594
    },
    {
      "epoch": 0.6421677802524127,
      "grad_norm": 0.03476111963391304,
      "learning_rate": 0.00038379564122015573,
      "loss": 0.0406,
      "step": 2595
    },
    {
      "epoch": 0.6424152437515467,
      "grad_norm": 0.052387017756700516,
      "learning_rate": 0.00038371343901337046,
      "loss": 0.0524,
      "step": 2596
    },
    {
      "epoch": 0.6426627072506805,
      "grad_norm": 0.025205863639712334,
      "learning_rate": 0.0003836312165525064,
      "loss": 0.0306,
      "step": 2597
    },
    {
      "epoch": 0.6429101707498144,
      "grad_norm": 0.040204279124736786,
      "learning_rate": 0.0003835489738500183,
      "loss": 0.0528,
      "step": 2598
    },
    {
      "epoch": 0.6431576342489482,
      "grad_norm": 0.05174612998962402,
      "learning_rate": 0.0003834667109183637,
      "loss": 0.084,
      "step": 2599
    },
    {
      "epoch": 0.6434050977480822,
      "grad_norm": 0.0342181958258152,
      "learning_rate": 0.00038338442777000324,
      "loss": 0.057,
      "step": 2600
    },
    {
      "epoch": 0.6434050977480822,
      "eval_loss": 0.29485368728637695,
      "eval_runtime": 202.5086,
      "eval_samples_per_second": 4.938,
      "eval_steps_per_second": 0.311,
      "step": 2600
    },
    {
      "epoch": 0.643652561247216,
      "grad_norm": 0.0357576422393322,
      "learning_rate": 0.0003833021244174008,
      "loss": 0.0554,
      "step": 2601
    },
    {
      "epoch": 0.6439000247463499,
      "grad_norm": 0.030916782096028328,
      "learning_rate": 0.00038321980087302287,
      "loss": 0.0824,
      "step": 2602
    },
    {
      "epoch": 0.6441474882454838,
      "grad_norm": 0.04977107048034668,
      "learning_rate": 0.0003831374571493397,
      "loss": 0.1062,
      "step": 2603
    },
    {
      "epoch": 0.6443949517446177,
      "grad_norm": 0.04311227798461914,
      "learning_rate": 0.000383055093258824,
      "loss": 0.0605,
      "step": 2604
    },
    {
      "epoch": 0.6446424152437515,
      "grad_norm": 0.03386778011918068,
      "learning_rate": 0.00038297270921395165,
      "loss": 0.0558,
      "step": 2605
    },
    {
      "epoch": 0.6448898787428854,
      "grad_norm": 0.07409419864416122,
      "learning_rate": 0.0003828903050272019,
      "loss": 0.112,
      "step": 2606
    },
    {
      "epoch": 0.6451373422420194,
      "grad_norm": 0.03911628946661949,
      "learning_rate": 0.00038280788071105664,
      "loss": 0.06,
      "step": 2607
    },
    {
      "epoch": 0.6453848057411532,
      "grad_norm": 0.06863750517368317,
      "learning_rate": 0.0003827254362780012,
      "loss": 0.0754,
      "step": 2608
    },
    {
      "epoch": 0.645632269240287,
      "grad_norm": 0.029117517173290253,
      "learning_rate": 0.00038264297174052343,
      "loss": 0.0602,
      "step": 2609
    },
    {
      "epoch": 0.6458797327394209,
      "grad_norm": 0.042376574128866196,
      "learning_rate": 0.0003825604871111148,
      "loss": 0.0258,
      "step": 2610
    },
    {
      "epoch": 0.6461271962385549,
      "grad_norm": 0.030273562297225,
      "learning_rate": 0.0003824779824022695,
      "loss": 0.049,
      "step": 2611
    },
    {
      "epoch": 0.6463746597376887,
      "grad_norm": 0.03511786833405495,
      "learning_rate": 0.0003823954576264848,
      "loss": 0.0589,
      "step": 2612
    },
    {
      "epoch": 0.6466221232368226,
      "grad_norm": 0.035814013332128525,
      "learning_rate": 0.000382312912796261,
      "loss": 0.0459,
      "step": 2613
    },
    {
      "epoch": 0.6468695867359564,
      "grad_norm": 0.037762489169836044,
      "learning_rate": 0.00038223034792410153,
      "loss": 0.0499,
      "step": 2614
    },
    {
      "epoch": 0.6471170502350904,
      "grad_norm": 0.04318425804376602,
      "learning_rate": 0.0003821477630225129,
      "loss": 0.045,
      "step": 2615
    },
    {
      "epoch": 0.6473645137342242,
      "grad_norm": 0.030687721446156502,
      "learning_rate": 0.00038206515810400445,
      "loss": 0.0503,
      "step": 2616
    },
    {
      "epoch": 0.6476119772333581,
      "grad_norm": 0.03210865706205368,
      "learning_rate": 0.00038198253318108853,
      "loss": 0.0561,
      "step": 2617
    },
    {
      "epoch": 0.6478594407324919,
      "grad_norm": 0.0439436212182045,
      "learning_rate": 0.00038189988826628095,
      "loss": 0.0895,
      "step": 2618
    },
    {
      "epoch": 0.6481069042316259,
      "grad_norm": 0.04520519822835922,
      "learning_rate": 0.0003818172233721,
      "loss": 0.0648,
      "step": 2619
    },
    {
      "epoch": 0.6483543677307597,
      "grad_norm": 0.07187381386756897,
      "learning_rate": 0.00038173453851106745,
      "loss": 0.0743,
      "step": 2620
    },
    {
      "epoch": 0.6486018312298936,
      "grad_norm": 0.026998350396752357,
      "learning_rate": 0.0003816518336957077,
      "loss": 0.0424,
      "step": 2621
    },
    {
      "epoch": 0.6488492947290274,
      "grad_norm": 0.05709988996386528,
      "learning_rate": 0.0003815691089385483,
      "loss": 0.0548,
      "step": 2622
    },
    {
      "epoch": 0.6490967582281614,
      "grad_norm": 0.04537331312894821,
      "learning_rate": 0.0003814863642521201,
      "loss": 0.0678,
      "step": 2623
    },
    {
      "epoch": 0.6493442217272952,
      "grad_norm": 0.039910025894641876,
      "learning_rate": 0.0003814035996489567,
      "loss": 0.0641,
      "step": 2624
    },
    {
      "epoch": 0.6495916852264291,
      "grad_norm": 0.040941767394542694,
      "learning_rate": 0.00038132081514159473,
      "loss": 0.027,
      "step": 2625
    },
    {
      "epoch": 0.649839148725563,
      "grad_norm": 0.03589576110243797,
      "learning_rate": 0.0003812380107425738,
      "loss": 0.0402,
      "step": 2626
    },
    {
      "epoch": 0.6500866122246969,
      "grad_norm": 0.04874832183122635,
      "learning_rate": 0.0003811551864644367,
      "loss": 0.1076,
      "step": 2627
    },
    {
      "epoch": 0.6503340757238307,
      "grad_norm": 0.031225191429257393,
      "learning_rate": 0.0003810723423197291,
      "loss": 0.0348,
      "step": 2628
    },
    {
      "epoch": 0.6505815392229646,
      "grad_norm": 0.05118999257683754,
      "learning_rate": 0.0003809894783209997,
      "loss": 0.1112,
      "step": 2629
    },
    {
      "epoch": 0.6508290027220985,
      "grad_norm": 0.02792651392519474,
      "learning_rate": 0.00038090659448080023,
      "loss": 0.0339,
      "step": 2630
    },
    {
      "epoch": 0.6510764662212324,
      "grad_norm": 0.0393390953540802,
      "learning_rate": 0.00038082369081168544,
      "loss": 0.0482,
      "step": 2631
    },
    {
      "epoch": 0.6513239297203662,
      "grad_norm": 0.06256836652755737,
      "learning_rate": 0.0003807407673262131,
      "loss": 0.0642,
      "step": 2632
    },
    {
      "epoch": 0.6515713932195001,
      "grad_norm": 0.045279935002326965,
      "learning_rate": 0.0003806578240369439,
      "loss": 0.0434,
      "step": 2633
    },
    {
      "epoch": 0.651818856718634,
      "grad_norm": 0.044829901307821274,
      "learning_rate": 0.0003805748609564415,
      "loss": 0.0875,
      "step": 2634
    },
    {
      "epoch": 0.6520663202177679,
      "grad_norm": 0.022336503490805626,
      "learning_rate": 0.00038049187809727274,
      "loss": 0.0519,
      "step": 2635
    },
    {
      "epoch": 0.6523137837169017,
      "grad_norm": 0.0518309511244297,
      "learning_rate": 0.0003804088754720073,
      "loss": 0.0484,
      "step": 2636
    },
    {
      "epoch": 0.6525612472160356,
      "grad_norm": 0.020869147032499313,
      "learning_rate": 0.00038032585309321785,
      "loss": 0.038,
      "step": 2637
    },
    {
      "epoch": 0.6528087107151695,
      "grad_norm": 0.04091648757457733,
      "learning_rate": 0.00038024281097348025,
      "loss": 0.0438,
      "step": 2638
    },
    {
      "epoch": 0.6530561742143034,
      "grad_norm": 0.03248954191803932,
      "learning_rate": 0.00038015974912537304,
      "loss": 0.0525,
      "step": 2639
    },
    {
      "epoch": 0.6533036377134372,
      "grad_norm": 0.034427616745233536,
      "learning_rate": 0.00038007666756147796,
      "loss": 0.0629,
      "step": 2640
    },
    {
      "epoch": 0.6535511012125711,
      "grad_norm": 0.02198866382241249,
      "learning_rate": 0.00037999356629437974,
      "loss": 0.0312,
      "step": 2641
    },
    {
      "epoch": 0.653798564711705,
      "grad_norm": 0.04890531301498413,
      "learning_rate": 0.0003799104453366661,
      "loss": 0.107,
      "step": 2642
    },
    {
      "epoch": 0.6540460282108389,
      "grad_norm": 0.03437897190451622,
      "learning_rate": 0.00037982730470092753,
      "loss": 0.034,
      "step": 2643
    },
    {
      "epoch": 0.6542934917099728,
      "grad_norm": 0.06374114751815796,
      "learning_rate": 0.00037974414439975767,
      "loss": 0.1236,
      "step": 2644
    },
    {
      "epoch": 0.6545409552091067,
      "grad_norm": 0.0471523143351078,
      "learning_rate": 0.0003796609644457532,
      "loss": 0.0665,
      "step": 2645
    },
    {
      "epoch": 0.6547884187082406,
      "grad_norm": 0.045001253485679626,
      "learning_rate": 0.00037957776485151364,
      "loss": 0.0533,
      "step": 2646
    },
    {
      "epoch": 0.6550358822073744,
      "grad_norm": 0.965971827507019,
      "learning_rate": 0.0003794945456296417,
      "loss": 0.0966,
      "step": 2647
    },
    {
      "epoch": 0.6552833457065083,
      "grad_norm": 0.04049934446811676,
      "learning_rate": 0.0003794113067927427,
      "loss": 0.03,
      "step": 2648
    },
    {
      "epoch": 0.6555308092056422,
      "grad_norm": 0.028821086511015892,
      "learning_rate": 0.0003793280483534253,
      "loss": 0.0497,
      "step": 2649
    },
    {
      "epoch": 0.6557782727047761,
      "grad_norm": 0.037329331040382385,
      "learning_rate": 0.0003792447703243008,
      "loss": 0.0542,
      "step": 2650
    },
    {
      "epoch": 0.6560257362039099,
      "grad_norm": 0.03231791406869888,
      "learning_rate": 0.0003791614727179838,
      "loss": 0.0418,
      "step": 2651
    },
    {
      "epoch": 0.6562731997030438,
      "grad_norm": 0.037179578095674515,
      "learning_rate": 0.00037907815554709157,
      "loss": 0.0498,
      "step": 2652
    },
    {
      "epoch": 0.6565206632021777,
      "grad_norm": 0.05004182085394859,
      "learning_rate": 0.0003789948188242447,
      "loss": 0.0765,
      "step": 2653
    },
    {
      "epoch": 0.6567681267013116,
      "grad_norm": 0.045324068516492844,
      "learning_rate": 0.0003789114625620663,
      "loss": 0.0435,
      "step": 2654
    },
    {
      "epoch": 0.6570155902004454,
      "grad_norm": 0.033629242330789566,
      "learning_rate": 0.0003788280867731827,
      "loss": 0.0347,
      "step": 2655
    },
    {
      "epoch": 0.6572630536995793,
      "grad_norm": 0.038371313363313675,
      "learning_rate": 0.00037874469147022307,
      "loss": 0.0234,
      "step": 2656
    },
    {
      "epoch": 0.6575105171987132,
      "grad_norm": 0.025048527866601944,
      "learning_rate": 0.00037866127666581984,
      "loss": 0.042,
      "step": 2657
    },
    {
      "epoch": 0.6577579806978471,
      "grad_norm": 0.027198659256100655,
      "learning_rate": 0.00037857784237260803,
      "loss": 0.0488,
      "step": 2658
    },
    {
      "epoch": 0.6580054441969809,
      "grad_norm": 0.03377086669206619,
      "learning_rate": 0.0003784943886032257,
      "loss": 0.0479,
      "step": 2659
    },
    {
      "epoch": 0.6582529076961148,
      "grad_norm": 0.031996794044971466,
      "learning_rate": 0.00037841091537031403,
      "loss": 0.0419,
      "step": 2660
    },
    {
      "epoch": 0.6585003711952487,
      "grad_norm": 0.03192130848765373,
      "learning_rate": 0.0003783274226865169,
      "loss": 0.0223,
      "step": 2661
    },
    {
      "epoch": 0.6587478346943826,
      "grad_norm": 0.04276735708117485,
      "learning_rate": 0.00037824391056448124,
      "loss": 0.05,
      "step": 2662
    },
    {
      "epoch": 0.6589952981935164,
      "grad_norm": 0.031712718307971954,
      "learning_rate": 0.0003781603790168571,
      "loss": 0.0754,
      "step": 2663
    },
    {
      "epoch": 0.6592427616926503,
      "grad_norm": 0.03836392983794212,
      "learning_rate": 0.0003780768280562972,
      "loss": 0.0509,
      "step": 2664
    },
    {
      "epoch": 0.6594902251917842,
      "grad_norm": 0.11009687185287476,
      "learning_rate": 0.00037799325769545745,
      "loss": 0.0954,
      "step": 2665
    },
    {
      "epoch": 0.6597376886909181,
      "grad_norm": 0.02293485589325428,
      "learning_rate": 0.00037790966794699633,
      "loss": 0.0241,
      "step": 2666
    },
    {
      "epoch": 0.6599851521900519,
      "grad_norm": 0.02622646652162075,
      "learning_rate": 0.0003778260588235758,
      "loss": 0.0376,
      "step": 2667
    },
    {
      "epoch": 0.6602326156891859,
      "grad_norm": 0.042873527854681015,
      "learning_rate": 0.00037774243033786016,
      "loss": 0.0449,
      "step": 2668
    },
    {
      "epoch": 0.6604800791883197,
      "grad_norm": 0.07123015075922012,
      "learning_rate": 0.00037765878250251706,
      "loss": 0.0655,
      "step": 2669
    },
    {
      "epoch": 0.6607275426874536,
      "grad_norm": 0.04041631147265434,
      "learning_rate": 0.00037757511533021706,
      "loss": 0.0682,
      "step": 2670
    },
    {
      "epoch": 0.6609750061865874,
      "grad_norm": 0.03441229090094566,
      "learning_rate": 0.0003774914288336334,
      "loss": 0.0321,
      "step": 2671
    },
    {
      "epoch": 0.6612224696857214,
      "grad_norm": 0.04110085964202881,
      "learning_rate": 0.0003774077230254424,
      "loss": 0.0519,
      "step": 2672
    },
    {
      "epoch": 0.6614699331848553,
      "grad_norm": 0.10320477932691574,
      "learning_rate": 0.00037732399791832327,
      "loss": 0.0389,
      "step": 2673
    },
    {
      "epoch": 0.6617173966839891,
      "grad_norm": 0.023587988689541817,
      "learning_rate": 0.0003772402535249583,
      "loss": 0.031,
      "step": 2674
    },
    {
      "epoch": 0.661964860183123,
      "grad_norm": 0.02845018357038498,
      "learning_rate": 0.0003771564898580324,
      "loss": 0.0287,
      "step": 2675
    },
    {
      "epoch": 0.6622123236822569,
      "grad_norm": 0.04253038763999939,
      "learning_rate": 0.0003770727069302337,
      "loss": 0.049,
      "step": 2676
    },
    {
      "epoch": 0.6624597871813908,
      "grad_norm": 0.05576512590050697,
      "learning_rate": 0.00037698890475425305,
      "loss": 0.0875,
      "step": 2677
    },
    {
      "epoch": 0.6627072506805246,
      "grad_norm": 0.048744481056928635,
      "learning_rate": 0.0003769050833427844,
      "loss": 0.0462,
      "step": 2678
    },
    {
      "epoch": 0.6629547141796585,
      "grad_norm": 0.08587385714054108,
      "learning_rate": 0.00037682124270852423,
      "loss": 0.127,
      "step": 2679
    },
    {
      "epoch": 0.6632021776787924,
      "grad_norm": 0.02908715233206749,
      "learning_rate": 0.0003767373828641725,
      "loss": 0.0541,
      "step": 2680
    },
    {
      "epoch": 0.6634496411779263,
      "grad_norm": 0.02874872460961342,
      "learning_rate": 0.00037665350382243154,
      "loss": 0.0438,
      "step": 2681
    },
    {
      "epoch": 0.6636971046770601,
      "grad_norm": 0.039423562586307526,
      "learning_rate": 0.000376569605596007,
      "loss": 0.0685,
      "step": 2682
    },
    {
      "epoch": 0.663944568176194,
      "grad_norm": 0.0513024665415287,
      "learning_rate": 0.0003764856881976071,
      "loss": 0.0554,
      "step": 2683
    },
    {
      "epoch": 0.6641920316753279,
      "grad_norm": 0.04835358262062073,
      "learning_rate": 0.0003764017516399433,
      "loss": 0.0997,
      "step": 2684
    },
    {
      "epoch": 0.6644394951744618,
      "grad_norm": 0.04227282851934433,
      "learning_rate": 0.0003763177959357297,
      "loss": 0.0403,
      "step": 2685
    },
    {
      "epoch": 0.6646869586735956,
      "grad_norm": 0.0397469736635685,
      "learning_rate": 0.00037623382109768335,
      "loss": 0.0196,
      "step": 2686
    },
    {
      "epoch": 0.6649344221727296,
      "grad_norm": 0.03267476707696915,
      "learning_rate": 0.00037614982713852417,
      "loss": 0.0565,
      "step": 2687
    },
    {
      "epoch": 0.6651818856718634,
      "grad_norm": 0.03921201080083847,
      "learning_rate": 0.00037606581407097526,
      "loss": 0.0751,
      "step": 2688
    },
    {
      "epoch": 0.6654293491709973,
      "grad_norm": 0.040106263011693954,
      "learning_rate": 0.0003759817819077623,
      "loss": 0.0448,
      "step": 2689
    },
    {
      "epoch": 0.6656768126701311,
      "grad_norm": 0.05938282981514931,
      "learning_rate": 0.0003758977306616139,
      "loss": 0.063,
      "step": 2690
    },
    {
      "epoch": 0.6659242761692651,
      "grad_norm": 0.057257842272520065,
      "learning_rate": 0.00037581366034526156,
      "loss": 0.0917,
      "step": 2691
    },
    {
      "epoch": 0.6661717396683989,
      "grad_norm": 0.03956431895494461,
      "learning_rate": 0.0003757295709714399,
      "loss": 0.088,
      "step": 2692
    },
    {
      "epoch": 0.6664192031675328,
      "grad_norm": 0.03562929108738899,
      "learning_rate": 0.00037564546255288616,
      "loss": 0.0514,
      "step": 2693
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.047146301716566086,
      "learning_rate": 0.00037556133510234063,
      "loss": 0.0806,
      "step": 2694
    },
    {
      "epoch": 0.6669141301658006,
      "grad_norm": 0.023209204897284508,
      "learning_rate": 0.00037547718863254625,
      "loss": 0.0274,
      "step": 2695
    },
    {
      "epoch": 0.6671615936649344,
      "grad_norm": 0.03869811072945595,
      "learning_rate": 0.00037539302315624923,
      "loss": 0.0568,
      "step": 2696
    },
    {
      "epoch": 0.6674090571640683,
      "grad_norm": 0.039692364633083344,
      "learning_rate": 0.00037530883868619827,
      "loss": 0.0271,
      "step": 2697
    },
    {
      "epoch": 0.6676565206632021,
      "grad_norm": 0.02983534336090088,
      "learning_rate": 0.00037522463523514515,
      "loss": 0.0312,
      "step": 2698
    },
    {
      "epoch": 0.6679039841623361,
      "grad_norm": 0.04203258454799652,
      "learning_rate": 0.00037514041281584443,
      "loss": 0.0727,
      "step": 2699
    },
    {
      "epoch": 0.6681514476614699,
      "grad_norm": 0.018679725006222725,
      "learning_rate": 0.0003750561714410537,
      "loss": 0.0334,
      "step": 2700
    },
    {
      "epoch": 0.6683989111606038,
      "grad_norm": 0.022105306386947632,
      "learning_rate": 0.0003749719111235332,
      "loss": 0.0324,
      "step": 2701
    },
    {
      "epoch": 0.6686463746597376,
      "grad_norm": 0.04602954164147377,
      "learning_rate": 0.00037488763187604635,
      "loss": 0.0944,
      "step": 2702
    },
    {
      "epoch": 0.6688938381588716,
      "grad_norm": 0.027358563616871834,
      "learning_rate": 0.00037480333371135905,
      "loss": 0.0314,
      "step": 2703
    },
    {
      "epoch": 0.6691413016580054,
      "grad_norm": 0.030468547716736794,
      "learning_rate": 0.0003747190166422404,
      "loss": 0.0358,
      "step": 2704
    },
    {
      "epoch": 0.6693887651571393,
      "grad_norm": 0.06279666721820831,
      "learning_rate": 0.00037463468068146203,
      "loss": 0.0705,
      "step": 2705
    },
    {
      "epoch": 0.6696362286562731,
      "grad_norm": 0.031067416071891785,
      "learning_rate": 0.0003745503258417989,
      "loss": 0.0383,
      "step": 2706
    },
    {
      "epoch": 0.6698836921554071,
      "grad_norm": 0.048371799290180206,
      "learning_rate": 0.0003744659521360283,
      "loss": 0.0703,
      "step": 2707
    },
    {
      "epoch": 0.670131155654541,
      "grad_norm": 0.035441040992736816,
      "learning_rate": 0.00037438155957693087,
      "loss": 0.0177,
      "step": 2708
    },
    {
      "epoch": 0.6703786191536748,
      "grad_norm": 0.0458381250500679,
      "learning_rate": 0.00037429714817728966,
      "loss": 0.0326,
      "step": 2709
    },
    {
      "epoch": 0.6706260826528088,
      "grad_norm": 0.03523711860179901,
      "learning_rate": 0.00037421271794989085,
      "loss": 0.0468,
      "step": 2710
    },
    {
      "epoch": 0.6708735461519426,
      "grad_norm": 0.025251757353544235,
      "learning_rate": 0.00037412826890752336,
      "loss": 0.0268,
      "step": 2711
    },
    {
      "epoch": 0.6711210096510765,
      "grad_norm": 0.040100473910570145,
      "learning_rate": 0.00037404380106297925,
      "loss": 0.0669,
      "step": 2712
    },
    {
      "epoch": 0.6713684731502103,
      "grad_norm": 0.024537527933716774,
      "learning_rate": 0.0003739593144290528,
      "loss": 0.0281,
      "step": 2713
    },
    {
      "epoch": 0.6716159366493443,
      "grad_norm": 0.030990835279226303,
      "learning_rate": 0.0003738748090185419,
      "loss": 0.0528,
      "step": 2714
    },
    {
      "epoch": 0.6718634001484781,
      "grad_norm": 0.04364334046840668,
      "learning_rate": 0.0003737902848442466,
      "loss": 0.0528,
      "step": 2715
    },
    {
      "epoch": 0.672110863647612,
      "grad_norm": 0.07016151398420334,
      "learning_rate": 0.00037370574191897027,
      "loss": 0.0684,
      "step": 2716
    },
    {
      "epoch": 0.6723583271467458,
      "grad_norm": 0.040274765342473984,
      "learning_rate": 0.0003736211802555188,
      "loss": 0.0491,
      "step": 2717
    },
    {
      "epoch": 0.6726057906458798,
      "grad_norm": 0.020994659513235092,
      "learning_rate": 0.0003735365998667013,
      "loss": 0.0286,
      "step": 2718
    },
    {
      "epoch": 0.6728532541450136,
      "grad_norm": 0.04881611466407776,
      "learning_rate": 0.00037345200076532923,
      "loss": 0.0668,
      "step": 2719
    },
    {
      "epoch": 0.6731007176441475,
      "grad_norm": 0.03187210485339165,
      "learning_rate": 0.0003733673829642173,
      "loss": 0.0456,
      "step": 2720
    },
    {
      "epoch": 0.6733481811432813,
      "grad_norm": 0.04037168622016907,
      "learning_rate": 0.00037328274647618274,
      "loss": 0.0475,
      "step": 2721
    },
    {
      "epoch": 0.6735956446424153,
      "grad_norm": 0.03812117502093315,
      "learning_rate": 0.00037319809131404594,
      "loss": 0.0349,
      "step": 2722
    },
    {
      "epoch": 0.6738431081415491,
      "grad_norm": 0.027528712525963783,
      "learning_rate": 0.0003731134174906298,
      "loss": 0.0278,
      "step": 2723
    },
    {
      "epoch": 0.674090571640683,
      "grad_norm": 0.022212281823158264,
      "learning_rate": 0.0003730287250187602,
      "loss": 0.0247,
      "step": 2724
    },
    {
      "epoch": 0.6743380351398168,
      "grad_norm": 0.03447945415973663,
      "learning_rate": 0.0003729440139112659,
      "loss": 0.0569,
      "step": 2725
    },
    {
      "epoch": 0.6745854986389508,
      "grad_norm": 0.03782547265291214,
      "learning_rate": 0.00037285928418097835,
      "loss": 0.0453,
      "step": 2726
    },
    {
      "epoch": 0.6748329621380846,
      "grad_norm": 0.03195065259933472,
      "learning_rate": 0.00037277453584073195,
      "loss": 0.0174,
      "step": 2727
    },
    {
      "epoch": 0.6750804256372185,
      "grad_norm": 0.04482446238398552,
      "learning_rate": 0.0003726897689033637,
      "loss": 0.0986,
      "step": 2728
    },
    {
      "epoch": 0.6753278891363523,
      "grad_norm": 0.046210628002882004,
      "learning_rate": 0.0003726049833817138,
      "loss": 0.0596,
      "step": 2729
    },
    {
      "epoch": 0.6755753526354863,
      "grad_norm": 0.03568826615810394,
      "learning_rate": 0.0003725201792886247,
      "loss": 0.048,
      "step": 2730
    },
    {
      "epoch": 0.6758228161346201,
      "grad_norm": 0.03406482934951782,
      "learning_rate": 0.0003724353566369423,
      "loss": 0.0531,
      "step": 2731
    },
    {
      "epoch": 0.676070279633754,
      "grad_norm": 0.03214597702026367,
      "learning_rate": 0.00037235051543951493,
      "loss": 0.041,
      "step": 2732
    },
    {
      "epoch": 0.676317743132888,
      "grad_norm": 0.03215204179286957,
      "learning_rate": 0.00037226565570919374,
      "loss": 0.0197,
      "step": 2733
    },
    {
      "epoch": 0.6765652066320218,
      "grad_norm": 0.03200281411409378,
      "learning_rate": 0.0003721807774588328,
      "loss": 0.0299,
      "step": 2734
    },
    {
      "epoch": 0.6768126701311556,
      "grad_norm": 0.01506018452346325,
      "learning_rate": 0.0003720958807012889,
      "loss": 0.0154,
      "step": 2735
    },
    {
      "epoch": 0.6770601336302895,
      "grad_norm": 0.023805750533938408,
      "learning_rate": 0.0003720109654494217,
      "loss": 0.0202,
      "step": 2736
    },
    {
      "epoch": 0.6773075971294235,
      "grad_norm": 0.03140024468302727,
      "learning_rate": 0.00037192603171609364,
      "loss": 0.0449,
      "step": 2737
    },
    {
      "epoch": 0.6775550606285573,
      "grad_norm": 0.022297050803899765,
      "learning_rate": 0.00037184107951416997,
      "loss": 0.03,
      "step": 2738
    },
    {
      "epoch": 0.6778025241276912,
      "grad_norm": 0.0390763096511364,
      "learning_rate": 0.0003717561088565187,
      "loss": 0.0508,
      "step": 2739
    },
    {
      "epoch": 0.678049987626825,
      "grad_norm": 0.05606766417622566,
      "learning_rate": 0.0003716711197560105,
      "loss": 0.0364,
      "step": 2740
    },
    {
      "epoch": 0.678297451125959,
      "grad_norm": 0.05871208757162094,
      "learning_rate": 0.0003715861122255193,
      "loss": 0.0587,
      "step": 2741
    },
    {
      "epoch": 0.6785449146250928,
      "grad_norm": 0.019660212099552155,
      "learning_rate": 0.0003715010862779213,
      "loss": 0.0335,
      "step": 2742
    },
    {
      "epoch": 0.6787923781242267,
      "grad_norm": 0.04543597996234894,
      "learning_rate": 0.00037141604192609585,
      "loss": 0.0335,
      "step": 2743
    },
    {
      "epoch": 0.6790398416233605,
      "grad_norm": 0.04949583113193512,
      "learning_rate": 0.00037133097918292475,
      "loss": 0.0917,
      "step": 2744
    },
    {
      "epoch": 0.6792873051224945,
      "grad_norm": 0.044961851090192795,
      "learning_rate": 0.00037124589806129284,
      "loss": 0.0668,
      "step": 2745
    },
    {
      "epoch": 0.6795347686216283,
      "grad_norm": 0.06612448394298553,
      "learning_rate": 0.0003711607985740877,
      "loss": 0.0573,
      "step": 2746
    },
    {
      "epoch": 0.6797822321207622,
      "grad_norm": 0.03452124446630478,
      "learning_rate": 0.0003710756807341997,
      "loss": 0.0298,
      "step": 2747
    },
    {
      "epoch": 0.680029695619896,
      "grad_norm": 0.02474481239914894,
      "learning_rate": 0.000370990544554522,
      "loss": 0.0383,
      "step": 2748
    },
    {
      "epoch": 0.68027715911903,
      "grad_norm": 0.03832683339715004,
      "learning_rate": 0.00037090539004795036,
      "loss": 0.04,
      "step": 2749
    },
    {
      "epoch": 0.6805246226181638,
      "grad_norm": 0.050544336438179016,
      "learning_rate": 0.0003708202172273835,
      "loss": 0.0977,
      "step": 2750
    },
    {
      "epoch": 0.6807720861172977,
      "grad_norm": 0.04238168150186539,
      "learning_rate": 0.00037073502610572296,
      "loss": 0.0648,
      "step": 2751
    },
    {
      "epoch": 0.6810195496164316,
      "grad_norm": 0.04269256442785263,
      "learning_rate": 0.00037064981669587284,
      "loss": 0.0834,
      "step": 2752
    },
    {
      "epoch": 0.6812670131155655,
      "grad_norm": 0.06180243566632271,
      "learning_rate": 0.00037056458901074023,
      "loss": 0.0608,
      "step": 2753
    },
    {
      "epoch": 0.6815144766146993,
      "grad_norm": 0.040908120572566986,
      "learning_rate": 0.0003704793430632348,
      "loss": 0.0771,
      "step": 2754
    },
    {
      "epoch": 0.6817619401138332,
      "grad_norm": 0.04250229150056839,
      "learning_rate": 0.00037039407886626917,
      "loss": 0.0677,
      "step": 2755
    },
    {
      "epoch": 0.6820094036129671,
      "grad_norm": 0.038105741143226624,
      "learning_rate": 0.0003703087964327585,
      "loss": 0.0512,
      "step": 2756
    },
    {
      "epoch": 0.682256867112101,
      "grad_norm": 0.0328921303153038,
      "learning_rate": 0.00037022349577562106,
      "loss": 0.0246,
      "step": 2757
    },
    {
      "epoch": 0.6825043306112348,
      "grad_norm": 0.02376484125852585,
      "learning_rate": 0.0003701381769077774,
      "loss": 0.0293,
      "step": 2758
    },
    {
      "epoch": 0.6827517941103687,
      "grad_norm": 0.02694113925099373,
      "learning_rate": 0.00037005283984215123,
      "loss": 0.0324,
      "step": 2759
    },
    {
      "epoch": 0.6829992576095026,
      "grad_norm": 0.020092060789465904,
      "learning_rate": 0.00036996748459166884,
      "loss": 0.0307,
      "step": 2760
    },
    {
      "epoch": 0.6832467211086365,
      "grad_norm": 0.034967001527547836,
      "learning_rate": 0.0003698821111692594,
      "loss": 0.0504,
      "step": 2761
    },
    {
      "epoch": 0.6834941846077703,
      "grad_norm": 0.06456306576728821,
      "learning_rate": 0.0003697967195878546,
      "loss": 0.068,
      "step": 2762
    },
    {
      "epoch": 0.6837416481069042,
      "grad_norm": 0.029348373413085938,
      "learning_rate": 0.00036971130986038914,
      "loss": 0.05,
      "step": 2763
    },
    {
      "epoch": 0.6839891116060381,
      "grad_norm": 0.03877430781722069,
      "learning_rate": 0.0003696258819998003,
      "loss": 0.0332,
      "step": 2764
    },
    {
      "epoch": 0.684236575105172,
      "grad_norm": 0.032218728214502335,
      "learning_rate": 0.0003695404360190282,
      "loss": 0.072,
      "step": 2765
    },
    {
      "epoch": 0.6844840386043058,
      "grad_norm": 0.022277481853961945,
      "learning_rate": 0.00036945497193101556,
      "loss": 0.0268,
      "step": 2766
    },
    {
      "epoch": 0.6847315021034397,
      "grad_norm": 0.02879038080573082,
      "learning_rate": 0.00036936948974870815,
      "loss": 0.0382,
      "step": 2767
    },
    {
      "epoch": 0.6849789656025737,
      "grad_norm": 0.04110099375247955,
      "learning_rate": 0.00036928398948505404,
      "loss": 0.0851,
      "step": 2768
    },
    {
      "epoch": 0.6852264291017075,
      "grad_norm": 0.037886250764131546,
      "learning_rate": 0.0003691984711530044,
      "loss": 0.0627,
      "step": 2769
    },
    {
      "epoch": 0.6854738926008414,
      "grad_norm": 0.021511679515242577,
      "learning_rate": 0.00036911293476551304,
      "loss": 0.0306,
      "step": 2770
    },
    {
      "epoch": 0.6857213560999752,
      "grad_norm": 0.02755870670080185,
      "learning_rate": 0.0003690273803355365,
      "loss": 0.0382,
      "step": 2771
    },
    {
      "epoch": 0.6859688195991092,
      "grad_norm": 0.046050846576690674,
      "learning_rate": 0.00036894180787603394,
      "loss": 0.0717,
      "step": 2772
    },
    {
      "epoch": 0.686216283098243,
      "grad_norm": 0.036949120461940765,
      "learning_rate": 0.0003688562173999673,
      "loss": 0.0386,
      "step": 2773
    },
    {
      "epoch": 0.6864637465973769,
      "grad_norm": 0.042577218264341354,
      "learning_rate": 0.0003687706089203015,
      "loss": 0.0605,
      "step": 2774
    },
    {
      "epoch": 0.6867112100965108,
      "grad_norm": 0.035391826182603836,
      "learning_rate": 0.00036868498245000377,
      "loss": 0.0689,
      "step": 2775
    },
    {
      "epoch": 0.6869586735956447,
      "grad_norm": 0.031058285385370255,
      "learning_rate": 0.0003685993380020443,
      "loss": 0.0448,
      "step": 2776
    },
    {
      "epoch": 0.6872061370947785,
      "grad_norm": 0.038005199283361435,
      "learning_rate": 0.00036851367558939606,
      "loss": 0.0525,
      "step": 2777
    },
    {
      "epoch": 0.6874536005939124,
      "grad_norm": 0.05473453924059868,
      "learning_rate": 0.0003684279952250347,
      "loss": 0.048,
      "step": 2778
    },
    {
      "epoch": 0.6877010640930463,
      "grad_norm": 0.04736247658729553,
      "learning_rate": 0.0003683422969219385,
      "loss": 0.0471,
      "step": 2779
    },
    {
      "epoch": 0.6879485275921802,
      "grad_norm": 0.05293905362486839,
      "learning_rate": 0.00036825658069308836,
      "loss": 0.0779,
      "step": 2780
    },
    {
      "epoch": 0.688195991091314,
      "grad_norm": 0.03741830214858055,
      "learning_rate": 0.00036817084655146827,
      "loss": 0.0645,
      "step": 2781
    },
    {
      "epoch": 0.6884434545904479,
      "grad_norm": 0.018978413194417953,
      "learning_rate": 0.0003680850945100646,
      "loss": 0.0342,
      "step": 2782
    },
    {
      "epoch": 0.6886909180895818,
      "grad_norm": 0.027546506375074387,
      "learning_rate": 0.00036799932458186647,
      "loss": 0.0397,
      "step": 2783
    },
    {
      "epoch": 0.6889383815887157,
      "grad_norm": 0.02818053774535656,
      "learning_rate": 0.0003679135367798658,
      "loss": 0.0382,
      "step": 2784
    },
    {
      "epoch": 0.6891858450878495,
      "grad_norm": 0.060853663831949234,
      "learning_rate": 0.00036782773111705724,
      "loss": 0.0464,
      "step": 2785
    },
    {
      "epoch": 0.6894333085869834,
      "grad_norm": 0.028076857328414917,
      "learning_rate": 0.00036774190760643824,
      "loss": 0.0525,
      "step": 2786
    },
    {
      "epoch": 0.6896807720861173,
      "grad_norm": 0.029821423813700676,
      "learning_rate": 0.0003676560662610085,
      "loss": 0.049,
      "step": 2787
    },
    {
      "epoch": 0.6899282355852512,
      "grad_norm": 0.034761250019073486,
      "learning_rate": 0.00036757020709377096,
      "loss": 0.0507,
      "step": 2788
    },
    {
      "epoch": 0.690175699084385,
      "grad_norm": 0.0407845564186573,
      "learning_rate": 0.00036748433011773085,
      "loss": 0.0729,
      "step": 2789
    },
    {
      "epoch": 0.6904231625835189,
      "grad_norm": 0.03783010318875313,
      "learning_rate": 0.0003673984353458965,
      "loss": 0.07,
      "step": 2790
    },
    {
      "epoch": 0.6906706260826528,
      "grad_norm": 0.07059955596923828,
      "learning_rate": 0.0003673125227912786,
      "loss": 0.0644,
      "step": 2791
    },
    {
      "epoch": 0.6909180895817867,
      "grad_norm": 0.027598122134804726,
      "learning_rate": 0.0003672265924668906,
      "loss": 0.0241,
      "step": 2792
    },
    {
      "epoch": 0.6911655530809205,
      "grad_norm": 0.033399548381567,
      "learning_rate": 0.00036714064438574877,
      "loss": 0.0574,
      "step": 2793
    },
    {
      "epoch": 0.6914130165800545,
      "grad_norm": 0.0489521250128746,
      "learning_rate": 0.000367054678560872,
      "loss": 0.0525,
      "step": 2794
    },
    {
      "epoch": 0.6916604800791883,
      "grad_norm": 0.035422392189502716,
      "learning_rate": 0.00036696869500528175,
      "loss": 0.0504,
      "step": 2795
    },
    {
      "epoch": 0.6919079435783222,
      "grad_norm": 0.0487804152071476,
      "learning_rate": 0.0003668826937320024,
      "loss": 0.084,
      "step": 2796
    },
    {
      "epoch": 0.692155407077456,
      "grad_norm": 0.062204938381910324,
      "learning_rate": 0.00036679667475406075,
      "loss": 0.1345,
      "step": 2797
    },
    {
      "epoch": 0.69240287057659,
      "grad_norm": 0.03419024869799614,
      "learning_rate": 0.00036671063808448663,
      "loss": 0.0419,
      "step": 2798
    },
    {
      "epoch": 0.6926503340757239,
      "grad_norm": 0.04144475981593132,
      "learning_rate": 0.0003666245837363121,
      "loss": 0.0429,
      "step": 2799
    },
    {
      "epoch": 0.6928977975748577,
      "grad_norm": 0.02956998720765114,
      "learning_rate": 0.0003665385117225723,
      "loss": 0.0311,
      "step": 2800
    },
    {
      "epoch": 0.6928977975748577,
      "eval_loss": 0.29440316557884216,
      "eval_runtime": 202.5678,
      "eval_samples_per_second": 4.937,
      "eval_steps_per_second": 0.311,
      "step": 2800
    },
    {
      "epoch": 0.6931452610739915,
      "grad_norm": 0.052061039954423904,
      "learning_rate": 0.0003664524220563048,
      "loss": 0.0487,
      "step": 2801
    },
    {
      "epoch": 0.6933927245731255,
      "grad_norm": 0.025738179683685303,
      "learning_rate": 0.0003663663147505499,
      "loss": 0.0346,
      "step": 2802
    },
    {
      "epoch": 0.6936401880722594,
      "grad_norm": 0.0714896023273468,
      "learning_rate": 0.0003662801898183508,
      "loss": 0.0666,
      "step": 2803
    },
    {
      "epoch": 0.6938876515713932,
      "grad_norm": 0.043621983379125595,
      "learning_rate": 0.00036619404727275285,
      "loss": 0.0445,
      "step": 2804
    },
    {
      "epoch": 0.694135115070527,
      "grad_norm": 0.05485047027468681,
      "learning_rate": 0.00036610788712680464,
      "loss": 0.0837,
      "step": 2805
    },
    {
      "epoch": 0.694382578569661,
      "grad_norm": 0.030383428558707237,
      "learning_rate": 0.0003660217093935571,
      "loss": 0.0625,
      "step": 2806
    },
    {
      "epoch": 0.6946300420687949,
      "grad_norm": 0.02456088177859783,
      "learning_rate": 0.0003659355140860638,
      "loss": 0.0516,
      "step": 2807
    },
    {
      "epoch": 0.6948775055679287,
      "grad_norm": 0.030993057414889336,
      "learning_rate": 0.0003658493012173813,
      "loss": 0.0295,
      "step": 2808
    },
    {
      "epoch": 0.6951249690670626,
      "grad_norm": 0.046911343932151794,
      "learning_rate": 0.00036576307080056833,
      "loss": 0.0836,
      "step": 2809
    },
    {
      "epoch": 0.6953724325661965,
      "grad_norm": 0.036097146570682526,
      "learning_rate": 0.00036567682284868665,
      "loss": 0.0603,
      "step": 2810
    },
    {
      "epoch": 0.6956198960653304,
      "grad_norm": 0.0537148118019104,
      "learning_rate": 0.00036559055737480056,
      "loss": 0.0857,
      "step": 2811
    },
    {
      "epoch": 0.6958673595644642,
      "grad_norm": 0.03651425987482071,
      "learning_rate": 0.0003655042743919771,
      "loss": 0.047,
      "step": 2812
    },
    {
      "epoch": 0.6961148230635981,
      "grad_norm": 0.027884449809789658,
      "learning_rate": 0.00036541797391328567,
      "loss": 0.0194,
      "step": 2813
    },
    {
      "epoch": 0.696362286562732,
      "grad_norm": 0.027042442932724953,
      "learning_rate": 0.0003653316559517987,
      "loss": 0.0351,
      "step": 2814
    },
    {
      "epoch": 0.6966097500618659,
      "grad_norm": 0.02194562740623951,
      "learning_rate": 0.0003652453205205911,
      "loss": 0.034,
      "step": 2815
    },
    {
      "epoch": 0.6968572135609997,
      "grad_norm": 0.03949260711669922,
      "learning_rate": 0.00036515896763274035,
      "loss": 0.1447,
      "step": 2816
    },
    {
      "epoch": 0.6971046770601337,
      "grad_norm": 0.028752798214554787,
      "learning_rate": 0.00036507259730132665,
      "loss": 0.0481,
      "step": 2817
    },
    {
      "epoch": 0.6973521405592675,
      "grad_norm": 0.02556697465479374,
      "learning_rate": 0.0003649862095394328,
      "loss": 0.0366,
      "step": 2818
    },
    {
      "epoch": 0.6975996040584014,
      "grad_norm": 0.03580290079116821,
      "learning_rate": 0.0003648998043601444,
      "loss": 0.0391,
      "step": 2819
    },
    {
      "epoch": 0.6978470675575352,
      "grad_norm": 0.042539920657873154,
      "learning_rate": 0.0003648133817765495,
      "loss": 0.0471,
      "step": 2820
    },
    {
      "epoch": 0.6980945310566692,
      "grad_norm": 0.027891075238585472,
      "learning_rate": 0.0003647269418017389,
      "loss": 0.0416,
      "step": 2821
    },
    {
      "epoch": 0.698341994555803,
      "grad_norm": 0.03249812126159668,
      "learning_rate": 0.00036464048444880595,
      "loss": 0.0659,
      "step": 2822
    },
    {
      "epoch": 0.6985894580549369,
      "grad_norm": 0.049223024398088455,
      "learning_rate": 0.0003645540097308466,
      "loss": 0.0259,
      "step": 2823
    },
    {
      "epoch": 0.6988369215540707,
      "grad_norm": 0.037583716213703156,
      "learning_rate": 0.0003644675176609597,
      "loss": 0.0592,
      "step": 2824
    },
    {
      "epoch": 0.6990843850532047,
      "grad_norm": 0.02659670263528824,
      "learning_rate": 0.0003643810082522463,
      "loss": 0.0321,
      "step": 2825
    },
    {
      "epoch": 0.6993318485523385,
      "grad_norm": 0.0314178392291069,
      "learning_rate": 0.0003642944815178104,
      "loss": 0.0398,
      "step": 2826
    },
    {
      "epoch": 0.6995793120514724,
      "grad_norm": 0.05572093650698662,
      "learning_rate": 0.00036420793747075865,
      "loss": 0.0791,
      "step": 2827
    },
    {
      "epoch": 0.6998267755506062,
      "grad_norm": 0.04347703605890274,
      "learning_rate": 0.0003641213761242,
      "loss": 0.0448,
      "step": 2828
    },
    {
      "epoch": 0.7000742390497402,
      "grad_norm": 0.06970293074846268,
      "learning_rate": 0.00036403479749124633,
      "loss": 0.0992,
      "step": 2829
    },
    {
      "epoch": 0.700321702548874,
      "grad_norm": 0.023222047835588455,
      "learning_rate": 0.00036394820158501204,
      "loss": 0.0435,
      "step": 2830
    },
    {
      "epoch": 0.7005691660480079,
      "grad_norm": 0.04638085514307022,
      "learning_rate": 0.0003638615884186141,
      "loss": 0.0732,
      "step": 2831
    },
    {
      "epoch": 0.7008166295471417,
      "grad_norm": 0.0337279811501503,
      "learning_rate": 0.00036377495800517213,
      "loss": 0.0579,
      "step": 2832
    },
    {
      "epoch": 0.7010640930462757,
      "grad_norm": 0.03439061716198921,
      "learning_rate": 0.00036368831035780844,
      "loss": 0.0348,
      "step": 2833
    },
    {
      "epoch": 0.7013115565454096,
      "grad_norm": 0.04489447548985481,
      "learning_rate": 0.00036360164548964775,
      "loss": 0.0341,
      "step": 2834
    },
    {
      "epoch": 0.7015590200445434,
      "grad_norm": 0.03321956843137741,
      "learning_rate": 0.0003635149634138177,
      "loss": 0.0831,
      "step": 2835
    },
    {
      "epoch": 0.7018064835436774,
      "grad_norm": 0.06638418138027191,
      "learning_rate": 0.00036342826414344817,
      "loss": 0.0722,
      "step": 2836
    },
    {
      "epoch": 0.7020539470428112,
      "grad_norm": 0.03539203107357025,
      "learning_rate": 0.0003633415476916719,
      "loss": 0.0514,
      "step": 2837
    },
    {
      "epoch": 0.7023014105419451,
      "grad_norm": 0.05167384445667267,
      "learning_rate": 0.0003632548140716241,
      "loss": 0.071,
      "step": 2838
    },
    {
      "epoch": 0.7025488740410789,
      "grad_norm": 0.02055610902607441,
      "learning_rate": 0.0003631680632964427,
      "loss": 0.024,
      "step": 2839
    },
    {
      "epoch": 0.7027963375402129,
      "grad_norm": 0.06953805685043335,
      "learning_rate": 0.0003630812953792682,
      "loss": 0.0791,
      "step": 2840
    },
    {
      "epoch": 0.7030438010393467,
      "grad_norm": 0.034134391695261,
      "learning_rate": 0.0003629945103332436,
      "loss": 0.0473,
      "step": 2841
    },
    {
      "epoch": 0.7032912645384806,
      "grad_norm": 0.050779227167367935,
      "learning_rate": 0.0003629077081715146,
      "loss": 0.0611,
      "step": 2842
    },
    {
      "epoch": 0.7035387280376144,
      "grad_norm": 0.09860623627901077,
      "learning_rate": 0.00036282088890722937,
      "loss": 0.0486,
      "step": 2843
    },
    {
      "epoch": 0.7037861915367484,
      "grad_norm": 0.09602988511323929,
      "learning_rate": 0.00036273405255353886,
      "loss": 0.0719,
      "step": 2844
    },
    {
      "epoch": 0.7040336550358822,
      "grad_norm": 0.040608398616313934,
      "learning_rate": 0.0003626471991235965,
      "loss": 0.0416,
      "step": 2845
    },
    {
      "epoch": 0.7042811185350161,
      "grad_norm": 0.02107364498078823,
      "learning_rate": 0.0003625603286305581,
      "loss": 0.0351,
      "step": 2846
    },
    {
      "epoch": 0.7045285820341499,
      "grad_norm": 0.023861806839704514,
      "learning_rate": 0.0003624734410875825,
      "loss": 0.038,
      "step": 2847
    },
    {
      "epoch": 0.7047760455332839,
      "grad_norm": 0.04146619141101837,
      "learning_rate": 0.00036238653650783075,
      "loss": 0.0632,
      "step": 2848
    },
    {
      "epoch": 0.7050235090324177,
      "grad_norm": 0.019869010895490646,
      "learning_rate": 0.00036229961490446676,
      "loss": 0.0245,
      "step": 2849
    },
    {
      "epoch": 0.7052709725315516,
      "grad_norm": 0.039534348994493484,
      "learning_rate": 0.0003622126762906567,
      "loss": 0.0772,
      "step": 2850
    },
    {
      "epoch": 0.7055184360306854,
      "grad_norm": 0.05773497745394707,
      "learning_rate": 0.00036212572067956966,
      "loss": 0.0905,
      "step": 2851
    },
    {
      "epoch": 0.7057658995298194,
      "grad_norm": 0.044003117829561234,
      "learning_rate": 0.00036203874808437696,
      "loss": 0.0387,
      "step": 2852
    },
    {
      "epoch": 0.7060133630289532,
      "grad_norm": 0.04482858628034592,
      "learning_rate": 0.0003619517585182527,
      "loss": 0.06,
      "step": 2853
    },
    {
      "epoch": 0.7062608265280871,
      "grad_norm": 0.044971615076065063,
      "learning_rate": 0.00036186475199437366,
      "loss": 0.0647,
      "step": 2854
    },
    {
      "epoch": 0.7065082900272209,
      "grad_norm": 0.03193631023168564,
      "learning_rate": 0.00036177772852591893,
      "loss": 0.0376,
      "step": 2855
    },
    {
      "epoch": 0.7067557535263549,
      "grad_norm": 0.045678768306970596,
      "learning_rate": 0.0003616906881260703,
      "loss": 0.0249,
      "step": 2856
    },
    {
      "epoch": 0.7070032170254887,
      "grad_norm": 0.053950149565935135,
      "learning_rate": 0.00036160363080801216,
      "loss": 0.0658,
      "step": 2857
    },
    {
      "epoch": 0.7072506805246226,
      "grad_norm": 0.03731100633740425,
      "learning_rate": 0.00036151655658493123,
      "loss": 0.0421,
      "step": 2858
    },
    {
      "epoch": 0.7074981440237565,
      "grad_norm": 0.02072007954120636,
      "learning_rate": 0.0003614294654700172,
      "loss": 0.0382,
      "step": 2859
    },
    {
      "epoch": 0.7077456075228904,
      "grad_norm": 0.038738250732421875,
      "learning_rate": 0.000361342357476462,
      "loss": 0.0348,
      "step": 2860
    },
    {
      "epoch": 0.7079930710220242,
      "grad_norm": 0.023066390305757523,
      "learning_rate": 0.0003612552326174602,
      "loss": 0.0195,
      "step": 2861
    },
    {
      "epoch": 0.7082405345211581,
      "grad_norm": 0.03808295726776123,
      "learning_rate": 0.0003611680909062088,
      "loss": 0.052,
      "step": 2862
    },
    {
      "epoch": 0.708487998020292,
      "grad_norm": 0.0782078430056572,
      "learning_rate": 0.00036108093235590776,
      "loss": 0.0876,
      "step": 2863
    },
    {
      "epoch": 0.7087354615194259,
      "grad_norm": 0.032879788428545,
      "learning_rate": 0.00036099375697975914,
      "loss": 0.039,
      "step": 2864
    },
    {
      "epoch": 0.7089829250185598,
      "grad_norm": 0.02315780706703663,
      "learning_rate": 0.0003609065647909677,
      "loss": 0.0331,
      "step": 2865
    },
    {
      "epoch": 0.7092303885176936,
      "grad_norm": 0.12348499894142151,
      "learning_rate": 0.00036081935580274086,
      "loss": 0.0803,
      "step": 2866
    },
    {
      "epoch": 0.7094778520168276,
      "grad_norm": 0.033535491675138474,
      "learning_rate": 0.00036073213002828843,
      "loss": 0.0448,
      "step": 2867
    },
    {
      "epoch": 0.7097253155159614,
      "grad_norm": 0.051866788417100906,
      "learning_rate": 0.0003606448874808228,
      "loss": 0.083,
      "step": 2868
    },
    {
      "epoch": 0.7099727790150953,
      "grad_norm": 0.034570373594760895,
      "learning_rate": 0.0003605576281735591,
      "loss": 0.0537,
      "step": 2869
    },
    {
      "epoch": 0.7102202425142291,
      "grad_norm": 0.03828684240579605,
      "learning_rate": 0.00036047035211971457,
      "loss": 0.0666,
      "step": 2870
    },
    {
      "epoch": 0.7104677060133631,
      "grad_norm": 0.06755081564188004,
      "learning_rate": 0.0003603830593325095,
      "loss": 0.0791,
      "step": 2871
    },
    {
      "epoch": 0.7107151695124969,
      "grad_norm": 0.03545359894633293,
      "learning_rate": 0.00036029574982516626,
      "loss": 0.054,
      "step": 2872
    },
    {
      "epoch": 0.7109626330116308,
      "grad_norm": 0.029483051970601082,
      "learning_rate": 0.00036020842361091,
      "loss": 0.0523,
      "step": 2873
    },
    {
      "epoch": 0.7112100965107646,
      "grad_norm": 0.07530030608177185,
      "learning_rate": 0.0003601210807029684,
      "loss": 0.0301,
      "step": 2874
    },
    {
      "epoch": 0.7114575600098986,
      "grad_norm": 0.033840566873550415,
      "learning_rate": 0.00036003372111457165,
      "loss": 0.109,
      "step": 2875
    },
    {
      "epoch": 0.7117050235090324,
      "grad_norm": 0.026176657527685165,
      "learning_rate": 0.00035994634485895236,
      "loss": 0.0374,
      "step": 2876
    },
    {
      "epoch": 0.7119524870081663,
      "grad_norm": 0.04469918832182884,
      "learning_rate": 0.0003598589519493457,
      "loss": 0.0476,
      "step": 2877
    },
    {
      "epoch": 0.7121999505073002,
      "grad_norm": 0.037686679512262344,
      "learning_rate": 0.00035977154239898954,
      "loss": 0.0657,
      "step": 2878
    },
    {
      "epoch": 0.7124474140064341,
      "grad_norm": 0.04546073451638222,
      "learning_rate": 0.000359684116221124,
      "loss": 0.0925,
      "step": 2879
    },
    {
      "epoch": 0.7126948775055679,
      "grad_norm": 0.06654953956604004,
      "learning_rate": 0.00035959667342899194,
      "loss": 0.0402,
      "step": 2880
    },
    {
      "epoch": 0.7129423410047018,
      "grad_norm": 0.02143755741417408,
      "learning_rate": 0.0003595092140358386,
      "loss": 0.0372,
      "step": 2881
    },
    {
      "epoch": 0.7131898045038357,
      "grad_norm": 0.034733012318611145,
      "learning_rate": 0.0003594217380549118,
      "loss": 0.0581,
      "step": 2882
    },
    {
      "epoch": 0.7134372680029696,
      "grad_norm": 0.039643626660108566,
      "learning_rate": 0.0003593342454994619,
      "loss": 0.0593,
      "step": 2883
    },
    {
      "epoch": 0.7136847315021034,
      "grad_norm": 0.042713966220617294,
      "learning_rate": 0.0003592467363827417,
      "loss": 0.0397,
      "step": 2884
    },
    {
      "epoch": 0.7139321950012373,
      "grad_norm": 0.025439629331231117,
      "learning_rate": 0.0003591592107180064,
      "loss": 0.0352,
      "step": 2885
    },
    {
      "epoch": 0.7141796585003712,
      "grad_norm": 0.03014153055846691,
      "learning_rate": 0.0003590716685185141,
      "loss": 0.0365,
      "step": 2886
    },
    {
      "epoch": 0.7144271219995051,
      "grad_norm": 0.0415978878736496,
      "learning_rate": 0.00035898410979752504,
      "loss": 0.0495,
      "step": 2887
    },
    {
      "epoch": 0.7146745854986389,
      "grad_norm": 0.04320184886455536,
      "learning_rate": 0.00035889653456830194,
      "loss": 0.0945,
      "step": 2888
    },
    {
      "epoch": 0.7149220489977728,
      "grad_norm": 0.04651755467057228,
      "learning_rate": 0.00035880894284411025,
      "loss": 0.0522,
      "step": 2889
    },
    {
      "epoch": 0.7151695124969067,
      "grad_norm": 0.0607682503759861,
      "learning_rate": 0.00035872133463821796,
      "loss": 0.1079,
      "step": 2890
    },
    {
      "epoch": 0.7154169759960406,
      "grad_norm": 0.035873327404260635,
      "learning_rate": 0.00035863370996389515,
      "loss": 0.0793,
      "step": 2891
    },
    {
      "epoch": 0.7156644394951744,
      "grad_norm": 0.02907879836857319,
      "learning_rate": 0.00035854606883441474,
      "loss": 0.0257,
      "step": 2892
    },
    {
      "epoch": 0.7159119029943083,
      "grad_norm": 0.052875857800245285,
      "learning_rate": 0.00035845841126305223,
      "loss": 0.0661,
      "step": 2893
    },
    {
      "epoch": 0.7161593664934423,
      "grad_norm": 0.04333766549825668,
      "learning_rate": 0.0003583707372630853,
      "loss": 0.0882,
      "step": 2894
    },
    {
      "epoch": 0.7164068299925761,
      "grad_norm": 0.05505667254328728,
      "learning_rate": 0.0003582830468477943,
      "loss": 0.0405,
      "step": 2895
    },
    {
      "epoch": 0.71665429349171,
      "grad_norm": 0.04451461881399155,
      "learning_rate": 0.00035819534003046197,
      "loss": 0.0934,
      "step": 2896
    },
    {
      "epoch": 0.7169017569908438,
      "grad_norm": 0.041603025048971176,
      "learning_rate": 0.0003581076168243736,
      "loss": 0.0535,
      "step": 2897
    },
    {
      "epoch": 0.7171492204899778,
      "grad_norm": 0.03508475422859192,
      "learning_rate": 0.000358019877242817,
      "loss": 0.0698,
      "step": 2898
    },
    {
      "epoch": 0.7173966839891116,
      "grad_norm": 0.03534997999668121,
      "learning_rate": 0.0003579321212990825,
      "loss": 0.0266,
      "step": 2899
    },
    {
      "epoch": 0.7176441474882455,
      "grad_norm": 0.027338974177837372,
      "learning_rate": 0.0003578443490064627,
      "loss": 0.0347,
      "step": 2900
    },
    {
      "epoch": 0.7178916109873794,
      "grad_norm": 0.056429751217365265,
      "learning_rate": 0.00035775656037825273,
      "loss": 0.0939,
      "step": 2901
    },
    {
      "epoch": 0.7181390744865133,
      "grad_norm": 0.03282984346151352,
      "learning_rate": 0.00035766875542775036,
      "loss": 0.0462,
      "step": 2902
    },
    {
      "epoch": 0.7183865379856471,
      "grad_norm": 0.10956468433141708,
      "learning_rate": 0.00035758093416825575,
      "loss": 0.0789,
      "step": 2903
    },
    {
      "epoch": 0.718634001484781,
      "grad_norm": 0.04707765579223633,
      "learning_rate": 0.0003574930966130716,
      "loss": 0.0848,
      "step": 2904
    },
    {
      "epoch": 0.7188814649839149,
      "grad_norm": 0.03675311803817749,
      "learning_rate": 0.00035740524277550286,
      "loss": 0.0502,
      "step": 2905
    },
    {
      "epoch": 0.7191289284830488,
      "grad_norm": 0.0359441302716732,
      "learning_rate": 0.00035731737266885715,
      "loss": 0.029,
      "step": 2906
    },
    {
      "epoch": 0.7193763919821826,
      "grad_norm": 0.0516531839966774,
      "learning_rate": 0.00035722948630644436,
      "loss": 0.0862,
      "step": 2907
    },
    {
      "epoch": 0.7196238554813165,
      "grad_norm": 0.03377589210867882,
      "learning_rate": 0.00035714158370157715,
      "loss": 0.0302,
      "step": 2908
    },
    {
      "epoch": 0.7198713189804504,
      "grad_norm": 0.029927410185337067,
      "learning_rate": 0.00035705366486757037,
      "loss": 0.0596,
      "step": 2909
    },
    {
      "epoch": 0.7201187824795843,
      "grad_norm": 0.05345943197607994,
      "learning_rate": 0.00035696572981774145,
      "loss": 0.0594,
      "step": 2910
    },
    {
      "epoch": 0.7203662459787181,
      "grad_norm": 0.02724754624068737,
      "learning_rate": 0.00035687777856541013,
      "loss": 0.0198,
      "step": 2911
    },
    {
      "epoch": 0.720613709477852,
      "grad_norm": 0.03504394739866257,
      "learning_rate": 0.00035678981112389883,
      "loss": 0.0511,
      "step": 2912
    },
    {
      "epoch": 0.7208611729769859,
      "grad_norm": 0.025756411254405975,
      "learning_rate": 0.0003567018275065323,
      "loss": 0.0358,
      "step": 2913
    },
    {
      "epoch": 0.7211086364761198,
      "grad_norm": 0.036339592188596725,
      "learning_rate": 0.00035661382772663776,
      "loss": 0.0364,
      "step": 2914
    },
    {
      "epoch": 0.7213560999752536,
      "grad_norm": 0.04504727944731712,
      "learning_rate": 0.00035652581179754483,
      "loss": 0.0548,
      "step": 2915
    },
    {
      "epoch": 0.7216035634743875,
      "grad_norm": 0.03149900957942009,
      "learning_rate": 0.0003564377797325856,
      "loss": 0.0351,
      "step": 2916
    },
    {
      "epoch": 0.7218510269735214,
      "grad_norm": 0.03403514623641968,
      "learning_rate": 0.00035634973154509463,
      "loss": 0.0533,
      "step": 2917
    },
    {
      "epoch": 0.7220984904726553,
      "grad_norm": 0.0404612235724926,
      "learning_rate": 0.000356261667248409,
      "loss": 0.0603,
      "step": 2918
    },
    {
      "epoch": 0.7223459539717891,
      "grad_norm": 0.04063718765974045,
      "learning_rate": 0.00035617358685586804,
      "loss": 0.076,
      "step": 2919
    },
    {
      "epoch": 0.7225934174709231,
      "grad_norm": 0.08214005082845688,
      "learning_rate": 0.00035608549038081366,
      "loss": 0.096,
      "step": 2920
    },
    {
      "epoch": 0.7228408809700569,
      "grad_norm": 0.035500023514032364,
      "learning_rate": 0.0003559973778365901,
      "loss": 0.0418,
      "step": 2921
    },
    {
      "epoch": 0.7230883444691908,
      "grad_norm": 0.045171644538640976,
      "learning_rate": 0.00035590924923654423,
      "loss": 0.0733,
      "step": 2922
    },
    {
      "epoch": 0.7233358079683246,
      "grad_norm": 0.022064482793211937,
      "learning_rate": 0.00035582110459402504,
      "loss": 0.0412,
      "step": 2923
    },
    {
      "epoch": 0.7235832714674586,
      "grad_norm": 0.039441533386707306,
      "learning_rate": 0.00035573294392238436,
      "loss": 0.0397,
      "step": 2924
    },
    {
      "epoch": 0.7238307349665924,
      "grad_norm": 0.04227723181247711,
      "learning_rate": 0.00035564476723497603,
      "loss": 0.0329,
      "step": 2925
    },
    {
      "epoch": 0.7240781984657263,
      "grad_norm": 0.03476080298423767,
      "learning_rate": 0.00035555657454515655,
      "loss": 0.0415,
      "step": 2926
    },
    {
      "epoch": 0.7243256619648601,
      "grad_norm": 0.03971000760793686,
      "learning_rate": 0.0003554683658662848,
      "loss": 0.0488,
      "step": 2927
    },
    {
      "epoch": 0.7245731254639941,
      "grad_norm": 0.119230255484581,
      "learning_rate": 0.0003553801412117222,
      "loss": 0.0763,
      "step": 2928
    },
    {
      "epoch": 0.724820588963128,
      "grad_norm": 0.05058324337005615,
      "learning_rate": 0.0003552919005948323,
      "loss": 0.0721,
      "step": 2929
    },
    {
      "epoch": 0.7250680524622618,
      "grad_norm": 0.043238215148448944,
      "learning_rate": 0.0003552036440289813,
      "loss": 0.0791,
      "step": 2930
    },
    {
      "epoch": 0.7253155159613957,
      "grad_norm": 0.029131926596164703,
      "learning_rate": 0.0003551153715275378,
      "loss": 0.0362,
      "step": 2931
    },
    {
      "epoch": 0.7255629794605296,
      "grad_norm": 0.04752261936664581,
      "learning_rate": 0.00035502708310387274,
      "loss": 0.0399,
      "step": 2932
    },
    {
      "epoch": 0.7258104429596635,
      "grad_norm": 0.034572429955005646,
      "learning_rate": 0.00035493877877135937,
      "loss": 0.0432,
      "step": 2933
    },
    {
      "epoch": 0.7260579064587973,
      "grad_norm": 0.024468058720231056,
      "learning_rate": 0.0003548504585433737,
      "loss": 0.0329,
      "step": 2934
    },
    {
      "epoch": 0.7263053699579312,
      "grad_norm": 0.03356872498989105,
      "learning_rate": 0.00035476212243329387,
      "loss": 0.0494,
      "step": 2935
    },
    {
      "epoch": 0.7265528334570651,
      "grad_norm": 0.07585480064153671,
      "learning_rate": 0.00035467377045450045,
      "loss": 0.103,
      "step": 2936
    },
    {
      "epoch": 0.726800296956199,
      "grad_norm": 0.034645576030015945,
      "learning_rate": 0.00035458540262037636,
      "loss": 0.0334,
      "step": 2937
    },
    {
      "epoch": 0.7270477604553328,
      "grad_norm": 0.023936599493026733,
      "learning_rate": 0.00035449701894430706,
      "loss": 0.0254,
      "step": 2938
    },
    {
      "epoch": 0.7272952239544667,
      "grad_norm": 0.0325830802321434,
      "learning_rate": 0.0003544086194396805,
      "loss": 0.0619,
      "step": 2939
    },
    {
      "epoch": 0.7275426874536006,
      "grad_norm": 0.03869204968214035,
      "learning_rate": 0.0003543202041198867,
      "loss": 0.051,
      "step": 2940
    },
    {
      "epoch": 0.7277901509527345,
      "grad_norm": 0.026684025302529335,
      "learning_rate": 0.00035423177299831845,
      "loss": 0.0256,
      "step": 2941
    },
    {
      "epoch": 0.7280376144518683,
      "grad_norm": 0.12968406081199646,
      "learning_rate": 0.0003541433260883705,
      "loss": 0.126,
      "step": 2942
    },
    {
      "epoch": 0.7282850779510023,
      "grad_norm": 0.07994022965431213,
      "learning_rate": 0.00035405486340344045,
      "loss": 0.0851,
      "step": 2943
    },
    {
      "epoch": 0.7285325414501361,
      "grad_norm": 0.050488971173763275,
      "learning_rate": 0.000353966384956928,
      "loss": 0.0601,
      "step": 2944
    },
    {
      "epoch": 0.72878000494927,
      "grad_norm": 0.05281640589237213,
      "learning_rate": 0.00035387789076223524,
      "loss": 0.0716,
      "step": 2945
    },
    {
      "epoch": 0.7290274684484038,
      "grad_norm": 0.03965461626648903,
      "learning_rate": 0.00035378938083276677,
      "loss": 0.0492,
      "step": 2946
    },
    {
      "epoch": 0.7292749319475378,
      "grad_norm": 0.02812867797911167,
      "learning_rate": 0.00035370085518192964,
      "loss": 0.0412,
      "step": 2947
    },
    {
      "epoch": 0.7295223954466716,
      "grad_norm": 0.019968118518590927,
      "learning_rate": 0.000353612313823133,
      "loss": 0.0265,
      "step": 2948
    },
    {
      "epoch": 0.7297698589458055,
      "grad_norm": 0.037023093551397324,
      "learning_rate": 0.00035352375676978866,
      "loss": 0.0231,
      "step": 2949
    },
    {
      "epoch": 0.7300173224449393,
      "grad_norm": 0.04785354435443878,
      "learning_rate": 0.0003534351840353106,
      "loss": 0.0633,
      "step": 2950
    },
    {
      "epoch": 0.7302647859440733,
      "grad_norm": 0.03324657678604126,
      "learning_rate": 0.00035334659563311535,
      "loss": 0.041,
      "step": 2951
    },
    {
      "epoch": 0.7305122494432071,
      "grad_norm": 0.034774765372276306,
      "learning_rate": 0.00035325799157662164,
      "loss": 0.0479,
      "step": 2952
    },
    {
      "epoch": 0.730759712942341,
      "grad_norm": 0.04519223794341087,
      "learning_rate": 0.0003531693718792507,
      "loss": 0.0634,
      "step": 2953
    },
    {
      "epoch": 0.7310071764414748,
      "grad_norm": 0.04638463631272316,
      "learning_rate": 0.00035308073655442615,
      "loss": 0.0577,
      "step": 2954
    },
    {
      "epoch": 0.7312546399406088,
      "grad_norm": 0.046507999300956726,
      "learning_rate": 0.0003529920856155738,
      "loss": 0.0367,
      "step": 2955
    },
    {
      "epoch": 0.7315021034397426,
      "grad_norm": 0.046382855623960495,
      "learning_rate": 0.000352903419076122,
      "loss": 0.0664,
      "step": 2956
    },
    {
      "epoch": 0.7317495669388765,
      "grad_norm": 0.03158707544207573,
      "learning_rate": 0.0003528147369495015,
      "loss": 0.0443,
      "step": 2957
    },
    {
      "epoch": 0.7319970304380103,
      "grad_norm": 0.03240172564983368,
      "learning_rate": 0.00035272603924914515,
      "loss": 0.0442,
      "step": 2958
    },
    {
      "epoch": 0.7322444939371443,
      "grad_norm": 0.04062187299132347,
      "learning_rate": 0.0003526373259884884,
      "loss": 0.0538,
      "step": 2959
    },
    {
      "epoch": 0.7324919574362782,
      "grad_norm": 0.03337433561682701,
      "learning_rate": 0.000352548597180969,
      "loss": 0.0471,
      "step": 2960
    },
    {
      "epoch": 0.732739420935412,
      "grad_norm": 0.03050457499921322,
      "learning_rate": 0.000352459852840027,
      "loss": 0.0388,
      "step": 2961
    },
    {
      "epoch": 0.732986884434546,
      "grad_norm": 0.0668942779302597,
      "learning_rate": 0.00035237109297910486,
      "loss": 0.1404,
      "step": 2962
    },
    {
      "epoch": 0.7332343479336798,
      "grad_norm": 0.036546140909194946,
      "learning_rate": 0.0003522823176116474,
      "loss": 0.0601,
      "step": 2963
    },
    {
      "epoch": 0.7334818114328137,
      "grad_norm": 0.055947382003068924,
      "learning_rate": 0.0003521935267511017,
      "loss": 0.0701,
      "step": 2964
    },
    {
      "epoch": 0.7337292749319475,
      "grad_norm": 0.038925375789403915,
      "learning_rate": 0.00035210472041091734,
      "loss": 0.0553,
      "step": 2965
    },
    {
      "epoch": 0.7339767384310815,
      "grad_norm": 0.04536188766360283,
      "learning_rate": 0.000352015898604546,
      "loss": 0.0397,
      "step": 2966
    },
    {
      "epoch": 0.7342242019302153,
      "grad_norm": 0.05181032419204712,
      "learning_rate": 0.00035192706134544194,
      "loss": 0.0736,
      "step": 2967
    },
    {
      "epoch": 0.7344716654293492,
      "grad_norm": 0.03274145722389221,
      "learning_rate": 0.0003518382086470617,
      "loss": 0.0537,
      "step": 2968
    },
    {
      "epoch": 0.734719128928483,
      "grad_norm": 0.059550195932388306,
      "learning_rate": 0.00035174934052286413,
      "loss": 0.0748,
      "step": 2969
    },
    {
      "epoch": 0.734966592427617,
      "grad_norm": 0.0316179096698761,
      "learning_rate": 0.0003516604569863104,
      "loss": 0.1115,
      "step": 2970
    },
    {
      "epoch": 0.7352140559267508,
      "grad_norm": 0.044354233890771866,
      "learning_rate": 0.000351571558050864,
      "loss": 0.0519,
      "step": 2971
    },
    {
      "epoch": 0.7354615194258847,
      "grad_norm": 0.056884124875068665,
      "learning_rate": 0.0003514826437299908,
      "loss": 0.0709,
      "step": 2972
    },
    {
      "epoch": 0.7357089829250185,
      "grad_norm": 0.025079380720853806,
      "learning_rate": 0.000351393714037159,
      "loss": 0.0646,
      "step": 2973
    },
    {
      "epoch": 0.7359564464241525,
      "grad_norm": 0.06730670481920242,
      "learning_rate": 0.0003513047689858392,
      "loss": 0.0797,
      "step": 2974
    },
    {
      "epoch": 0.7362039099232863,
      "grad_norm": 0.05193821340799332,
      "learning_rate": 0.0003512158085895041,
      "loss": 0.0673,
      "step": 2975
    },
    {
      "epoch": 0.7364513734224202,
      "grad_norm": 0.04720687493681908,
      "learning_rate": 0.00035112683286162895,
      "loss": 0.0581,
      "step": 2976
    },
    {
      "epoch": 0.736698836921554,
      "grad_norm": 0.03470895439386368,
      "learning_rate": 0.00035103784181569127,
      "loss": 0.0701,
      "step": 2977
    },
    {
      "epoch": 0.736946300420688,
      "grad_norm": 0.03349664807319641,
      "learning_rate": 0.00035094883546517075,
      "loss": 0.0424,
      "step": 2978
    },
    {
      "epoch": 0.7371937639198218,
      "grad_norm": 0.02981209196150303,
      "learning_rate": 0.0003508598138235496,
      "loss": 0.0396,
      "step": 2979
    },
    {
      "epoch": 0.7374412274189557,
      "grad_norm": 0.04544321075081825,
      "learning_rate": 0.00035077077690431224,
      "loss": 0.0603,
      "step": 2980
    },
    {
      "epoch": 0.7376886909180895,
      "grad_norm": 0.04411564767360687,
      "learning_rate": 0.00035068172472094546,
      "loss": 0.0337,
      "step": 2981
    },
    {
      "epoch": 0.7379361544172235,
      "grad_norm": 0.03359556943178177,
      "learning_rate": 0.0003505926572869383,
      "loss": 0.033,
      "step": 2982
    },
    {
      "epoch": 0.7381836179163573,
      "grad_norm": 0.06937862187623978,
      "learning_rate": 0.0003505035746157822,
      "loss": 0.0895,
      "step": 2983
    },
    {
      "epoch": 0.7384310814154912,
      "grad_norm": 0.030936438590288162,
      "learning_rate": 0.0003504144767209707,
      "loss": 0.0535,
      "step": 2984
    },
    {
      "epoch": 0.7386785449146251,
      "grad_norm": 0.030405832454562187,
      "learning_rate": 0.0003503253636159999,
      "loss": 0.0431,
      "step": 2985
    },
    {
      "epoch": 0.738926008413759,
      "grad_norm": 0.04335198923945427,
      "learning_rate": 0.0003502362353143681,
      "loss": 0.0568,
      "step": 2986
    },
    {
      "epoch": 0.7391734719128928,
      "grad_norm": 0.0465807169675827,
      "learning_rate": 0.000350147091829576,
      "loss": 0.0399,
      "step": 2987
    },
    {
      "epoch": 0.7394209354120267,
      "grad_norm": 0.07439375668764114,
      "learning_rate": 0.00035005793317512623,
      "loss": 0.0556,
      "step": 2988
    },
    {
      "epoch": 0.7396683989111607,
      "grad_norm": 0.02773824892938137,
      "learning_rate": 0.0003499687593645242,
      "loss": 0.0581,
      "step": 2989
    },
    {
      "epoch": 0.7399158624102945,
      "grad_norm": 0.06352584064006805,
      "learning_rate": 0.0003498795704112774,
      "loss": 0.0398,
      "step": 2990
    },
    {
      "epoch": 0.7401633259094283,
      "grad_norm": 0.028790345415472984,
      "learning_rate": 0.0003497903663288955,
      "loss": 0.0468,
      "step": 2991
    },
    {
      "epoch": 0.7404107894085622,
      "grad_norm": 0.046019893139600754,
      "learning_rate": 0.0003497011471308907,
      "loss": 0.0654,
      "step": 2992
    },
    {
      "epoch": 0.7406582529076962,
      "grad_norm": 0.029755789786577225,
      "learning_rate": 0.0003496119128307772,
      "loss": 0.0407,
      "step": 2993
    },
    {
      "epoch": 0.74090571640683,
      "grad_norm": 0.03239033371210098,
      "learning_rate": 0.0003495226634420719,
      "loss": 0.047,
      "step": 2994
    },
    {
      "epoch": 0.7411531799059639,
      "grad_norm": 0.02381131425499916,
      "learning_rate": 0.0003494333989782935,
      "loss": 0.0219,
      "step": 2995
    },
    {
      "epoch": 0.7414006434050977,
      "grad_norm": 0.059831403195858,
      "learning_rate": 0.0003493441194529634,
      "loss": 0.0777,
      "step": 2996
    },
    {
      "epoch": 0.7416481069042317,
      "grad_norm": 0.03393726423382759,
      "learning_rate": 0.000349254824879605,
      "loss": 0.0238,
      "step": 2997
    },
    {
      "epoch": 0.7418955704033655,
      "grad_norm": 0.02457336150109768,
      "learning_rate": 0.00034916551527174424,
      "loss": 0.0387,
      "step": 2998
    },
    {
      "epoch": 0.7421430339024994,
      "grad_norm": 0.05517300218343735,
      "learning_rate": 0.000349076190642909,
      "loss": 0.0909,
      "step": 2999
    },
    {
      "epoch": 0.7423904974016332,
      "grad_norm": 0.03176598250865936,
      "learning_rate": 0.0003489868510066296,
      "loss": 0.0516,
      "step": 3000
    },
    {
      "epoch": 0.7423904974016332,
      "eval_loss": 0.2943735718727112,
      "eval_runtime": 202.7059,
      "eval_samples_per_second": 4.933,
      "eval_steps_per_second": 0.311,
      "step": 3000
    },
    {
      "epoch": 0.7426379609007672,
      "grad_norm": 0.04263310879468918,
      "learning_rate": 0.00034889749637643874,
      "loss": 0.0569,
      "step": 3001
    },
    {
      "epoch": 0.742885424399901,
      "grad_norm": 0.050301019102334976,
      "learning_rate": 0.00034880812676587144,
      "loss": 0.0587,
      "step": 3002
    },
    {
      "epoch": 0.7431328878990349,
      "grad_norm": 0.042001210153102875,
      "learning_rate": 0.00034871874218846463,
      "loss": 0.042,
      "step": 3003
    },
    {
      "epoch": 0.7433803513981687,
      "grad_norm": 0.0362122468650341,
      "learning_rate": 0.0003486293426577578,
      "loss": 0.0345,
      "step": 3004
    },
    {
      "epoch": 0.7436278148973027,
      "grad_norm": 0.07581409066915512,
      "learning_rate": 0.0003485399281872927,
      "loss": 0.0818,
      "step": 3005
    },
    {
      "epoch": 0.7438752783964365,
      "grad_norm": 0.0482758954167366,
      "learning_rate": 0.00034845049879061313,
      "loss": 0.0516,
      "step": 3006
    },
    {
      "epoch": 0.7441227418955704,
      "grad_norm": 0.040715351700782776,
      "learning_rate": 0.0003483610544812654,
      "loss": 0.047,
      "step": 3007
    },
    {
      "epoch": 0.7443702053947043,
      "grad_norm": 0.06184582784771919,
      "learning_rate": 0.0003482715952727981,
      "loss": 0.0809,
      "step": 3008
    },
    {
      "epoch": 0.7446176688938382,
      "grad_norm": 0.06015343219041824,
      "learning_rate": 0.0003481821211787617,
      "loss": 0.0649,
      "step": 3009
    },
    {
      "epoch": 0.744865132392972,
      "grad_norm": 0.03556248918175697,
      "learning_rate": 0.0003480926322127093,
      "loss": 0.0459,
      "step": 3010
    },
    {
      "epoch": 0.7451125958921059,
      "grad_norm": 0.0887352004647255,
      "learning_rate": 0.0003480031283881961,
      "loss": 0.0818,
      "step": 3011
    },
    {
      "epoch": 0.7453600593912398,
      "grad_norm": 0.05961500480771065,
      "learning_rate": 0.0003479136097187798,
      "loss": 0.0399,
      "step": 3012
    },
    {
      "epoch": 0.7456075228903737,
      "grad_norm": 0.02624593675136566,
      "learning_rate": 0.00034782407621801974,
      "loss": 0.0243,
      "step": 3013
    },
    {
      "epoch": 0.7458549863895075,
      "grad_norm": 0.04283909872174263,
      "learning_rate": 0.00034773452789947824,
      "loss": 0.0512,
      "step": 3014
    },
    {
      "epoch": 0.7461024498886414,
      "grad_norm": 0.08816249668598175,
      "learning_rate": 0.00034764496477671927,
      "loss": 0.0933,
      "step": 3015
    },
    {
      "epoch": 0.7463499133877753,
      "grad_norm": 0.028139682486653328,
      "learning_rate": 0.00034755538686330944,
      "loss": 0.043,
      "step": 3016
    },
    {
      "epoch": 0.7465973768869092,
      "grad_norm": 0.04103044420480728,
      "learning_rate": 0.00034746579417281744,
      "loss": 0.0554,
      "step": 3017
    },
    {
      "epoch": 0.746844840386043,
      "grad_norm": 0.07955171912908554,
      "learning_rate": 0.00034737618671881423,
      "loss": 0.0834,
      "step": 3018
    },
    {
      "epoch": 0.7470923038851769,
      "grad_norm": 0.07451873272657394,
      "learning_rate": 0.0003472865645148728,
      "loss": 0.1021,
      "step": 3019
    },
    {
      "epoch": 0.7473397673843108,
      "grad_norm": 0.03664511814713478,
      "learning_rate": 0.00034719692757456883,
      "loss": 0.055,
      "step": 3020
    },
    {
      "epoch": 0.7475872308834447,
      "grad_norm": 0.056076500564813614,
      "learning_rate": 0.0003471072759114798,
      "loss": 0.0906,
      "step": 3021
    },
    {
      "epoch": 0.7478346943825785,
      "grad_norm": 0.030174387618899345,
      "learning_rate": 0.0003470176095391857,
      "loss": 0.0461,
      "step": 3022
    },
    {
      "epoch": 0.7480821578817124,
      "grad_norm": 0.03216397762298584,
      "learning_rate": 0.0003469279284712685,
      "loss": 0.0391,
      "step": 3023
    },
    {
      "epoch": 0.7483296213808464,
      "grad_norm": 0.028492091223597527,
      "learning_rate": 0.00034683823272131263,
      "loss": 0.0265,
      "step": 3024
    },
    {
      "epoch": 0.7485770848799802,
      "grad_norm": 0.035666514188051224,
      "learning_rate": 0.00034674852230290456,
      "loss": 0.0397,
      "step": 3025
    },
    {
      "epoch": 0.748824548379114,
      "grad_norm": 0.03832787275314331,
      "learning_rate": 0.0003466587972296332,
      "loss": 0.0335,
      "step": 3026
    },
    {
      "epoch": 0.749072011878248,
      "grad_norm": 0.04107518494129181,
      "learning_rate": 0.0003465690575150894,
      "loss": 0.0846,
      "step": 3027
    },
    {
      "epoch": 0.7493194753773819,
      "grad_norm": 0.056475572288036346,
      "learning_rate": 0.00034647930317286644,
      "loss": 0.0875,
      "step": 3028
    },
    {
      "epoch": 0.7495669388765157,
      "grad_norm": 0.04130066558718681,
      "learning_rate": 0.0003463895342165597,
      "loss": 0.0788,
      "step": 3029
    },
    {
      "epoch": 0.7498144023756496,
      "grad_norm": 0.024167843163013458,
      "learning_rate": 0.0003462997506597669,
      "loss": 0.0311,
      "step": 3030
    },
    {
      "epoch": 0.7500618658747835,
      "grad_norm": 0.035350460559129715,
      "learning_rate": 0.00034620995251608786,
      "loss": 0.0344,
      "step": 3031
    },
    {
      "epoch": 0.7503093293739174,
      "grad_norm": 0.04033831134438515,
      "learning_rate": 0.00034612013979912464,
      "loss": 0.0568,
      "step": 3032
    },
    {
      "epoch": 0.7505567928730512,
      "grad_norm": 0.06535568833351135,
      "learning_rate": 0.0003460303125224815,
      "loss": 0.1158,
      "step": 3033
    },
    {
      "epoch": 0.7508042563721851,
      "grad_norm": 0.033950403332710266,
      "learning_rate": 0.0003459404706997649,
      "loss": 0.0339,
      "step": 3034
    },
    {
      "epoch": 0.751051719871319,
      "grad_norm": 0.05921526998281479,
      "learning_rate": 0.0003458506143445836,
      "loss": 0.0816,
      "step": 3035
    },
    {
      "epoch": 0.7512991833704529,
      "grad_norm": 0.04587625339627266,
      "learning_rate": 0.00034576074347054844,
      "loss": 0.0702,
      "step": 3036
    },
    {
      "epoch": 0.7515466468695867,
      "grad_norm": 0.016276588663458824,
      "learning_rate": 0.0003456708580912725,
      "loss": 0.0184,
      "step": 3037
    },
    {
      "epoch": 0.7517941103687206,
      "grad_norm": 0.08203975111246109,
      "learning_rate": 0.00034558095822037095,
      "loss": 0.0773,
      "step": 3038
    },
    {
      "epoch": 0.7520415738678545,
      "grad_norm": 0.02734135463833809,
      "learning_rate": 0.00034549104387146144,
      "loss": 0.0338,
      "step": 3039
    },
    {
      "epoch": 0.7522890373669884,
      "grad_norm": 0.01801295019686222,
      "learning_rate": 0.0003454011150581635,
      "loss": 0.0242,
      "step": 3040
    },
    {
      "epoch": 0.7525365008661222,
      "grad_norm": 0.07450036704540253,
      "learning_rate": 0.0003453111717940991,
      "loss": 0.0387,
      "step": 3041
    },
    {
      "epoch": 0.7527839643652561,
      "grad_norm": 0.025441868230700493,
      "learning_rate": 0.00034522121409289217,
      "loss": 0.0355,
      "step": 3042
    },
    {
      "epoch": 0.75303142786439,
      "grad_norm": 0.08636169135570526,
      "learning_rate": 0.000345131241968169,
      "loss": 0.0884,
      "step": 3043
    },
    {
      "epoch": 0.7532788913635239,
      "grad_norm": 0.06009035184979439,
      "learning_rate": 0.000345041255433558,
      "loss": 0.0665,
      "step": 3044
    },
    {
      "epoch": 0.7535263548626577,
      "grad_norm": 0.033142685890197754,
      "learning_rate": 0.00034495125450268984,
      "loss": 0.0633,
      "step": 3045
    },
    {
      "epoch": 0.7537738183617916,
      "grad_norm": 0.01835213601589203,
      "learning_rate": 0.0003448612391891971,
      "loss": 0.0217,
      "step": 3046
    },
    {
      "epoch": 0.7540212818609255,
      "grad_norm": 0.05770641565322876,
      "learning_rate": 0.00034477120950671505,
      "loss": 0.0475,
      "step": 3047
    },
    {
      "epoch": 0.7542687453600594,
      "grad_norm": 0.11438173055648804,
      "learning_rate": 0.0003446811654688805,
      "loss": 0.1077,
      "step": 3048
    },
    {
      "epoch": 0.7545162088591932,
      "grad_norm": 0.03773662820458412,
      "learning_rate": 0.000344591107089333,
      "loss": 0.0877,
      "step": 3049
    },
    {
      "epoch": 0.7547636723583272,
      "grad_norm": 0.024747055023908615,
      "learning_rate": 0.0003445010343817138,
      "loss": 0.0261,
      "step": 3050
    },
    {
      "epoch": 0.755011135857461,
      "grad_norm": 0.027634147554636,
      "learning_rate": 0.0003444109473596668,
      "loss": 0.0341,
      "step": 3051
    },
    {
      "epoch": 0.7552585993565949,
      "grad_norm": 0.03817013278603554,
      "learning_rate": 0.0003443208460368377,
      "loss": 0.04,
      "step": 3052
    },
    {
      "epoch": 0.7555060628557287,
      "grad_norm": 0.038672491908073425,
      "learning_rate": 0.0003442307304268746,
      "loss": 0.0651,
      "step": 3053
    },
    {
      "epoch": 0.7557535263548627,
      "grad_norm": 0.03260371834039688,
      "learning_rate": 0.00034414060054342743,
      "loss": 0.0442,
      "step": 3054
    },
    {
      "epoch": 0.7560009898539966,
      "grad_norm": 0.12245763838291168,
      "learning_rate": 0.0003440504564001486,
      "loss": 0.0682,
      "step": 3055
    },
    {
      "epoch": 0.7562484533531304,
      "grad_norm": 0.05492492765188217,
      "learning_rate": 0.0003439602980106926,
      "loss": 0.0747,
      "step": 3056
    },
    {
      "epoch": 0.7564959168522642,
      "grad_norm": 0.05185329541563988,
      "learning_rate": 0.0003438701253887162,
      "loss": 0.0594,
      "step": 3057
    },
    {
      "epoch": 0.7567433803513982,
      "grad_norm": 0.04104764759540558,
      "learning_rate": 0.000343779938547878,
      "loss": 0.0402,
      "step": 3058
    },
    {
      "epoch": 0.7569908438505321,
      "grad_norm": 0.029996437951922417,
      "learning_rate": 0.00034368973750183895,
      "loss": 0.0625,
      "step": 3059
    },
    {
      "epoch": 0.7572383073496659,
      "grad_norm": 0.03956412896513939,
      "learning_rate": 0.00034359952226426215,
      "loss": 0.0643,
      "step": 3060
    },
    {
      "epoch": 0.7574857708487998,
      "grad_norm": 0.05634066462516785,
      "learning_rate": 0.000343509292848813,
      "loss": 0.0688,
      "step": 3061
    },
    {
      "epoch": 0.7577332343479337,
      "grad_norm": 0.03693857789039612,
      "learning_rate": 0.00034341904926915866,
      "loss": 0.0415,
      "step": 3062
    },
    {
      "epoch": 0.7579806978470676,
      "grad_norm": 0.03652140870690346,
      "learning_rate": 0.0003433287915389689,
      "loss": 0.0434,
      "step": 3063
    },
    {
      "epoch": 0.7582281613462014,
      "grad_norm": 0.03687924146652222,
      "learning_rate": 0.0003432385196719152,
      "loss": 0.0408,
      "step": 3064
    },
    {
      "epoch": 0.7584756248453353,
      "grad_norm": 0.03965305536985397,
      "learning_rate": 0.00034314823368167144,
      "loss": 0.041,
      "step": 3065
    },
    {
      "epoch": 0.7587230883444692,
      "grad_norm": 0.024688132107257843,
      "learning_rate": 0.00034305793358191356,
      "loss": 0.0452,
      "step": 3066
    },
    {
      "epoch": 0.7589705518436031,
      "grad_norm": 0.03680253401398659,
      "learning_rate": 0.0003429676193863198,
      "loss": 0.0325,
      "step": 3067
    },
    {
      "epoch": 0.7592180153427369,
      "grad_norm": 0.04845409840345383,
      "learning_rate": 0.0003428772911085702,
      "loss": 0.0714,
      "step": 3068
    },
    {
      "epoch": 0.7594654788418709,
      "grad_norm": 0.08785786479711533,
      "learning_rate": 0.0003427869487623472,
      "loss": 0.0472,
      "step": 3069
    },
    {
      "epoch": 0.7597129423410047,
      "grad_norm": 0.027600785717368126,
      "learning_rate": 0.0003426965923613353,
      "loss": 0.0354,
      "step": 3070
    },
    {
      "epoch": 0.7599604058401386,
      "grad_norm": 0.036006372421979904,
      "learning_rate": 0.0003426062219192212,
      "loss": 0.0413,
      "step": 3071
    },
    {
      "epoch": 0.7602078693392724,
      "grad_norm": 0.03683434799313545,
      "learning_rate": 0.0003425158374496935,
      "loss": 0.0497,
      "step": 3072
    },
    {
      "epoch": 0.7604553328384064,
      "grad_norm": 0.027918484061956406,
      "learning_rate": 0.0003424254389664433,
      "loss": 0.0318,
      "step": 3073
    },
    {
      "epoch": 0.7607027963375402,
      "grad_norm": 0.057360339909791946,
      "learning_rate": 0.00034233502648316325,
      "loss": 0.0417,
      "step": 3074
    },
    {
      "epoch": 0.7609502598366741,
      "grad_norm": 0.03159065172076225,
      "learning_rate": 0.0003422446000135488,
      "loss": 0.0634,
      "step": 3075
    },
    {
      "epoch": 0.7611977233358079,
      "grad_norm": 0.03343046456575394,
      "learning_rate": 0.00034215415957129704,
      "loss": 0.0518,
      "step": 3076
    },
    {
      "epoch": 0.7614451868349419,
      "grad_norm": 0.05505616217851639,
      "learning_rate": 0.00034206370517010735,
      "loss": 0.0634,
      "step": 3077
    },
    {
      "epoch": 0.7616926503340757,
      "grad_norm": 0.05588321387767792,
      "learning_rate": 0.00034197323682368114,
      "loss": 0.0289,
      "step": 3078
    },
    {
      "epoch": 0.7619401138332096,
      "grad_norm": 0.02793971821665764,
      "learning_rate": 0.000341882754545722,
      "loss": 0.03,
      "step": 3079
    },
    {
      "epoch": 0.7621875773323434,
      "grad_norm": 0.029484780505299568,
      "learning_rate": 0.00034179225834993584,
      "loss": 0.0465,
      "step": 3080
    },
    {
      "epoch": 0.7624350408314774,
      "grad_norm": 0.040579698979854584,
      "learning_rate": 0.0003417017482500302,
      "loss": 0.0347,
      "step": 3081
    },
    {
      "epoch": 0.7626825043306112,
      "grad_norm": 0.06160145625472069,
      "learning_rate": 0.0003416112242597151,
      "loss": 0.0474,
      "step": 3082
    },
    {
      "epoch": 0.7629299678297451,
      "grad_norm": 0.03923697769641876,
      "learning_rate": 0.00034152068639270244,
      "loss": 0.0432,
      "step": 3083
    },
    {
      "epoch": 0.7631774313288789,
      "grad_norm": 0.03857006877660751,
      "learning_rate": 0.00034143013466270643,
      "loss": 0.0815,
      "step": 3084
    },
    {
      "epoch": 0.7634248948280129,
      "grad_norm": 0.03220030292868614,
      "learning_rate": 0.00034133956908344337,
      "loss": 0.0437,
      "step": 3085
    },
    {
      "epoch": 0.7636723583271467,
      "grad_norm": 0.035437341779470444,
      "learning_rate": 0.0003412489896686314,
      "loss": 0.0459,
      "step": 3086
    },
    {
      "epoch": 0.7639198218262806,
      "grad_norm": 0.04882223159074783,
      "learning_rate": 0.00034115839643199096,
      "loss": 0.059,
      "step": 3087
    },
    {
      "epoch": 0.7641672853254144,
      "grad_norm": 0.051056668162345886,
      "learning_rate": 0.0003410677893872447,
      "loss": 0.0752,
      "step": 3088
    },
    {
      "epoch": 0.7644147488245484,
      "grad_norm": 0.1890375167131424,
      "learning_rate": 0.000340977168548117,
      "loss": 0.0947,
      "step": 3089
    },
    {
      "epoch": 0.7646622123236823,
      "grad_norm": 0.04047168046236038,
      "learning_rate": 0.0003408865339283347,
      "loss": 0.0529,
      "step": 3090
    },
    {
      "epoch": 0.7649096758228161,
      "grad_norm": 0.02554137445986271,
      "learning_rate": 0.0003407958855416264,
      "loss": 0.0361,
      "step": 3091
    },
    {
      "epoch": 0.7651571393219501,
      "grad_norm": 0.02919938415288925,
      "learning_rate": 0.0003407052234017232,
      "loss": 0.0259,
      "step": 3092
    },
    {
      "epoch": 0.7654046028210839,
      "grad_norm": 0.04149129241704941,
      "learning_rate": 0.00034061454752235774,
      "loss": 0.0635,
      "step": 3093
    },
    {
      "epoch": 0.7656520663202178,
      "grad_norm": 0.029136020690202713,
      "learning_rate": 0.0003405238579172652,
      "loss": 0.0326,
      "step": 3094
    },
    {
      "epoch": 0.7658995298193516,
      "grad_norm": 0.02622685581445694,
      "learning_rate": 0.00034043315460018277,
      "loss": 0.0324,
      "step": 3095
    },
    {
      "epoch": 0.7661469933184856,
      "grad_norm": 0.048571594059467316,
      "learning_rate": 0.0003403424375848495,
      "loss": 0.0879,
      "step": 3096
    },
    {
      "epoch": 0.7663944568176194,
      "grad_norm": 0.07142825424671173,
      "learning_rate": 0.0003402517068850065,
      "loss": 0.0633,
      "step": 3097
    },
    {
      "epoch": 0.7666419203167533,
      "grad_norm": 0.040193263441324234,
      "learning_rate": 0.00034016096251439744,
      "loss": 0.0625,
      "step": 3098
    },
    {
      "epoch": 0.7668893838158871,
      "grad_norm": 0.03274761512875557,
      "learning_rate": 0.0003400702044867674,
      "loss": 0.0385,
      "step": 3099
    },
    {
      "epoch": 0.7671368473150211,
      "grad_norm": 0.07853472232818604,
      "learning_rate": 0.00033997943281586395,
      "loss": 0.1077,
      "step": 3100
    },
    {
      "epoch": 0.7673843108141549,
      "grad_norm": 0.0975194051861763,
      "learning_rate": 0.00033988864751543666,
      "loss": 0.1121,
      "step": 3101
    },
    {
      "epoch": 0.7676317743132888,
      "grad_norm": 0.0480571910738945,
      "learning_rate": 0.00033979784859923713,
      "loss": 0.0701,
      "step": 3102
    },
    {
      "epoch": 0.7678792378124226,
      "grad_norm": 0.08498524874448776,
      "learning_rate": 0.00033970703608101894,
      "loss": 0.0863,
      "step": 3103
    },
    {
      "epoch": 0.7681267013115566,
      "grad_norm": 0.05933518335223198,
      "learning_rate": 0.0003396162099745378,
      "loss": 0.0736,
      "step": 3104
    },
    {
      "epoch": 0.7683741648106904,
      "grad_norm": 0.051088180392980576,
      "learning_rate": 0.0003395253702935516,
      "loss": 0.0376,
      "step": 3105
    },
    {
      "epoch": 0.7686216283098243,
      "grad_norm": 0.14174480736255646,
      "learning_rate": 0.00033943451705182004,
      "loss": 0.0819,
      "step": 3106
    },
    {
      "epoch": 0.7688690918089581,
      "grad_norm": 0.05242229625582695,
      "learning_rate": 0.000339343650263105,
      "loss": 0.0717,
      "step": 3107
    },
    {
      "epoch": 0.7691165553080921,
      "grad_norm": 0.06991757452487946,
      "learning_rate": 0.0003392527699411706,
      "loss": 0.0683,
      "step": 3108
    },
    {
      "epoch": 0.7693640188072259,
      "grad_norm": 0.04799938574433327,
      "learning_rate": 0.0003391618760997825,
      "loss": 0.0387,
      "step": 3109
    },
    {
      "epoch": 0.7696114823063598,
      "grad_norm": 0.04852146655321121,
      "learning_rate": 0.00033907096875270904,
      "loss": 0.0504,
      "step": 3110
    },
    {
      "epoch": 0.7698589458054937,
      "grad_norm": 0.040391191840171814,
      "learning_rate": 0.00033898004791372015,
      "loss": 0.0524,
      "step": 3111
    },
    {
      "epoch": 0.7701064093046276,
      "grad_norm": 0.03744666650891304,
      "learning_rate": 0.00033888911359658807,
      "loss": 0.0736,
      "step": 3112
    },
    {
      "epoch": 0.7703538728037614,
      "grad_norm": 0.050705939531326294,
      "learning_rate": 0.00033879816581508683,
      "loss": 0.0519,
      "step": 3113
    },
    {
      "epoch": 0.7706013363028953,
      "grad_norm": 0.041675109416246414,
      "learning_rate": 0.00033870720458299263,
      "loss": 0.0932,
      "step": 3114
    },
    {
      "epoch": 0.7708487998020292,
      "grad_norm": 0.033853016793727875,
      "learning_rate": 0.00033861622991408373,
      "loss": 0.0936,
      "step": 3115
    },
    {
      "epoch": 0.7710962633011631,
      "grad_norm": 0.05037635564804077,
      "learning_rate": 0.00033852524182214055,
      "loss": 0.0241,
      "step": 3116
    },
    {
      "epoch": 0.771343726800297,
      "grad_norm": 0.08376960456371307,
      "learning_rate": 0.00033843424032094525,
      "loss": 0.0668,
      "step": 3117
    },
    {
      "epoch": 0.7715911902994308,
      "grad_norm": 0.03591614589095116,
      "learning_rate": 0.0003383432254242823,
      "loss": 0.0608,
      "step": 3118
    },
    {
      "epoch": 0.7718386537985648,
      "grad_norm": 0.024663789197802544,
      "learning_rate": 0.00033825219714593774,
      "loss": 0.0212,
      "step": 3119
    },
    {
      "epoch": 0.7720861172976986,
      "grad_norm": 0.02068348042666912,
      "learning_rate": 0.00033816115549970037,
      "loss": 0.0279,
      "step": 3120
    },
    {
      "epoch": 0.7723335807968325,
      "grad_norm": 0.042823679745197296,
      "learning_rate": 0.0003380701004993604,
      "loss": 0.0466,
      "step": 3121
    },
    {
      "epoch": 0.7725810442959663,
      "grad_norm": 0.06010512635111809,
      "learning_rate": 0.0003379790321587104,
      "loss": 0.0454,
      "step": 3122
    },
    {
      "epoch": 0.7728285077951003,
      "grad_norm": 0.023654941469430923,
      "learning_rate": 0.0003378879504915446,
      "loss": 0.0461,
      "step": 3123
    },
    {
      "epoch": 0.7730759712942341,
      "grad_norm": 0.05020834505558014,
      "learning_rate": 0.00033779685551165974,
      "loss": 0.0755,
      "step": 3124
    },
    {
      "epoch": 0.773323434793368,
      "grad_norm": 0.027890536934137344,
      "learning_rate": 0.0003377057472328542,
      "loss": 0.039,
      "step": 3125
    },
    {
      "epoch": 0.7735708982925018,
      "grad_norm": 0.02024225704371929,
      "learning_rate": 0.00033761462566892853,
      "loss": 0.0316,
      "step": 3126
    },
    {
      "epoch": 0.7738183617916358,
      "grad_norm": 0.055271126329898834,
      "learning_rate": 0.0003375234908336852,
      "loss": 0.0306,
      "step": 3127
    },
    {
      "epoch": 0.7740658252907696,
      "grad_norm": 0.02203730307519436,
      "learning_rate": 0.0003374323427409289,
      "loss": 0.0193,
      "step": 3128
    },
    {
      "epoch": 0.7743132887899035,
      "grad_norm": 0.038083113729953766,
      "learning_rate": 0.00033734118140446584,
      "loss": 0.0389,
      "step": 3129
    },
    {
      "epoch": 0.7745607522890373,
      "grad_norm": 0.05485178902745247,
      "learning_rate": 0.000337250006838105,
      "loss": 0.0654,
      "step": 3130
    },
    {
      "epoch": 0.7748082157881713,
      "grad_norm": 0.02038496546447277,
      "learning_rate": 0.0003371588190556566,
      "loss": 0.0342,
      "step": 3131
    },
    {
      "epoch": 0.7750556792873051,
      "grad_norm": 0.03468623384833336,
      "learning_rate": 0.0003370676180709334,
      "loss": 0.0703,
      "step": 3132
    },
    {
      "epoch": 0.775303142786439,
      "grad_norm": 0.03686300292611122,
      "learning_rate": 0.0003369764038977498,
      "loss": 0.0207,
      "step": 3133
    },
    {
      "epoch": 0.7755506062855729,
      "grad_norm": 0.028417695313692093,
      "learning_rate": 0.0003368851765499225,
      "loss": 0.0281,
      "step": 3134
    },
    {
      "epoch": 0.7757980697847068,
      "grad_norm": 0.07046246528625488,
      "learning_rate": 0.00033679393604126994,
      "loss": 0.0423,
      "step": 3135
    },
    {
      "epoch": 0.7760455332838406,
      "grad_norm": 0.06682214140892029,
      "learning_rate": 0.0003367026823856128,
      "loss": 0.1377,
      "step": 3136
    },
    {
      "epoch": 0.7762929967829745,
      "grad_norm": 0.03144377842545509,
      "learning_rate": 0.0003366114155967734,
      "loss": 0.0423,
      "step": 3137
    },
    {
      "epoch": 0.7765404602821084,
      "grad_norm": 0.060684021562337875,
      "learning_rate": 0.0003365201356885765,
      "loss": 0.0657,
      "step": 3138
    },
    {
      "epoch": 0.7767879237812423,
      "grad_norm": 0.06563454121351242,
      "learning_rate": 0.00033642884267484846,
      "loss": 0.0858,
      "step": 3139
    },
    {
      "epoch": 0.7770353872803761,
      "grad_norm": 0.034222740679979324,
      "learning_rate": 0.00033633753656941783,
      "loss": 0.0732,
      "step": 3140
    },
    {
      "epoch": 0.77728285077951,
      "grad_norm": 0.03740501031279564,
      "learning_rate": 0.00033624621738611503,
      "loss": 0.0432,
      "step": 3141
    },
    {
      "epoch": 0.7775303142786439,
      "grad_norm": 0.033952973783016205,
      "learning_rate": 0.00033615488513877263,
      "loss": 0.0389,
      "step": 3142
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.037406373769044876,
      "learning_rate": 0.00033606353984122494,
      "loss": 0.0791,
      "step": 3143
    },
    {
      "epoch": 0.7780252412769116,
      "grad_norm": 0.04401783645153046,
      "learning_rate": 0.00033597218150730856,
      "loss": 0.054,
      "step": 3144
    },
    {
      "epoch": 0.7782727047760455,
      "grad_norm": 0.05990288406610489,
      "learning_rate": 0.0003358808101508617,
      "loss": 0.1284,
      "step": 3145
    },
    {
      "epoch": 0.7785201682751794,
      "grad_norm": 0.03252928704023361,
      "learning_rate": 0.00033578942578572473,
      "loss": 0.0595,
      "step": 3146
    },
    {
      "epoch": 0.7787676317743133,
      "grad_norm": 0.032060034573078156,
      "learning_rate": 0.00033569802842574017,
      "loss": 0.0323,
      "step": 3147
    },
    {
      "epoch": 0.7790150952734471,
      "grad_norm": 0.055676236748695374,
      "learning_rate": 0.0003356066180847521,
      "loss": 0.0949,
      "step": 3148
    },
    {
      "epoch": 0.779262558772581,
      "grad_norm": 0.034499600529670715,
      "learning_rate": 0.0003355151947766069,
      "loss": 0.0273,
      "step": 3149
    },
    {
      "epoch": 0.779510022271715,
      "grad_norm": 0.03666648268699646,
      "learning_rate": 0.0003354237585151528,
      "loss": 0.041,
      "step": 3150
    },
    {
      "epoch": 0.7797574857708488,
      "grad_norm": 0.03839319571852684,
      "learning_rate": 0.00033533230931424005,
      "loss": 0.0501,
      "step": 3151
    },
    {
      "epoch": 0.7800049492699827,
      "grad_norm": 0.031552061438560486,
      "learning_rate": 0.0003352408471877207,
      "loss": 0.0432,
      "step": 3152
    },
    {
      "epoch": 0.7802524127691166,
      "grad_norm": 0.05447722226381302,
      "learning_rate": 0.00033514937214944887,
      "loss": 0.0733,
      "step": 3153
    },
    {
      "epoch": 0.7804998762682505,
      "grad_norm": 0.02331661991775036,
      "learning_rate": 0.00033505788421328065,
      "loss": 0.0264,
      "step": 3154
    },
    {
      "epoch": 0.7807473397673843,
      "grad_norm": 0.044074852019548416,
      "learning_rate": 0.0003349663833930741,
      "loss": 0.0415,
      "step": 3155
    },
    {
      "epoch": 0.7809948032665182,
      "grad_norm": 0.03957337886095047,
      "learning_rate": 0.0003348748697026892,
      "loss": 0.0583,
      "step": 3156
    },
    {
      "epoch": 0.7812422667656521,
      "grad_norm": 0.03815812990069389,
      "learning_rate": 0.00033478334315598785,
      "loss": 0.0483,
      "step": 3157
    },
    {
      "epoch": 0.781489730264786,
      "grad_norm": 0.04861074686050415,
      "learning_rate": 0.00033469180376683377,
      "loss": 0.0436,
      "step": 3158
    },
    {
      "epoch": 0.7817371937639198,
      "grad_norm": 0.047727350145578384,
      "learning_rate": 0.00033460025154909313,
      "loss": 0.0379,
      "step": 3159
    },
    {
      "epoch": 0.7819846572630537,
      "grad_norm": 0.0338301956653595,
      "learning_rate": 0.0003345086865166334,
      "loss": 0.0405,
      "step": 3160
    },
    {
      "epoch": 0.7822321207621876,
      "grad_norm": 0.03364020586013794,
      "learning_rate": 0.0003344171086833243,
      "loss": 0.0546,
      "step": 3161
    },
    {
      "epoch": 0.7824795842613215,
      "grad_norm": 0.032178059220314026,
      "learning_rate": 0.0003343255180630376,
      "loss": 0.04,
      "step": 3162
    },
    {
      "epoch": 0.7827270477604553,
      "grad_norm": 0.034734200686216354,
      "learning_rate": 0.0003342339146696467,
      "loss": 0.0409,
      "step": 3163
    },
    {
      "epoch": 0.7829745112595892,
      "grad_norm": 0.02914332039654255,
      "learning_rate": 0.0003341422985170273,
      "loss": 0.0295,
      "step": 3164
    },
    {
      "epoch": 0.7832219747587231,
      "grad_norm": 0.04648888111114502,
      "learning_rate": 0.0003340506696190568,
      "loss": 0.0588,
      "step": 3165
    },
    {
      "epoch": 0.783469438257857,
      "grad_norm": 0.040232423692941666,
      "learning_rate": 0.0003339590279896144,
      "loss": 0.0802,
      "step": 3166
    },
    {
      "epoch": 0.7837169017569908,
      "grad_norm": 0.0264685470610857,
      "learning_rate": 0.00033386737364258167,
      "loss": 0.0507,
      "step": 3167
    },
    {
      "epoch": 0.7839643652561247,
      "grad_norm": 0.038930535316467285,
      "learning_rate": 0.00033377570659184165,
      "loss": 0.0462,
      "step": 3168
    },
    {
      "epoch": 0.7842118287552586,
      "grad_norm": 0.04239130765199661,
      "learning_rate": 0.0003336840268512796,
      "loss": 0.0682,
      "step": 3169
    },
    {
      "epoch": 0.7844592922543925,
      "grad_norm": 0.04360295832157135,
      "learning_rate": 0.0003335923344347825,
      "loss": 0.0695,
      "step": 3170
    },
    {
      "epoch": 0.7847067557535263,
      "grad_norm": 0.04325376823544502,
      "learning_rate": 0.0003335006293562394,
      "loss": 0.0715,
      "step": 3171
    },
    {
      "epoch": 0.7849542192526602,
      "grad_norm": 0.024208320304751396,
      "learning_rate": 0.0003334089116295412,
      "loss": 0.0378,
      "step": 3172
    },
    {
      "epoch": 0.7852016827517941,
      "grad_norm": 0.051129840314388275,
      "learning_rate": 0.0003333171812685808,
      "loss": 0.0652,
      "step": 3173
    },
    {
      "epoch": 0.785449146250928,
      "grad_norm": 0.07443839311599731,
      "learning_rate": 0.0003332254382872528,
      "loss": 0.0873,
      "step": 3174
    },
    {
      "epoch": 0.7856966097500618,
      "grad_norm": 0.025589635595679283,
      "learning_rate": 0.00033313368269945404,
      "loss": 0.0535,
      "step": 3175
    },
    {
      "epoch": 0.7859440732491958,
      "grad_norm": 0.05955842137336731,
      "learning_rate": 0.00033304191451908287,
      "loss": 0.0585,
      "step": 3176
    },
    {
      "epoch": 0.7861915367483296,
      "grad_norm": 0.0330023393034935,
      "learning_rate": 0.00033295013376004,
      "loss": 0.0378,
      "step": 3177
    },
    {
      "epoch": 0.7864390002474635,
      "grad_norm": 0.03125142678618431,
      "learning_rate": 0.00033285834043622755,
      "loss": 0.0587,
      "step": 3178
    },
    {
      "epoch": 0.7866864637465973,
      "grad_norm": 0.041120175272226334,
      "learning_rate": 0.00033276653456155,
      "loss": 0.0529,
      "step": 3179
    },
    {
      "epoch": 0.7869339272457313,
      "grad_norm": 0.04687928035855293,
      "learning_rate": 0.0003326747161499134,
      "loss": 0.0486,
      "step": 3180
    },
    {
      "epoch": 0.7871813907448652,
      "grad_norm": 0.0606394037604332,
      "learning_rate": 0.000332582885215226,
      "loss": 0.0642,
      "step": 3181
    },
    {
      "epoch": 0.787428854243999,
      "grad_norm": 0.03244477137923241,
      "learning_rate": 0.0003324910417713976,
      "loss": 0.0718,
      "step": 3182
    },
    {
      "epoch": 0.7876763177431328,
      "grad_norm": 0.02334127388894558,
      "learning_rate": 0.0003323991858323401,
      "loss": 0.0277,
      "step": 3183
    },
    {
      "epoch": 0.7879237812422668,
      "grad_norm": 0.05984571948647499,
      "learning_rate": 0.0003323073174119674,
      "loss": 0.0539,
      "step": 3184
    },
    {
      "epoch": 0.7881712447414007,
      "grad_norm": 0.028796257451176643,
      "learning_rate": 0.00033221543652419506,
      "loss": 0.0413,
      "step": 3185
    },
    {
      "epoch": 0.7884187082405345,
      "grad_norm": 0.03036872111260891,
      "learning_rate": 0.00033212354318294055,
      "loss": 0.0396,
      "step": 3186
    },
    {
      "epoch": 0.7886661717396684,
      "grad_norm": 0.03453581780195236,
      "learning_rate": 0.0003320316374021235,
      "loss": 0.0422,
      "step": 3187
    },
    {
      "epoch": 0.7889136352388023,
      "grad_norm": 0.02771654725074768,
      "learning_rate": 0.000331939719195665,
      "loss": 0.0372,
      "step": 3188
    },
    {
      "epoch": 0.7891610987379362,
      "grad_norm": 0.09823941439390182,
      "learning_rate": 0.00033184778857748843,
      "loss": 0.0275,
      "step": 3189
    },
    {
      "epoch": 0.78940856223707,
      "grad_norm": 0.027540354058146477,
      "learning_rate": 0.00033175584556151874,
      "loss": 0.0234,
      "step": 3190
    },
    {
      "epoch": 0.7896560257362039,
      "grad_norm": 0.04887859895825386,
      "learning_rate": 0.00033166389016168297,
      "loss": 0.0919,
      "step": 3191
    },
    {
      "epoch": 0.7899034892353378,
      "grad_norm": 0.03183165565133095,
      "learning_rate": 0.00033157192239190994,
      "loss": 0.0505,
      "step": 3192
    },
    {
      "epoch": 0.7901509527344717,
      "grad_norm": 0.04767000675201416,
      "learning_rate": 0.0003314799422661303,
      "loss": 0.0382,
      "step": 3193
    },
    {
      "epoch": 0.7903984162336055,
      "grad_norm": 0.07046545296907425,
      "learning_rate": 0.00033138794979827664,
      "loss": 0.0531,
      "step": 3194
    },
    {
      "epoch": 0.7906458797327395,
      "grad_norm": 0.027783052995800972,
      "learning_rate": 0.0003312959450022834,
      "loss": 0.0224,
      "step": 3195
    },
    {
      "epoch": 0.7908933432318733,
      "grad_norm": 0.028844358399510384,
      "learning_rate": 0.00033120392789208697,
      "loss": 0.071,
      "step": 3196
    },
    {
      "epoch": 0.7911408067310072,
      "grad_norm": 0.06082805618643761,
      "learning_rate": 0.0003311118984816255,
      "loss": 0.0769,
      "step": 3197
    },
    {
      "epoch": 0.791388270230141,
      "grad_norm": 0.034075066447257996,
      "learning_rate": 0.00033101985678483893,
      "loss": 0.0342,
      "step": 3198
    },
    {
      "epoch": 0.791635733729275,
      "grad_norm": 0.055462855845689774,
      "learning_rate": 0.00033092780281566926,
      "loss": 0.075,
      "step": 3199
    },
    {
      "epoch": 0.7918831972284088,
      "grad_norm": 0.020936530083417892,
      "learning_rate": 0.0003308357365880603,
      "loss": 0.042,
      "step": 3200
    },
    {
      "epoch": 0.7918831972284088,
      "eval_loss": 0.29363560676574707,
      "eval_runtime": 202.6201,
      "eval_samples_per_second": 4.935,
      "eval_steps_per_second": 0.311,
      "step": 3200
    },
    {
      "epoch": 0.7921306607275427,
      "grad_norm": 0.03949823975563049,
      "learning_rate": 0.00033074365811595755,
      "loss": 0.0572,
      "step": 3201
    },
    {
      "epoch": 0.7923781242266765,
      "grad_norm": 0.02811598591506481,
      "learning_rate": 0.00033065156741330856,
      "loss": 0.0504,
      "step": 3202
    },
    {
      "epoch": 0.7926255877258105,
      "grad_norm": 0.02615027315914631,
      "learning_rate": 0.0003305594644940626,
      "loss": 0.0289,
      "step": 3203
    },
    {
      "epoch": 0.7928730512249443,
      "grad_norm": 0.029491104185581207,
      "learning_rate": 0.00033046734937217093,
      "loss": 0.0255,
      "step": 3204
    },
    {
      "epoch": 0.7931205147240782,
      "grad_norm": 0.08711069822311401,
      "learning_rate": 0.00033037522206158645,
      "loss": 0.1053,
      "step": 3205
    },
    {
      "epoch": 0.793367978223212,
      "grad_norm": 0.03588009625673294,
      "learning_rate": 0.00033028308257626414,
      "loss": 0.0635,
      "step": 3206
    },
    {
      "epoch": 0.793615441722346,
      "grad_norm": 0.037068627774715424,
      "learning_rate": 0.0003301909309301606,
      "loss": 0.0451,
      "step": 3207
    },
    {
      "epoch": 0.7938629052214798,
      "grad_norm": 0.022516537457704544,
      "learning_rate": 0.00033009876713723456,
      "loss": 0.0362,
      "step": 3208
    },
    {
      "epoch": 0.7941103687206137,
      "grad_norm": 0.05169098824262619,
      "learning_rate": 0.0003300065912114463,
      "loss": 0.0693,
      "step": 3209
    },
    {
      "epoch": 0.7943578322197475,
      "grad_norm": 0.02382432110607624,
      "learning_rate": 0.0003299144031667581,
      "loss": 0.0267,
      "step": 3210
    },
    {
      "epoch": 0.7946052957188815,
      "grad_norm": 0.05215682089328766,
      "learning_rate": 0.000329822203017134,
      "loss": 0.0502,
      "step": 3211
    },
    {
      "epoch": 0.7948527592180153,
      "grad_norm": 0.022023271769285202,
      "learning_rate": 0.00032972999077653987,
      "loss": 0.0134,
      "step": 3212
    },
    {
      "epoch": 0.7951002227171492,
      "grad_norm": 0.05606259033083916,
      "learning_rate": 0.0003296377664589435,
      "loss": 0.0916,
      "step": 3213
    },
    {
      "epoch": 0.795347686216283,
      "grad_norm": 0.043250277638435364,
      "learning_rate": 0.0003295455300783145,
      "loss": 0.0391,
      "step": 3214
    },
    {
      "epoch": 0.795595149715417,
      "grad_norm": 0.05078098922967911,
      "learning_rate": 0.00032945328164862413,
      "loss": 0.1278,
      "step": 3215
    },
    {
      "epoch": 0.7958426132145509,
      "grad_norm": 0.0388418510556221,
      "learning_rate": 0.00032936102118384577,
      "loss": 0.0411,
      "step": 3216
    },
    {
      "epoch": 0.7960900767136847,
      "grad_norm": 0.04103633761405945,
      "learning_rate": 0.00032926874869795424,
      "loss": 0.0739,
      "step": 3217
    },
    {
      "epoch": 0.7963375402128187,
      "grad_norm": 0.03200320154428482,
      "learning_rate": 0.00032917646420492665,
      "loss": 0.0556,
      "step": 3218
    },
    {
      "epoch": 0.7965850037119525,
      "grad_norm": 0.04015055298805237,
      "learning_rate": 0.0003290841677187415,
      "loss": 0.0397,
      "step": 3219
    },
    {
      "epoch": 0.7968324672110864,
      "grad_norm": 0.054596658796072006,
      "learning_rate": 0.00032899185925337943,
      "loss": 0.0721,
      "step": 3220
    },
    {
      "epoch": 0.7970799307102202,
      "grad_norm": 0.03455119580030441,
      "learning_rate": 0.0003288995388228226,
      "loss": 0.0593,
      "step": 3221
    },
    {
      "epoch": 0.7973273942093542,
      "grad_norm": 0.034863367676734924,
      "learning_rate": 0.0003288072064410552,
      "loss": 0.0466,
      "step": 3222
    },
    {
      "epoch": 0.797574857708488,
      "grad_norm": 0.021588223055005074,
      "learning_rate": 0.0003287148621220632,
      "loss": 0.0319,
      "step": 3223
    },
    {
      "epoch": 0.7978223212076219,
      "grad_norm": 0.0610320083796978,
      "learning_rate": 0.0003286225058798343,
      "loss": 0.0645,
      "step": 3224
    },
    {
      "epoch": 0.7980697847067557,
      "grad_norm": 0.027451248839497566,
      "learning_rate": 0.00032853013772835807,
      "loss": 0.0479,
      "step": 3225
    },
    {
      "epoch": 0.7983172482058897,
      "grad_norm": 0.037193890661001205,
      "learning_rate": 0.0003284377576816259,
      "loss": 0.0516,
      "step": 3226
    },
    {
      "epoch": 0.7985647117050235,
      "grad_norm": 0.045318733900785446,
      "learning_rate": 0.00032834536575363074,
      "loss": 0.0722,
      "step": 3227
    },
    {
      "epoch": 0.7988121752041574,
      "grad_norm": 0.06936261802911758,
      "learning_rate": 0.00032825296195836785,
      "loss": 0.0971,
      "step": 3228
    },
    {
      "epoch": 0.7990596387032912,
      "grad_norm": 0.028072508051991463,
      "learning_rate": 0.0003281605463098337,
      "loss": 0.0253,
      "step": 3229
    },
    {
      "epoch": 0.7993071022024252,
      "grad_norm": 0.025834975764155388,
      "learning_rate": 0.00032806811882202713,
      "loss": 0.0169,
      "step": 3230
    },
    {
      "epoch": 0.799554565701559,
      "grad_norm": 0.04529469832777977,
      "learning_rate": 0.00032797567950894823,
      "loss": 0.0713,
      "step": 3231
    },
    {
      "epoch": 0.7998020292006929,
      "grad_norm": 0.052537817507982254,
      "learning_rate": 0.00032788322838459916,
      "loss": 0.0615,
      "step": 3232
    },
    {
      "epoch": 0.8000494926998267,
      "grad_norm": 0.04472094029188156,
      "learning_rate": 0.00032779076546298396,
      "loss": 0.0347,
      "step": 3233
    },
    {
      "epoch": 0.8002969561989607,
      "grad_norm": 0.02815347909927368,
      "learning_rate": 0.0003276982907581083,
      "loss": 0.0241,
      "step": 3234
    },
    {
      "epoch": 0.8005444196980945,
      "grad_norm": 0.04661433771252632,
      "learning_rate": 0.0003276058042839796,
      "loss": 0.057,
      "step": 3235
    },
    {
      "epoch": 0.8007918831972284,
      "grad_norm": 0.03225890174508095,
      "learning_rate": 0.00032751330605460726,
      "loss": 0.0332,
      "step": 3236
    },
    {
      "epoch": 0.8010393466963622,
      "grad_norm": 0.0595218800008297,
      "learning_rate": 0.00032742079608400214,
      "loss": 0.0752,
      "step": 3237
    },
    {
      "epoch": 0.8012868101954962,
      "grad_norm": 0.03620988130569458,
      "learning_rate": 0.0003273282743861773,
      "loss": 0.0406,
      "step": 3238
    },
    {
      "epoch": 0.80153427369463,
      "grad_norm": 0.03816146031022072,
      "learning_rate": 0.00032723574097514717,
      "loss": 0.053,
      "step": 3239
    },
    {
      "epoch": 0.8017817371937639,
      "grad_norm": 0.04918317124247551,
      "learning_rate": 0.0003271431958649283,
      "loss": 0.0616,
      "step": 3240
    },
    {
      "epoch": 0.8020292006928978,
      "grad_norm": 0.02773427963256836,
      "learning_rate": 0.00032705063906953867,
      "loss": 0.0504,
      "step": 3241
    },
    {
      "epoch": 0.8022766641920317,
      "grad_norm": 0.07128343731164932,
      "learning_rate": 0.0003269580706029983,
      "loss": 0.0371,
      "step": 3242
    },
    {
      "epoch": 0.8025241276911655,
      "grad_norm": 0.05829593166708946,
      "learning_rate": 0.0003268654904793289,
      "loss": 0.0923,
      "step": 3243
    },
    {
      "epoch": 0.8027715911902994,
      "grad_norm": 0.03362617641687393,
      "learning_rate": 0.0003267728987125539,
      "loss": 0.0411,
      "step": 3244
    },
    {
      "epoch": 0.8030190546894334,
      "grad_norm": 0.04260895028710365,
      "learning_rate": 0.00032668029531669854,
      "loss": 0.0542,
      "step": 3245
    },
    {
      "epoch": 0.8032665181885672,
      "grad_norm": 0.04403899982571602,
      "learning_rate": 0.0003265876803057897,
      "loss": 0.0732,
      "step": 3246
    },
    {
      "epoch": 0.803513981687701,
      "grad_norm": 0.05818270519375801,
      "learning_rate": 0.00032649505369385627,
      "loss": 0.0588,
      "step": 3247
    },
    {
      "epoch": 0.8037614451868349,
      "grad_norm": 0.09498108923435211,
      "learning_rate": 0.00032640241549492877,
      "loss": 0.0555,
      "step": 3248
    },
    {
      "epoch": 0.8040089086859689,
      "grad_norm": 0.044862471520900726,
      "learning_rate": 0.0003263097657230394,
      "loss": 0.0607,
      "step": 3249
    },
    {
      "epoch": 0.8042563721851027,
      "grad_norm": 0.03459244593977928,
      "learning_rate": 0.0003262171043922221,
      "loss": 0.047,
      "step": 3250
    },
    {
      "epoch": 0.8045038356842366,
      "grad_norm": 0.04510566592216492,
      "learning_rate": 0.0003261244315165127,
      "loss": 0.0272,
      "step": 3251
    },
    {
      "epoch": 0.8047512991833704,
      "grad_norm": 0.04959813132882118,
      "learning_rate": 0.0003260317471099487,
      "loss": 0.1142,
      "step": 3252
    },
    {
      "epoch": 0.8049987626825044,
      "grad_norm": 0.044967297464609146,
      "learning_rate": 0.0003259390511865693,
      "loss": 0.0414,
      "step": 3253
    },
    {
      "epoch": 0.8052462261816382,
      "grad_norm": 0.09930045157670975,
      "learning_rate": 0.0003258463437604156,
      "loss": 0.2032,
      "step": 3254
    },
    {
      "epoch": 0.8054936896807721,
      "grad_norm": 0.0426117442548275,
      "learning_rate": 0.0003257536248455303,
      "loss": 0.0534,
      "step": 3255
    },
    {
      "epoch": 0.8057411531799059,
      "grad_norm": 0.039853304624557495,
      "learning_rate": 0.00032566089445595795,
      "loss": 0.0743,
      "step": 3256
    },
    {
      "epoch": 0.8059886166790399,
      "grad_norm": 0.043614957481622696,
      "learning_rate": 0.00032556815260574465,
      "loss": 0.066,
      "step": 3257
    },
    {
      "epoch": 0.8062360801781737,
      "grad_norm": 0.044844865798950195,
      "learning_rate": 0.0003254753993089384,
      "loss": 0.0692,
      "step": 3258
    },
    {
      "epoch": 0.8064835436773076,
      "grad_norm": 0.029220325872302055,
      "learning_rate": 0.00032538263457958895,
      "loss": 0.0229,
      "step": 3259
    },
    {
      "epoch": 0.8067310071764415,
      "grad_norm": 0.07006491720676422,
      "learning_rate": 0.00032528985843174765,
      "loss": 0.0419,
      "step": 3260
    },
    {
      "epoch": 0.8069784706755754,
      "grad_norm": 0.05736098438501358,
      "learning_rate": 0.0003251970708794676,
      "loss": 0.0791,
      "step": 3261
    },
    {
      "epoch": 0.8072259341747092,
      "grad_norm": 0.03624983876943588,
      "learning_rate": 0.0003251042719368038,
      "loss": 0.0444,
      "step": 3262
    },
    {
      "epoch": 0.8074733976738431,
      "grad_norm": 0.05231252312660217,
      "learning_rate": 0.0003250114616178129,
      "loss": 0.0628,
      "step": 3263
    },
    {
      "epoch": 0.807720861172977,
      "grad_norm": 0.054359059780836105,
      "learning_rate": 0.0003249186399365531,
      "loss": 0.0454,
      "step": 3264
    },
    {
      "epoch": 0.8079683246721109,
      "grad_norm": 0.04920567572116852,
      "learning_rate": 0.0003248258069070846,
      "loss": 0.0634,
      "step": 3265
    },
    {
      "epoch": 0.8082157881712447,
      "grad_norm": 0.055424220860004425,
      "learning_rate": 0.00032473296254346886,
      "loss": 0.0805,
      "step": 3266
    },
    {
      "epoch": 0.8084632516703786,
      "grad_norm": 0.05413173511624336,
      "learning_rate": 0.00032464010685976974,
      "loss": 0.0796,
      "step": 3267
    },
    {
      "epoch": 0.8087107151695125,
      "grad_norm": 0.040724240243434906,
      "learning_rate": 0.0003245472398700522,
      "loss": 0.0584,
      "step": 3268
    },
    {
      "epoch": 0.8089581786686464,
      "grad_norm": 0.044512297958135605,
      "learning_rate": 0.00032445436158838327,
      "loss": 0.0751,
      "step": 3269
    },
    {
      "epoch": 0.8092056421677802,
      "grad_norm": 0.04523032531142235,
      "learning_rate": 0.00032436147202883145,
      "loss": 0.0714,
      "step": 3270
    },
    {
      "epoch": 0.8094531056669141,
      "grad_norm": 0.03172460198402405,
      "learning_rate": 0.00032426857120546716,
      "loss": 0.0415,
      "step": 3271
    },
    {
      "epoch": 0.809700569166048,
      "grad_norm": 0.06426667422056198,
      "learning_rate": 0.00032417565913236247,
      "loss": 0.0745,
      "step": 3272
    },
    {
      "epoch": 0.8099480326651819,
      "grad_norm": 0.04240565001964569,
      "learning_rate": 0.00032408273582359103,
      "loss": 0.0614,
      "step": 3273
    },
    {
      "epoch": 0.8101954961643157,
      "grad_norm": 0.048155736178159714,
      "learning_rate": 0.0003239898012932284,
      "loss": 0.0514,
      "step": 3274
    },
    {
      "epoch": 0.8104429596634496,
      "grad_norm": 0.04075194522738457,
      "learning_rate": 0.00032389685555535167,
      "loss": 0.0699,
      "step": 3275
    },
    {
      "epoch": 0.8106904231625836,
      "grad_norm": 0.06356235593557358,
      "learning_rate": 0.0003238038986240396,
      "loss": 0.0653,
      "step": 3276
    },
    {
      "epoch": 0.8109378866617174,
      "grad_norm": 0.04307625815272331,
      "learning_rate": 0.0003237109305133728,
      "loss": 0.0781,
      "step": 3277
    },
    {
      "epoch": 0.8111853501608512,
      "grad_norm": 0.04585617035627365,
      "learning_rate": 0.00032361795123743355,
      "loss": 0.066,
      "step": 3278
    },
    {
      "epoch": 0.8114328136599851,
      "grad_norm": 0.05327466130256653,
      "learning_rate": 0.00032352496081030567,
      "loss": 0.0576,
      "step": 3279
    },
    {
      "epoch": 0.8116802771591191,
      "grad_norm": 0.022113854065537453,
      "learning_rate": 0.0003234319592460748,
      "loss": 0.0247,
      "step": 3280
    },
    {
      "epoch": 0.8119277406582529,
      "grad_norm": 0.04576323181390762,
      "learning_rate": 0.0003233389465588283,
      "loss": 0.093,
      "step": 3281
    },
    {
      "epoch": 0.8121752041573868,
      "grad_norm": 0.04501279816031456,
      "learning_rate": 0.0003232459227626551,
      "loss": 0.0595,
      "step": 3282
    },
    {
      "epoch": 0.8124226676565207,
      "grad_norm": 0.03620445355772972,
      "learning_rate": 0.0003231528878716459,
      "loss": 0.0461,
      "step": 3283
    },
    {
      "epoch": 0.8126701311556546,
      "grad_norm": 0.033111706376075745,
      "learning_rate": 0.0003230598418998929,
      "loss": 0.0385,
      "step": 3284
    },
    {
      "epoch": 0.8129175946547884,
      "grad_norm": 0.03347893804311752,
      "learning_rate": 0.00032296678486149036,
      "loss": 0.0463,
      "step": 3285
    },
    {
      "epoch": 0.8131650581539223,
      "grad_norm": 0.019520677626132965,
      "learning_rate": 0.00032287371677053377,
      "loss": 0.0302,
      "step": 3286
    },
    {
      "epoch": 0.8134125216530562,
      "grad_norm": 0.03377610072493553,
      "learning_rate": 0.0003227806376411206,
      "loss": 0.0864,
      "step": 3287
    },
    {
      "epoch": 0.8136599851521901,
      "grad_norm": 0.05603278800845146,
      "learning_rate": 0.00032268754748734994,
      "loss": 0.0403,
      "step": 3288
    },
    {
      "epoch": 0.8139074486513239,
      "grad_norm": 0.03001122549176216,
      "learning_rate": 0.0003225944463233226,
      "loss": 0.0238,
      "step": 3289
    },
    {
      "epoch": 0.8141549121504578,
      "grad_norm": 0.032866470515728,
      "learning_rate": 0.0003225013341631406,
      "loss": 0.0377,
      "step": 3290
    },
    {
      "epoch": 0.8144023756495917,
      "grad_norm": 0.03352874889969826,
      "learning_rate": 0.0003224082110209083,
      "loss": 0.0524,
      "step": 3291
    },
    {
      "epoch": 0.8146498391487256,
      "grad_norm": 0.028211472555994987,
      "learning_rate": 0.0003223150769107313,
      "loss": 0.0389,
      "step": 3292
    },
    {
      "epoch": 0.8148973026478594,
      "grad_norm": 0.05779856815934181,
      "learning_rate": 0.0003222219318467171,
      "loss": 0.0945,
      "step": 3293
    },
    {
      "epoch": 0.8151447661469933,
      "grad_norm": 0.03998423367738724,
      "learning_rate": 0.00032212877584297453,
      "loss": 0.0986,
      "step": 3294
    },
    {
      "epoch": 0.8153922296461272,
      "grad_norm": 0.04137613996863365,
      "learning_rate": 0.0003220356089136144,
      "loss": 0.08,
      "step": 3295
    },
    {
      "epoch": 0.8156396931452611,
      "grad_norm": 0.03250770643353462,
      "learning_rate": 0.0003219424310727491,
      "loss": 0.0455,
      "step": 3296
    },
    {
      "epoch": 0.8158871566443949,
      "grad_norm": 0.07215747237205505,
      "learning_rate": 0.0003218492423344925,
      "loss": 0.0554,
      "step": 3297
    },
    {
      "epoch": 0.8161346201435288,
      "grad_norm": 0.04181814566254616,
      "learning_rate": 0.0003217560427129603,
      "loss": 0.0971,
      "step": 3298
    },
    {
      "epoch": 0.8163820836426627,
      "grad_norm": 0.03581192344427109,
      "learning_rate": 0.0003216628322222699,
      "loss": 0.0407,
      "step": 3299
    },
    {
      "epoch": 0.8166295471417966,
      "grad_norm": 0.054700177162885666,
      "learning_rate": 0.00032156961087654017,
      "loss": 0.0448,
      "step": 3300
    },
    {
      "epoch": 0.8168770106409304,
      "grad_norm": 0.048003971576690674,
      "learning_rate": 0.0003214763786898917,
      "loss": 0.0363,
      "step": 3301
    },
    {
      "epoch": 0.8171244741400644,
      "grad_norm": 0.04176715388894081,
      "learning_rate": 0.0003213831356764466,
      "loss": 0.0843,
      "step": 3302
    },
    {
      "epoch": 0.8173719376391982,
      "grad_norm": 0.05790676176548004,
      "learning_rate": 0.0003212898818503289,
      "loss": 0.0564,
      "step": 3303
    },
    {
      "epoch": 0.8176194011383321,
      "grad_norm": 0.029545988887548447,
      "learning_rate": 0.0003211966172256642,
      "loss": 0.029,
      "step": 3304
    },
    {
      "epoch": 0.8178668646374659,
      "grad_norm": 0.0513620600104332,
      "learning_rate": 0.0003211033418165794,
      "loss": 0.073,
      "step": 3305
    },
    {
      "epoch": 0.8181143281365999,
      "grad_norm": 0.03917355090379715,
      "learning_rate": 0.00032101005563720333,
      "loss": 0.0558,
      "step": 3306
    },
    {
      "epoch": 0.8183617916357337,
      "grad_norm": 0.031057346612215042,
      "learning_rate": 0.00032091675870166646,
      "loss": 0.0316,
      "step": 3307
    },
    {
      "epoch": 0.8186092551348676,
      "grad_norm": 0.05833421275019646,
      "learning_rate": 0.0003208234510241009,
      "loss": 0.1098,
      "step": 3308
    },
    {
      "epoch": 0.8188567186340014,
      "grad_norm": 0.05156641826033592,
      "learning_rate": 0.0003207301326186402,
      "loss": 0.0643,
      "step": 3309
    },
    {
      "epoch": 0.8191041821331354,
      "grad_norm": 0.04032192379236221,
      "learning_rate": 0.0003206368034994197,
      "loss": 0.058,
      "step": 3310
    },
    {
      "epoch": 0.8193516456322693,
      "grad_norm": 0.030873777344822884,
      "learning_rate": 0.0003205434636805763,
      "loss": 0.049,
      "step": 3311
    },
    {
      "epoch": 0.8195991091314031,
      "grad_norm": 0.025054484605789185,
      "learning_rate": 0.0003204501131762485,
      "loss": 0.0251,
      "step": 3312
    },
    {
      "epoch": 0.819846572630537,
      "grad_norm": 0.04334467276930809,
      "learning_rate": 0.00032035675200057655,
      "loss": 0.0272,
      "step": 3313
    },
    {
      "epoch": 0.8200940361296709,
      "grad_norm": 0.017242781817913055,
      "learning_rate": 0.00032026338016770216,
      "loss": 0.0298,
      "step": 3314
    },
    {
      "epoch": 0.8203414996288048,
      "grad_norm": 0.04659800976514816,
      "learning_rate": 0.0003201699976917687,
      "loss": 0.0625,
      "step": 3315
    },
    {
      "epoch": 0.8205889631279386,
      "grad_norm": 0.05084367096424103,
      "learning_rate": 0.00032007660458692114,
      "loss": 0.0646,
      "step": 3316
    },
    {
      "epoch": 0.8208364266270725,
      "grad_norm": 0.03407611325383186,
      "learning_rate": 0.0003199832008673061,
      "loss": 0.0587,
      "step": 3317
    },
    {
      "epoch": 0.8210838901262064,
      "grad_norm": 0.04884278029203415,
      "learning_rate": 0.0003198897865470719,
      "loss": 0.0916,
      "step": 3318
    },
    {
      "epoch": 0.8213313536253403,
      "grad_norm": 0.03338263928890228,
      "learning_rate": 0.00031979636164036816,
      "loss": 0.0504,
      "step": 3319
    },
    {
      "epoch": 0.8215788171244741,
      "grad_norm": 0.030556529760360718,
      "learning_rate": 0.00031970292616134656,
      "loss": 0.0459,
      "step": 3320
    },
    {
      "epoch": 0.821826280623608,
      "grad_norm": 0.049159370362758636,
      "learning_rate": 0.0003196094801241599,
      "loss": 0.0719,
      "step": 3321
    },
    {
      "epoch": 0.8220737441227419,
      "grad_norm": 0.10904859751462936,
      "learning_rate": 0.0003195160235429629,
      "loss": 0.0708,
      "step": 3322
    },
    {
      "epoch": 0.8223212076218758,
      "grad_norm": 0.09470497816801071,
      "learning_rate": 0.00031942255643191176,
      "loss": 0.1499,
      "step": 3323
    },
    {
      "epoch": 0.8225686711210096,
      "grad_norm": 0.029939664527773857,
      "learning_rate": 0.0003193290788051643,
      "loss": 0.0508,
      "step": 3324
    },
    {
      "epoch": 0.8228161346201436,
      "grad_norm": 0.028726642951369286,
      "learning_rate": 0.00031923559067687993,
      "loss": 0.0518,
      "step": 3325
    },
    {
      "epoch": 0.8230635981192774,
      "grad_norm": 0.03715290501713753,
      "learning_rate": 0.00031914209206121967,
      "loss": 0.0652,
      "step": 3326
    },
    {
      "epoch": 0.8233110616184113,
      "grad_norm": 0.03019072860479355,
      "learning_rate": 0.00031904858297234607,
      "loss": 0.0211,
      "step": 3327
    },
    {
      "epoch": 0.8235585251175451,
      "grad_norm": 0.04390506073832512,
      "learning_rate": 0.00031895506342442337,
      "loss": 0.067,
      "step": 3328
    },
    {
      "epoch": 0.8238059886166791,
      "grad_norm": 0.025328505784273148,
      "learning_rate": 0.0003188615334316172,
      "loss": 0.031,
      "step": 3329
    },
    {
      "epoch": 0.8240534521158129,
      "grad_norm": 0.04992508515715599,
      "learning_rate": 0.000318767993008095,
      "loss": 0.0548,
      "step": 3330
    },
    {
      "epoch": 0.8243009156149468,
      "grad_norm": 0.05083809792995453,
      "learning_rate": 0.00031867444216802574,
      "loss": 0.0496,
      "step": 3331
    },
    {
      "epoch": 0.8245483791140806,
      "grad_norm": 0.040816787630319595,
      "learning_rate": 0.00031858088092557977,
      "loss": 0.0593,
      "step": 3332
    },
    {
      "epoch": 0.8247958426132146,
      "grad_norm": 0.04989750310778618,
      "learning_rate": 0.0003184873092949293,
      "loss": 0.0637,
      "step": 3333
    },
    {
      "epoch": 0.8250433061123484,
      "grad_norm": 0.05539499223232269,
      "learning_rate": 0.000318393727290248,
      "loss": 0.03,
      "step": 3334
    },
    {
      "epoch": 0.8252907696114823,
      "grad_norm": 0.07851479947566986,
      "learning_rate": 0.0003183001349257107,
      "loss": 0.07,
      "step": 3335
    },
    {
      "epoch": 0.8255382331106161,
      "grad_norm": 0.03683549165725708,
      "learning_rate": 0.00031820653221549484,
      "loss": 0.0384,
      "step": 3336
    },
    {
      "epoch": 0.8257856966097501,
      "grad_norm": 0.02932286262512207,
      "learning_rate": 0.0003181129191737783,
      "loss": 0.0266,
      "step": 3337
    },
    {
      "epoch": 0.826033160108884,
      "grad_norm": 0.04858605936169624,
      "learning_rate": 0.00031801929581474114,
      "loss": 0.096,
      "step": 3338
    },
    {
      "epoch": 0.8262806236080178,
      "grad_norm": 0.04225529730319977,
      "learning_rate": 0.00031792566215256486,
      "loss": 0.0622,
      "step": 3339
    },
    {
      "epoch": 0.8265280871071516,
      "grad_norm": 0.026168217882514,
      "learning_rate": 0.00031783201820143243,
      "loss": 0.0558,
      "step": 3340
    },
    {
      "epoch": 0.8267755506062856,
      "grad_norm": 0.08704346418380737,
      "learning_rate": 0.0003177383639755285,
      "loss": 0.0777,
      "step": 3341
    },
    {
      "epoch": 0.8270230141054195,
      "grad_norm": 0.04090065509080887,
      "learning_rate": 0.0003176446994890393,
      "loss": 0.0345,
      "step": 3342
    },
    {
      "epoch": 0.8272704776045533,
      "grad_norm": 0.036404211074113846,
      "learning_rate": 0.00031755102475615243,
      "loss": 0.0491,
      "step": 3343
    },
    {
      "epoch": 0.8275179411036873,
      "grad_norm": 0.03336641937494278,
      "learning_rate": 0.00031745733979105726,
      "loss": 0.0458,
      "step": 3344
    },
    {
      "epoch": 0.8277654046028211,
      "grad_norm": 0.02581116557121277,
      "learning_rate": 0.0003173636446079444,
      "loss": 0.0212,
      "step": 3345
    },
    {
      "epoch": 0.828012868101955,
      "grad_norm": 0.04734337702393532,
      "learning_rate": 0.00031726993922100654,
      "loss": 0.1046,
      "step": 3346
    },
    {
      "epoch": 0.8282603316010888,
      "grad_norm": 0.03577285632491112,
      "learning_rate": 0.0003171762236444373,
      "loss": 0.0384,
      "step": 3347
    },
    {
      "epoch": 0.8285077951002228,
      "grad_norm": 0.03400909900665283,
      "learning_rate": 0.00031708249789243225,
      "loss": 0.0655,
      "step": 3348
    },
    {
      "epoch": 0.8287552585993566,
      "grad_norm": 0.038599953055381775,
      "learning_rate": 0.0003169887619791884,
      "loss": 0.0508,
      "step": 3349
    },
    {
      "epoch": 0.8290027220984905,
      "grad_norm": 0.03769144043326378,
      "learning_rate": 0.00031689501591890425,
      "loss": 0.0315,
      "step": 3350
    },
    {
      "epoch": 0.8292501855976243,
      "grad_norm": 0.0594886839389801,
      "learning_rate": 0.0003168012597257798,
      "loss": 0.0834,
      "step": 3351
    },
    {
      "epoch": 0.8294976490967583,
      "grad_norm": 0.09666852653026581,
      "learning_rate": 0.00031670749341401684,
      "loss": 0.0433,
      "step": 3352
    },
    {
      "epoch": 0.8297451125958921,
      "grad_norm": 0.041769903153181076,
      "learning_rate": 0.00031661371699781825,
      "loss": 0.0508,
      "step": 3353
    },
    {
      "epoch": 0.829992576095026,
      "grad_norm": 0.050141606479883194,
      "learning_rate": 0.00031651993049138893,
      "loss": 0.0442,
      "step": 3354
    },
    {
      "epoch": 0.8302400395941598,
      "grad_norm": 0.02898954600095749,
      "learning_rate": 0.00031642613390893496,
      "loss": 0.0405,
      "step": 3355
    },
    {
      "epoch": 0.8304875030932938,
      "grad_norm": 0.036123041063547134,
      "learning_rate": 0.00031633232726466416,
      "loss": 0.0403,
      "step": 3356
    },
    {
      "epoch": 0.8307349665924276,
      "grad_norm": 0.04559781029820442,
      "learning_rate": 0.00031623851057278565,
      "loss": 0.0341,
      "step": 3357
    },
    {
      "epoch": 0.8309824300915615,
      "grad_norm": 0.053697310388088226,
      "learning_rate": 0.0003161446838475102,
      "loss": 0.0404,
      "step": 3358
    },
    {
      "epoch": 0.8312298935906953,
      "grad_norm": 0.023877134546637535,
      "learning_rate": 0.00031605084710305026,
      "loss": 0.0215,
      "step": 3359
    },
    {
      "epoch": 0.8314773570898293,
      "grad_norm": 0.058046381920576096,
      "learning_rate": 0.0003159570003536195,
      "loss": 0.0478,
      "step": 3360
    },
    {
      "epoch": 0.8317248205889631,
      "grad_norm": 0.0285743810236454,
      "learning_rate": 0.00031586314361343315,
      "loss": 0.046,
      "step": 3361
    },
    {
      "epoch": 0.831972284088097,
      "grad_norm": 0.036439284682273865,
      "learning_rate": 0.0003157692768967082,
      "loss": 0.0759,
      "step": 3362
    },
    {
      "epoch": 0.8322197475872308,
      "grad_norm": 0.07193590700626373,
      "learning_rate": 0.00031567540021766306,
      "loss": 0.0965,
      "step": 3363
    },
    {
      "epoch": 0.8324672110863648,
      "grad_norm": 0.0563240647315979,
      "learning_rate": 0.0003155815135905174,
      "loss": 0.0409,
      "step": 3364
    },
    {
      "epoch": 0.8327146745854986,
      "grad_norm": 0.024185029789805412,
      "learning_rate": 0.00031548761702949264,
      "loss": 0.0316,
      "step": 3365
    },
    {
      "epoch": 0.8329621380846325,
      "grad_norm": 0.055772315710783005,
      "learning_rate": 0.0003153937105488117,
      "loss": 0.0736,
      "step": 3366
    },
    {
      "epoch": 0.8332096015837664,
      "grad_norm": 0.04130030795931816,
      "learning_rate": 0.00031529979416269893,
      "loss": 0.0537,
      "step": 3367
    },
    {
      "epoch": 0.8334570650829003,
      "grad_norm": 0.04836469516158104,
      "learning_rate": 0.0003152058678853802,
      "loss": 0.0822,
      "step": 3368
    },
    {
      "epoch": 0.8337045285820341,
      "grad_norm": 0.03648659586906433,
      "learning_rate": 0.0003151119317310829,
      "loss": 0.0524,
      "step": 3369
    },
    {
      "epoch": 0.833951992081168,
      "grad_norm": 0.045853592455387115,
      "learning_rate": 0.0003150179857140357,
      "loss": 0.0744,
      "step": 3370
    },
    {
      "epoch": 0.834199455580302,
      "grad_norm": 0.02511308528482914,
      "learning_rate": 0.0003149240298484692,
      "loss": 0.0371,
      "step": 3371
    },
    {
      "epoch": 0.8344469190794358,
      "grad_norm": 0.04353027418255806,
      "learning_rate": 0.0003148300641486151,
      "loss": 0.0717,
      "step": 3372
    },
    {
      "epoch": 0.8346943825785696,
      "grad_norm": 0.047293227165937424,
      "learning_rate": 0.00031473608862870694,
      "loss": 0.0933,
      "step": 3373
    },
    {
      "epoch": 0.8349418460777035,
      "grad_norm": 0.04433967173099518,
      "learning_rate": 0.0003146421033029793,
      "loss": 0.0676,
      "step": 3374
    },
    {
      "epoch": 0.8351893095768375,
      "grad_norm": 0.037607163190841675,
      "learning_rate": 0.00031454810818566865,
      "loss": 0.0435,
      "step": 3375
    },
    {
      "epoch": 0.8354367730759713,
      "grad_norm": 0.03444785624742508,
      "learning_rate": 0.00031445410329101266,
      "loss": 0.0456,
      "step": 3376
    },
    {
      "epoch": 0.8356842365751052,
      "grad_norm": 0.0347161740064621,
      "learning_rate": 0.00031436008863325077,
      "loss": 0.0345,
      "step": 3377
    },
    {
      "epoch": 0.835931700074239,
      "grad_norm": 0.08687067031860352,
      "learning_rate": 0.00031426606422662364,
      "loss": 0.0519,
      "step": 3378
    },
    {
      "epoch": 0.836179163573373,
      "grad_norm": 0.03910711407661438,
      "learning_rate": 0.00031417203008537347,
      "loss": 0.0462,
      "step": 3379
    },
    {
      "epoch": 0.8364266270725068,
      "grad_norm": 0.0386650487780571,
      "learning_rate": 0.00031407798622374396,
      "loss": 0.0357,
      "step": 3380
    },
    {
      "epoch": 0.8366740905716407,
      "grad_norm": 0.02656557783484459,
      "learning_rate": 0.0003139839326559804,
      "loss": 0.0193,
      "step": 3381
    },
    {
      "epoch": 0.8369215540707745,
      "grad_norm": 0.060669295489788055,
      "learning_rate": 0.0003138898693963293,
      "loss": 0.0622,
      "step": 3382
    },
    {
      "epoch": 0.8371690175699085,
      "grad_norm": 0.03554720804095268,
      "learning_rate": 0.00031379579645903887,
      "loss": 0.021,
      "step": 3383
    },
    {
      "epoch": 0.8374164810690423,
      "grad_norm": 0.058810438960790634,
      "learning_rate": 0.00031370171385835865,
      "loss": 0.0496,
      "step": 3384
    },
    {
      "epoch": 0.8376639445681762,
      "grad_norm": 0.05456243455410004,
      "learning_rate": 0.00031360762160853974,
      "loss": 0.0717,
      "step": 3385
    },
    {
      "epoch": 0.8379114080673101,
      "grad_norm": 0.036145344376564026,
      "learning_rate": 0.00031351351972383457,
      "loss": 0.0655,
      "step": 3386
    },
    {
      "epoch": 0.838158871566444,
      "grad_norm": 0.07132502645254135,
      "learning_rate": 0.00031341940821849726,
      "loss": 0.096,
      "step": 3387
    },
    {
      "epoch": 0.8384063350655778,
      "grad_norm": 0.04594054073095322,
      "learning_rate": 0.000313325287106783,
      "loss": 0.0677,
      "step": 3388
    },
    {
      "epoch": 0.8386537985647117,
      "grad_norm": 0.03431184962391853,
      "learning_rate": 0.0003132311564029488,
      "loss": 0.0369,
      "step": 3389
    },
    {
      "epoch": 0.8389012620638456,
      "grad_norm": 0.0625060498714447,
      "learning_rate": 0.000313137016121253,
      "loss": 0.0583,
      "step": 3390
    },
    {
      "epoch": 0.8391487255629795,
      "grad_norm": 0.03280968219041824,
      "learning_rate": 0.0003130428662759554,
      "loss": 0.0371,
      "step": 3391
    },
    {
      "epoch": 0.8393961890621133,
      "grad_norm": 0.019509781152009964,
      "learning_rate": 0.00031294870688131717,
      "loss": 0.0252,
      "step": 3392
    },
    {
      "epoch": 0.8396436525612472,
      "grad_norm": 0.069908007979393,
      "learning_rate": 0.00031285453795160107,
      "loss": 0.0516,
      "step": 3393
    },
    {
      "epoch": 0.8398911160603811,
      "grad_norm": 0.03387478366494179,
      "learning_rate": 0.0003127603595010711,
      "loss": 0.0458,
      "step": 3394
    },
    {
      "epoch": 0.840138579559515,
      "grad_norm": 0.04064776003360748,
      "learning_rate": 0.0003126661715439929,
      "loss": 0.064,
      "step": 3395
    },
    {
      "epoch": 0.8403860430586488,
      "grad_norm": 0.0503527894616127,
      "learning_rate": 0.0003125719740946335,
      "loss": 0.0414,
      "step": 3396
    },
    {
      "epoch": 0.8406335065577827,
      "grad_norm": 0.0417158305644989,
      "learning_rate": 0.00031247776716726133,
      "loss": 0.0514,
      "step": 3397
    },
    {
      "epoch": 0.8408809700569166,
      "grad_norm": 0.028270503506064415,
      "learning_rate": 0.0003123835507761462,
      "loss": 0.0413,
      "step": 3398
    },
    {
      "epoch": 0.8411284335560505,
      "grad_norm": 0.05661873146891594,
      "learning_rate": 0.0003122893249355595,
      "loss": 0.0406,
      "step": 3399
    },
    {
      "epoch": 0.8413758970551843,
      "grad_norm": 0.038923412561416626,
      "learning_rate": 0.00031219508965977394,
      "loss": 0.0384,
      "step": 3400
    },
    {
      "epoch": 0.8413758970551843,
      "eval_loss": 0.29278817772865295,
      "eval_runtime": 202.5032,
      "eval_samples_per_second": 4.938,
      "eval_steps_per_second": 0.311,
      "step": 3400
    },
    {
      "epoch": 0.8416233605543182,
      "grad_norm": 0.03532251715660095,
      "learning_rate": 0.00031210084496306376,
      "loss": 0.0573,
      "step": 3401
    },
    {
      "epoch": 0.8418708240534521,
      "grad_norm": 0.19873273372650146,
      "learning_rate": 0.0003120065908597044,
      "loss": 0.079,
      "step": 3402
    },
    {
      "epoch": 0.842118287552586,
      "grad_norm": 0.03970174863934517,
      "learning_rate": 0.000311912327363973,
      "loss": 0.0565,
      "step": 3403
    },
    {
      "epoch": 0.8423657510517198,
      "grad_norm": 0.04335176199674606,
      "learning_rate": 0.000311818054490148,
      "loss": 0.0579,
      "step": 3404
    },
    {
      "epoch": 0.8426132145508537,
      "grad_norm": 0.023111553862690926,
      "learning_rate": 0.0003117237722525093,
      "loss": 0.0262,
      "step": 3405
    },
    {
      "epoch": 0.8428606780499877,
      "grad_norm": 0.03508841618895531,
      "learning_rate": 0.00031162948066533816,
      "loss": 0.0476,
      "step": 3406
    },
    {
      "epoch": 0.8431081415491215,
      "grad_norm": 0.03946821764111519,
      "learning_rate": 0.00031153517974291716,
      "loss": 0.0493,
      "step": 3407
    },
    {
      "epoch": 0.8433556050482554,
      "grad_norm": 0.05003785341978073,
      "learning_rate": 0.00031144086949953066,
      "loss": 0.051,
      "step": 3408
    },
    {
      "epoch": 0.8436030685473893,
      "grad_norm": 0.03379150107502937,
      "learning_rate": 0.00031134654994946395,
      "loss": 0.0393,
      "step": 3409
    },
    {
      "epoch": 0.8438505320465232,
      "grad_norm": 0.05212021619081497,
      "learning_rate": 0.00031125222110700405,
      "loss": 0.0765,
      "step": 3410
    },
    {
      "epoch": 0.844097995545657,
      "grad_norm": 0.04991363734006882,
      "learning_rate": 0.0003111578829864394,
      "loss": 0.0635,
      "step": 3411
    },
    {
      "epoch": 0.8443454590447909,
      "grad_norm": 0.023038450628519058,
      "learning_rate": 0.00031106353560205965,
      "loss": 0.0186,
      "step": 3412
    },
    {
      "epoch": 0.8445929225439248,
      "grad_norm": 0.041510049253702164,
      "learning_rate": 0.00031096917896815597,
      "loss": 0.0455,
      "step": 3413
    },
    {
      "epoch": 0.8448403860430587,
      "grad_norm": 0.03638453036546707,
      "learning_rate": 0.0003108748130990209,
      "loss": 0.0535,
      "step": 3414
    },
    {
      "epoch": 0.8450878495421925,
      "grad_norm": 0.0550110749900341,
      "learning_rate": 0.0003107804380089484,
      "loss": 0.0526,
      "step": 3415
    },
    {
      "epoch": 0.8453353130413264,
      "grad_norm": 0.040599677711725235,
      "learning_rate": 0.0003106860537122339,
      "loss": 0.0563,
      "step": 3416
    },
    {
      "epoch": 0.8455827765404603,
      "grad_norm": 0.04438232630491257,
      "learning_rate": 0.000310591660223174,
      "loss": 0.0532,
      "step": 3417
    },
    {
      "epoch": 0.8458302400395942,
      "grad_norm": 0.06687714904546738,
      "learning_rate": 0.00031049725755606704,
      "loss": 0.0635,
      "step": 3418
    },
    {
      "epoch": 0.846077703538728,
      "grad_norm": 0.02513321489095688,
      "learning_rate": 0.00031040284572521234,
      "loss": 0.0224,
      "step": 3419
    },
    {
      "epoch": 0.8463251670378619,
      "grad_norm": 0.05035790055990219,
      "learning_rate": 0.0003103084247449109,
      "loss": 0.0375,
      "step": 3420
    },
    {
      "epoch": 0.8465726305369958,
      "grad_norm": 0.03578770160675049,
      "learning_rate": 0.0003102139946294651,
      "loss": 0.0375,
      "step": 3421
    },
    {
      "epoch": 0.8468200940361297,
      "grad_norm": 0.02279803343117237,
      "learning_rate": 0.0003101195553931785,
      "loss": 0.029,
      "step": 3422
    },
    {
      "epoch": 0.8470675575352635,
      "grad_norm": 0.06312551349401474,
      "learning_rate": 0.0003100251070503562,
      "loss": 0.0811,
      "step": 3423
    },
    {
      "epoch": 0.8473150210343974,
      "grad_norm": 0.06880306452512741,
      "learning_rate": 0.00030993064961530473,
      "loss": 0.0939,
      "step": 3424
    },
    {
      "epoch": 0.8475624845335313,
      "grad_norm": 0.05235544219613075,
      "learning_rate": 0.00030983618310233176,
      "loss": 0.0908,
      "step": 3425
    },
    {
      "epoch": 0.8478099480326652,
      "grad_norm": 0.11190931499004364,
      "learning_rate": 0.00030974170752574664,
      "loss": 0.0625,
      "step": 3426
    },
    {
      "epoch": 0.848057411531799,
      "grad_norm": 0.034124214202165604,
      "learning_rate": 0.00030964722289985984,
      "loss": 0.0606,
      "step": 3427
    },
    {
      "epoch": 0.848304875030933,
      "grad_norm": 0.05076669156551361,
      "learning_rate": 0.00030955272923898343,
      "loss": 0.0354,
      "step": 3428
    },
    {
      "epoch": 0.8485523385300668,
      "grad_norm": 0.032100070267915726,
      "learning_rate": 0.0003094582265574305,
      "loss": 0.049,
      "step": 3429
    },
    {
      "epoch": 0.8487998020292007,
      "grad_norm": 0.059219010174274445,
      "learning_rate": 0.00030936371486951596,
      "loss": 0.0682,
      "step": 3430
    },
    {
      "epoch": 0.8490472655283345,
      "grad_norm": 0.0561065748333931,
      "learning_rate": 0.00030926919418955575,
      "loss": 0.0511,
      "step": 3431
    },
    {
      "epoch": 0.8492947290274685,
      "grad_norm": 0.029888801276683807,
      "learning_rate": 0.00030917466453186726,
      "loss": 0.0339,
      "step": 3432
    },
    {
      "epoch": 0.8495421925266023,
      "grad_norm": 0.03216870501637459,
      "learning_rate": 0.0003090801259107692,
      "loss": 0.0574,
      "step": 3433
    },
    {
      "epoch": 0.8497896560257362,
      "grad_norm": 0.02989691123366356,
      "learning_rate": 0.0003089855783405818,
      "loss": 0.0369,
      "step": 3434
    },
    {
      "epoch": 0.85003711952487,
      "grad_norm": 0.056847214698791504,
      "learning_rate": 0.0003088910218356265,
      "loss": 0.0892,
      "step": 3435
    },
    {
      "epoch": 0.850284583024004,
      "grad_norm": 0.03278122469782829,
      "learning_rate": 0.0003087964564102262,
      "loss": 0.0523,
      "step": 3436
    },
    {
      "epoch": 0.8505320465231379,
      "grad_norm": 0.03528047353029251,
      "learning_rate": 0.00030870188207870493,
      "loss": 0.061,
      "step": 3437
    },
    {
      "epoch": 0.8507795100222717,
      "grad_norm": 0.06796428561210632,
      "learning_rate": 0.0003086072988553884,
      "loss": 0.0705,
      "step": 3438
    },
    {
      "epoch": 0.8510269735214055,
      "grad_norm": 0.027175573632121086,
      "learning_rate": 0.0003085127067546032,
      "loss": 0.0458,
      "step": 3439
    },
    {
      "epoch": 0.8512744370205395,
      "grad_norm": 0.04117520526051521,
      "learning_rate": 0.00030841810579067784,
      "loss": 0.0666,
      "step": 3440
    },
    {
      "epoch": 0.8515219005196734,
      "grad_norm": 0.04305184632539749,
      "learning_rate": 0.0003083234959779418,
      "loss": 0.0969,
      "step": 3441
    },
    {
      "epoch": 0.8517693640188072,
      "grad_norm": 0.03388594090938568,
      "learning_rate": 0.00030822887733072604,
      "loss": 0.0402,
      "step": 3442
    },
    {
      "epoch": 0.8520168275179411,
      "grad_norm": 0.020735183730721474,
      "learning_rate": 0.0003081342498633626,
      "loss": 0.0332,
      "step": 3443
    },
    {
      "epoch": 0.852264291017075,
      "grad_norm": 0.03319716453552246,
      "learning_rate": 0.00030803961359018525,
      "loss": 0.0475,
      "step": 3444
    },
    {
      "epoch": 0.8525117545162089,
      "grad_norm": 0.05394148454070091,
      "learning_rate": 0.00030794496852552876,
      "loss": 0.0451,
      "step": 3445
    },
    {
      "epoch": 0.8527592180153427,
      "grad_norm": 0.02310708723962307,
      "learning_rate": 0.00030785031468372957,
      "loss": 0.0527,
      "step": 3446
    },
    {
      "epoch": 0.8530066815144766,
      "grad_norm": 0.03444906324148178,
      "learning_rate": 0.00030775565207912507,
      "loss": 0.0469,
      "step": 3447
    },
    {
      "epoch": 0.8532541450136105,
      "grad_norm": 0.05590604245662689,
      "learning_rate": 0.0003076609807260543,
      "loss": 0.0609,
      "step": 3448
    },
    {
      "epoch": 0.8535016085127444,
      "grad_norm": 0.02700785920023918,
      "learning_rate": 0.0003075663006388574,
      "loss": 0.0264,
      "step": 3449
    },
    {
      "epoch": 0.8537490720118782,
      "grad_norm": 0.041237398982048035,
      "learning_rate": 0.00030747161183187593,
      "loss": 0.0422,
      "step": 3450
    },
    {
      "epoch": 0.8539965355110122,
      "grad_norm": 0.07038528472185135,
      "learning_rate": 0.00030737691431945277,
      "loss": 0.0825,
      "step": 3451
    },
    {
      "epoch": 0.854243999010146,
      "grad_norm": 0.07240702211856842,
      "learning_rate": 0.0003072822081159321,
      "loss": 0.1077,
      "step": 3452
    },
    {
      "epoch": 0.8544914625092799,
      "grad_norm": 0.04541891813278198,
      "learning_rate": 0.0003071874932356594,
      "loss": 0.0903,
      "step": 3453
    },
    {
      "epoch": 0.8547389260084137,
      "grad_norm": 0.032325297594070435,
      "learning_rate": 0.0003070927696929816,
      "loss": 0.0405,
      "step": 3454
    },
    {
      "epoch": 0.8549863895075477,
      "grad_norm": 0.038359224796295166,
      "learning_rate": 0.00030699803750224675,
      "loss": 0.0744,
      "step": 3455
    },
    {
      "epoch": 0.8552338530066815,
      "grad_norm": 0.03377167880535126,
      "learning_rate": 0.0003069032966778043,
      "loss": 0.0389,
      "step": 3456
    },
    {
      "epoch": 0.8554813165058154,
      "grad_norm": 0.04895671457052231,
      "learning_rate": 0.00030680854723400496,
      "loss": 0.0334,
      "step": 3457
    },
    {
      "epoch": 0.8557287800049492,
      "grad_norm": 0.024432973936200142,
      "learning_rate": 0.0003067137891852008,
      "loss": 0.028,
      "step": 3458
    },
    {
      "epoch": 0.8559762435040832,
      "grad_norm": 0.05112036317586899,
      "learning_rate": 0.00030661902254574527,
      "loss": 0.085,
      "step": 3459
    },
    {
      "epoch": 0.856223707003217,
      "grad_norm": 0.057486578822135925,
      "learning_rate": 0.00030652424732999293,
      "loss": 0.0565,
      "step": 3460
    },
    {
      "epoch": 0.8564711705023509,
      "grad_norm": 0.037517745047807693,
      "learning_rate": 0.00030642946355229983,
      "loss": 0.0489,
      "step": 3461
    },
    {
      "epoch": 0.8567186340014847,
      "grad_norm": 0.03586706146597862,
      "learning_rate": 0.0003063346712270231,
      "loss": 0.0647,
      "step": 3462
    },
    {
      "epoch": 0.8569660975006187,
      "grad_norm": 0.04019206017255783,
      "learning_rate": 0.0003062398703685214,
      "loss": 0.0465,
      "step": 3463
    },
    {
      "epoch": 0.8572135609997525,
      "grad_norm": 0.04006287828087807,
      "learning_rate": 0.0003061450609911545,
      "loss": 0.0547,
      "step": 3464
    },
    {
      "epoch": 0.8574610244988864,
      "grad_norm": 0.029153309762477875,
      "learning_rate": 0.0003060502431092836,
      "loss": 0.0341,
      "step": 3465
    },
    {
      "epoch": 0.8577084879980202,
      "grad_norm": 0.038836102932691574,
      "learning_rate": 0.00030595541673727106,
      "loss": 0.0477,
      "step": 3466
    },
    {
      "epoch": 0.8579559514971542,
      "grad_norm": 0.07465203106403351,
      "learning_rate": 0.0003058605818894807,
      "loss": 0.0779,
      "step": 3467
    },
    {
      "epoch": 0.858203414996288,
      "grad_norm": 0.03191037476062775,
      "learning_rate": 0.00030576573858027734,
      "loss": 0.0484,
      "step": 3468
    },
    {
      "epoch": 0.8584508784954219,
      "grad_norm": 0.042138420045375824,
      "learning_rate": 0.0003056708868240273,
      "loss": 0.039,
      "step": 3469
    },
    {
      "epoch": 0.8586983419945559,
      "grad_norm": 0.10168299078941345,
      "learning_rate": 0.00030557602663509826,
      "loss": 0.052,
      "step": 3470
    },
    {
      "epoch": 0.8589458054936897,
      "grad_norm": 0.040694963186979294,
      "learning_rate": 0.00030548115802785905,
      "loss": 0.0383,
      "step": 3471
    },
    {
      "epoch": 0.8591932689928236,
      "grad_norm": 0.08203563839197159,
      "learning_rate": 0.00030538628101667954,
      "loss": 0.1448,
      "step": 3472
    },
    {
      "epoch": 0.8594407324919574,
      "grad_norm": 0.020379599183797836,
      "learning_rate": 0.00030529139561593125,
      "loss": 0.0195,
      "step": 3473
    },
    {
      "epoch": 0.8596881959910914,
      "grad_norm": 0.05028738081455231,
      "learning_rate": 0.0003051965018399869,
      "loss": 0.0409,
      "step": 3474
    },
    {
      "epoch": 0.8599356594902252,
      "grad_norm": 0.037961527705192566,
      "learning_rate": 0.0003051015997032204,
      "loss": 0.0383,
      "step": 3475
    },
    {
      "epoch": 0.8601831229893591,
      "grad_norm": 0.036483943462371826,
      "learning_rate": 0.00030500668922000683,
      "loss": 0.0655,
      "step": 3476
    },
    {
      "epoch": 0.8604305864884929,
      "grad_norm": 0.023670218884944916,
      "learning_rate": 0.00030491177040472276,
      "loss": 0.0368,
      "step": 3477
    },
    {
      "epoch": 0.8606780499876269,
      "grad_norm": 0.08603847771883011,
      "learning_rate": 0.0003048168432717457,
      "loss": 0.0749,
      "step": 3478
    },
    {
      "epoch": 0.8609255134867607,
      "grad_norm": 0.054354213178157806,
      "learning_rate": 0.00030472190783545485,
      "loss": 0.0673,
      "step": 3479
    },
    {
      "epoch": 0.8611729769858946,
      "grad_norm": 0.03237907961010933,
      "learning_rate": 0.0003046269641102303,
      "loss": 0.0418,
      "step": 3480
    },
    {
      "epoch": 0.8614204404850284,
      "grad_norm": 0.06447884440422058,
      "learning_rate": 0.00030453201211045364,
      "loss": 0.099,
      "step": 3481
    },
    {
      "epoch": 0.8616679039841624,
      "grad_norm": 0.027177980169653893,
      "learning_rate": 0.00030443705185050746,
      "loss": 0.045,
      "step": 3482
    },
    {
      "epoch": 0.8619153674832962,
      "grad_norm": 0.037103768438100815,
      "learning_rate": 0.0003043420833447759,
      "loss": 0.0585,
      "step": 3483
    },
    {
      "epoch": 0.8621628309824301,
      "grad_norm": 0.052891992032527924,
      "learning_rate": 0.000304247106607644,
      "loss": 0.0697,
      "step": 3484
    },
    {
      "epoch": 0.8624102944815639,
      "grad_norm": 0.029685158282518387,
      "learning_rate": 0.0003041521216534985,
      "loss": 0.042,
      "step": 3485
    },
    {
      "epoch": 0.8626577579806979,
      "grad_norm": 0.06563043594360352,
      "learning_rate": 0.000304057128496727,
      "loss": 0.079,
      "step": 3486
    },
    {
      "epoch": 0.8629052214798317,
      "grad_norm": 0.031140105798840523,
      "learning_rate": 0.0003039621271517184,
      "loss": 0.0326,
      "step": 3487
    },
    {
      "epoch": 0.8631526849789656,
      "grad_norm": 0.02231648936867714,
      "learning_rate": 0.000303867117632863,
      "loss": 0.0357,
      "step": 3488
    },
    {
      "epoch": 0.8634001484780994,
      "grad_norm": 0.029236238449811935,
      "learning_rate": 0.0003037720999545523,
      "loss": 0.0368,
      "step": 3489
    },
    {
      "epoch": 0.8636476119772334,
      "grad_norm": 0.050909239798784256,
      "learning_rate": 0.00030367707413117887,
      "loss": 0.0586,
      "step": 3490
    },
    {
      "epoch": 0.8638950754763672,
      "grad_norm": 0.03680165112018585,
      "learning_rate": 0.0003035820401771367,
      "loss": 0.0532,
      "step": 3491
    },
    {
      "epoch": 0.8641425389755011,
      "grad_norm": 0.03031117655336857,
      "learning_rate": 0.0003034869981068209,
      "loss": 0.0455,
      "step": 3492
    },
    {
      "epoch": 0.864390002474635,
      "grad_norm": 0.029424330219626427,
      "learning_rate": 0.0003033919479346279,
      "loss": 0.0354,
      "step": 3493
    },
    {
      "epoch": 0.8646374659737689,
      "grad_norm": 0.05563594400882721,
      "learning_rate": 0.00030329688967495517,
      "loss": 0.0724,
      "step": 3494
    },
    {
      "epoch": 0.8648849294729027,
      "grad_norm": 0.05978972464799881,
      "learning_rate": 0.00030320182334220183,
      "loss": 0.0689,
      "step": 3495
    },
    {
      "epoch": 0.8651323929720366,
      "grad_norm": 0.03059561550617218,
      "learning_rate": 0.0003031067489507676,
      "loss": 0.0501,
      "step": 3496
    },
    {
      "epoch": 0.8653798564711705,
      "grad_norm": 0.03521260246634483,
      "learning_rate": 0.0003030116665150541,
      "loss": 0.0249,
      "step": 3497
    },
    {
      "epoch": 0.8656273199703044,
      "grad_norm": 0.03228010982275009,
      "learning_rate": 0.0003029165760494634,
      "loss": 0.0528,
      "step": 3498
    },
    {
      "epoch": 0.8658747834694382,
      "grad_norm": 0.05017945170402527,
      "learning_rate": 0.0003028214775683997,
      "loss": 0.0754,
      "step": 3499
    },
    {
      "epoch": 0.8661222469685721,
      "grad_norm": 0.041069768369197845,
      "learning_rate": 0.00030272637108626747,
      "loss": 0.0725,
      "step": 3500
    },
    {
      "epoch": 0.8663697104677061,
      "grad_norm": 0.027103060856461525,
      "learning_rate": 0.0003026312566174732,
      "loss": 0.0341,
      "step": 3501
    },
    {
      "epoch": 0.8666171739668399,
      "grad_norm": 0.036904096603393555,
      "learning_rate": 0.000302536134176424,
      "loss": 0.0304,
      "step": 3502
    },
    {
      "epoch": 0.8668646374659738,
      "grad_norm": 0.05864458158612251,
      "learning_rate": 0.0003024410037775286,
      "loss": 0.0991,
      "step": 3503
    },
    {
      "epoch": 0.8671121009651076,
      "grad_norm": 0.025903986766934395,
      "learning_rate": 0.0003023458654351966,
      "loss": 0.0474,
      "step": 3504
    },
    {
      "epoch": 0.8673595644642416,
      "grad_norm": 0.03688926622271538,
      "learning_rate": 0.00030225071916383905,
      "loss": 0.0638,
      "step": 3505
    },
    {
      "epoch": 0.8676070279633754,
      "grad_norm": 0.05960642546415329,
      "learning_rate": 0.00030215556497786817,
      "loss": 0.0722,
      "step": 3506
    },
    {
      "epoch": 0.8678544914625093,
      "grad_norm": 0.05942024663090706,
      "learning_rate": 0.00030206040289169715,
      "loss": 0.0756,
      "step": 3507
    },
    {
      "epoch": 0.8681019549616431,
      "grad_norm": 0.06566822528839111,
      "learning_rate": 0.0003019652329197407,
      "loss": 0.0637,
      "step": 3508
    },
    {
      "epoch": 0.8683494184607771,
      "grad_norm": 0.057095129042863846,
      "learning_rate": 0.00030187005507641466,
      "loss": 0.0955,
      "step": 3509
    },
    {
      "epoch": 0.8685968819599109,
      "grad_norm": 0.029769614338874817,
      "learning_rate": 0.0003017748693761357,
      "loss": 0.0557,
      "step": 3510
    },
    {
      "epoch": 0.8688443454590448,
      "grad_norm": 0.028184982016682625,
      "learning_rate": 0.0003016796758333221,
      "loss": 0.0287,
      "step": 3511
    },
    {
      "epoch": 0.8690918089581786,
      "grad_norm": 0.0319366455078125,
      "learning_rate": 0.00030158447446239323,
      "loss": 0.0526,
      "step": 3512
    },
    {
      "epoch": 0.8693392724573126,
      "grad_norm": 0.04314996302127838,
      "learning_rate": 0.0003014892652777696,
      "loss": 0.0522,
      "step": 3513
    },
    {
      "epoch": 0.8695867359564464,
      "grad_norm": 0.02135942503809929,
      "learning_rate": 0.00030139404829387283,
      "loss": 0.0346,
      "step": 3514
    },
    {
      "epoch": 0.8698341994555803,
      "grad_norm": 0.04221007972955704,
      "learning_rate": 0.00030129882352512583,
      "loss": 0.071,
      "step": 3515
    },
    {
      "epoch": 0.8700816629547142,
      "grad_norm": 0.036638278514146805,
      "learning_rate": 0.00030120359098595266,
      "loss": 0.0399,
      "step": 3516
    },
    {
      "epoch": 0.8703291264538481,
      "grad_norm": 0.020136289298534393,
      "learning_rate": 0.0003011083506907785,
      "loss": 0.0239,
      "step": 3517
    },
    {
      "epoch": 0.8705765899529819,
      "grad_norm": 0.036432623863220215,
      "learning_rate": 0.00030101310265402985,
      "loss": 0.0397,
      "step": 3518
    },
    {
      "epoch": 0.8708240534521158,
      "grad_norm": 0.022293346002697945,
      "learning_rate": 0.00030091784689013424,
      "loss": 0.0407,
      "step": 3519
    },
    {
      "epoch": 0.8710715169512497,
      "grad_norm": 0.019397282972931862,
      "learning_rate": 0.0003008225834135204,
      "loss": 0.0191,
      "step": 3520
    },
    {
      "epoch": 0.8713189804503836,
      "grad_norm": 0.06540483236312866,
      "learning_rate": 0.0003007273122386183,
      "loss": 0.098,
      "step": 3521
    },
    {
      "epoch": 0.8715664439495174,
      "grad_norm": 0.05174461752176285,
      "learning_rate": 0.000300632033379859,
      "loss": 0.1023,
      "step": 3522
    },
    {
      "epoch": 0.8718139074486513,
      "grad_norm": 0.03555186837911606,
      "learning_rate": 0.0003005367468516747,
      "loss": 0.0435,
      "step": 3523
    },
    {
      "epoch": 0.8720613709477852,
      "grad_norm": 0.050919048488140106,
      "learning_rate": 0.0003004414526684989,
      "loss": 0.0495,
      "step": 3524
    },
    {
      "epoch": 0.8723088344469191,
      "grad_norm": 0.0604366660118103,
      "learning_rate": 0.00030034615084476615,
      "loss": 0.0433,
      "step": 3525
    },
    {
      "epoch": 0.8725562979460529,
      "grad_norm": 0.02786049246788025,
      "learning_rate": 0.0003002508413949121,
      "loss": 0.0317,
      "step": 3526
    },
    {
      "epoch": 0.8728037614451868,
      "grad_norm": 0.0427236333489418,
      "learning_rate": 0.00030015552433337357,
      "loss": 0.0657,
      "step": 3527
    },
    {
      "epoch": 0.8730512249443207,
      "grad_norm": 0.05326475575566292,
      "learning_rate": 0.00030006019967458887,
      "loss": 0.0665,
      "step": 3528
    },
    {
      "epoch": 0.8732986884434546,
      "grad_norm": 0.02891598455607891,
      "learning_rate": 0.00029996486743299696,
      "loss": 0.0452,
      "step": 3529
    },
    {
      "epoch": 0.8735461519425884,
      "grad_norm": 0.024220267310738564,
      "learning_rate": 0.0002998695276230383,
      "loss": 0.0289,
      "step": 3530
    },
    {
      "epoch": 0.8737936154417223,
      "grad_norm": 0.03815581277012825,
      "learning_rate": 0.00029977418025915417,
      "loss": 0.0455,
      "step": 3531
    },
    {
      "epoch": 0.8740410789408563,
      "grad_norm": 0.03093753755092621,
      "learning_rate": 0.0002996788253557874,
      "loss": 0.0245,
      "step": 3532
    },
    {
      "epoch": 0.8742885424399901,
      "grad_norm": 0.0407579205930233,
      "learning_rate": 0.00029958346292738167,
      "loss": 0.0532,
      "step": 3533
    },
    {
      "epoch": 0.874536005939124,
      "grad_norm": 0.052784327417612076,
      "learning_rate": 0.00029948809298838185,
      "loss": 0.0674,
      "step": 3534
    },
    {
      "epoch": 0.8747834694382579,
      "grad_norm": 0.02852584421634674,
      "learning_rate": 0.0002993927155532341,
      "loss": 0.0439,
      "step": 3535
    },
    {
      "epoch": 0.8750309329373918,
      "grad_norm": 0.021543659269809723,
      "learning_rate": 0.0002992973306363856,
      "loss": 0.025,
      "step": 3536
    },
    {
      "epoch": 0.8752783964365256,
      "grad_norm": 0.035017143934965134,
      "learning_rate": 0.00029920193825228443,
      "loss": 0.0519,
      "step": 3537
    },
    {
      "epoch": 0.8755258599356595,
      "grad_norm": 0.05360685661435127,
      "learning_rate": 0.0002991065384153803,
      "loss": 0.0703,
      "step": 3538
    },
    {
      "epoch": 0.8757733234347934,
      "grad_norm": 0.028134942054748535,
      "learning_rate": 0.0002990111311401237,
      "loss": 0.0425,
      "step": 3539
    },
    {
      "epoch": 0.8760207869339273,
      "grad_norm": 0.04068060591816902,
      "learning_rate": 0.0002989157164409663,
      "loss": 0.0552,
      "step": 3540
    },
    {
      "epoch": 0.8762682504330611,
      "grad_norm": 0.027614831924438477,
      "learning_rate": 0.000298820294332361,
      "loss": 0.0385,
      "step": 3541
    },
    {
      "epoch": 0.876515713932195,
      "grad_norm": 0.05249042436480522,
      "learning_rate": 0.00029872486482876155,
      "loss": 0.0402,
      "step": 3542
    },
    {
      "epoch": 0.8767631774313289,
      "grad_norm": 0.053672853857278824,
      "learning_rate": 0.0002986294279446232,
      "loss": 0.0849,
      "step": 3543
    },
    {
      "epoch": 0.8770106409304628,
      "grad_norm": 0.02239184081554413,
      "learning_rate": 0.00029853398369440216,
      "loss": 0.0255,
      "step": 3544
    },
    {
      "epoch": 0.8772581044295966,
      "grad_norm": 0.06336171925067902,
      "learning_rate": 0.00029843853209255555,
      "loss": 0.0433,
      "step": 3545
    },
    {
      "epoch": 0.8775055679287305,
      "grad_norm": 0.026824621483683586,
      "learning_rate": 0.000298343073153542,
      "loss": 0.0253,
      "step": 3546
    },
    {
      "epoch": 0.8777530314278644,
      "grad_norm": 0.03126296401023865,
      "learning_rate": 0.0002982476068918208,
      "loss": 0.0585,
      "step": 3547
    },
    {
      "epoch": 0.8780004949269983,
      "grad_norm": 0.030215388163924217,
      "learning_rate": 0.00029815213332185284,
      "loss": 0.0493,
      "step": 3548
    },
    {
      "epoch": 0.8782479584261321,
      "grad_norm": 0.030793488025665283,
      "learning_rate": 0.00029805665245809964,
      "loss": 0.0743,
      "step": 3549
    },
    {
      "epoch": 0.878495421925266,
      "grad_norm": 0.04671405255794525,
      "learning_rate": 0.00029796116431502417,
      "loss": 0.1044,
      "step": 3550
    },
    {
      "epoch": 0.8787428854243999,
      "grad_norm": 0.057684339582920074,
      "learning_rate": 0.00029786566890709033,
      "loss": 0.1309,
      "step": 3551
    },
    {
      "epoch": 0.8789903489235338,
      "grad_norm": 0.032514069229364395,
      "learning_rate": 0.00029777016624876317,
      "loss": 0.0498,
      "step": 3552
    },
    {
      "epoch": 0.8792378124226676,
      "grad_norm": 0.03814935311675072,
      "learning_rate": 0.00029767465635450893,
      "loss": 0.0479,
      "step": 3553
    },
    {
      "epoch": 0.8794852759218015,
      "grad_norm": 0.07243417203426361,
      "learning_rate": 0.0002975791392387948,
      "loss": 0.0986,
      "step": 3554
    },
    {
      "epoch": 0.8797327394209354,
      "grad_norm": 0.04693886265158653,
      "learning_rate": 0.00029748361491608894,
      "loss": 0.043,
      "step": 3555
    },
    {
      "epoch": 0.8799802029200693,
      "grad_norm": 0.04570804536342621,
      "learning_rate": 0.0002973880834008611,
      "loss": 0.0666,
      "step": 3556
    },
    {
      "epoch": 0.8802276664192031,
      "grad_norm": 0.07011096924543381,
      "learning_rate": 0.0002972925447075815,
      "loss": 0.0572,
      "step": 3557
    },
    {
      "epoch": 0.8804751299183371,
      "grad_norm": 0.04749603196978569,
      "learning_rate": 0.0002971969988507219,
      "loss": 0.0653,
      "step": 3558
    },
    {
      "epoch": 0.880722593417471,
      "grad_norm": 0.03171459957957268,
      "learning_rate": 0.000297101445844755,
      "loss": 0.0474,
      "step": 3559
    },
    {
      "epoch": 0.8809700569166048,
      "grad_norm": 0.059343304485082626,
      "learning_rate": 0.0002970058857041546,
      "loss": 0.1332,
      "step": 3560
    },
    {
      "epoch": 0.8812175204157386,
      "grad_norm": 0.03740965574979782,
      "learning_rate": 0.00029691031844339546,
      "loss": 0.0656,
      "step": 3561
    },
    {
      "epoch": 0.8814649839148726,
      "grad_norm": 0.047199126332998276,
      "learning_rate": 0.00029681474407695354,
      "loss": 0.054,
      "step": 3562
    },
    {
      "epoch": 0.8817124474140065,
      "grad_norm": 0.025511302053928375,
      "learning_rate": 0.0002967191626193058,
      "loss": 0.0407,
      "step": 3563
    },
    {
      "epoch": 0.8819599109131403,
      "grad_norm": 0.03225066885352135,
      "learning_rate": 0.0002966235740849305,
      "loss": 0.0513,
      "step": 3564
    },
    {
      "epoch": 0.8822073744122741,
      "grad_norm": 0.03160463646054268,
      "learning_rate": 0.00029652797848830664,
      "loss": 0.0493,
      "step": 3565
    },
    {
      "epoch": 0.8824548379114081,
      "grad_norm": 0.048803992569446564,
      "learning_rate": 0.0002964323758439145,
      "loss": 0.0372,
      "step": 3566
    },
    {
      "epoch": 0.882702301410542,
      "grad_norm": 0.04246385395526886,
      "learning_rate": 0.0002963367661662353,
      "loss": 0.0826,
      "step": 3567
    },
    {
      "epoch": 0.8829497649096758,
      "grad_norm": 0.021309569478034973,
      "learning_rate": 0.0002962411494697515,
      "loss": 0.0201,
      "step": 3568
    },
    {
      "epoch": 0.8831972284088097,
      "grad_norm": 0.036758411675691605,
      "learning_rate": 0.0002961455257689465,
      "loss": 0.0338,
      "step": 3569
    },
    {
      "epoch": 0.8834446919079436,
      "grad_norm": 0.025380857288837433,
      "learning_rate": 0.0002960498950783047,
      "loss": 0.0327,
      "step": 3570
    },
    {
      "epoch": 0.8836921554070775,
      "grad_norm": 0.037782371044158936,
      "learning_rate": 0.0002959542574123118,
      "loss": 0.0488,
      "step": 3571
    },
    {
      "epoch": 0.8839396189062113,
      "grad_norm": 0.052043016999959946,
      "learning_rate": 0.00029585861278545424,
      "loss": 0.0644,
      "step": 3572
    },
    {
      "epoch": 0.8841870824053452,
      "grad_norm": 0.04189633950591087,
      "learning_rate": 0.00029576296121221966,
      "loss": 0.0585,
      "step": 3573
    },
    {
      "epoch": 0.8844345459044791,
      "grad_norm": 0.047629471868276596,
      "learning_rate": 0.0002956673027070969,
      "loss": 0.0404,
      "step": 3574
    },
    {
      "epoch": 0.884682009403613,
      "grad_norm": 0.05125616118311882,
      "learning_rate": 0.00029557163728457576,
      "loss": 0.067,
      "step": 3575
    },
    {
      "epoch": 0.8849294729027468,
      "grad_norm": 0.06362637132406235,
      "learning_rate": 0.0002954759649591468,
      "loss": 0.0667,
      "step": 3576
    },
    {
      "epoch": 0.8851769364018808,
      "grad_norm": 0.07511649280786514,
      "learning_rate": 0.000295380285745302,
      "loss": 0.0678,
      "step": 3577
    },
    {
      "epoch": 0.8854243999010146,
      "grad_norm": 0.03860118240118027,
      "learning_rate": 0.0002952845996575343,
      "loss": 0.0419,
      "step": 3578
    },
    {
      "epoch": 0.8856718634001485,
      "grad_norm": 0.051841042935848236,
      "learning_rate": 0.0002951889067103376,
      "loss": 0.0635,
      "step": 3579
    },
    {
      "epoch": 0.8859193268992823,
      "grad_norm": 0.06486198306083679,
      "learning_rate": 0.00029509320691820687,
      "loss": 0.0825,
      "step": 3580
    },
    {
      "epoch": 0.8861667903984163,
      "grad_norm": 0.04117492213845253,
      "learning_rate": 0.00029499750029563806,
      "loss": 0.079,
      "step": 3581
    },
    {
      "epoch": 0.8864142538975501,
      "grad_norm": 0.02877133898437023,
      "learning_rate": 0.0002949017868571282,
      "loss": 0.0574,
      "step": 3582
    },
    {
      "epoch": 0.886661717396684,
      "grad_norm": 0.03793917968869209,
      "learning_rate": 0.0002948060666171756,
      "loss": 0.053,
      "step": 3583
    },
    {
      "epoch": 0.8869091808958178,
      "grad_norm": 0.04066360369324684,
      "learning_rate": 0.000294710339590279,
      "loss": 0.0441,
      "step": 3584
    },
    {
      "epoch": 0.8871566443949518,
      "grad_norm": 0.031121907755732536,
      "learning_rate": 0.00029461460579093885,
      "loss": 0.0433,
      "step": 3585
    },
    {
      "epoch": 0.8874041078940856,
      "grad_norm": 0.030097847804427147,
      "learning_rate": 0.00029451886523365606,
      "loss": 0.0711,
      "step": 3586
    },
    {
      "epoch": 0.8876515713932195,
      "grad_norm": 0.10585130751132965,
      "learning_rate": 0.0002944231179329331,
      "loss": 0.0384,
      "step": 3587
    },
    {
      "epoch": 0.8878990348923533,
      "grad_norm": 0.03486407548189163,
      "learning_rate": 0.00029432736390327285,
      "loss": 0.0328,
      "step": 3588
    },
    {
      "epoch": 0.8881464983914873,
      "grad_norm": 0.024293914437294006,
      "learning_rate": 0.0002942316031591798,
      "loss": 0.0267,
      "step": 3589
    },
    {
      "epoch": 0.8883939618906211,
      "grad_norm": 0.03166751191020012,
      "learning_rate": 0.0002941358357151591,
      "loss": 0.0325,
      "step": 3590
    },
    {
      "epoch": 0.888641425389755,
      "grad_norm": 0.033401839435100555,
      "learning_rate": 0.0002940400615857169,
      "loss": 0.0335,
      "step": 3591
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.032465919852256775,
      "learning_rate": 0.00029394428078536054,
      "loss": 0.0327,
      "step": 3592
    },
    {
      "epoch": 0.8891363523880228,
      "grad_norm": 0.0256704930216074,
      "learning_rate": 0.00029384849332859844,
      "loss": 0.0262,
      "step": 3593
    },
    {
      "epoch": 0.8893838158871566,
      "grad_norm": 0.0416538380086422,
      "learning_rate": 0.00029375269922993967,
      "loss": 0.063,
      "step": 3594
    },
    {
      "epoch": 0.8896312793862905,
      "grad_norm": 0.04079778492450714,
      "learning_rate": 0.0002936568985038947,
      "loss": 0.142,
      "step": 3595
    },
    {
      "epoch": 0.8898787428854243,
      "grad_norm": 0.038679055869579315,
      "learning_rate": 0.00029356109116497457,
      "loss": 0.0568,
      "step": 3596
    },
    {
      "epoch": 0.8901262063845583,
      "grad_norm": 0.045154523104429245,
      "learning_rate": 0.00029346527722769194,
      "loss": 0.0607,
      "step": 3597
    },
    {
      "epoch": 0.8903736698836922,
      "grad_norm": 0.03883758559823036,
      "learning_rate": 0.00029336945670655986,
      "loss": 0.0689,
      "step": 3598
    },
    {
      "epoch": 0.890621133382826,
      "grad_norm": 0.052267298102378845,
      "learning_rate": 0.0002932736296160927,
      "loss": 0.0588,
      "step": 3599
    },
    {
      "epoch": 0.89086859688196,
      "grad_norm": 0.03072446584701538,
      "learning_rate": 0.00029317779597080575,
      "loss": 0.04,
      "step": 3600
    },
    {
      "epoch": 0.89086859688196,
      "eval_loss": 0.29262542724609375,
      "eval_runtime": 202.8579,
      "eval_samples_per_second": 4.93,
      "eval_steps_per_second": 0.311,
      "step": 3600
    },
    {
      "epoch": 0.8911160603810938,
      "grad_norm": 0.0422472283244133,
      "learning_rate": 0.00029308195578521524,
      "loss": 0.0567,
      "step": 3601
    },
    {
      "epoch": 0.8913635238802277,
      "grad_norm": 0.05421879142522812,
      "learning_rate": 0.00029298610907383847,
      "loss": 0.0795,
      "step": 3602
    },
    {
      "epoch": 0.8916109873793615,
      "grad_norm": 0.024328958243131638,
      "learning_rate": 0.00029289025585119377,
      "loss": 0.0258,
      "step": 3603
    },
    {
      "epoch": 0.8918584508784955,
      "grad_norm": 0.07400143146514893,
      "learning_rate": 0.0002927943961318003,
      "loss": 0.0755,
      "step": 3604
    },
    {
      "epoch": 0.8921059143776293,
      "grad_norm": 0.03573387861251831,
      "learning_rate": 0.00029269852993017844,
      "loss": 0.0454,
      "step": 3605
    },
    {
      "epoch": 0.8923533778767632,
      "grad_norm": 0.04259311035275459,
      "learning_rate": 0.00029260265726084914,
      "loss": 0.0663,
      "step": 3606
    },
    {
      "epoch": 0.892600841375897,
      "grad_norm": 0.05523712560534477,
      "learning_rate": 0.0002925067781383348,
      "loss": 0.0984,
      "step": 3607
    },
    {
      "epoch": 0.892848304875031,
      "grad_norm": 0.05236425995826721,
      "learning_rate": 0.0002924108925771585,
      "loss": 0.0702,
      "step": 3608
    },
    {
      "epoch": 0.8930957683741648,
      "grad_norm": 0.045128222554922104,
      "learning_rate": 0.0002923150005918446,
      "loss": 0.0609,
      "step": 3609
    },
    {
      "epoch": 0.8933432318732987,
      "grad_norm": 0.06168694421648979,
      "learning_rate": 0.0002922191021969179,
      "loss": 0.0653,
      "step": 3610
    },
    {
      "epoch": 0.8935906953724325,
      "grad_norm": 0.028918184340000153,
      "learning_rate": 0.0002921231974069046,
      "loss": 0.0334,
      "step": 3611
    },
    {
      "epoch": 0.8938381588715665,
      "grad_norm": 0.029154036194086075,
      "learning_rate": 0.0002920272862363319,
      "loss": 0.0339,
      "step": 3612
    },
    {
      "epoch": 0.8940856223707003,
      "grad_norm": 0.04175645858049393,
      "learning_rate": 0.00029193136869972775,
      "loss": 0.0421,
      "step": 3613
    },
    {
      "epoch": 0.8943330858698342,
      "grad_norm": 0.06680497527122498,
      "learning_rate": 0.0002918354448116211,
      "loss": 0.0644,
      "step": 3614
    },
    {
      "epoch": 0.894580549368968,
      "grad_norm": 0.04448161646723747,
      "learning_rate": 0.00029173951458654184,
      "loss": 0.0308,
      "step": 3615
    },
    {
      "epoch": 0.894828012868102,
      "grad_norm": 0.030568968504667282,
      "learning_rate": 0.00029164357803902096,
      "loss": 0.0297,
      "step": 3616
    },
    {
      "epoch": 0.8950754763672358,
      "grad_norm": 0.02501297928392887,
      "learning_rate": 0.0002915476351835905,
      "loss": 0.0382,
      "step": 3617
    },
    {
      "epoch": 0.8953229398663697,
      "grad_norm": 0.029183190315961838,
      "learning_rate": 0.000291451686034783,
      "loss": 0.0348,
      "step": 3618
    },
    {
      "epoch": 0.8955704033655036,
      "grad_norm": 0.036908019334077835,
      "learning_rate": 0.00029135573060713233,
      "loss": 0.067,
      "step": 3619
    },
    {
      "epoch": 0.8958178668646375,
      "grad_norm": 0.0487799346446991,
      "learning_rate": 0.0002912597689151734,
      "loss": 0.0544,
      "step": 3620
    },
    {
      "epoch": 0.8960653303637713,
      "grad_norm": 0.04013493284583092,
      "learning_rate": 0.0002911638009734416,
      "loss": 0.0697,
      "step": 3621
    },
    {
      "epoch": 0.8963127938629052,
      "grad_norm": 0.06936226040124893,
      "learning_rate": 0.0002910678267964737,
      "loss": 0.1265,
      "step": 3622
    },
    {
      "epoch": 0.8965602573620391,
      "grad_norm": 0.03674672171473503,
      "learning_rate": 0.0002909718463988073,
      "loss": 0.0586,
      "step": 3623
    },
    {
      "epoch": 0.896807720861173,
      "grad_norm": 0.03890179097652435,
      "learning_rate": 0.0002908758597949809,
      "loss": 0.0377,
      "step": 3624
    },
    {
      "epoch": 0.8970551843603068,
      "grad_norm": 0.02813340723514557,
      "learning_rate": 0.0002907798669995339,
      "loss": 0.0312,
      "step": 3625
    },
    {
      "epoch": 0.8973026478594407,
      "grad_norm": 0.07876330614089966,
      "learning_rate": 0.0002906838680270067,
      "loss": 0.1155,
      "step": 3626
    },
    {
      "epoch": 0.8975501113585747,
      "grad_norm": 0.08734628558158875,
      "learning_rate": 0.00029058786289194065,
      "loss": 0.0702,
      "step": 3627
    },
    {
      "epoch": 0.8977975748577085,
      "grad_norm": 0.04091063141822815,
      "learning_rate": 0.00029049185160887807,
      "loss": 0.0632,
      "step": 3628
    },
    {
      "epoch": 0.8980450383568424,
      "grad_norm": 0.03945143520832062,
      "learning_rate": 0.00029039583419236204,
      "loss": 0.033,
      "step": 3629
    },
    {
      "epoch": 0.8982925018559762,
      "grad_norm": 0.047470323741436005,
      "learning_rate": 0.00029029981065693675,
      "loss": 0.0399,
      "step": 3630
    },
    {
      "epoch": 0.8985399653551102,
      "grad_norm": 0.05150194466114044,
      "learning_rate": 0.0002902037810171472,
      "loss": 0.0825,
      "step": 3631
    },
    {
      "epoch": 0.898787428854244,
      "grad_norm": 0.03234158828854561,
      "learning_rate": 0.0002901077452875395,
      "loss": 0.0413,
      "step": 3632
    },
    {
      "epoch": 0.8990348923533779,
      "grad_norm": 0.06601953506469727,
      "learning_rate": 0.00029001170348266027,
      "loss": 0.0555,
      "step": 3633
    },
    {
      "epoch": 0.8992823558525117,
      "grad_norm": 0.5941337943077087,
      "learning_rate": 0.0002899156556170576,
      "loss": 0.0684,
      "step": 3634
    },
    {
      "epoch": 0.8995298193516457,
      "grad_norm": 0.08264609426259995,
      "learning_rate": 0.00028981960170528003,
      "loss": 0.0906,
      "step": 3635
    },
    {
      "epoch": 0.8997772828507795,
      "grad_norm": 0.08511495590209961,
      "learning_rate": 0.0002897235417618773,
      "loss": 0.0847,
      "step": 3636
    },
    {
      "epoch": 0.9000247463499134,
      "grad_norm": 0.0850846990942955,
      "learning_rate": 0.0002896274758014,
      "loss": 0.0792,
      "step": 3637
    },
    {
      "epoch": 0.9002722098490472,
      "grad_norm": 0.029547207057476044,
      "learning_rate": 0.0002895314038383996,
      "loss": 0.0449,
      "step": 3638
    },
    {
      "epoch": 0.9005196733481812,
      "grad_norm": 0.03462203964591026,
      "learning_rate": 0.0002894353258874283,
      "loss": 0.0495,
      "step": 3639
    },
    {
      "epoch": 0.900767136847315,
      "grad_norm": 0.03193754330277443,
      "learning_rate": 0.0002893392419630396,
      "loss": 0.0371,
      "step": 3640
    },
    {
      "epoch": 0.9010146003464489,
      "grad_norm": 0.06320648640394211,
      "learning_rate": 0.0002892431520797877,
      "loss": 0.068,
      "step": 3641
    },
    {
      "epoch": 0.9012620638455828,
      "grad_norm": 0.04932953789830208,
      "learning_rate": 0.0002891470562522276,
      "loss": 0.0896,
      "step": 3642
    },
    {
      "epoch": 0.9015095273447167,
      "grad_norm": 0.035871122032403946,
      "learning_rate": 0.00028905095449491524,
      "loss": 0.0542,
      "step": 3643
    },
    {
      "epoch": 0.9017569908438505,
      "grad_norm": 0.04309915006160736,
      "learning_rate": 0.00028895484682240767,
      "loss": 0.059,
      "step": 3644
    },
    {
      "epoch": 0.9020044543429844,
      "grad_norm": 0.02846485935151577,
      "learning_rate": 0.00028885873324926254,
      "loss": 0.0436,
      "step": 3645
    },
    {
      "epoch": 0.9022519178421183,
      "grad_norm": 0.02625178173184395,
      "learning_rate": 0.0002887626137900387,
      "loss": 0.0316,
      "step": 3646
    },
    {
      "epoch": 0.9024993813412522,
      "grad_norm": 0.05954017490148544,
      "learning_rate": 0.00028866648845929556,
      "loss": 0.0517,
      "step": 3647
    },
    {
      "epoch": 0.902746844840386,
      "grad_norm": 0.04023442789912224,
      "learning_rate": 0.0002885703572715937,
      "loss": 0.0547,
      "step": 3648
    },
    {
      "epoch": 0.9029943083395199,
      "grad_norm": 0.04130489379167557,
      "learning_rate": 0.0002884742202414944,
      "loss": 0.0882,
      "step": 3649
    },
    {
      "epoch": 0.9032417718386538,
      "grad_norm": 0.057139698415994644,
      "learning_rate": 0.00028837807738355984,
      "loss": 0.0581,
      "step": 3650
    },
    {
      "epoch": 0.9034892353377877,
      "grad_norm": 0.05219906568527222,
      "learning_rate": 0.00028828192871235337,
      "loss": 0.0465,
      "step": 3651
    },
    {
      "epoch": 0.9037366988369215,
      "grad_norm": 0.036797549575567245,
      "learning_rate": 0.0002881857742424388,
      "loss": 0.0458,
      "step": 3652
    },
    {
      "epoch": 0.9039841623360554,
      "grad_norm": 0.04953831434249878,
      "learning_rate": 0.0002880896139883811,
      "loss": 0.0447,
      "step": 3653
    },
    {
      "epoch": 0.9042316258351893,
      "grad_norm": 0.027391182258725166,
      "learning_rate": 0.0002879934479647459,
      "loss": 0.0534,
      "step": 3654
    },
    {
      "epoch": 0.9044790893343232,
      "grad_norm": 0.06478574126958847,
      "learning_rate": 0.00028789727618609994,
      "loss": 0.0552,
      "step": 3655
    },
    {
      "epoch": 0.904726552833457,
      "grad_norm": 0.04854155331850052,
      "learning_rate": 0.0002878010986670108,
      "loss": 0.0972,
      "step": 3656
    },
    {
      "epoch": 0.9049740163325909,
      "grad_norm": 0.05745529383420944,
      "learning_rate": 0.00028770491542204655,
      "loss": 0.0352,
      "step": 3657
    },
    {
      "epoch": 0.9052214798317249,
      "grad_norm": 0.051000192761421204,
      "learning_rate": 0.00028760872646577687,
      "loss": 0.0845,
      "step": 3658
    },
    {
      "epoch": 0.9054689433308587,
      "grad_norm": 0.04462452232837677,
      "learning_rate": 0.00028751253181277147,
      "loss": 0.0427,
      "step": 3659
    },
    {
      "epoch": 0.9057164068299925,
      "grad_norm": 0.055600520223379135,
      "learning_rate": 0.00028741633147760147,
      "loss": 0.05,
      "step": 3660
    },
    {
      "epoch": 0.9059638703291265,
      "grad_norm": 0.02841491438448429,
      "learning_rate": 0.0002873201254748387,
      "loss": 0.0371,
      "step": 3661
    },
    {
      "epoch": 0.9062113338282604,
      "grad_norm": 0.03642707318067551,
      "learning_rate": 0.00028722391381905593,
      "loss": 0.0479,
      "step": 3662
    },
    {
      "epoch": 0.9064587973273942,
      "grad_norm": 0.028425859287381172,
      "learning_rate": 0.00028712769652482656,
      "loss": 0.0355,
      "step": 3663
    },
    {
      "epoch": 0.906706260826528,
      "grad_norm": 0.028387457132339478,
      "learning_rate": 0.00028703147360672506,
      "loss": 0.0355,
      "step": 3664
    },
    {
      "epoch": 0.906953724325662,
      "grad_norm": 0.027083316817879677,
      "learning_rate": 0.00028693524507932656,
      "loss": 0.0339,
      "step": 3665
    },
    {
      "epoch": 0.9072011878247959,
      "grad_norm": 0.03523759916424751,
      "learning_rate": 0.00028683901095720736,
      "loss": 0.0315,
      "step": 3666
    },
    {
      "epoch": 0.9074486513239297,
      "grad_norm": 0.024722009897232056,
      "learning_rate": 0.0002867427712549443,
      "loss": 0.0451,
      "step": 3667
    },
    {
      "epoch": 0.9076961148230636,
      "grad_norm": 0.02587270364165306,
      "learning_rate": 0.0002866465259871152,
      "loss": 0.0424,
      "step": 3668
    },
    {
      "epoch": 0.9079435783221975,
      "grad_norm": 0.04340089112520218,
      "learning_rate": 0.0002865502751682986,
      "loss": 0.0669,
      "step": 3669
    },
    {
      "epoch": 0.9081910418213314,
      "grad_norm": 0.04698026180267334,
      "learning_rate": 0.000286454018813074,
      "loss": 0.0867,
      "step": 3670
    },
    {
      "epoch": 0.9084385053204652,
      "grad_norm": 0.0628252923488617,
      "learning_rate": 0.0002863577569360218,
      "loss": 0.0669,
      "step": 3671
    },
    {
      "epoch": 0.9086859688195991,
      "grad_norm": 0.031674403697252274,
      "learning_rate": 0.00028626148955172316,
      "loss": 0.0221,
      "step": 3672
    },
    {
      "epoch": 0.908933432318733,
      "grad_norm": 0.026211470365524292,
      "learning_rate": 0.0002861652166747599,
      "loss": 0.0311,
      "step": 3673
    },
    {
      "epoch": 0.9091808958178669,
      "grad_norm": 0.024851372465491295,
      "learning_rate": 0.00028606893831971506,
      "loss": 0.0413,
      "step": 3674
    },
    {
      "epoch": 0.9094283593170007,
      "grad_norm": 0.05633268132805824,
      "learning_rate": 0.00028597265450117207,
      "loss": 0.0578,
      "step": 3675
    },
    {
      "epoch": 0.9096758228161346,
      "grad_norm": 0.0394303984940052,
      "learning_rate": 0.00028587636523371557,
      "loss": 0.0453,
      "step": 3676
    },
    {
      "epoch": 0.9099232863152685,
      "grad_norm": 0.02827819436788559,
      "learning_rate": 0.00028578007053193074,
      "loss": 0.0333,
      "step": 3677
    },
    {
      "epoch": 0.9101707498144024,
      "grad_norm": 0.0666874423623085,
      "learning_rate": 0.0002856837704104038,
      "loss": 0.0723,
      "step": 3678
    },
    {
      "epoch": 0.9104182133135362,
      "grad_norm": 0.06146746873855591,
      "learning_rate": 0.0002855874648837216,
      "loss": 0.0439,
      "step": 3679
    },
    {
      "epoch": 0.9106656768126701,
      "grad_norm": 0.06319642812013626,
      "learning_rate": 0.00028549115396647197,
      "loss": 0.0516,
      "step": 3680
    },
    {
      "epoch": 0.910913140311804,
      "grad_norm": 0.030273566022515297,
      "learning_rate": 0.00028539483767324345,
      "loss": 0.0283,
      "step": 3681
    },
    {
      "epoch": 0.9111606038109379,
      "grad_norm": 0.0516953207552433,
      "learning_rate": 0.00028529851601862536,
      "loss": 0.073,
      "step": 3682
    },
    {
      "epoch": 0.9114080673100717,
      "grad_norm": 0.055828921496868134,
      "learning_rate": 0.0002852021890172081,
      "loss": 0.0359,
      "step": 3683
    },
    {
      "epoch": 0.9116555308092057,
      "grad_norm": 0.049647122621536255,
      "learning_rate": 0.0002851058566835826,
      "loss": 0.0402,
      "step": 3684
    },
    {
      "epoch": 0.9119029943083395,
      "grad_norm": 0.03657376766204834,
      "learning_rate": 0.0002850095190323405,
      "loss": 0.0458,
      "step": 3685
    },
    {
      "epoch": 0.9121504578074734,
      "grad_norm": 0.0448615588247776,
      "learning_rate": 0.0002849131760780747,
      "loss": 0.0626,
      "step": 3686
    },
    {
      "epoch": 0.9123979213066072,
      "grad_norm": 0.04542333632707596,
      "learning_rate": 0.0002848168278353785,
      "loss": 0.0464,
      "step": 3687
    },
    {
      "epoch": 0.9126453848057412,
      "grad_norm": 0.06558733433485031,
      "learning_rate": 0.0002847204743188461,
      "loss": 0.0631,
      "step": 3688
    },
    {
      "epoch": 0.912892848304875,
      "grad_norm": 0.03954744711518288,
      "learning_rate": 0.00028462411554307265,
      "loss": 0.0303,
      "step": 3689
    },
    {
      "epoch": 0.9131403118040089,
      "grad_norm": 0.027226613834500313,
      "learning_rate": 0.00028452775152265375,
      "loss": 0.038,
      "step": 3690
    },
    {
      "epoch": 0.9133877753031427,
      "grad_norm": 0.04881559684872627,
      "learning_rate": 0.0002844313822721863,
      "loss": 0.097,
      "step": 3691
    },
    {
      "epoch": 0.9136352388022767,
      "grad_norm": 0.041723016649484634,
      "learning_rate": 0.0002843350078062676,
      "loss": 0.0472,
      "step": 3692
    },
    {
      "epoch": 0.9138827023014106,
      "grad_norm": 0.05270853638648987,
      "learning_rate": 0.0002842386281394958,
      "loss": 0.0782,
      "step": 3693
    },
    {
      "epoch": 0.9141301658005444,
      "grad_norm": 0.0298173725605011,
      "learning_rate": 0.00028414224328646994,
      "loss": 0.0358,
      "step": 3694
    },
    {
      "epoch": 0.9143776292996783,
      "grad_norm": 0.0458509586751461,
      "learning_rate": 0.0002840458532617897,
      "loss": 0.0564,
      "step": 3695
    },
    {
      "epoch": 0.9146250927988122,
      "grad_norm": 0.03616419434547424,
      "learning_rate": 0.00028394945808005585,
      "loss": 0.0427,
      "step": 3696
    },
    {
      "epoch": 0.9148725562979461,
      "grad_norm": 0.032830171287059784,
      "learning_rate": 0.00028385305775586957,
      "loss": 0.0379,
      "step": 3697
    },
    {
      "epoch": 0.9151200197970799,
      "grad_norm": 0.07696054130792618,
      "learning_rate": 0.00028375665230383294,
      "loss": 0.0505,
      "step": 3698
    },
    {
      "epoch": 0.9153674832962138,
      "grad_norm": 0.09253362566232681,
      "learning_rate": 0.0002836602417385491,
      "loss": 0.0807,
      "step": 3699
    },
    {
      "epoch": 0.9156149467953477,
      "grad_norm": 0.08384037762880325,
      "learning_rate": 0.0002835638260746214,
      "loss": 0.1249,
      "step": 3700
    },
    {
      "epoch": 0.9158624102944816,
      "grad_norm": 0.04269932582974434,
      "learning_rate": 0.00028346740532665454,
      "loss": 0.0519,
      "step": 3701
    },
    {
      "epoch": 0.9161098737936154,
      "grad_norm": 0.03708578646183014,
      "learning_rate": 0.00028337097950925357,
      "loss": 0.0252,
      "step": 3702
    },
    {
      "epoch": 0.9163573372927494,
      "grad_norm": 0.0272962749004364,
      "learning_rate": 0.0002832745486370246,
      "loss": 0.0382,
      "step": 3703
    },
    {
      "epoch": 0.9166048007918832,
      "grad_norm": 0.0646519884467125,
      "learning_rate": 0.00028317811272457423,
      "loss": 0.0366,
      "step": 3704
    },
    {
      "epoch": 0.9168522642910171,
      "grad_norm": 0.04707009345293045,
      "learning_rate": 0.0002830816717865102,
      "loss": 0.0515,
      "step": 3705
    },
    {
      "epoch": 0.9170997277901509,
      "grad_norm": 0.04257091134786606,
      "learning_rate": 0.00028298522583744047,
      "loss": 0.0485,
      "step": 3706
    },
    {
      "epoch": 0.9173471912892849,
      "grad_norm": 0.04745860397815704,
      "learning_rate": 0.0002828887748919744,
      "loss": 0.0687,
      "step": 3707
    },
    {
      "epoch": 0.9175946547884187,
      "grad_norm": 0.0433807410299778,
      "learning_rate": 0.0002827923189647215,
      "loss": 0.0454,
      "step": 3708
    },
    {
      "epoch": 0.9178421182875526,
      "grad_norm": 0.06512998044490814,
      "learning_rate": 0.00028269585807029246,
      "loss": 0.077,
      "step": 3709
    },
    {
      "epoch": 0.9180895817866864,
      "grad_norm": 0.03778126835823059,
      "learning_rate": 0.00028259939222329857,
      "loss": 0.0532,
      "step": 3710
    },
    {
      "epoch": 0.9183370452858204,
      "grad_norm": 0.03510742262005806,
      "learning_rate": 0.0002825029214383519,
      "loss": 0.0242,
      "step": 3711
    },
    {
      "epoch": 0.9185845087849542,
      "grad_norm": 0.0287933312356472,
      "learning_rate": 0.00028240644573006513,
      "loss": 0.0399,
      "step": 3712
    },
    {
      "epoch": 0.9188319722840881,
      "grad_norm": 0.024103913456201553,
      "learning_rate": 0.0002823099651130519,
      "loss": 0.0387,
      "step": 3713
    },
    {
      "epoch": 0.9190794357832219,
      "grad_norm": 0.07176008075475693,
      "learning_rate": 0.0002822134796019264,
      "loss": 0.1292,
      "step": 3714
    },
    {
      "epoch": 0.9193268992823559,
      "grad_norm": 0.034454796463251114,
      "learning_rate": 0.0002821169892113038,
      "loss": 0.0282,
      "step": 3715
    },
    {
      "epoch": 0.9195743627814897,
      "grad_norm": 0.02849438227713108,
      "learning_rate": 0.0002820204939557997,
      "loss": 0.0403,
      "step": 3716
    },
    {
      "epoch": 0.9198218262806236,
      "grad_norm": 0.03083902597427368,
      "learning_rate": 0.00028192399385003076,
      "loss": 0.0455,
      "step": 3717
    },
    {
      "epoch": 0.9200692897797574,
      "grad_norm": 0.053843624889850616,
      "learning_rate": 0.000281827488908614,
      "loss": 0.053,
      "step": 3718
    },
    {
      "epoch": 0.9203167532788914,
      "grad_norm": 0.04174676164984703,
      "learning_rate": 0.00028173097914616763,
      "loss": 0.054,
      "step": 3719
    },
    {
      "epoch": 0.9205642167780252,
      "grad_norm": 0.04662962257862091,
      "learning_rate": 0.00028163446457731017,
      "loss": 0.0435,
      "step": 3720
    },
    {
      "epoch": 0.9208116802771591,
      "grad_norm": 0.03075965866446495,
      "learning_rate": 0.00028153794521666123,
      "loss": 0.0186,
      "step": 3721
    },
    {
      "epoch": 0.9210591437762929,
      "grad_norm": 0.03423637896776199,
      "learning_rate": 0.00028144142107884075,
      "loss": 0.0443,
      "step": 3722
    },
    {
      "epoch": 0.9213066072754269,
      "grad_norm": 0.06083688512444496,
      "learning_rate": 0.00028134489217846966,
      "loss": 0.0798,
      "step": 3723
    },
    {
      "epoch": 0.9215540707745608,
      "grad_norm": 0.038752246648073196,
      "learning_rate": 0.00028124835853016964,
      "loss": 0.0396,
      "step": 3724
    },
    {
      "epoch": 0.9218015342736946,
      "grad_norm": 0.039416369050741196,
      "learning_rate": 0.000281151820148563,
      "loss": 0.0384,
      "step": 3725
    },
    {
      "epoch": 0.9220489977728286,
      "grad_norm": 0.046477049589157104,
      "learning_rate": 0.00028105527704827267,
      "loss": 0.0628,
      "step": 3726
    },
    {
      "epoch": 0.9222964612719624,
      "grad_norm": 0.025583088397979736,
      "learning_rate": 0.00028095872924392245,
      "loss": 0.0236,
      "step": 3727
    },
    {
      "epoch": 0.9225439247710963,
      "grad_norm": 0.07780144363641739,
      "learning_rate": 0.0002808621767501369,
      "loss": 0.0895,
      "step": 3728
    },
    {
      "epoch": 0.9227913882702301,
      "grad_norm": 0.031794410198926926,
      "learning_rate": 0.000280765619581541,
      "loss": 0.0334,
      "step": 3729
    },
    {
      "epoch": 0.9230388517693641,
      "grad_norm": 0.04336371645331383,
      "learning_rate": 0.00028066905775276074,
      "loss": 0.0438,
      "step": 3730
    },
    {
      "epoch": 0.9232863152684979,
      "grad_norm": 0.048503924161195755,
      "learning_rate": 0.0002805724912784227,
      "loss": 0.0446,
      "step": 3731
    },
    {
      "epoch": 0.9235337787676318,
      "grad_norm": 0.057599179446697235,
      "learning_rate": 0.0002804759201731543,
      "loss": 0.1376,
      "step": 3732
    },
    {
      "epoch": 0.9237812422667656,
      "grad_norm": 0.09596066176891327,
      "learning_rate": 0.0002803793444515832,
      "loss": 0.0486,
      "step": 3733
    },
    {
      "epoch": 0.9240287057658996,
      "grad_norm": 0.023084482178092003,
      "learning_rate": 0.0002802827641283383,
      "loss": 0.0297,
      "step": 3734
    },
    {
      "epoch": 0.9242761692650334,
      "grad_norm": 0.027933018282055855,
      "learning_rate": 0.000280186179218049,
      "loss": 0.0369,
      "step": 3735
    },
    {
      "epoch": 0.9245236327641673,
      "grad_norm": 0.060982175171375275,
      "learning_rate": 0.00028008958973534543,
      "loss": 0.0709,
      "step": 3736
    },
    {
      "epoch": 0.9247710962633011,
      "grad_norm": 0.0384097658097744,
      "learning_rate": 0.0002799929956948582,
      "loss": 0.0465,
      "step": 3737
    },
    {
      "epoch": 0.9250185597624351,
      "grad_norm": 0.04396243020892143,
      "learning_rate": 0.00027989639711121887,
      "loss": 0.0449,
      "step": 3738
    },
    {
      "epoch": 0.9252660232615689,
      "grad_norm": 0.041387706995010376,
      "learning_rate": 0.00027979979399905954,
      "loss": 0.0457,
      "step": 3739
    },
    {
      "epoch": 0.9255134867607028,
      "grad_norm": 0.030484722927212715,
      "learning_rate": 0.0002797031863730131,
      "loss": 0.0441,
      "step": 3740
    },
    {
      "epoch": 0.9257609502598366,
      "grad_norm": 0.09658710658550262,
      "learning_rate": 0.000279606574247713,
      "loss": 0.1031,
      "step": 3741
    },
    {
      "epoch": 0.9260084137589706,
      "grad_norm": 0.03563420847058296,
      "learning_rate": 0.00027950995763779366,
      "loss": 0.0548,
      "step": 3742
    },
    {
      "epoch": 0.9262558772581044,
      "grad_norm": 0.054248910397291183,
      "learning_rate": 0.0002794133365578896,
      "loss": 0.0728,
      "step": 3743
    },
    {
      "epoch": 0.9265033407572383,
      "grad_norm": 0.04042825847864151,
      "learning_rate": 0.00027931671102263665,
      "loss": 0.0669,
      "step": 3744
    },
    {
      "epoch": 0.9267508042563722,
      "grad_norm": 0.0330435074865818,
      "learning_rate": 0.00027922008104667095,
      "loss": 0.0348,
      "step": 3745
    },
    {
      "epoch": 0.9269982677555061,
      "grad_norm": 0.04808798432350159,
      "learning_rate": 0.00027912344664462953,
      "loss": 0.063,
      "step": 3746
    },
    {
      "epoch": 0.9272457312546399,
      "grad_norm": 0.052107829600572586,
      "learning_rate": 0.0002790268078311498,
      "loss": 0.1071,
      "step": 3747
    },
    {
      "epoch": 0.9274931947537738,
      "grad_norm": 0.043077923357486725,
      "learning_rate": 0.00027893016462087,
      "loss": 0.0461,
      "step": 3748
    },
    {
      "epoch": 0.9277406582529077,
      "grad_norm": 0.02989727444946766,
      "learning_rate": 0.0002788335170284291,
      "loss": 0.05,
      "step": 3749
    },
    {
      "epoch": 0.9279881217520416,
      "grad_norm": 0.04556617885828018,
      "learning_rate": 0.0002787368650684668,
      "loss": 0.0542,
      "step": 3750
    },
    {
      "epoch": 0.9282355852511754,
      "grad_norm": 0.058111440390348434,
      "learning_rate": 0.00027864020875562317,
      "loss": 0.0912,
      "step": 3751
    },
    {
      "epoch": 0.9284830487503093,
      "grad_norm": 0.042666807770729065,
      "learning_rate": 0.0002785435481045392,
      "loss": 0.0535,
      "step": 3752
    },
    {
      "epoch": 0.9287305122494433,
      "grad_norm": 0.06609299033880234,
      "learning_rate": 0.0002784468831298563,
      "loss": 0.0521,
      "step": 3753
    },
    {
      "epoch": 0.9289779757485771,
      "grad_norm": 0.05670025199651718,
      "learning_rate": 0.00027835021384621685,
      "loss": 0.0944,
      "step": 3754
    },
    {
      "epoch": 0.929225439247711,
      "grad_norm": 0.03563275188207626,
      "learning_rate": 0.0002782535402682636,
      "loss": 0.0336,
      "step": 3755
    },
    {
      "epoch": 0.9294729027468448,
      "grad_norm": 0.04307149350643158,
      "learning_rate": 0.0002781568624106402,
      "loss": 0.1184,
      "step": 3756
    },
    {
      "epoch": 0.9297203662459788,
      "grad_norm": 0.0871143490076065,
      "learning_rate": 0.0002780601802879906,
      "loss": 0.0737,
      "step": 3757
    },
    {
      "epoch": 0.9299678297451126,
      "grad_norm": 0.04805029183626175,
      "learning_rate": 0.00027796349391495976,
      "loss": 0.0487,
      "step": 3758
    },
    {
      "epoch": 0.9302152932442465,
      "grad_norm": 0.04023217409849167,
      "learning_rate": 0.00027786680330619303,
      "loss": 0.0608,
      "step": 3759
    },
    {
      "epoch": 0.9304627567433803,
      "grad_norm": 0.03016405925154686,
      "learning_rate": 0.0002777701084763367,
      "loss": 0.0428,
      "step": 3760
    },
    {
      "epoch": 0.9307102202425143,
      "grad_norm": 0.061032794415950775,
      "learning_rate": 0.0002776734094400372,
      "loss": 0.0476,
      "step": 3761
    },
    {
      "epoch": 0.9309576837416481,
      "grad_norm": 0.025127297267317772,
      "learning_rate": 0.00027757670621194214,
      "loss": 0.0509,
      "step": 3762
    },
    {
      "epoch": 0.931205147240782,
      "grad_norm": 0.03199096396565437,
      "learning_rate": 0.00027747999880669937,
      "loss": 0.0453,
      "step": 3763
    },
    {
      "epoch": 0.9314526107399158,
      "grad_norm": 0.02231556363403797,
      "learning_rate": 0.0002773832872389577,
      "loss": 0.0255,
      "step": 3764
    },
    {
      "epoch": 0.9317000742390498,
      "grad_norm": 0.0272163525223732,
      "learning_rate": 0.0002772865715233662,
      "loss": 0.0477,
      "step": 3765
    },
    {
      "epoch": 0.9319475377381836,
      "grad_norm": 0.03315539285540581,
      "learning_rate": 0.0002771898516745749,
      "loss": 0.0222,
      "step": 3766
    },
    {
      "epoch": 0.9321950012373175,
      "grad_norm": 0.05251189321279526,
      "learning_rate": 0.00027709312770723423,
      "loss": 0.0492,
      "step": 3767
    },
    {
      "epoch": 0.9324424647364514,
      "grad_norm": 0.046361081302165985,
      "learning_rate": 0.00027699639963599536,
      "loss": 0.0442,
      "step": 3768
    },
    {
      "epoch": 0.9326899282355853,
      "grad_norm": 0.042008429765701294,
      "learning_rate": 0.0002768996674755101,
      "loss": 0.0511,
      "step": 3769
    },
    {
      "epoch": 0.9329373917347191,
      "grad_norm": 0.046475157141685486,
      "learning_rate": 0.0002768029312404309,
      "loss": 0.0726,
      "step": 3770
    },
    {
      "epoch": 0.933184855233853,
      "grad_norm": 0.03556036576628685,
      "learning_rate": 0.00027670619094541057,
      "loss": 0.0825,
      "step": 3771
    },
    {
      "epoch": 0.9334323187329869,
      "grad_norm": 0.056523922830820084,
      "learning_rate": 0.00027660944660510287,
      "loss": 0.1015,
      "step": 3772
    },
    {
      "epoch": 0.9336797822321208,
      "grad_norm": 0.04516666382551193,
      "learning_rate": 0.00027651269823416187,
      "loss": 0.0527,
      "step": 3773
    },
    {
      "epoch": 0.9339272457312546,
      "grad_norm": 0.02745635434985161,
      "learning_rate": 0.00027641594584724267,
      "loss": 0.0427,
      "step": 3774
    },
    {
      "epoch": 0.9341747092303885,
      "grad_norm": 0.04704441875219345,
      "learning_rate": 0.0002763191894590005,
      "loss": 0.0532,
      "step": 3775
    },
    {
      "epoch": 0.9344221727295224,
      "grad_norm": 0.050318267196416855,
      "learning_rate": 0.0002762224290840915,
      "loss": 0.0581,
      "step": 3776
    },
    {
      "epoch": 0.9346696362286563,
      "grad_norm": 0.03243562579154968,
      "learning_rate": 0.0002761256647371723,
      "loss": 0.0477,
      "step": 3777
    },
    {
      "epoch": 0.9349170997277901,
      "grad_norm": 0.06289669871330261,
      "learning_rate": 0.00027602889643290023,
      "loss": 0.0543,
      "step": 3778
    },
    {
      "epoch": 0.935164563226924,
      "grad_norm": 0.07387159019708633,
      "learning_rate": 0.0002759321241859331,
      "loss": 0.0235,
      "step": 3779
    },
    {
      "epoch": 0.9354120267260579,
      "grad_norm": 0.08709192276000977,
      "learning_rate": 0.00027583534801092936,
      "loss": 0.0543,
      "step": 3780
    },
    {
      "epoch": 0.9356594902251918,
      "grad_norm": 0.03365423157811165,
      "learning_rate": 0.00027573856792254805,
      "loss": 0.0298,
      "step": 3781
    },
    {
      "epoch": 0.9359069537243256,
      "grad_norm": 0.02170618437230587,
      "learning_rate": 0.0002756417839354488,
      "loss": 0.024,
      "step": 3782
    },
    {
      "epoch": 0.9361544172234595,
      "grad_norm": 0.04324209317564964,
      "learning_rate": 0.0002755449960642919,
      "loss": 0.0527,
      "step": 3783
    },
    {
      "epoch": 0.9364018807225934,
      "grad_norm": 0.053040970116853714,
      "learning_rate": 0.0002754482043237382,
      "loss": 0.0682,
      "step": 3784
    },
    {
      "epoch": 0.9366493442217273,
      "grad_norm": 0.05850011110305786,
      "learning_rate": 0.000275351408728449,
      "loss": 0.0974,
      "step": 3785
    },
    {
      "epoch": 0.9368968077208611,
      "grad_norm": 0.029238156974315643,
      "learning_rate": 0.0002752546092930863,
      "loss": 0.0346,
      "step": 3786
    },
    {
      "epoch": 0.937144271219995,
      "grad_norm": 0.06486111879348755,
      "learning_rate": 0.0002751578060323129,
      "loss": 0.0833,
      "step": 3787
    },
    {
      "epoch": 0.937391734719129,
      "grad_norm": 0.03853238373994827,
      "learning_rate": 0.0002750609989607917,
      "loss": 0.0609,
      "step": 3788
    },
    {
      "epoch": 0.9376391982182628,
      "grad_norm": 0.047132063657045364,
      "learning_rate": 0.00027496418809318647,
      "loss": 0.0897,
      "step": 3789
    },
    {
      "epoch": 0.9378866617173967,
      "grad_norm": 0.04832138121128082,
      "learning_rate": 0.0002748673734441616,
      "loss": 0.0687,
      "step": 3790
    },
    {
      "epoch": 0.9381341252165306,
      "grad_norm": 0.03146980702877045,
      "learning_rate": 0.000274770555028382,
      "loss": 0.0339,
      "step": 3791
    },
    {
      "epoch": 0.9383815887156645,
      "grad_norm": 0.030478132888674736,
      "learning_rate": 0.00027467373286051296,
      "loss": 0.0456,
      "step": 3792
    },
    {
      "epoch": 0.9386290522147983,
      "grad_norm": 0.08005114644765854,
      "learning_rate": 0.0002745769069552206,
      "loss": 0.0779,
      "step": 3793
    },
    {
      "epoch": 0.9388765157139322,
      "grad_norm": 0.09250778704881668,
      "learning_rate": 0.0002744800773271715,
      "loss": 0.108,
      "step": 3794
    },
    {
      "epoch": 0.9391239792130661,
      "grad_norm": 0.03437555581331253,
      "learning_rate": 0.00027438324399103295,
      "loss": 0.0328,
      "step": 3795
    },
    {
      "epoch": 0.9393714427122,
      "grad_norm": 0.08376381546258926,
      "learning_rate": 0.00027428640696147233,
      "loss": 0.1035,
      "step": 3796
    },
    {
      "epoch": 0.9396189062113338,
      "grad_norm": 0.03984188660979271,
      "learning_rate": 0.00027418956625315823,
      "loss": 0.0551,
      "step": 3797
    },
    {
      "epoch": 0.9398663697104677,
      "grad_norm": 0.03717036172747612,
      "learning_rate": 0.00027409272188075925,
      "loss": 0.0389,
      "step": 3798
    },
    {
      "epoch": 0.9401138332096016,
      "grad_norm": 0.07666964083909988,
      "learning_rate": 0.0002739958738589449,
      "loss": 0.0939,
      "step": 3799
    },
    {
      "epoch": 0.9403612967087355,
      "grad_norm": 0.038412660360336304,
      "learning_rate": 0.00027389902220238503,
      "loss": 0.0397,
      "step": 3800
    },
    {
      "epoch": 0.9403612967087355,
      "eval_loss": 0.292087197303772,
      "eval_runtime": 202.5377,
      "eval_samples_per_second": 4.937,
      "eval_steps_per_second": 0.311,
      "step": 3800
    },
    {
      "epoch": 0.9406087602078693,
      "grad_norm": 0.04810366779565811,
      "learning_rate": 0.0002738021669257503,
      "loss": 0.0476,
      "step": 3801
    },
    {
      "epoch": 0.9408562237070032,
      "grad_norm": 0.06494127213954926,
      "learning_rate": 0.0002737053080437116,
      "loss": 0.0512,
      "step": 3802
    },
    {
      "epoch": 0.9411036872061371,
      "grad_norm": 0.09157708287239075,
      "learning_rate": 0.00027360844557094044,
      "loss": 0.0565,
      "step": 3803
    },
    {
      "epoch": 0.941351150705271,
      "grad_norm": 0.08058518916368484,
      "learning_rate": 0.00027351157952210906,
      "loss": 0.0632,
      "step": 3804
    },
    {
      "epoch": 0.9415986142044048,
      "grad_norm": 0.06362111866474152,
      "learning_rate": 0.00027341470991189016,
      "loss": 0.0483,
      "step": 3805
    },
    {
      "epoch": 0.9418460777035387,
      "grad_norm": 0.043757542967796326,
      "learning_rate": 0.0002733178367549568,
      "loss": 0.0518,
      "step": 3806
    },
    {
      "epoch": 0.9420935412026726,
      "grad_norm": 0.0329570509493351,
      "learning_rate": 0.0002732209600659829,
      "loss": 0.068,
      "step": 3807
    },
    {
      "epoch": 0.9423410047018065,
      "grad_norm": 0.1368408203125,
      "learning_rate": 0.0002731240798596425,
      "loss": 0.0584,
      "step": 3808
    },
    {
      "epoch": 0.9425884682009403,
      "grad_norm": 0.0398695282638073,
      "learning_rate": 0.00027302719615061074,
      "loss": 0.0476,
      "step": 3809
    },
    {
      "epoch": 0.9428359317000743,
      "grad_norm": 0.08117951452732086,
      "learning_rate": 0.0002729303089535627,
      "loss": 0.0568,
      "step": 3810
    },
    {
      "epoch": 0.9430833951992081,
      "grad_norm": 0.035656437277793884,
      "learning_rate": 0.0002728334182831743,
      "loss": 0.0497,
      "step": 3811
    },
    {
      "epoch": 0.943330858698342,
      "grad_norm": 0.041106753051280975,
      "learning_rate": 0.000272736524154122,
      "loss": 0.0764,
      "step": 3812
    },
    {
      "epoch": 0.9435783221974758,
      "grad_norm": 0.04165680333971977,
      "learning_rate": 0.0002726396265810827,
      "loss": 0.0615,
      "step": 3813
    },
    {
      "epoch": 0.9438257856966098,
      "grad_norm": 0.04521676525473595,
      "learning_rate": 0.0002725427255787337,
      "loss": 0.0559,
      "step": 3814
    },
    {
      "epoch": 0.9440732491957436,
      "grad_norm": 0.04321299120783806,
      "learning_rate": 0.0002724458211617532,
      "loss": 0.0992,
      "step": 3815
    },
    {
      "epoch": 0.9443207126948775,
      "grad_norm": 0.030958084389567375,
      "learning_rate": 0.0002723489133448195,
      "loss": 0.0431,
      "step": 3816
    },
    {
      "epoch": 0.9445681761940113,
      "grad_norm": 0.04070119932293892,
      "learning_rate": 0.0002722520021426117,
      "loss": 0.0454,
      "step": 3817
    },
    {
      "epoch": 0.9448156396931453,
      "grad_norm": 0.02756941318511963,
      "learning_rate": 0.0002721550875698091,
      "loss": 0.04,
      "step": 3818
    },
    {
      "epoch": 0.9450631031922792,
      "grad_norm": 0.03486606106162071,
      "learning_rate": 0.00027205816964109207,
      "loss": 0.0311,
      "step": 3819
    },
    {
      "epoch": 0.945310566691413,
      "grad_norm": 0.030396558344364166,
      "learning_rate": 0.00027196124837114085,
      "loss": 0.0307,
      "step": 3820
    },
    {
      "epoch": 0.9455580301905468,
      "grad_norm": 0.03373820334672928,
      "learning_rate": 0.0002718643237746365,
      "loss": 0.0305,
      "step": 3821
    },
    {
      "epoch": 0.9458054936896808,
      "grad_norm": 0.07487089931964874,
      "learning_rate": 0.0002717673958662607,
      "loss": 0.0543,
      "step": 3822
    },
    {
      "epoch": 0.9460529571888147,
      "grad_norm": 0.03801766410470009,
      "learning_rate": 0.00027167046466069534,
      "loss": 0.0421,
      "step": 3823
    },
    {
      "epoch": 0.9463004206879485,
      "grad_norm": 0.026684653013944626,
      "learning_rate": 0.00027157353017262303,
      "loss": 0.0381,
      "step": 3824
    },
    {
      "epoch": 0.9465478841870824,
      "grad_norm": 0.038509875535964966,
      "learning_rate": 0.00027147659241672683,
      "loss": 0.052,
      "step": 3825
    },
    {
      "epoch": 0.9467953476862163,
      "grad_norm": 0.03395827114582062,
      "learning_rate": 0.0002713796514076902,
      "loss": 0.0355,
      "step": 3826
    },
    {
      "epoch": 0.9470428111853502,
      "grad_norm": 0.04415019974112511,
      "learning_rate": 0.000271282707160197,
      "loss": 0.0578,
      "step": 3827
    },
    {
      "epoch": 0.947290274684484,
      "grad_norm": 0.028224781155586243,
      "learning_rate": 0.0002711857596889321,
      "loss": 0.1166,
      "step": 3828
    },
    {
      "epoch": 0.9475377381836179,
      "grad_norm": 0.03408017382025719,
      "learning_rate": 0.00027108880900858035,
      "loss": 0.0336,
      "step": 3829
    },
    {
      "epoch": 0.9477852016827518,
      "grad_norm": 0.03865210339426994,
      "learning_rate": 0.00027099185513382713,
      "loss": 0.0837,
      "step": 3830
    },
    {
      "epoch": 0.9480326651818857,
      "grad_norm": 0.04711972549557686,
      "learning_rate": 0.0002708948980793585,
      "loss": 0.0532,
      "step": 3831
    },
    {
      "epoch": 0.9482801286810195,
      "grad_norm": 0.02553107962012291,
      "learning_rate": 0.00027079793785986077,
      "loss": 0.0406,
      "step": 3832
    },
    {
      "epoch": 0.9485275921801535,
      "grad_norm": 0.041239939630031586,
      "learning_rate": 0.0002707009744900212,
      "loss": 0.0607,
      "step": 3833
    },
    {
      "epoch": 0.9487750556792873,
      "grad_norm": 0.028210096061229706,
      "learning_rate": 0.00027060400798452686,
      "loss": 0.0388,
      "step": 3834
    },
    {
      "epoch": 0.9490225191784212,
      "grad_norm": 0.052814800292253494,
      "learning_rate": 0.00027050703835806576,
      "loss": 0.0528,
      "step": 3835
    },
    {
      "epoch": 0.949269982677555,
      "grad_norm": 0.05705508589744568,
      "learning_rate": 0.0002704100656253263,
      "loss": 0.0533,
      "step": 3836
    },
    {
      "epoch": 0.949517446176689,
      "grad_norm": 0.02193387970328331,
      "learning_rate": 0.00027031308980099726,
      "loss": 0.0199,
      "step": 3837
    },
    {
      "epoch": 0.9497649096758228,
      "grad_norm": 0.0256638266146183,
      "learning_rate": 0.00027021611089976793,
      "loss": 0.0399,
      "step": 3838
    },
    {
      "epoch": 0.9500123731749567,
      "grad_norm": 0.0810638815164566,
      "learning_rate": 0.00027011912893632807,
      "loss": 0.093,
      "step": 3839
    },
    {
      "epoch": 0.9502598366740905,
      "grad_norm": 0.028301341459155083,
      "learning_rate": 0.00027002214392536796,
      "loss": 0.0354,
      "step": 3840
    },
    {
      "epoch": 0.9505073001732245,
      "grad_norm": 0.05055158585309982,
      "learning_rate": 0.00026992515588157817,
      "loss": 0.0338,
      "step": 3841
    },
    {
      "epoch": 0.9507547636723583,
      "grad_norm": 0.0355767160654068,
      "learning_rate": 0.0002698281648196499,
      "loss": 0.0457,
      "step": 3842
    },
    {
      "epoch": 0.9510022271714922,
      "grad_norm": 0.04804877191781998,
      "learning_rate": 0.00026973117075427476,
      "loss": 0.0437,
      "step": 3843
    },
    {
      "epoch": 0.951249690670626,
      "grad_norm": 0.02310306206345558,
      "learning_rate": 0.00026963417370014486,
      "loss": 0.0386,
      "step": 3844
    },
    {
      "epoch": 0.95149715416976,
      "grad_norm": 0.03240377828478813,
      "learning_rate": 0.0002695371736719526,
      "loss": 0.0472,
      "step": 3845
    },
    {
      "epoch": 0.9517446176688938,
      "grad_norm": 0.04458369314670563,
      "learning_rate": 0.00026944017068439107,
      "loss": 0.0399,
      "step": 3846
    },
    {
      "epoch": 0.9519920811680277,
      "grad_norm": 0.046504031866788864,
      "learning_rate": 0.00026934316475215356,
      "loss": 0.0467,
      "step": 3847
    },
    {
      "epoch": 0.9522395446671615,
      "grad_norm": 0.032650597393512726,
      "learning_rate": 0.000269246155889934,
      "loss": 0.0581,
      "step": 3848
    },
    {
      "epoch": 0.9524870081662955,
      "grad_norm": 0.05342574417591095,
      "learning_rate": 0.0002691491441124266,
      "loss": 0.0571,
      "step": 3849
    },
    {
      "epoch": 0.9527344716654293,
      "grad_norm": 0.05234880372881889,
      "learning_rate": 0.0002690521294343262,
      "loss": 0.0306,
      "step": 3850
    },
    {
      "epoch": 0.9529819351645632,
      "grad_norm": 0.05556529387831688,
      "learning_rate": 0.00026895511187032797,
      "loss": 0.0899,
      "step": 3851
    },
    {
      "epoch": 0.9532293986636972,
      "grad_norm": 0.027847889810800552,
      "learning_rate": 0.0002688580914351274,
      "loss": 0.036,
      "step": 3852
    },
    {
      "epoch": 0.953476862162831,
      "grad_norm": 0.030024074018001556,
      "learning_rate": 0.00026876106814342066,
      "loss": 0.037,
      "step": 3853
    },
    {
      "epoch": 0.9537243256619649,
      "grad_norm": 0.0348537303507328,
      "learning_rate": 0.00026866404200990424,
      "loss": 0.0591,
      "step": 3854
    },
    {
      "epoch": 0.9539717891610987,
      "grad_norm": 0.09340909868478775,
      "learning_rate": 0.000268567013049275,
      "loss": 0.0874,
      "step": 3855
    },
    {
      "epoch": 0.9542192526602327,
      "grad_norm": 0.082057423889637,
      "learning_rate": 0.0002684699812762303,
      "loss": 0.0659,
      "step": 3856
    },
    {
      "epoch": 0.9544667161593665,
      "grad_norm": 0.02505819872021675,
      "learning_rate": 0.00026837294670546794,
      "loss": 0.0316,
      "step": 3857
    },
    {
      "epoch": 0.9547141796585004,
      "grad_norm": 0.034362249076366425,
      "learning_rate": 0.0002682759093516861,
      "loss": 0.0427,
      "step": 3858
    },
    {
      "epoch": 0.9549616431576342,
      "grad_norm": 0.04829096794128418,
      "learning_rate": 0.00026817886922958327,
      "loss": 0.0463,
      "step": 3859
    },
    {
      "epoch": 0.9552091066567682,
      "grad_norm": 0.04480405151844025,
      "learning_rate": 0.00026808182635385875,
      "loss": 0.0706,
      "step": 3860
    },
    {
      "epoch": 0.955456570155902,
      "grad_norm": 0.021062346175312996,
      "learning_rate": 0.00026798478073921175,
      "loss": 0.0375,
      "step": 3861
    },
    {
      "epoch": 0.9557040336550359,
      "grad_norm": 0.04101051762700081,
      "learning_rate": 0.00026788773240034216,
      "loss": 0.0467,
      "step": 3862
    },
    {
      "epoch": 0.9559514971541697,
      "grad_norm": 0.03081304393708706,
      "learning_rate": 0.00026779068135195035,
      "loss": 0.0463,
      "step": 3863
    },
    {
      "epoch": 0.9561989606533037,
      "grad_norm": 0.03182263299822807,
      "learning_rate": 0.000267693627608737,
      "loss": 0.0378,
      "step": 3864
    },
    {
      "epoch": 0.9564464241524375,
      "grad_norm": 0.033718209713697433,
      "learning_rate": 0.00026759657118540317,
      "loss": 0.0532,
      "step": 3865
    },
    {
      "epoch": 0.9566938876515714,
      "grad_norm": 0.0384630486369133,
      "learning_rate": 0.0002674995120966504,
      "loss": 0.0482,
      "step": 3866
    },
    {
      "epoch": 0.9569413511507052,
      "grad_norm": 0.05819743871688843,
      "learning_rate": 0.00026740245035718045,
      "loss": 0.0383,
      "step": 3867
    },
    {
      "epoch": 0.9571888146498392,
      "grad_norm": 0.02664116397500038,
      "learning_rate": 0.00026730538598169584,
      "loss": 0.0274,
      "step": 3868
    },
    {
      "epoch": 0.957436278148973,
      "grad_norm": 0.035076990723609924,
      "learning_rate": 0.00026720831898489916,
      "loss": 0.0762,
      "step": 3869
    },
    {
      "epoch": 0.9576837416481069,
      "grad_norm": 0.02704058215022087,
      "learning_rate": 0.0002671112493814936,
      "loss": 0.0425,
      "step": 3870
    },
    {
      "epoch": 0.9579312051472407,
      "grad_norm": 0.036692067980766296,
      "learning_rate": 0.00026701417718618254,
      "loss": 0.0417,
      "step": 3871
    },
    {
      "epoch": 0.9581786686463747,
      "grad_norm": 0.03474491089582443,
      "learning_rate": 0.0002669171024136699,
      "loss": 0.0531,
      "step": 3872
    },
    {
      "epoch": 0.9584261321455085,
      "grad_norm": 0.09354110062122345,
      "learning_rate": 0.00026682002507866,
      "loss": 0.0877,
      "step": 3873
    },
    {
      "epoch": 0.9586735956446424,
      "grad_norm": 0.0424899198114872,
      "learning_rate": 0.0002667229451958575,
      "loss": 0.0729,
      "step": 3874
    },
    {
      "epoch": 0.9589210591437763,
      "grad_norm": 0.025743158534169197,
      "learning_rate": 0.00026662586277996747,
      "loss": 0.0157,
      "step": 3875
    },
    {
      "epoch": 0.9591685226429102,
      "grad_norm": 0.02685108408331871,
      "learning_rate": 0.00026652877784569533,
      "loss": 0.0334,
      "step": 3876
    },
    {
      "epoch": 0.959415986142044,
      "grad_norm": 0.02893228642642498,
      "learning_rate": 0.00026643169040774684,
      "loss": 0.0268,
      "step": 3877
    },
    {
      "epoch": 0.9596634496411779,
      "grad_norm": 0.06457052379846573,
      "learning_rate": 0.0002663346004808284,
      "loss": 0.0851,
      "step": 3878
    },
    {
      "epoch": 0.9599109131403119,
      "grad_norm": 0.03163319081068039,
      "learning_rate": 0.0002662375080796463,
      "loss": 0.0487,
      "step": 3879
    },
    {
      "epoch": 0.9601583766394457,
      "grad_norm": 0.03083273209631443,
      "learning_rate": 0.00026614041321890776,
      "loss": 0.0314,
      "step": 3880
    },
    {
      "epoch": 0.9604058401385795,
      "grad_norm": 0.049483753740787506,
      "learning_rate": 0.0002660433159133199,
      "loss": 0.0859,
      "step": 3881
    },
    {
      "epoch": 0.9606533036377134,
      "grad_norm": 0.03844766318798065,
      "learning_rate": 0.0002659462161775905,
      "loss": 0.0572,
      "step": 3882
    },
    {
      "epoch": 0.9609007671368474,
      "grad_norm": 0.028499405831098557,
      "learning_rate": 0.0002658491140264277,
      "loss": 0.0468,
      "step": 3883
    },
    {
      "epoch": 0.9611482306359812,
      "grad_norm": 0.027627700939774513,
      "learning_rate": 0.0002657520094745398,
      "loss": 0.0278,
      "step": 3884
    },
    {
      "epoch": 0.961395694135115,
      "grad_norm": 0.0332038551568985,
      "learning_rate": 0.00026565490253663564,
      "loss": 0.0415,
      "step": 3885
    },
    {
      "epoch": 0.9616431576342489,
      "grad_norm": 0.03255773335695267,
      "learning_rate": 0.0002655577932274244,
      "loss": 0.0342,
      "step": 3886
    },
    {
      "epoch": 0.9618906211333829,
      "grad_norm": 0.05649880692362785,
      "learning_rate": 0.00026546068156161555,
      "loss": 0.0696,
      "step": 3887
    },
    {
      "epoch": 0.9621380846325167,
      "grad_norm": 0.037120189517736435,
      "learning_rate": 0.00026536356755391913,
      "loss": 0.0295,
      "step": 3888
    },
    {
      "epoch": 0.9623855481316506,
      "grad_norm": 0.026691587641835213,
      "learning_rate": 0.00026526645121904504,
      "loss": 0.0402,
      "step": 3889
    },
    {
      "epoch": 0.9626330116307844,
      "grad_norm": 0.0715339332818985,
      "learning_rate": 0.00026516933257170406,
      "loss": 0.0396,
      "step": 3890
    },
    {
      "epoch": 0.9628804751299184,
      "grad_norm": 0.06135562062263489,
      "learning_rate": 0.0002650722116266071,
      "loss": 0.0863,
      "step": 3891
    },
    {
      "epoch": 0.9631279386290522,
      "grad_norm": 0.03475521132349968,
      "learning_rate": 0.0002649750883984655,
      "loss": 0.0677,
      "step": 3892
    },
    {
      "epoch": 0.9633754021281861,
      "grad_norm": 0.043271664530038834,
      "learning_rate": 0.00026487796290199073,
      "loss": 0.0589,
      "step": 3893
    },
    {
      "epoch": 0.96362286562732,
      "grad_norm": 0.03711339458823204,
      "learning_rate": 0.0002647808351518949,
      "loss": 0.0242,
      "step": 3894
    },
    {
      "epoch": 0.9638703291264539,
      "grad_norm": 0.06391856074333191,
      "learning_rate": 0.00026468370516289025,
      "loss": 0.0598,
      "step": 3895
    },
    {
      "epoch": 0.9641177926255877,
      "grad_norm": 0.06257625669240952,
      "learning_rate": 0.00026458657294968935,
      "loss": 0.0888,
      "step": 3896
    },
    {
      "epoch": 0.9643652561247216,
      "grad_norm": 0.049301400780677795,
      "learning_rate": 0.0002644894385270053,
      "loss": 0.085,
      "step": 3897
    },
    {
      "epoch": 0.9646127196238555,
      "grad_norm": 0.04092220589518547,
      "learning_rate": 0.00026439230190955135,
      "loss": 0.0225,
      "step": 3898
    },
    {
      "epoch": 0.9648601831229894,
      "grad_norm": 0.038097135722637177,
      "learning_rate": 0.0002642951631120413,
      "loss": 0.0217,
      "step": 3899
    },
    {
      "epoch": 0.9651076466221232,
      "grad_norm": 0.03674677014350891,
      "learning_rate": 0.0002641980221491889,
      "loss": 0.0276,
      "step": 3900
    },
    {
      "epoch": 0.9653551101212571,
      "grad_norm": 0.05215861648321152,
      "learning_rate": 0.00026410087903570856,
      "loss": 0.0936,
      "step": 3901
    },
    {
      "epoch": 0.965602573620391,
      "grad_norm": 0.05248233675956726,
      "learning_rate": 0.0002640037337863149,
      "loss": 0.0899,
      "step": 3902
    },
    {
      "epoch": 0.9658500371195249,
      "grad_norm": 0.03200202062726021,
      "learning_rate": 0.000263906586415723,
      "loss": 0.0391,
      "step": 3903
    },
    {
      "epoch": 0.9660975006186587,
      "grad_norm": 0.03279602527618408,
      "learning_rate": 0.00026380943693864796,
      "loss": 0.0462,
      "step": 3904
    },
    {
      "epoch": 0.9663449641177926,
      "grad_norm": 0.03924598917365074,
      "learning_rate": 0.0002637122853698055,
      "loss": 0.0949,
      "step": 3905
    },
    {
      "epoch": 0.9665924276169265,
      "grad_norm": 0.028410278260707855,
      "learning_rate": 0.0002636151317239114,
      "loss": 0.0332,
      "step": 3906
    },
    {
      "epoch": 0.9668398911160604,
      "grad_norm": 0.029081175103783607,
      "learning_rate": 0.000263517976015682,
      "loss": 0.0362,
      "step": 3907
    },
    {
      "epoch": 0.9670873546151942,
      "grad_norm": 0.04249626025557518,
      "learning_rate": 0.00026342081825983385,
      "loss": 0.0525,
      "step": 3908
    },
    {
      "epoch": 0.9673348181143281,
      "grad_norm": 0.0318472795188427,
      "learning_rate": 0.0002633236584710838,
      "loss": 0.0387,
      "step": 3909
    },
    {
      "epoch": 0.967582281613462,
      "grad_norm": 0.03583240136504173,
      "learning_rate": 0.00026322649666414886,
      "loss": 0.0418,
      "step": 3910
    },
    {
      "epoch": 0.9678297451125959,
      "grad_norm": 0.02668687142431736,
      "learning_rate": 0.0002631293328537467,
      "loss": 0.0287,
      "step": 3911
    },
    {
      "epoch": 0.9680772086117297,
      "grad_norm": 0.05954049527645111,
      "learning_rate": 0.000263032167054595,
      "loss": 0.0598,
      "step": 3912
    },
    {
      "epoch": 0.9683246721108636,
      "grad_norm": 0.03242543712258339,
      "learning_rate": 0.00026293499928141184,
      "loss": 0.0479,
      "step": 3913
    },
    {
      "epoch": 0.9685721356099976,
      "grad_norm": 0.06294508278369904,
      "learning_rate": 0.0002628378295489155,
      "loss": 0.0511,
      "step": 3914
    },
    {
      "epoch": 0.9688195991091314,
      "grad_norm": 0.052285969257354736,
      "learning_rate": 0.00026274065787182486,
      "loss": 0.0688,
      "step": 3915
    },
    {
      "epoch": 0.9690670626082653,
      "grad_norm": 0.058582428842782974,
      "learning_rate": 0.0002626434842648586,
      "loss": 0.0759,
      "step": 3916
    },
    {
      "epoch": 0.9693145261073992,
      "grad_norm": 0.04029868543148041,
      "learning_rate": 0.0002625463087427362,
      "loss": 0.068,
      "step": 3917
    },
    {
      "epoch": 0.9695619896065331,
      "grad_norm": 0.036435890942811966,
      "learning_rate": 0.00026244913132017706,
      "loss": 0.0383,
      "step": 3918
    },
    {
      "epoch": 0.9698094531056669,
      "grad_norm": 0.0578017421066761,
      "learning_rate": 0.00026235195201190114,
      "loss": 0.0754,
      "step": 3919
    },
    {
      "epoch": 0.9700569166048008,
      "grad_norm": 0.030804021283984184,
      "learning_rate": 0.0002622547708326284,
      "loss": 0.0315,
      "step": 3920
    },
    {
      "epoch": 0.9703043801039347,
      "grad_norm": 0.04179300367832184,
      "learning_rate": 0.00026215758779707946,
      "loss": 0.0935,
      "step": 3921
    },
    {
      "epoch": 0.9705518436030686,
      "grad_norm": 0.03576100990176201,
      "learning_rate": 0.0002620604029199747,
      "loss": 0.0584,
      "step": 3922
    },
    {
      "epoch": 0.9707993071022024,
      "grad_norm": 0.04640800133347511,
      "learning_rate": 0.0002619632162160354,
      "loss": 0.0339,
      "step": 3923
    },
    {
      "epoch": 0.9710467706013363,
      "grad_norm": 0.044788580387830734,
      "learning_rate": 0.0002618660276999826,
      "loss": 0.0652,
      "step": 3924
    },
    {
      "epoch": 0.9712942341004702,
      "grad_norm": 0.036081839352846146,
      "learning_rate": 0.00026176883738653783,
      "loss": 0.0327,
      "step": 3925
    },
    {
      "epoch": 0.9715416975996041,
      "grad_norm": 0.031212303787469864,
      "learning_rate": 0.00026167164529042287,
      "loss": 0.0415,
      "step": 3926
    },
    {
      "epoch": 0.9717891610987379,
      "grad_norm": 0.03842180222272873,
      "learning_rate": 0.0002615744514263598,
      "loss": 0.07,
      "step": 3927
    },
    {
      "epoch": 0.9720366245978718,
      "grad_norm": 0.0644618347287178,
      "learning_rate": 0.000261477255809071,
      "loss": 0.0395,
      "step": 3928
    },
    {
      "epoch": 0.9722840880970057,
      "grad_norm": 0.03460933640599251,
      "learning_rate": 0.000261380058453279,
      "loss": 0.048,
      "step": 3929
    },
    {
      "epoch": 0.9725315515961396,
      "grad_norm": 0.041019540280103683,
      "learning_rate": 0.0002612828593737066,
      "loss": 0.0594,
      "step": 3930
    },
    {
      "epoch": 0.9727790150952734,
      "grad_norm": 0.047863442450761795,
      "learning_rate": 0.000261185658585077,
      "loss": 0.0638,
      "step": 3931
    },
    {
      "epoch": 0.9730264785944073,
      "grad_norm": 0.03704773634672165,
      "learning_rate": 0.00026108845610211347,
      "loss": 0.0498,
      "step": 3932
    },
    {
      "epoch": 0.9732739420935412,
      "grad_norm": 0.037387292832136154,
      "learning_rate": 0.0002609912519395398,
      "loss": 0.0454,
      "step": 3933
    },
    {
      "epoch": 0.9735214055926751,
      "grad_norm": 0.05212124437093735,
      "learning_rate": 0.00026089404611207966,
      "loss": 0.0493,
      "step": 3934
    },
    {
      "epoch": 0.9737688690918089,
      "grad_norm": 0.05727126821875572,
      "learning_rate": 0.0002607968386344573,
      "loss": 0.0679,
      "step": 3935
    },
    {
      "epoch": 0.9740163325909429,
      "grad_norm": 0.032723668962717056,
      "learning_rate": 0.0002606996295213971,
      "loss": 0.0599,
      "step": 3936
    },
    {
      "epoch": 0.9742637960900767,
      "grad_norm": 0.031063539907336235,
      "learning_rate": 0.00026060241878762374,
      "loss": 0.0327,
      "step": 3937
    },
    {
      "epoch": 0.9745112595892106,
      "grad_norm": 0.036911796778440475,
      "learning_rate": 0.000260505206447862,
      "loss": 0.0496,
      "step": 3938
    },
    {
      "epoch": 0.9747587230883444,
      "grad_norm": 0.028424391523003578,
      "learning_rate": 0.00026040799251683704,
      "loss": 0.0431,
      "step": 3939
    },
    {
      "epoch": 0.9750061865874784,
      "grad_norm": 0.05801880732178688,
      "learning_rate": 0.00026031077700927423,
      "loss": 0.059,
      "step": 3940
    },
    {
      "epoch": 0.9752536500866122,
      "grad_norm": 0.03331974148750305,
      "learning_rate": 0.00026021355993989916,
      "loss": 0.0405,
      "step": 3941
    },
    {
      "epoch": 0.9755011135857461,
      "grad_norm": 0.038058340549468994,
      "learning_rate": 0.00026011634132343763,
      "loss": 0.043,
      "step": 3942
    },
    {
      "epoch": 0.9757485770848799,
      "grad_norm": 0.030018066987395287,
      "learning_rate": 0.0002600191211746158,
      "loss": 0.0219,
      "step": 3943
    },
    {
      "epoch": 0.9759960405840139,
      "grad_norm": 0.026250306516885757,
      "learning_rate": 0.00025992189950815994,
      "loss": 0.041,
      "step": 3944
    },
    {
      "epoch": 0.9762435040831478,
      "grad_norm": 0.04496731981635094,
      "learning_rate": 0.00025982467633879646,
      "loss": 0.043,
      "step": 3945
    },
    {
      "epoch": 0.9764909675822816,
      "grad_norm": 0.04184640198945999,
      "learning_rate": 0.0002597274516812522,
      "loss": 0.0675,
      "step": 3946
    },
    {
      "epoch": 0.9767384310814154,
      "grad_norm": 0.051552701741456985,
      "learning_rate": 0.00025963022555025426,
      "loss": 0.036,
      "step": 3947
    },
    {
      "epoch": 0.9769858945805494,
      "grad_norm": 0.0367719829082489,
      "learning_rate": 0.00025953299796052973,
      "loss": 0.0412,
      "step": 3948
    },
    {
      "epoch": 0.9772333580796833,
      "grad_norm": 0.0330343171954155,
      "learning_rate": 0.0002594357689268061,
      "loss": 0.0488,
      "step": 3949
    },
    {
      "epoch": 0.9774808215788171,
      "grad_norm": 0.04323523864150047,
      "learning_rate": 0.00025933853846381094,
      "loss": 0.0524,
      "step": 3950
    },
    {
      "epoch": 0.977728285077951,
      "grad_norm": 0.03280399367213249,
      "learning_rate": 0.0002592413065862721,
      "loss": 0.034,
      "step": 3951
    },
    {
      "epoch": 0.9779757485770849,
      "grad_norm": 0.04127269983291626,
      "learning_rate": 0.0002591440733089178,
      "loss": 0.0575,
      "step": 3952
    },
    {
      "epoch": 0.9782232120762188,
      "grad_norm": 0.03014710359275341,
      "learning_rate": 0.00025904683864647624,
      "loss": 0.0787,
      "step": 3953
    },
    {
      "epoch": 0.9784706755753526,
      "grad_norm": 0.05331893265247345,
      "learning_rate": 0.00025894960261367594,
      "loss": 0.0779,
      "step": 3954
    },
    {
      "epoch": 0.9787181390744865,
      "grad_norm": 0.05765842646360397,
      "learning_rate": 0.0002588523652252455,
      "loss": 0.0728,
      "step": 3955
    },
    {
      "epoch": 0.9789656025736204,
      "grad_norm": 0.029986228793859482,
      "learning_rate": 0.00025875512649591406,
      "loss": 0.046,
      "step": 3956
    },
    {
      "epoch": 0.9792130660727543,
      "grad_norm": 0.07178862392902374,
      "learning_rate": 0.00025865788644041047,
      "loss": 0.0852,
      "step": 3957
    },
    {
      "epoch": 0.9794605295718881,
      "grad_norm": 0.030306851491332054,
      "learning_rate": 0.00025856064507346434,
      "loss": 0.0419,
      "step": 3958
    },
    {
      "epoch": 0.9797079930710221,
      "grad_norm": 0.02337048016488552,
      "learning_rate": 0.0002584634024098049,
      "loss": 0.0215,
      "step": 3959
    },
    {
      "epoch": 0.9799554565701559,
      "grad_norm": 0.07489633560180664,
      "learning_rate": 0.000258366158464162,
      "loss": 0.0635,
      "step": 3960
    },
    {
      "epoch": 0.9802029200692898,
      "grad_norm": 0.04981861263513565,
      "learning_rate": 0.0002582689132512656,
      "loss": 0.0626,
      "step": 3961
    },
    {
      "epoch": 0.9804503835684236,
      "grad_norm": 0.057709142565727234,
      "learning_rate": 0.00025817166678584574,
      "loss": 0.091,
      "step": 3962
    },
    {
      "epoch": 0.9806978470675576,
      "grad_norm": 0.05516841635107994,
      "learning_rate": 0.0002580744190826327,
      "loss": 0.047,
      "step": 3963
    },
    {
      "epoch": 0.9809453105666914,
      "grad_norm": 0.03968621790409088,
      "learning_rate": 0.00025797717015635694,
      "loss": 0.0938,
      "step": 3964
    },
    {
      "epoch": 0.9811927740658253,
      "grad_norm": 0.0232058297842741,
      "learning_rate": 0.0002578799200217491,
      "loss": 0.0312,
      "step": 3965
    },
    {
      "epoch": 0.9814402375649591,
      "grad_norm": 0.049952343106269836,
      "learning_rate": 0.0002577826686935402,
      "loss": 0.1102,
      "step": 3966
    },
    {
      "epoch": 0.9816877010640931,
      "grad_norm": 0.0503806434571743,
      "learning_rate": 0.000257685416186461,
      "loss": 0.0317,
      "step": 3967
    },
    {
      "epoch": 0.9819351645632269,
      "grad_norm": 0.026819005608558655,
      "learning_rate": 0.00025758816251524296,
      "loss": 0.0419,
      "step": 3968
    },
    {
      "epoch": 0.9821826280623608,
      "grad_norm": 0.050974756479263306,
      "learning_rate": 0.00025749090769461725,
      "loss": 0.0627,
      "step": 3969
    },
    {
      "epoch": 0.9824300915614946,
      "grad_norm": 0.028621789067983627,
      "learning_rate": 0.0002573936517393156,
      "loss": 0.0502,
      "step": 3970
    },
    {
      "epoch": 0.9826775550606286,
      "grad_norm": 0.0575203113257885,
      "learning_rate": 0.0002572963946640695,
      "loss": 0.0467,
      "step": 3971
    },
    {
      "epoch": 0.9829250185597624,
      "grad_norm": 0.028424717485904694,
      "learning_rate": 0.00025719913648361114,
      "loss": 0.0212,
      "step": 3972
    },
    {
      "epoch": 0.9831724820588963,
      "grad_norm": 0.03313787281513214,
      "learning_rate": 0.00025710187721267243,
      "loss": 0.026,
      "step": 3973
    },
    {
      "epoch": 0.9834199455580301,
      "grad_norm": 0.04959508776664734,
      "learning_rate": 0.00025700461686598556,
      "loss": 0.039,
      "step": 3974
    },
    {
      "epoch": 0.9836674090571641,
      "grad_norm": 0.08575943857431412,
      "learning_rate": 0.00025690735545828294,
      "loss": 0.0955,
      "step": 3975
    },
    {
      "epoch": 0.983914872556298,
      "grad_norm": 0.05173584073781967,
      "learning_rate": 0.0002568100930042972,
      "loss": 0.0802,
      "step": 3976
    },
    {
      "epoch": 0.9841623360554318,
      "grad_norm": 0.15450309216976166,
      "learning_rate": 0.00025671282951876095,
      "loss": 0.0972,
      "step": 3977
    },
    {
      "epoch": 0.9844097995545658,
      "grad_norm": 0.033633992075920105,
      "learning_rate": 0.0002566155650164072,
      "loss": 0.0305,
      "step": 3978
    },
    {
      "epoch": 0.9846572630536996,
      "grad_norm": 0.04885895177721977,
      "learning_rate": 0.0002565182995119688,
      "loss": 0.0906,
      "step": 3979
    },
    {
      "epoch": 0.9849047265528335,
      "grad_norm": 0.08194766193628311,
      "learning_rate": 0.00025642103302017904,
      "loss": 0.1363,
      "step": 3980
    },
    {
      "epoch": 0.9851521900519673,
      "grad_norm": 0.06291204690933228,
      "learning_rate": 0.00025632376555577115,
      "loss": 0.1114,
      "step": 3981
    },
    {
      "epoch": 0.9853996535511013,
      "grad_norm": 0.12484254688024521,
      "learning_rate": 0.0002562264971334787,
      "loss": 0.1579,
      "step": 3982
    },
    {
      "epoch": 0.9856471170502351,
      "grad_norm": 0.056972239166498184,
      "learning_rate": 0.00025612922776803524,
      "loss": 0.0477,
      "step": 3983
    },
    {
      "epoch": 0.985894580549369,
      "grad_norm": 0.026771530508995056,
      "learning_rate": 0.0002560319574741746,
      "loss": 0.0372,
      "step": 3984
    },
    {
      "epoch": 0.9861420440485028,
      "grad_norm": 0.07442040741443634,
      "learning_rate": 0.0002559346862666305,
      "loss": 0.056,
      "step": 3985
    },
    {
      "epoch": 0.9863895075476368,
      "grad_norm": 0.04113194718956947,
      "learning_rate": 0.0002558374141601373,
      "loss": 0.0628,
      "step": 3986
    },
    {
      "epoch": 0.9866369710467706,
      "grad_norm": 0.02580890618264675,
      "learning_rate": 0.0002557401411694288,
      "loss": 0.043,
      "step": 3987
    },
    {
      "epoch": 0.9868844345459045,
      "grad_norm": 0.07168138027191162,
      "learning_rate": 0.0002556428673092395,
      "loss": 0.0633,
      "step": 3988
    },
    {
      "epoch": 0.9871318980450383,
      "grad_norm": 0.041350264102220535,
      "learning_rate": 0.00025554559259430396,
      "loss": 0.0349,
      "step": 3989
    },
    {
      "epoch": 0.9873793615441723,
      "grad_norm": 0.03898297995328903,
      "learning_rate": 0.00025544831703935644,
      "loss": 0.0313,
      "step": 3990
    },
    {
      "epoch": 0.9876268250433061,
      "grad_norm": 0.03281628340482712,
      "learning_rate": 0.00025535104065913184,
      "loss": 0.0497,
      "step": 3991
    },
    {
      "epoch": 0.98787428854244,
      "grad_norm": 0.0420871339738369,
      "learning_rate": 0.00025525376346836496,
      "loss": 0.0516,
      "step": 3992
    },
    {
      "epoch": 0.9881217520415738,
      "grad_norm": 0.03841589018702507,
      "learning_rate": 0.0002551564854817907,
      "loss": 0.0776,
      "step": 3993
    },
    {
      "epoch": 0.9883692155407078,
      "grad_norm": 0.038491155952215195,
      "learning_rate": 0.0002550592067141442,
      "loss": 0.0564,
      "step": 3994
    },
    {
      "epoch": 0.9886166790398416,
      "grad_norm": 0.032198358327150345,
      "learning_rate": 0.0002549619271801605,
      "loss": 0.0479,
      "step": 3995
    },
    {
      "epoch": 0.9888641425389755,
      "grad_norm": 0.03896049037575722,
      "learning_rate": 0.00025486464689457506,
      "loss": 0.0302,
      "step": 3996
    },
    {
      "epoch": 0.9891116060381093,
      "grad_norm": 0.03693017363548279,
      "learning_rate": 0.00025476736587212316,
      "loss": 0.0497,
      "step": 3997
    },
    {
      "epoch": 0.9893590695372433,
      "grad_norm": 0.06783358007669449,
      "learning_rate": 0.00025467008412754046,
      "loss": 0.1058,
      "step": 3998
    },
    {
      "epoch": 0.9896065330363771,
      "grad_norm": 0.024322928860783577,
      "learning_rate": 0.00025457280167556247,
      "loss": 0.0312,
      "step": 3999
    },
    {
      "epoch": 0.989853996535511,
      "grad_norm": 0.026043418794870377,
      "learning_rate": 0.000254475518530925,
      "loss": 0.0446,
      "step": 4000
    },
    {
      "epoch": 0.989853996535511,
      "eval_loss": 0.29177379608154297,
      "eval_runtime": 202.5381,
      "eval_samples_per_second": 4.937,
      "eval_steps_per_second": 0.311,
      "step": 4000
    },
    {
      "epoch": 0.9901014600346449,
      "grad_norm": 0.09864072501659393,
      "learning_rate": 0.0002543782347083638,
      "loss": 0.0727,
      "step": 4001
    },
    {
      "epoch": 0.9903489235337788,
      "grad_norm": 0.03505851328372955,
      "learning_rate": 0.0002542809502226149,
      "loss": 0.0438,
      "step": 4002
    },
    {
      "epoch": 0.9905963870329126,
      "grad_norm": 0.068672776222229,
      "learning_rate": 0.0002541836650884144,
      "loss": 0.1229,
      "step": 4003
    },
    {
      "epoch": 0.9908438505320465,
      "grad_norm": 0.057676561176776886,
      "learning_rate": 0.0002540863793204983,
      "loss": 0.0798,
      "step": 4004
    },
    {
      "epoch": 0.9910913140311804,
      "grad_norm": 0.03528449311852455,
      "learning_rate": 0.00025398909293360296,
      "loss": 0.056,
      "step": 4005
    },
    {
      "epoch": 0.9913387775303143,
      "grad_norm": 0.031189892441034317,
      "learning_rate": 0.00025389180594246463,
      "loss": 0.0257,
      "step": 4006
    },
    {
      "epoch": 0.9915862410294481,
      "grad_norm": 0.027555717155337334,
      "learning_rate": 0.0002537945183618199,
      "loss": 0.0306,
      "step": 4007
    },
    {
      "epoch": 0.991833704528582,
      "grad_norm": 0.03286689519882202,
      "learning_rate": 0.00025369723020640504,
      "loss": 0.0371,
      "step": 4008
    },
    {
      "epoch": 0.992081168027716,
      "grad_norm": 0.0354846753180027,
      "learning_rate": 0.0002535999414909569,
      "loss": 0.0976,
      "step": 4009
    },
    {
      "epoch": 0.9923286315268498,
      "grad_norm": 0.06048410385847092,
      "learning_rate": 0.0002535026522302119,
      "loss": 0.1058,
      "step": 4010
    },
    {
      "epoch": 0.9925760950259837,
      "grad_norm": 0.052290573716163635,
      "learning_rate": 0.0002534053624389071,
      "loss": 0.0544,
      "step": 4011
    },
    {
      "epoch": 0.9928235585251175,
      "grad_norm": 0.0490659661591053,
      "learning_rate": 0.00025330807213177917,
      "loss": 0.0617,
      "step": 4012
    },
    {
      "epoch": 0.9930710220242515,
      "grad_norm": 0.08304398506879807,
      "learning_rate": 0.00025321078132356506,
      "loss": 0.0508,
      "step": 4013
    },
    {
      "epoch": 0.9933184855233853,
      "grad_norm": 0.06016132980585098,
      "learning_rate": 0.0002531134900290018,
      "loss": 0.0488,
      "step": 4014
    },
    {
      "epoch": 0.9935659490225192,
      "grad_norm": 0.03961125388741493,
      "learning_rate": 0.0002530161982628265,
      "loss": 0.0923,
      "step": 4015
    },
    {
      "epoch": 0.993813412521653,
      "grad_norm": 0.04623891040682793,
      "learning_rate": 0.0002529189060397762,
      "loss": 0.0403,
      "step": 4016
    },
    {
      "epoch": 0.994060876020787,
      "grad_norm": 0.04451937973499298,
      "learning_rate": 0.00025282161337458834,
      "loss": 0.0682,
      "step": 4017
    },
    {
      "epoch": 0.9943083395199208,
      "grad_norm": 0.04363994300365448,
      "learning_rate": 0.000252724320282,
      "loss": 0.0512,
      "step": 4018
    },
    {
      "epoch": 0.9945558030190547,
      "grad_norm": 0.025311926379799843,
      "learning_rate": 0.0002526270267767485,
      "loss": 0.0508,
      "step": 4019
    },
    {
      "epoch": 0.9948032665181886,
      "grad_norm": 0.02763415314257145,
      "learning_rate": 0.0002525297328735715,
      "loss": 0.0394,
      "step": 4020
    },
    {
      "epoch": 0.9950507300173225,
      "grad_norm": 0.043966084718704224,
      "learning_rate": 0.0002524324385872063,
      "loss": 0.0562,
      "step": 4021
    },
    {
      "epoch": 0.9952981935164563,
      "grad_norm": 0.20582611858844757,
      "learning_rate": 0.0002523351439323904,
      "loss": 0.0484,
      "step": 4022
    },
    {
      "epoch": 0.9955456570155902,
      "grad_norm": 0.05127502232789993,
      "learning_rate": 0.0002522378489238615,
      "loss": 0.0493,
      "step": 4023
    },
    {
      "epoch": 0.9957931205147241,
      "grad_norm": 0.06729800254106522,
      "learning_rate": 0.00025214055357635704,
      "loss": 0.1051,
      "step": 4024
    },
    {
      "epoch": 0.996040584013858,
      "grad_norm": 0.031454090029001236,
      "learning_rate": 0.00025204325790461506,
      "loss": 0.0581,
      "step": 4025
    },
    {
      "epoch": 0.9962880475129918,
      "grad_norm": 0.03522645682096481,
      "learning_rate": 0.000251945961923373,
      "loss": 0.0483,
      "step": 4026
    },
    {
      "epoch": 0.9965355110121257,
      "grad_norm": 0.07088697701692581,
      "learning_rate": 0.0002518486656473688,
      "loss": 0.055,
      "step": 4027
    },
    {
      "epoch": 0.9967829745112596,
      "grad_norm": 0.036963578313589096,
      "learning_rate": 0.0002517513690913402,
      "loss": 0.0657,
      "step": 4028
    },
    {
      "epoch": 0.9970304380103935,
      "grad_norm": 0.027032146230340004,
      "learning_rate": 0.00025165407227002517,
      "loss": 0.0296,
      "step": 4029
    },
    {
      "epoch": 0.9972779015095273,
      "grad_norm": 0.030858051031827927,
      "learning_rate": 0.0002515567751981615,
      "loss": 0.03,
      "step": 4030
    },
    {
      "epoch": 0.9975253650086612,
      "grad_norm": 0.025940919294953346,
      "learning_rate": 0.0002514594778904873,
      "loss": 0.0474,
      "step": 4031
    },
    {
      "epoch": 0.9977728285077951,
      "grad_norm": 0.01587725803256035,
      "learning_rate": 0.00025136218036174045,
      "loss": 0.0131,
      "step": 4032
    },
    {
      "epoch": 0.998020292006929,
      "grad_norm": 0.07339359074831009,
      "learning_rate": 0.0002512648826266591,
      "loss": 0.0974,
      "step": 4033
    },
    {
      "epoch": 0.9982677555060628,
      "grad_norm": 0.08025604486465454,
      "learning_rate": 0.00025116758469998103,
      "loss": 0.0678,
      "step": 4034
    },
    {
      "epoch": 0.9985152190051967,
      "grad_norm": 0.06517297029495239,
      "learning_rate": 0.0002510702865964446,
      "loss": 0.0434,
      "step": 4035
    },
    {
      "epoch": 0.9987626825043306,
      "grad_norm": 0.038928914815187454,
      "learning_rate": 0.0002509729883307878,
      "loss": 0.0591,
      "step": 4036
    },
    {
      "epoch": 0.9990101460034645,
      "grad_norm": 0.04703410342335701,
      "learning_rate": 0.0002508756899177489,
      "loss": 0.0609,
      "step": 4037
    },
    {
      "epoch": 0.9992576095025983,
      "grad_norm": 0.027914753183722496,
      "learning_rate": 0.00025077839137206584,
      "loss": 0.0472,
      "step": 4038
    },
    {
      "epoch": 0.9995050730017322,
      "grad_norm": 0.03601641207933426,
      "learning_rate": 0.0002506810927084769,
      "loss": 0.056,
      "step": 4039
    },
    {
      "epoch": 0.9997525365008662,
      "grad_norm": 0.06436987221240997,
      "learning_rate": 0.0002505837939417203,
      "loss": 0.1293,
      "step": 4040
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.3093368113040924,
      "learning_rate": 0.00025048649508653423,
      "loss": 0.1949,
      "step": 4041
    },
    {
      "epoch": 1.0002474634991338,
      "grad_norm": 0.049063973128795624,
      "learning_rate": 0.00025038919615765685,
      "loss": 0.0483,
      "step": 4042
    },
    {
      "epoch": 1.0004949269982677,
      "grad_norm": 0.06910372525453568,
      "learning_rate": 0.00025029189716982653,
      "loss": 0.1151,
      "step": 4043
    },
    {
      "epoch": 1.0007423904974015,
      "grad_norm": 0.03633300960063934,
      "learning_rate": 0.00025019459813778146,
      "loss": 0.037,
      "step": 4044
    },
    {
      "epoch": 1.0009898539965356,
      "grad_norm": 0.050197359174489975,
      "learning_rate": 0.00025009729907625987,
      "loss": 0.0934,
      "step": 4045
    },
    {
      "epoch": 1.0012373174956695,
      "grad_norm": 0.03347782790660858,
      "learning_rate": 0.00025,
      "loss": 0.0172,
      "step": 4046
    },
    {
      "epoch": 1.0014847809948033,
      "grad_norm": 0.023877954110503197,
      "learning_rate": 0.00024990270092374014,
      "loss": 0.0352,
      "step": 4047
    },
    {
      "epoch": 1.0017322444939372,
      "grad_norm": 0.08700431883335114,
      "learning_rate": 0.0002498054018622186,
      "loss": 0.0505,
      "step": 4048
    },
    {
      "epoch": 1.001979707993071,
      "grad_norm": 0.05931776762008667,
      "learning_rate": 0.0002497081028301735,
      "loss": 0.0684,
      "step": 4049
    },
    {
      "epoch": 1.0022271714922049,
      "grad_norm": 0.037174660712480545,
      "learning_rate": 0.0002496108038423432,
      "loss": 0.0294,
      "step": 4050
    },
    {
      "epoch": 1.0024746349913387,
      "grad_norm": 0.05070015788078308,
      "learning_rate": 0.0002495135049134658,
      "loss": 0.0567,
      "step": 4051
    },
    {
      "epoch": 1.0027220984904726,
      "grad_norm": 0.026898331940174103,
      "learning_rate": 0.00024941620605827973,
      "loss": 0.0383,
      "step": 4052
    },
    {
      "epoch": 1.0029695619896066,
      "grad_norm": 0.04771450534462929,
      "learning_rate": 0.00024931890729152316,
      "loss": 0.0887,
      "step": 4053
    },
    {
      "epoch": 1.0032170254887405,
      "grad_norm": 0.027968497946858406,
      "learning_rate": 0.00024922160862793423,
      "loss": 0.0165,
      "step": 4054
    },
    {
      "epoch": 1.0034644889878743,
      "grad_norm": 0.053073327988386154,
      "learning_rate": 0.00024912431008225125,
      "loss": 0.0349,
      "step": 4055
    },
    {
      "epoch": 1.0037119524870082,
      "grad_norm": 0.020021310076117516,
      "learning_rate": 0.0002490270116692122,
      "loss": 0.017,
      "step": 4056
    },
    {
      "epoch": 1.003959415986142,
      "grad_norm": 0.036219071596860886,
      "learning_rate": 0.0002489297134035554,
      "loss": 0.0422,
      "step": 4057
    },
    {
      "epoch": 1.0042068794852759,
      "grad_norm": 0.03284837305545807,
      "learning_rate": 0.00024883241530001903,
      "loss": 0.0562,
      "step": 4058
    },
    {
      "epoch": 1.0044543429844097,
      "grad_norm": 0.038011495023965836,
      "learning_rate": 0.00024873511737334096,
      "loss": 0.0338,
      "step": 4059
    },
    {
      "epoch": 1.0047018064835436,
      "grad_norm": 0.022861357778310776,
      "learning_rate": 0.0002486378196382596,
      "loss": 0.0464,
      "step": 4060
    },
    {
      "epoch": 1.0049492699826776,
      "grad_norm": 0.026074029505252838,
      "learning_rate": 0.00024854052210951274,
      "loss": 0.0288,
      "step": 4061
    },
    {
      "epoch": 1.0051967334818115,
      "grad_norm": 0.04838128760457039,
      "learning_rate": 0.0002484432248018385,
      "loss": 0.0721,
      "step": 4062
    },
    {
      "epoch": 1.0054441969809453,
      "grad_norm": 0.02373766154050827,
      "learning_rate": 0.0002483459277299749,
      "loss": 0.0253,
      "step": 4063
    },
    {
      "epoch": 1.0056916604800792,
      "grad_norm": 0.03399401530623436,
      "learning_rate": 0.0002482486309086598,
      "loss": 0.0344,
      "step": 4064
    },
    {
      "epoch": 1.005939123979213,
      "grad_norm": 0.026562457904219627,
      "learning_rate": 0.00024815133435263125,
      "loss": 0.0358,
      "step": 4065
    },
    {
      "epoch": 1.0061865874783469,
      "grad_norm": 0.03336372599005699,
      "learning_rate": 0.000248054038076627,
      "loss": 0.0622,
      "step": 4066
    },
    {
      "epoch": 1.0064340509774807,
      "grad_norm": 0.03467939794063568,
      "learning_rate": 0.0002479567420953849,
      "loss": 0.0335,
      "step": 4067
    },
    {
      "epoch": 1.0066815144766148,
      "grad_norm": 0.053122468292713165,
      "learning_rate": 0.00024785944642364297,
      "loss": 0.0661,
      "step": 4068
    },
    {
      "epoch": 1.0069289779757487,
      "grad_norm": 0.024571184068918228,
      "learning_rate": 0.00024776215107613853,
      "loss": 0.022,
      "step": 4069
    },
    {
      "epoch": 1.0071764414748825,
      "grad_norm": 0.041323237121105194,
      "learning_rate": 0.0002476648560676097,
      "loss": 0.0511,
      "step": 4070
    },
    {
      "epoch": 1.0074239049740163,
      "grad_norm": 0.028709853067994118,
      "learning_rate": 0.0002475675614127937,
      "loss": 0.044,
      "step": 4071
    },
    {
      "epoch": 1.0076713684731502,
      "grad_norm": 0.03568616509437561,
      "learning_rate": 0.0002474702671264285,
      "loss": 0.0368,
      "step": 4072
    },
    {
      "epoch": 1.007918831972284,
      "grad_norm": 0.036837346851825714,
      "learning_rate": 0.0002473729732232515,
      "loss": 0.066,
      "step": 4073
    },
    {
      "epoch": 1.008166295471418,
      "grad_norm": 0.026220884174108505,
      "learning_rate": 0.00024727567971800007,
      "loss": 0.0426,
      "step": 4074
    },
    {
      "epoch": 1.0084137589705517,
      "grad_norm": 0.04045395180583,
      "learning_rate": 0.0002471783866254118,
      "loss": 0.0326,
      "step": 4075
    },
    {
      "epoch": 1.0086612224696858,
      "grad_norm": 0.0824037492275238,
      "learning_rate": 0.0002470810939602238,
      "loss": 0.101,
      "step": 4076
    },
    {
      "epoch": 1.0089086859688197,
      "grad_norm": 0.0864928737282753,
      "learning_rate": 0.0002469838017371735,
      "loss": 0.0911,
      "step": 4077
    },
    {
      "epoch": 1.0091561494679535,
      "grad_norm": 0.033985547721385956,
      "learning_rate": 0.0002468865099709983,
      "loss": 0.0529,
      "step": 4078
    },
    {
      "epoch": 1.0094036129670874,
      "grad_norm": 0.03927365690469742,
      "learning_rate": 0.00024678921867643495,
      "loss": 0.0634,
      "step": 4079
    },
    {
      "epoch": 1.0096510764662212,
      "grad_norm": 0.06347379833459854,
      "learning_rate": 0.0002466919278682209,
      "loss": 0.0821,
      "step": 4080
    },
    {
      "epoch": 1.009898539965355,
      "grad_norm": 0.03174550086259842,
      "learning_rate": 0.00024659463756109294,
      "loss": 0.0379,
      "step": 4081
    },
    {
      "epoch": 1.010146003464489,
      "grad_norm": 0.04761435464024544,
      "learning_rate": 0.0002464973477697881,
      "loss": 0.0372,
      "step": 4082
    },
    {
      "epoch": 1.010393466963623,
      "grad_norm": 0.03661966323852539,
      "learning_rate": 0.00024640005850904323,
      "loss": 0.0588,
      "step": 4083
    },
    {
      "epoch": 1.0106409304627568,
      "grad_norm": 0.04942243918776512,
      "learning_rate": 0.000246302769793595,
      "loss": 0.0808,
      "step": 4084
    },
    {
      "epoch": 1.0108883939618907,
      "grad_norm": 0.05786413326859474,
      "learning_rate": 0.00024620548163818023,
      "loss": 0.1509,
      "step": 4085
    },
    {
      "epoch": 1.0111358574610245,
      "grad_norm": 0.04318402707576752,
      "learning_rate": 0.0002461081940575354,
      "loss": 0.0186,
      "step": 4086
    },
    {
      "epoch": 1.0113833209601584,
      "grad_norm": 0.03400596231222153,
      "learning_rate": 0.00024601090706639705,
      "loss": 0.0403,
      "step": 4087
    },
    {
      "epoch": 1.0116307844592922,
      "grad_norm": 0.03218872472643852,
      "learning_rate": 0.0002459136206795017,
      "loss": 0.0655,
      "step": 4088
    },
    {
      "epoch": 1.011878247958426,
      "grad_norm": 0.04493182525038719,
      "learning_rate": 0.00024581633491158565,
      "loss": 0.0357,
      "step": 4089
    },
    {
      "epoch": 1.01212571145756,
      "grad_norm": 0.029078926891088486,
      "learning_rate": 0.00024571904977738516,
      "loss": 0.045,
      "step": 4090
    },
    {
      "epoch": 1.012373174956694,
      "grad_norm": 0.03780406340956688,
      "learning_rate": 0.0002456217652916363,
      "loss": 0.0351,
      "step": 4091
    },
    {
      "epoch": 1.0126206384558278,
      "grad_norm": 0.04011503979563713,
      "learning_rate": 0.00024552448146907506,
      "loss": 0.0277,
      "step": 4092
    },
    {
      "epoch": 1.0128681019549617,
      "grad_norm": 0.04423149675130844,
      "learning_rate": 0.0002454271983244376,
      "loss": 0.0645,
      "step": 4093
    },
    {
      "epoch": 1.0131155654540955,
      "grad_norm": 0.030063092708587646,
      "learning_rate": 0.0002453299158724596,
      "loss": 0.0318,
      "step": 4094
    },
    {
      "epoch": 1.0133630289532294,
      "grad_norm": 0.03268326073884964,
      "learning_rate": 0.0002452326341278769,
      "loss": 0.0286,
      "step": 4095
    },
    {
      "epoch": 1.0136104924523632,
      "grad_norm": 0.046463899314403534,
      "learning_rate": 0.00024513535310542495,
      "loss": 0.051,
      "step": 4096
    },
    {
      "epoch": 1.013857955951497,
      "grad_norm": 0.06197037175297737,
      "learning_rate": 0.00024503807281983945,
      "loss": 0.0764,
      "step": 4097
    },
    {
      "epoch": 1.014105419450631,
      "grad_norm": 0.03155982494354248,
      "learning_rate": 0.00024494079328585586,
      "loss": 0.067,
      "step": 4098
    },
    {
      "epoch": 1.014352882949765,
      "grad_norm": 0.03996339440345764,
      "learning_rate": 0.0002448435145182093,
      "loss": 0.0371,
      "step": 4099
    },
    {
      "epoch": 1.0146003464488988,
      "grad_norm": 0.039786238223314285,
      "learning_rate": 0.0002447462365316351,
      "loss": 0.0421,
      "step": 4100
    },
    {
      "epoch": 1.0148478099480327,
      "grad_norm": 0.04647200182080269,
      "learning_rate": 0.0002446489593408682,
      "loss": 0.0479,
      "step": 4101
    },
    {
      "epoch": 1.0150952734471665,
      "grad_norm": 0.030739208683371544,
      "learning_rate": 0.00024455168296064363,
      "loss": 0.0335,
      "step": 4102
    },
    {
      "epoch": 1.0153427369463004,
      "grad_norm": 0.035285256803035736,
      "learning_rate": 0.00024445440740569616,
      "loss": 0.0497,
      "step": 4103
    },
    {
      "epoch": 1.0155902004454342,
      "grad_norm": 0.03745584934949875,
      "learning_rate": 0.0002443571326907605,
      "loss": 0.0392,
      "step": 4104
    },
    {
      "epoch": 1.015837663944568,
      "grad_norm": 0.0353362038731575,
      "learning_rate": 0.0002442598588305713,
      "loss": 0.0634,
      "step": 4105
    },
    {
      "epoch": 1.0160851274437022,
      "grad_norm": 0.04978550225496292,
      "learning_rate": 0.00024416258583986276,
      "loss": 0.0449,
      "step": 4106
    },
    {
      "epoch": 1.016332590942836,
      "grad_norm": 0.04185254126787186,
      "learning_rate": 0.00024406531373336943,
      "loss": 0.0841,
      "step": 4107
    },
    {
      "epoch": 1.0165800544419699,
      "grad_norm": 0.033792998641729355,
      "learning_rate": 0.00024396804252582548,
      "loss": 0.0632,
      "step": 4108
    },
    {
      "epoch": 1.0168275179411037,
      "grad_norm": 0.1135513186454773,
      "learning_rate": 0.00024387077223196477,
      "loss": 0.1016,
      "step": 4109
    },
    {
      "epoch": 1.0170749814402376,
      "grad_norm": 0.03832370415329933,
      "learning_rate": 0.00024377350286652132,
      "loss": 0.0564,
      "step": 4110
    },
    {
      "epoch": 1.0173224449393714,
      "grad_norm": 0.027329307049512863,
      "learning_rate": 0.00024367623444422889,
      "loss": 0.0368,
      "step": 4111
    },
    {
      "epoch": 1.0175699084385053,
      "grad_norm": 0.02537551149725914,
      "learning_rate": 0.00024357896697982102,
      "loss": 0.038,
      "step": 4112
    },
    {
      "epoch": 1.017817371937639,
      "grad_norm": 0.027492450550198555,
      "learning_rate": 0.00024348170048803128,
      "loss": 0.0321,
      "step": 4113
    },
    {
      "epoch": 1.0180648354367732,
      "grad_norm": 0.037484217435121536,
      "learning_rate": 0.00024338443498359285,
      "loss": 0.0679,
      "step": 4114
    },
    {
      "epoch": 1.018312298935907,
      "grad_norm": 0.08301033079624176,
      "learning_rate": 0.00024328717048123903,
      "loss": 0.0563,
      "step": 4115
    },
    {
      "epoch": 1.0185597624350409,
      "grad_norm": 0.06698343902826309,
      "learning_rate": 0.00024318990699570285,
      "loss": 0.0823,
      "step": 4116
    },
    {
      "epoch": 1.0188072259341747,
      "grad_norm": 0.0398639515042305,
      "learning_rate": 0.00024309264454171707,
      "loss": 0.0722,
      "step": 4117
    },
    {
      "epoch": 1.0190546894333086,
      "grad_norm": 0.042752254754304886,
      "learning_rate": 0.00024299538313401453,
      "loss": 0.0618,
      "step": 4118
    },
    {
      "epoch": 1.0193021529324424,
      "grad_norm": 0.028764845803380013,
      "learning_rate": 0.00024289812278732766,
      "loss": 0.0366,
      "step": 4119
    },
    {
      "epoch": 1.0195496164315763,
      "grad_norm": 0.036965977400541306,
      "learning_rate": 0.0002428008635163889,
      "loss": 0.0561,
      "step": 4120
    },
    {
      "epoch": 1.0197970799307101,
      "grad_norm": 0.05769829824566841,
      "learning_rate": 0.0002427036053359305,
      "loss": 0.0634,
      "step": 4121
    },
    {
      "epoch": 1.0200445434298442,
      "grad_norm": 0.028374379500746727,
      "learning_rate": 0.00024260634826068442,
      "loss": 0.0445,
      "step": 4122
    },
    {
      "epoch": 1.020292006928978,
      "grad_norm": 0.04169129580259323,
      "learning_rate": 0.0002425090923053828,
      "loss": 0.0517,
      "step": 4123
    },
    {
      "epoch": 1.0205394704281119,
      "grad_norm": 0.06615476310253143,
      "learning_rate": 0.00024241183748475707,
      "loss": 0.0883,
      "step": 4124
    },
    {
      "epoch": 1.0207869339272457,
      "grad_norm": 0.05988084897398949,
      "learning_rate": 0.00024231458381353897,
      "loss": 0.0559,
      "step": 4125
    },
    {
      "epoch": 1.0210343974263796,
      "grad_norm": 0.07271682471036911,
      "learning_rate": 0.00024221733130645986,
      "loss": 0.0837,
      "step": 4126
    },
    {
      "epoch": 1.0212818609255134,
      "grad_norm": 0.04411160573363304,
      "learning_rate": 0.00024212007997825088,
      "loss": 0.1121,
      "step": 4127
    },
    {
      "epoch": 1.0215293244246473,
      "grad_norm": 0.06369533389806747,
      "learning_rate": 0.00024202282984364312,
      "loss": 0.0595,
      "step": 4128
    },
    {
      "epoch": 1.0217767879237813,
      "grad_norm": 0.016807064414024353,
      "learning_rate": 0.00024192558091736736,
      "loss": 0.0208,
      "step": 4129
    },
    {
      "epoch": 1.0220242514229152,
      "grad_norm": 0.042318109422922134,
      "learning_rate": 0.00024182833321415424,
      "loss": 0.0515,
      "step": 4130
    },
    {
      "epoch": 1.022271714922049,
      "grad_norm": 0.02657907083630562,
      "learning_rate": 0.00024173108674873446,
      "loss": 0.0447,
      "step": 4131
    },
    {
      "epoch": 1.022519178421183,
      "grad_norm": 0.038829296827316284,
      "learning_rate": 0.00024163384153583794,
      "loss": 0.0618,
      "step": 4132
    },
    {
      "epoch": 1.0227666419203167,
      "grad_norm": 0.02888103388249874,
      "learning_rate": 0.0002415365975901952,
      "loss": 0.0337,
      "step": 4133
    },
    {
      "epoch": 1.0230141054194506,
      "grad_norm": 0.03532019630074501,
      "learning_rate": 0.00024143935492653573,
      "loss": 0.0363,
      "step": 4134
    },
    {
      "epoch": 1.0232615689185844,
      "grad_norm": 0.056882914155721664,
      "learning_rate": 0.0002413421135595895,
      "loss": 0.0743,
      "step": 4135
    },
    {
      "epoch": 1.0235090324177183,
      "grad_norm": 0.04032082483172417,
      "learning_rate": 0.000241244873504086,
      "loss": 0.0353,
      "step": 4136
    },
    {
      "epoch": 1.0237564959168524,
      "grad_norm": 0.02647574432194233,
      "learning_rate": 0.00024114763477475448,
      "loss": 0.0383,
      "step": 4137
    },
    {
      "epoch": 1.0240039594159862,
      "grad_norm": 0.03200173005461693,
      "learning_rate": 0.00024105039738632416,
      "loss": 0.05,
      "step": 4138
    },
    {
      "epoch": 1.02425142291512,
      "grad_norm": 0.02783455327153206,
      "learning_rate": 0.0002409531613535238,
      "loss": 0.0344,
      "step": 4139
    },
    {
      "epoch": 1.024498886414254,
      "grad_norm": 0.03697407245635986,
      "learning_rate": 0.00024085592669108216,
      "loss": 0.0414,
      "step": 4140
    },
    {
      "epoch": 1.0247463499133878,
      "grad_norm": 0.046524226665496826,
      "learning_rate": 0.00024075869341372793,
      "loss": 0.0561,
      "step": 4141
    },
    {
      "epoch": 1.0249938134125216,
      "grad_norm": 0.04858894273638725,
      "learning_rate": 0.00024066146153618907,
      "loss": 0.0534,
      "step": 4142
    },
    {
      "epoch": 1.0252412769116555,
      "grad_norm": 0.04082120954990387,
      "learning_rate": 0.00024056423107319394,
      "loss": 0.0532,
      "step": 4143
    },
    {
      "epoch": 1.0254887404107893,
      "grad_norm": 0.031067492440342903,
      "learning_rate": 0.00024046700203947028,
      "loss": 0.0265,
      "step": 4144
    },
    {
      "epoch": 1.0257362039099234,
      "grad_norm": 0.039695605635643005,
      "learning_rate": 0.00024036977444974572,
      "loss": 0.086,
      "step": 4145
    },
    {
      "epoch": 1.0259836674090572,
      "grad_norm": 0.032032713294029236,
      "learning_rate": 0.00024027254831874778,
      "loss": 0.0499,
      "step": 4146
    },
    {
      "epoch": 1.026231130908191,
      "grad_norm": 0.09200192242860794,
      "learning_rate": 0.0002401753236612036,
      "loss": 0.0539,
      "step": 4147
    },
    {
      "epoch": 1.026478594407325,
      "grad_norm": 0.05286726355552673,
      "learning_rate": 0.0002400781004918402,
      "loss": 0.0461,
      "step": 4148
    },
    {
      "epoch": 1.0267260579064588,
      "grad_norm": 0.03380788117647171,
      "learning_rate": 0.00023998087882538428,
      "loss": 0.0308,
      "step": 4149
    },
    {
      "epoch": 1.0269735214055926,
      "grad_norm": 0.02845810540020466,
      "learning_rate": 0.00023988365867656233,
      "loss": 0.0463,
      "step": 4150
    },
    {
      "epoch": 1.0272209849047265,
      "grad_norm": 0.04413831979036331,
      "learning_rate": 0.00023978644006010085,
      "loss": 0.0317,
      "step": 4151
    },
    {
      "epoch": 1.0274684484038605,
      "grad_norm": 0.06807389110326767,
      "learning_rate": 0.00023968922299072578,
      "loss": 0.0268,
      "step": 4152
    },
    {
      "epoch": 1.0277159119029944,
      "grad_norm": 0.05053911730647087,
      "learning_rate": 0.00023959200748316302,
      "loss": 0.0396,
      "step": 4153
    },
    {
      "epoch": 1.0279633754021282,
      "grad_norm": 0.030922790989279747,
      "learning_rate": 0.00023949479355213802,
      "loss": 0.0481,
      "step": 4154
    },
    {
      "epoch": 1.028210838901262,
      "grad_norm": 0.061892036348581314,
      "learning_rate": 0.0002393975812123763,
      "loss": 0.0356,
      "step": 4155
    },
    {
      "epoch": 1.028458302400396,
      "grad_norm": 0.06027645990252495,
      "learning_rate": 0.00023930037047860292,
      "loss": 0.0553,
      "step": 4156
    },
    {
      "epoch": 1.0287057658995298,
      "grad_norm": 0.04142504930496216,
      "learning_rate": 0.0002392031613655427,
      "loss": 0.0476,
      "step": 4157
    },
    {
      "epoch": 1.0289532293986636,
      "grad_norm": 0.022832611575722694,
      "learning_rate": 0.00023910595388792043,
      "loss": 0.034,
      "step": 4158
    },
    {
      "epoch": 1.0292006928977975,
      "grad_norm": 0.03029404766857624,
      "learning_rate": 0.00023900874806046026,
      "loss": 0.0301,
      "step": 4159
    },
    {
      "epoch": 1.0294481563969315,
      "grad_norm": 0.08685967326164246,
      "learning_rate": 0.0002389115438978865,
      "loss": 0.0436,
      "step": 4160
    },
    {
      "epoch": 1.0296956198960654,
      "grad_norm": 0.04166773334145546,
      "learning_rate": 0.00023881434141492306,
      "loss": 0.0738,
      "step": 4161
    },
    {
      "epoch": 1.0299430833951992,
      "grad_norm": 0.04786469414830208,
      "learning_rate": 0.0002387171406262934,
      "loss": 0.0575,
      "step": 4162
    },
    {
      "epoch": 1.030190546894333,
      "grad_norm": 0.034817636013031006,
      "learning_rate": 0.00023861994154672107,
      "loss": 0.0303,
      "step": 4163
    },
    {
      "epoch": 1.030438010393467,
      "grad_norm": 0.16023369133472443,
      "learning_rate": 0.00023852274419092903,
      "loss": 0.0772,
      "step": 4164
    },
    {
      "epoch": 1.0306854738926008,
      "grad_norm": 0.048931851983070374,
      "learning_rate": 0.0002384255485736402,
      "loss": 0.0912,
      "step": 4165
    },
    {
      "epoch": 1.0309329373917346,
      "grad_norm": 0.026618795469403267,
      "learning_rate": 0.0002383283547095772,
      "loss": 0.0441,
      "step": 4166
    },
    {
      "epoch": 1.0311804008908685,
      "grad_norm": 0.04212675988674164,
      "learning_rate": 0.0002382311626134622,
      "loss": 0.0377,
      "step": 4167
    },
    {
      "epoch": 1.0314278643900026,
      "grad_norm": 0.13300372660160065,
      "learning_rate": 0.0002381339723000175,
      "loss": 0.1311,
      "step": 4168
    },
    {
      "epoch": 1.0316753278891364,
      "grad_norm": 0.05736646428704262,
      "learning_rate": 0.00023803678378396463,
      "loss": 0.0526,
      "step": 4169
    },
    {
      "epoch": 1.0319227913882703,
      "grad_norm": 0.03531758487224579,
      "learning_rate": 0.00023793959708002523,
      "loss": 0.0312,
      "step": 4170
    },
    {
      "epoch": 1.032170254887404,
      "grad_norm": 0.03057601861655712,
      "learning_rate": 0.00023784241220292063,
      "loss": 0.0387,
      "step": 4171
    },
    {
      "epoch": 1.032417718386538,
      "grad_norm": 0.0403023287653923,
      "learning_rate": 0.00023774522916737158,
      "loss": 0.0332,
      "step": 4172
    },
    {
      "epoch": 1.0326651818856718,
      "grad_norm": 0.033894337713718414,
      "learning_rate": 0.00023764804798809895,
      "loss": 0.0364,
      "step": 4173
    },
    {
      "epoch": 1.0329126453848056,
      "grad_norm": 0.032178014516830444,
      "learning_rate": 0.00023755086867982298,
      "loss": 0.0312,
      "step": 4174
    },
    {
      "epoch": 1.0331601088839397,
      "grad_norm": 0.024432383477687836,
      "learning_rate": 0.0002374536912572638,
      "loss": 0.0297,
      "step": 4175
    },
    {
      "epoch": 1.0334075723830736,
      "grad_norm": 0.057446349412202835,
      "learning_rate": 0.00023735651573514148,
      "loss": 0.0843,
      "step": 4176
    },
    {
      "epoch": 1.0336550358822074,
      "grad_norm": 0.03359261527657509,
      "learning_rate": 0.0002372593421281752,
      "loss": 0.0491,
      "step": 4177
    },
    {
      "epoch": 1.0339024993813413,
      "grad_norm": 0.047640588134527206,
      "learning_rate": 0.00023716217045108458,
      "loss": 0.0539,
      "step": 4178
    },
    {
      "epoch": 1.0341499628804751,
      "grad_norm": 0.03915596753358841,
      "learning_rate": 0.00023706500071858823,
      "loss": 0.0259,
      "step": 4179
    },
    {
      "epoch": 1.034397426379609,
      "grad_norm": 0.03184492141008377,
      "learning_rate": 0.00023696783294540501,
      "loss": 0.0425,
      "step": 4180
    },
    {
      "epoch": 1.0346448898787428,
      "grad_norm": 0.05530840530991554,
      "learning_rate": 0.00023687066714625334,
      "loss": 0.0745,
      "step": 4181
    },
    {
      "epoch": 1.0348923533778767,
      "grad_norm": 0.035886943340301514,
      "learning_rate": 0.00023677350333585115,
      "loss": 0.0416,
      "step": 4182
    },
    {
      "epoch": 1.0351398168770107,
      "grad_norm": 0.024876080453395844,
      "learning_rate": 0.00023667634152891632,
      "loss": 0.0323,
      "step": 4183
    },
    {
      "epoch": 1.0353872803761446,
      "grad_norm": 0.055410224944353104,
      "learning_rate": 0.0002365791817401662,
      "loss": 0.097,
      "step": 4184
    },
    {
      "epoch": 1.0356347438752784,
      "grad_norm": 0.03852277249097824,
      "learning_rate": 0.00023648202398431794,
      "loss": 0.0655,
      "step": 4185
    },
    {
      "epoch": 1.0358822073744123,
      "grad_norm": 0.02498190850019455,
      "learning_rate": 0.0002363848682760887,
      "loss": 0.017,
      "step": 4186
    },
    {
      "epoch": 1.0361296708735461,
      "grad_norm": 0.03161734342575073,
      "learning_rate": 0.00023628771463019455,
      "loss": 0.0348,
      "step": 4187
    },
    {
      "epoch": 1.03637713437268,
      "grad_norm": 0.046063292771577835,
      "learning_rate": 0.00023619056306135207,
      "loss": 0.0973,
      "step": 4188
    },
    {
      "epoch": 1.0366245978718138,
      "grad_norm": 0.07400227338075638,
      "learning_rate": 0.00023609341358427706,
      "loss": 0.0409,
      "step": 4189
    },
    {
      "epoch": 1.0368720613709477,
      "grad_norm": 0.03666578233242035,
      "learning_rate": 0.00023599626621368506,
      "loss": 0.0605,
      "step": 4190
    },
    {
      "epoch": 1.0371195248700817,
      "grad_norm": 0.03269293159246445,
      "learning_rate": 0.00023589912096429148,
      "loss": 0.0344,
      "step": 4191
    },
    {
      "epoch": 1.0373669883692156,
      "grad_norm": 0.11223572492599487,
      "learning_rate": 0.00023580197785081115,
      "loss": 0.073,
      "step": 4192
    },
    {
      "epoch": 1.0376144518683494,
      "grad_norm": 0.0531841479241848,
      "learning_rate": 0.00023570483688795883,
      "loss": 0.0554,
      "step": 4193
    },
    {
      "epoch": 1.0378619153674833,
      "grad_norm": 0.04828519746661186,
      "learning_rate": 0.00023560769809044866,
      "loss": 0.0856,
      "step": 4194
    },
    {
      "epoch": 1.0381093788666171,
      "grad_norm": 0.04416150972247124,
      "learning_rate": 0.00023551056147299467,
      "loss": 0.0863,
      "step": 4195
    },
    {
      "epoch": 1.038356842365751,
      "grad_norm": 0.050244301557540894,
      "learning_rate": 0.00023541342705031066,
      "loss": 0.0371,
      "step": 4196
    },
    {
      "epoch": 1.0386043058648848,
      "grad_norm": 0.04414781555533409,
      "learning_rate": 0.0002353162948371098,
      "loss": 0.0617,
      "step": 4197
    },
    {
      "epoch": 1.038851769364019,
      "grad_norm": 0.02722323313355446,
      "learning_rate": 0.00023521916484810516,
      "loss": 0.0328,
      "step": 4198
    },
    {
      "epoch": 1.0390992328631528,
      "grad_norm": 0.027653302997350693,
      "learning_rate": 0.00023512203709800928,
      "loss": 0.0334,
      "step": 4199
    },
    {
      "epoch": 1.0393466963622866,
      "grad_norm": 0.038181982934474945,
      "learning_rate": 0.0002350249116015345,
      "loss": 0.0277,
      "step": 4200
    },
    {
      "epoch": 1.0393466963622866,
      "eval_loss": 0.29199138283729553,
      "eval_runtime": 202.7808,
      "eval_samples_per_second": 4.931,
      "eval_steps_per_second": 0.311,
      "step": 4200
    },
    {
      "epoch": 1.0395941598614205,
      "grad_norm": 0.04156839847564697,
      "learning_rate": 0.00023492778837339294,
      "loss": 0.0711,
      "step": 4201
    },
    {
      "epoch": 1.0398416233605543,
      "grad_norm": 0.025836361572146416,
      "learning_rate": 0.00023483066742829598,
      "loss": 0.0387,
      "step": 4202
    },
    {
      "epoch": 1.0400890868596881,
      "grad_norm": 0.04488589987158775,
      "learning_rate": 0.00023473354878095508,
      "loss": 0.0554,
      "step": 4203
    },
    {
      "epoch": 1.040336550358822,
      "grad_norm": 0.050054457038640976,
      "learning_rate": 0.00023463643244608096,
      "loss": 0.0682,
      "step": 4204
    },
    {
      "epoch": 1.0405840138579558,
      "grad_norm": 0.034004129469394684,
      "learning_rate": 0.0002345393184383844,
      "loss": 0.0428,
      "step": 4205
    },
    {
      "epoch": 1.04083147735709,
      "grad_norm": 0.03449151664972305,
      "learning_rate": 0.00023444220677257564,
      "loss": 0.0299,
      "step": 4206
    },
    {
      "epoch": 1.0410789408562238,
      "grad_norm": 0.03667733445763588,
      "learning_rate": 0.00023434509746336434,
      "loss": 0.0433,
      "step": 4207
    },
    {
      "epoch": 1.0413264043553576,
      "grad_norm": 0.047826141119003296,
      "learning_rate": 0.00023424799052546025,
      "loss": 0.0498,
      "step": 4208
    },
    {
      "epoch": 1.0415738678544915,
      "grad_norm": 0.05572633817791939,
      "learning_rate": 0.00023415088597357237,
      "loss": 0.0919,
      "step": 4209
    },
    {
      "epoch": 1.0418213313536253,
      "grad_norm": 0.04805757477879524,
      "learning_rate": 0.00023405378382240953,
      "loss": 0.0338,
      "step": 4210
    },
    {
      "epoch": 1.0420687948527592,
      "grad_norm": 0.07974382489919662,
      "learning_rate": 0.0002339566840866802,
      "loss": 0.1113,
      "step": 4211
    },
    {
      "epoch": 1.042316258351893,
      "grad_norm": 0.020005591213703156,
      "learning_rate": 0.00023385958678109227,
      "loss": 0.0292,
      "step": 4212
    },
    {
      "epoch": 1.042563721851027,
      "grad_norm": 0.04046489670872688,
      "learning_rate": 0.00023376249192035378,
      "loss": 0.0327,
      "step": 4213
    },
    {
      "epoch": 1.042811185350161,
      "grad_norm": 0.03583111613988876,
      "learning_rate": 0.00023366539951917169,
      "loss": 0.0506,
      "step": 4214
    },
    {
      "epoch": 1.0430586488492948,
      "grad_norm": 0.07575049996376038,
      "learning_rate": 0.00023356830959225314,
      "loss": 0.0489,
      "step": 4215
    },
    {
      "epoch": 1.0433061123484286,
      "grad_norm": 0.047368161380290985,
      "learning_rate": 0.00023347122215430476,
      "loss": 0.0621,
      "step": 4216
    },
    {
      "epoch": 1.0435535758475625,
      "grad_norm": 0.058917537331581116,
      "learning_rate": 0.0002333741372200326,
      "loss": 0.0991,
      "step": 4217
    },
    {
      "epoch": 1.0438010393466963,
      "grad_norm": 0.09859666973352432,
      "learning_rate": 0.00023327705480414258,
      "loss": 0.1173,
      "step": 4218
    },
    {
      "epoch": 1.0440485028458302,
      "grad_norm": 0.04977050796151161,
      "learning_rate": 0.00023317997492134008,
      "loss": 0.031,
      "step": 4219
    },
    {
      "epoch": 1.044295966344964,
      "grad_norm": 0.05789288878440857,
      "learning_rate": 0.00023308289758633017,
      "loss": 0.0377,
      "step": 4220
    },
    {
      "epoch": 1.044543429844098,
      "grad_norm": 0.0315215177834034,
      "learning_rate": 0.00023298582281381757,
      "loss": 0.0371,
      "step": 4221
    },
    {
      "epoch": 1.044790893343232,
      "grad_norm": 0.06990973651409149,
      "learning_rate": 0.00023288875061850645,
      "loss": 0.0809,
      "step": 4222
    },
    {
      "epoch": 1.0450383568423658,
      "grad_norm": 0.030321579426527023,
      "learning_rate": 0.00023279168101510093,
      "loss": 0.0179,
      "step": 4223
    },
    {
      "epoch": 1.0452858203414996,
      "grad_norm": 0.03447651490569115,
      "learning_rate": 0.00023269461401830417,
      "loss": 0.0416,
      "step": 4224
    },
    {
      "epoch": 1.0455332838406335,
      "grad_norm": 0.13807256519794464,
      "learning_rate": 0.00023259754964281954,
      "loss": 0.0562,
      "step": 4225
    },
    {
      "epoch": 1.0457807473397673,
      "grad_norm": 0.05054190009832382,
      "learning_rate": 0.0002325004879033497,
      "loss": 0.054,
      "step": 4226
    },
    {
      "epoch": 1.0460282108389012,
      "grad_norm": 0.08559759706258774,
      "learning_rate": 0.0002324034288145969,
      "loss": 0.0515,
      "step": 4227
    },
    {
      "epoch": 1.046275674338035,
      "grad_norm": 0.032579001039266586,
      "learning_rate": 0.00023230637239126308,
      "loss": 0.0285,
      "step": 4228
    },
    {
      "epoch": 1.046523137837169,
      "grad_norm": 0.04491838067770004,
      "learning_rate": 0.0002322093186480497,
      "loss": 0.04,
      "step": 4229
    },
    {
      "epoch": 1.046770601336303,
      "grad_norm": 0.039254944771528244,
      "learning_rate": 0.00023211226759965785,
      "loss": 0.0339,
      "step": 4230
    },
    {
      "epoch": 1.0470180648354368,
      "grad_norm": 0.05268038809299469,
      "learning_rate": 0.0002320152192607884,
      "loss": 0.0667,
      "step": 4231
    },
    {
      "epoch": 1.0472655283345707,
      "grad_norm": 0.0692434310913086,
      "learning_rate": 0.00023191817364614131,
      "loss": 0.0837,
      "step": 4232
    },
    {
      "epoch": 1.0475129918337045,
      "grad_norm": 0.035184480249881744,
      "learning_rate": 0.0002318211307704168,
      "loss": 0.0463,
      "step": 4233
    },
    {
      "epoch": 1.0477604553328383,
      "grad_norm": 0.03887702897191048,
      "learning_rate": 0.000231724090648314,
      "loss": 0.0364,
      "step": 4234
    },
    {
      "epoch": 1.0480079188319722,
      "grad_norm": 0.06909196078777313,
      "learning_rate": 0.0002316270532945321,
      "loss": 0.0594,
      "step": 4235
    },
    {
      "epoch": 1.0482553823311063,
      "grad_norm": 0.025025738403201103,
      "learning_rate": 0.00023153001872376974,
      "loss": 0.0398,
      "step": 4236
    },
    {
      "epoch": 1.0485028458302401,
      "grad_norm": 0.06699416786432266,
      "learning_rate": 0.00023143298695072503,
      "loss": 0.053,
      "step": 4237
    },
    {
      "epoch": 1.048750309329374,
      "grad_norm": 0.07503928989171982,
      "learning_rate": 0.00023133595799009572,
      "loss": 0.039,
      "step": 4238
    },
    {
      "epoch": 1.0489977728285078,
      "grad_norm": 0.029390960931777954,
      "learning_rate": 0.00023123893185657938,
      "loss": 0.0306,
      "step": 4239
    },
    {
      "epoch": 1.0492452363276417,
      "grad_norm": 0.041035011410713196,
      "learning_rate": 0.00023114190856487262,
      "loss": 0.0515,
      "step": 4240
    },
    {
      "epoch": 1.0494926998267755,
      "grad_norm": 0.026082798838615417,
      "learning_rate": 0.00023104488812967217,
      "loss": 0.036,
      "step": 4241
    },
    {
      "epoch": 1.0497401633259094,
      "grad_norm": 0.043302759528160095,
      "learning_rate": 0.0002309478705656738,
      "loss": 0.0628,
      "step": 4242
    },
    {
      "epoch": 1.0499876268250432,
      "grad_norm": 0.05887509882450104,
      "learning_rate": 0.00023085085588757338,
      "loss": 0.0939,
      "step": 4243
    },
    {
      "epoch": 1.0502350903241773,
      "grad_norm": 0.05824275687336922,
      "learning_rate": 0.00023075384411006607,
      "loss": 0.065,
      "step": 4244
    },
    {
      "epoch": 1.0504825538233111,
      "grad_norm": 0.05050227791070938,
      "learning_rate": 0.00023065683524784645,
      "loss": 0.0687,
      "step": 4245
    },
    {
      "epoch": 1.050730017322445,
      "grad_norm": 0.047159433364868164,
      "learning_rate": 0.00023055982931560897,
      "loss": 0.0566,
      "step": 4246
    },
    {
      "epoch": 1.0509774808215788,
      "grad_norm": 0.037270233035087585,
      "learning_rate": 0.00023046282632804737,
      "loss": 0.0465,
      "step": 4247
    },
    {
      "epoch": 1.0512249443207127,
      "grad_norm": 0.04228666052222252,
      "learning_rate": 0.0002303658262998551,
      "loss": 0.037,
      "step": 4248
    },
    {
      "epoch": 1.0514724078198465,
      "grad_norm": 0.030990317463874817,
      "learning_rate": 0.00023026882924572525,
      "loss": 0.0341,
      "step": 4249
    },
    {
      "epoch": 1.0517198713189804,
      "grad_norm": 0.04026566073298454,
      "learning_rate": 0.00023017183518035008,
      "loss": 0.0456,
      "step": 4250
    },
    {
      "epoch": 1.0519673348181144,
      "grad_norm": 0.05147616192698479,
      "learning_rate": 0.0002300748441184219,
      "loss": 0.0604,
      "step": 4251
    },
    {
      "epoch": 1.0522147983172483,
      "grad_norm": 0.034533318132162094,
      "learning_rate": 0.00022997785607463208,
      "loss": 0.051,
      "step": 4252
    },
    {
      "epoch": 1.0524622618163821,
      "grad_norm": 0.03413247689604759,
      "learning_rate": 0.00022988087106367192,
      "loss": 0.0488,
      "step": 4253
    },
    {
      "epoch": 1.052709725315516,
      "grad_norm": 0.051135096698999405,
      "learning_rate": 0.0002297838891002321,
      "loss": 0.0438,
      "step": 4254
    },
    {
      "epoch": 1.0529571888146498,
      "grad_norm": 0.03117258846759796,
      "learning_rate": 0.00022968691019900275,
      "loss": 0.0312,
      "step": 4255
    },
    {
      "epoch": 1.0532046523137837,
      "grad_norm": 0.034302931278944016,
      "learning_rate": 0.00022958993437467373,
      "loss": 0.0561,
      "step": 4256
    },
    {
      "epoch": 1.0534521158129175,
      "grad_norm": 0.031270407140254974,
      "learning_rate": 0.00022949296164193425,
      "loss": 0.0403,
      "step": 4257
    },
    {
      "epoch": 1.0536995793120514,
      "grad_norm": 0.05327674373984337,
      "learning_rate": 0.00022939599201547313,
      "loss": 0.0505,
      "step": 4258
    },
    {
      "epoch": 1.0539470428111855,
      "grad_norm": 0.03554330766201019,
      "learning_rate": 0.00022929902550997887,
      "loss": 0.0524,
      "step": 4259
    },
    {
      "epoch": 1.0541945063103193,
      "grad_norm": 0.025498101487755775,
      "learning_rate": 0.00022920206214013916,
      "loss": 0.0435,
      "step": 4260
    },
    {
      "epoch": 1.0544419698094532,
      "grad_norm": 0.06596457213163376,
      "learning_rate": 0.00022910510192064161,
      "loss": 0.0638,
      "step": 4261
    },
    {
      "epoch": 1.054689433308587,
      "grad_norm": 0.031422216445207596,
      "learning_rate": 0.00022900814486617293,
      "loss": 0.0349,
      "step": 4262
    },
    {
      "epoch": 1.0549368968077208,
      "grad_norm": 0.03700647130608559,
      "learning_rate": 0.00022891119099141972,
      "loss": 0.0609,
      "step": 4263
    },
    {
      "epoch": 1.0551843603068547,
      "grad_norm": 0.03266005590558052,
      "learning_rate": 0.00022881424031106793,
      "loss": 0.0218,
      "step": 4264
    },
    {
      "epoch": 1.0554318238059885,
      "grad_norm": 0.029074661433696747,
      "learning_rate": 0.00022871729283980297,
      "loss": 0.0427,
      "step": 4265
    },
    {
      "epoch": 1.0556792873051224,
      "grad_norm": 0.04016367346048355,
      "learning_rate": 0.00022862034859230996,
      "loss": 0.0408,
      "step": 4266
    },
    {
      "epoch": 1.0559267508042565,
      "grad_norm": 0.06969396770000458,
      "learning_rate": 0.00022852340758327324,
      "loss": 0.0771,
      "step": 4267
    },
    {
      "epoch": 1.0561742143033903,
      "grad_norm": 0.03257341310381889,
      "learning_rate": 0.00022842646982737698,
      "loss": 0.0393,
      "step": 4268
    },
    {
      "epoch": 1.0564216778025242,
      "grad_norm": 0.07436996698379517,
      "learning_rate": 0.00022832953533930467,
      "loss": 0.0368,
      "step": 4269
    },
    {
      "epoch": 1.056669141301658,
      "grad_norm": 0.041797395795583725,
      "learning_rate": 0.00022823260413373932,
      "loss": 0.0895,
      "step": 4270
    },
    {
      "epoch": 1.0569166048007919,
      "grad_norm": 0.03190123289823532,
      "learning_rate": 0.00022813567622536352,
      "loss": 0.0395,
      "step": 4271
    },
    {
      "epoch": 1.0571640682999257,
      "grad_norm": 0.03779128938913345,
      "learning_rate": 0.0002280387516288592,
      "loss": 0.0445,
      "step": 4272
    },
    {
      "epoch": 1.0574115317990596,
      "grad_norm": 0.04811125993728638,
      "learning_rate": 0.00022794183035890797,
      "loss": 0.0262,
      "step": 4273
    },
    {
      "epoch": 1.0576589952981936,
      "grad_norm": 0.03538716956973076,
      "learning_rate": 0.0002278449124301909,
      "loss": 0.0571,
      "step": 4274
    },
    {
      "epoch": 1.0579064587973275,
      "grad_norm": 0.059475984424352646,
      "learning_rate": 0.00022774799785738834,
      "loss": 0.1459,
      "step": 4275
    },
    {
      "epoch": 1.0581539222964613,
      "grad_norm": 0.0357363186776638,
      "learning_rate": 0.0002276510866551806,
      "loss": 0.0482,
      "step": 4276
    },
    {
      "epoch": 1.0584013857955952,
      "grad_norm": 0.0369037464261055,
      "learning_rate": 0.00022755417883824687,
      "loss": 0.0505,
      "step": 4277
    },
    {
      "epoch": 1.058648849294729,
      "grad_norm": 0.04599189758300781,
      "learning_rate": 0.00022745727442126628,
      "loss": 0.043,
      "step": 4278
    },
    {
      "epoch": 1.0588963127938629,
      "grad_norm": 0.03353029116988182,
      "learning_rate": 0.00022736037341891742,
      "loss": 0.0359,
      "step": 4279
    },
    {
      "epoch": 1.0591437762929967,
      "grad_norm": 0.039066098630428314,
      "learning_rate": 0.00022726347584587804,
      "loss": 0.0359,
      "step": 4280
    },
    {
      "epoch": 1.0593912397921306,
      "grad_norm": 0.03193221241235733,
      "learning_rate": 0.00022716658171682575,
      "loss": 0.03,
      "step": 4281
    },
    {
      "epoch": 1.0596387032912646,
      "grad_norm": 0.09627215564250946,
      "learning_rate": 0.00022706969104643735,
      "loss": 0.0726,
      "step": 4282
    },
    {
      "epoch": 1.0598861667903985,
      "grad_norm": 0.041323285549879074,
      "learning_rate": 0.00022697280384938924,
      "loss": 0.094,
      "step": 4283
    },
    {
      "epoch": 1.0601336302895323,
      "grad_norm": 0.02622910775244236,
      "learning_rate": 0.0002268759201403575,
      "loss": 0.0498,
      "step": 4284
    },
    {
      "epoch": 1.0603810937886662,
      "grad_norm": 0.0455872006714344,
      "learning_rate": 0.00022677903993401712,
      "loss": 0.0551,
      "step": 4285
    },
    {
      "epoch": 1.0606285572878,
      "grad_norm": 0.11672249436378479,
      "learning_rate": 0.00022668216324504327,
      "loss": 0.053,
      "step": 4286
    },
    {
      "epoch": 1.0608760207869339,
      "grad_norm": 0.052230242639780045,
      "learning_rate": 0.00022658529008810988,
      "loss": 0.0838,
      "step": 4287
    },
    {
      "epoch": 1.0611234842860677,
      "grad_norm": 0.03100452572107315,
      "learning_rate": 0.00022648842047789093,
      "loss": 0.0288,
      "step": 4288
    },
    {
      "epoch": 1.0613709477852016,
      "grad_norm": 0.04639707878232002,
      "learning_rate": 0.00022639155442905962,
      "loss": 0.0511,
      "step": 4289
    },
    {
      "epoch": 1.0616184112843357,
      "grad_norm": 0.038758717477321625,
      "learning_rate": 0.00022629469195628848,
      "loss": 0.0456,
      "step": 4290
    },
    {
      "epoch": 1.0618658747834695,
      "grad_norm": 0.03787446394562721,
      "learning_rate": 0.0002261978330742498,
      "loss": 0.0442,
      "step": 4291
    },
    {
      "epoch": 1.0621133382826033,
      "grad_norm": 0.08224284648895264,
      "learning_rate": 0.00022610097779761495,
      "loss": 0.0845,
      "step": 4292
    },
    {
      "epoch": 1.0623608017817372,
      "grad_norm": 0.022507572546601295,
      "learning_rate": 0.0002260041261410551,
      "loss": 0.0413,
      "step": 4293
    },
    {
      "epoch": 1.062608265280871,
      "grad_norm": 0.0289597287774086,
      "learning_rate": 0.00022590727811924084,
      "loss": 0.0363,
      "step": 4294
    },
    {
      "epoch": 1.062855728780005,
      "grad_norm": 0.026125339791178703,
      "learning_rate": 0.0002258104337468418,
      "loss": 0.04,
      "step": 4295
    },
    {
      "epoch": 1.0631031922791387,
      "grad_norm": 0.026392871513962746,
      "learning_rate": 0.0002257135930385277,
      "loss": 0.0259,
      "step": 4296
    },
    {
      "epoch": 1.0633506557782728,
      "grad_norm": 0.07197372615337372,
      "learning_rate": 0.00022561675600896714,
      "loss": 0.1337,
      "step": 4297
    },
    {
      "epoch": 1.0635981192774067,
      "grad_norm": 0.030109426006674767,
      "learning_rate": 0.00022551992267282845,
      "loss": 0.0486,
      "step": 4298
    },
    {
      "epoch": 1.0638455827765405,
      "grad_norm": 0.03728478029370308,
      "learning_rate": 0.00022542309304477941,
      "loss": 0.0562,
      "step": 4299
    },
    {
      "epoch": 1.0640930462756744,
      "grad_norm": 0.030533453449606895,
      "learning_rate": 0.0002253262671394871,
      "loss": 0.022,
      "step": 4300
    },
    {
      "epoch": 1.0643405097748082,
      "grad_norm": 0.02338554710149765,
      "learning_rate": 0.0002252294449716181,
      "loss": 0.0244,
      "step": 4301
    },
    {
      "epoch": 1.064587973273942,
      "grad_norm": 0.0367557667195797,
      "learning_rate": 0.00022513262655583845,
      "loss": 0.0432,
      "step": 4302
    },
    {
      "epoch": 1.064835436773076,
      "grad_norm": 0.04399411007761955,
      "learning_rate": 0.0002250358119068135,
      "loss": 0.0464,
      "step": 4303
    },
    {
      "epoch": 1.0650829002722098,
      "grad_norm": 0.040037188678979874,
      "learning_rate": 0.00022493900103920836,
      "loss": 0.0431,
      "step": 4304
    },
    {
      "epoch": 1.0653303637713438,
      "grad_norm": 0.032520271837711334,
      "learning_rate": 0.00022484219396768715,
      "loss": 0.0315,
      "step": 4305
    },
    {
      "epoch": 1.0655778272704777,
      "grad_norm": 0.0369427315890789,
      "learning_rate": 0.0002247453907069137,
      "loss": 0.0299,
      "step": 4306
    },
    {
      "epoch": 1.0658252907696115,
      "grad_norm": 0.03283606097102165,
      "learning_rate": 0.00022464859127155107,
      "loss": 0.0412,
      "step": 4307
    },
    {
      "epoch": 1.0660727542687454,
      "grad_norm": 0.05906312167644501,
      "learning_rate": 0.00022455179567626186,
      "loss": 0.081,
      "step": 4308
    },
    {
      "epoch": 1.0663202177678792,
      "grad_norm": 0.04146648943424225,
      "learning_rate": 0.00022445500393570816,
      "loss": 0.0356,
      "step": 4309
    },
    {
      "epoch": 1.066567681267013,
      "grad_norm": 0.047882795333862305,
      "learning_rate": 0.00022435821606455126,
      "loss": 0.0761,
      "step": 4310
    },
    {
      "epoch": 1.066815144766147,
      "grad_norm": 0.039998311549425125,
      "learning_rate": 0.0002242614320774521,
      "loss": 0.0453,
      "step": 4311
    },
    {
      "epoch": 1.0670626082652808,
      "grad_norm": 0.032451264560222626,
      "learning_rate": 0.0002241646519890707,
      "loss": 0.0369,
      "step": 4312
    },
    {
      "epoch": 1.0673100717644148,
      "grad_norm": 0.056366633623838425,
      "learning_rate": 0.00022406787581406693,
      "loss": 0.0679,
      "step": 4313
    },
    {
      "epoch": 1.0675575352635487,
      "grad_norm": 0.05585365369915962,
      "learning_rate": 0.00022397110356709983,
      "loss": 0.0604,
      "step": 4314
    },
    {
      "epoch": 1.0678049987626825,
      "grad_norm": 0.053986627608537674,
      "learning_rate": 0.0002238743352628277,
      "loss": 0.077,
      "step": 4315
    },
    {
      "epoch": 1.0680524622618164,
      "grad_norm": 0.029465528205037117,
      "learning_rate": 0.00022377757091590855,
      "loss": 0.033,
      "step": 4316
    },
    {
      "epoch": 1.0682999257609502,
      "grad_norm": 0.05461987480521202,
      "learning_rate": 0.00022368081054099955,
      "loss": 0.0805,
      "step": 4317
    },
    {
      "epoch": 1.068547389260084,
      "grad_norm": 0.046937257051467896,
      "learning_rate": 0.0002235840541527574,
      "loss": 0.0648,
      "step": 4318
    },
    {
      "epoch": 1.068794852759218,
      "grad_norm": 0.05156581848859787,
      "learning_rate": 0.00022348730176583817,
      "loss": 0.0519,
      "step": 4319
    },
    {
      "epoch": 1.069042316258352,
      "grad_norm": 0.044083286076784134,
      "learning_rate": 0.0002233905533948972,
      "loss": 0.0513,
      "step": 4320
    },
    {
      "epoch": 1.0692897797574858,
      "grad_norm": 0.07689382135868073,
      "learning_rate": 0.00022329380905458955,
      "loss": 0.1162,
      "step": 4321
    },
    {
      "epoch": 1.0695372432566197,
      "grad_norm": 0.0335310660302639,
      "learning_rate": 0.00022319706875956915,
      "loss": 0.0413,
      "step": 4322
    },
    {
      "epoch": 1.0697847067557535,
      "grad_norm": 0.04091612622141838,
      "learning_rate": 0.00022310033252448985,
      "loss": 0.0475,
      "step": 4323
    },
    {
      "epoch": 1.0700321702548874,
      "grad_norm": 0.03591950237751007,
      "learning_rate": 0.00022300360036400465,
      "loss": 0.0497,
      "step": 4324
    },
    {
      "epoch": 1.0702796337540212,
      "grad_norm": 0.0378585159778595,
      "learning_rate": 0.0002229068722927658,
      "loss": 0.0337,
      "step": 4325
    },
    {
      "epoch": 1.070527097253155,
      "grad_norm": 0.04445488005876541,
      "learning_rate": 0.0002228101483254252,
      "loss": 0.0338,
      "step": 4326
    },
    {
      "epoch": 1.070774560752289,
      "grad_norm": 0.049066245555877686,
      "learning_rate": 0.00022271342847663387,
      "loss": 0.049,
      "step": 4327
    },
    {
      "epoch": 1.071022024251423,
      "grad_norm": 0.04145931079983711,
      "learning_rate": 0.00022261671276104232,
      "loss": 0.0523,
      "step": 4328
    },
    {
      "epoch": 1.0712694877505569,
      "grad_norm": 0.0421026386320591,
      "learning_rate": 0.00022252000119330067,
      "loss": 0.0522,
      "step": 4329
    },
    {
      "epoch": 1.0715169512496907,
      "grad_norm": 0.035681743174791336,
      "learning_rate": 0.00022242329378805787,
      "loss": 0.0421,
      "step": 4330
    },
    {
      "epoch": 1.0717644147488246,
      "grad_norm": 0.03506549075245857,
      "learning_rate": 0.0002223265905599629,
      "loss": 0.0605,
      "step": 4331
    },
    {
      "epoch": 1.0720118782479584,
      "grad_norm": 0.0754697173833847,
      "learning_rate": 0.00022222989152366338,
      "loss": 0.0823,
      "step": 4332
    },
    {
      "epoch": 1.0722593417470923,
      "grad_norm": 0.11030976474285126,
      "learning_rate": 0.00022213319669380695,
      "loss": 0.0772,
      "step": 4333
    },
    {
      "epoch": 1.072506805246226,
      "grad_norm": 0.06410490721464157,
      "learning_rate": 0.0002220365060850403,
      "loss": 0.047,
      "step": 4334
    },
    {
      "epoch": 1.07275426874536,
      "grad_norm": 0.05407809093594551,
      "learning_rate": 0.00022193981971200944,
      "loss": 0.0588,
      "step": 4335
    },
    {
      "epoch": 1.073001732244494,
      "grad_norm": 0.04122697189450264,
      "learning_rate": 0.00022184313758935993,
      "loss": 0.0337,
      "step": 4336
    },
    {
      "epoch": 1.0732491957436279,
      "grad_norm": 0.057381778955459595,
      "learning_rate": 0.00022174645973173645,
      "loss": 0.0355,
      "step": 4337
    },
    {
      "epoch": 1.0734966592427617,
      "grad_norm": 0.05147232487797737,
      "learning_rate": 0.00022164978615378314,
      "loss": 0.066,
      "step": 4338
    },
    {
      "epoch": 1.0737441227418956,
      "grad_norm": 0.0369790755212307,
      "learning_rate": 0.00022155311687014376,
      "loss": 0.0392,
      "step": 4339
    },
    {
      "epoch": 1.0739915862410294,
      "grad_norm": 0.07766517996788025,
      "learning_rate": 0.00022145645189546086,
      "loss": 0.0803,
      "step": 4340
    },
    {
      "epoch": 1.0742390497401633,
      "grad_norm": 0.0380689799785614,
      "learning_rate": 0.00022135979124437695,
      "loss": 0.0344,
      "step": 4341
    },
    {
      "epoch": 1.0744865132392971,
      "grad_norm": 0.036447081714868546,
      "learning_rate": 0.00022126313493153323,
      "loss": 0.0321,
      "step": 4342
    },
    {
      "epoch": 1.0747339767384312,
      "grad_norm": 0.036413971334695816,
      "learning_rate": 0.00022116648297157087,
      "loss": 0.0674,
      "step": 4343
    },
    {
      "epoch": 1.074981440237565,
      "grad_norm": 0.03754256293177605,
      "learning_rate": 0.00022106983537913005,
      "loss": 0.0489,
      "step": 4344
    },
    {
      "epoch": 1.0752289037366989,
      "grad_norm": 0.03448772430419922,
      "learning_rate": 0.00022097319216885032,
      "loss": 0.0582,
      "step": 4345
    },
    {
      "epoch": 1.0754763672358327,
      "grad_norm": 0.043863046914339066,
      "learning_rate": 0.0002208765533553706,
      "loss": 0.0443,
      "step": 4346
    },
    {
      "epoch": 1.0757238307349666,
      "grad_norm": 0.04378112405538559,
      "learning_rate": 0.0002207799189533291,
      "loss": 0.0366,
      "step": 4347
    },
    {
      "epoch": 1.0759712942341004,
      "grad_norm": 0.03425491601228714,
      "learning_rate": 0.00022068328897736333,
      "loss": 0.0463,
      "step": 4348
    },
    {
      "epoch": 1.0762187577332343,
      "grad_norm": 0.028142603114247322,
      "learning_rate": 0.00022058666344211049,
      "loss": 0.0267,
      "step": 4349
    },
    {
      "epoch": 1.0764662212323681,
      "grad_norm": 0.05808975547552109,
      "learning_rate": 0.00022049004236220643,
      "loss": 0.0654,
      "step": 4350
    },
    {
      "epoch": 1.0767136847315022,
      "grad_norm": 0.03977560997009277,
      "learning_rate": 0.000220393425752287,
      "loss": 0.0521,
      "step": 4351
    },
    {
      "epoch": 1.076961148230636,
      "grad_norm": 0.05638420954346657,
      "learning_rate": 0.00022029681362698694,
      "loss": 0.0643,
      "step": 4352
    },
    {
      "epoch": 1.07720861172977,
      "grad_norm": 0.03629923239350319,
      "learning_rate": 0.0002202002060009405,
      "loss": 0.0308,
      "step": 4353
    },
    {
      "epoch": 1.0774560752289037,
      "grad_norm": 0.03448617830872536,
      "learning_rate": 0.00022010360288878122,
      "loss": 0.0345,
      "step": 4354
    },
    {
      "epoch": 1.0777035387280376,
      "grad_norm": 0.03953889012336731,
      "learning_rate": 0.00022000700430514186,
      "loss": 0.0577,
      "step": 4355
    },
    {
      "epoch": 1.0779510022271714,
      "grad_norm": 0.04360634461045265,
      "learning_rate": 0.00021991041026465466,
      "loss": 0.0573,
      "step": 4356
    },
    {
      "epoch": 1.0781984657263053,
      "grad_norm": 0.04645049199461937,
      "learning_rate": 0.000219813820781951,
      "loss": 0.0425,
      "step": 4357
    },
    {
      "epoch": 1.0784459292254391,
      "grad_norm": 0.020021963864564896,
      "learning_rate": 0.00021971723587166164,
      "loss": 0.0328,
      "step": 4358
    },
    {
      "epoch": 1.0786933927245732,
      "grad_norm": 0.04056544974446297,
      "learning_rate": 0.00021962065554841683,
      "loss": 0.1092,
      "step": 4359
    },
    {
      "epoch": 1.078940856223707,
      "grad_norm": 0.049504972994327545,
      "learning_rate": 0.0002195240798268458,
      "loss": 0.0373,
      "step": 4360
    },
    {
      "epoch": 1.079188319722841,
      "grad_norm": 0.03774910047650337,
      "learning_rate": 0.00021942750872157733,
      "loss": 0.0252,
      "step": 4361
    },
    {
      "epoch": 1.0794357832219748,
      "grad_norm": 0.10347683727741241,
      "learning_rate": 0.0002193309422472393,
      "loss": 0.0594,
      "step": 4362
    },
    {
      "epoch": 1.0796832467211086,
      "grad_norm": 0.04427259787917137,
      "learning_rate": 0.00021923438041845903,
      "loss": 0.051,
      "step": 4363
    },
    {
      "epoch": 1.0799307102202425,
      "grad_norm": 0.03221619129180908,
      "learning_rate": 0.00021913782324986322,
      "loss": 0.0612,
      "step": 4364
    },
    {
      "epoch": 1.0801781737193763,
      "grad_norm": 0.05065833777189255,
      "learning_rate": 0.0002190412707560776,
      "loss": 0.0379,
      "step": 4365
    },
    {
      "epoch": 1.0804256372185104,
      "grad_norm": 0.039836421608924866,
      "learning_rate": 0.00021894472295172734,
      "loss": 0.0551,
      "step": 4366
    },
    {
      "epoch": 1.0806731007176442,
      "grad_norm": 0.04752777889370918,
      "learning_rate": 0.00021884817985143708,
      "loss": 0.0582,
      "step": 4367
    },
    {
      "epoch": 1.080920564216778,
      "grad_norm": 0.06197037175297737,
      "learning_rate": 0.00021875164146983037,
      "loss": 0.0527,
      "step": 4368
    },
    {
      "epoch": 1.081168027715912,
      "grad_norm": 0.044319603592157364,
      "learning_rate": 0.0002186551078215304,
      "loss": 0.0727,
      "step": 4369
    },
    {
      "epoch": 1.0814154912150458,
      "grad_norm": 0.04486612230539322,
      "learning_rate": 0.00021855857892115932,
      "loss": 0.0552,
      "step": 4370
    },
    {
      "epoch": 1.0816629547141796,
      "grad_norm": 0.09787023812532425,
      "learning_rate": 0.00021846205478333883,
      "loss": 0.0527,
      "step": 4371
    },
    {
      "epoch": 1.0819104182133135,
      "grad_norm": 0.05144360661506653,
      "learning_rate": 0.00021836553542268987,
      "loss": 0.0926,
      "step": 4372
    },
    {
      "epoch": 1.0821578817124473,
      "grad_norm": 0.02728385105729103,
      "learning_rate": 0.0002182690208538324,
      "loss": 0.0244,
      "step": 4373
    },
    {
      "epoch": 1.0824053452115814,
      "grad_norm": 0.06808751076459885,
      "learning_rate": 0.00021817251109138605,
      "loss": 0.0759,
      "step": 4374
    },
    {
      "epoch": 1.0826528087107152,
      "grad_norm": 0.04117904230952263,
      "learning_rate": 0.0002180760061499693,
      "loss": 0.026,
      "step": 4375
    },
    {
      "epoch": 1.082900272209849,
      "grad_norm": 0.08819013833999634,
      "learning_rate": 0.0002179795060442003,
      "loss": 0.0768,
      "step": 4376
    },
    {
      "epoch": 1.083147735708983,
      "grad_norm": 0.040230873972177505,
      "learning_rate": 0.00021788301078869626,
      "loss": 0.0517,
      "step": 4377
    },
    {
      "epoch": 1.0833951992081168,
      "grad_norm": 0.08995991945266724,
      "learning_rate": 0.0002177865203980736,
      "loss": 0.0457,
      "step": 4378
    },
    {
      "epoch": 1.0836426627072506,
      "grad_norm": 0.05208728089928627,
      "learning_rate": 0.00021769003488694816,
      "loss": 0.035,
      "step": 4379
    },
    {
      "epoch": 1.0838901262063845,
      "grad_norm": 0.032912708818912506,
      "learning_rate": 0.0002175935542699349,
      "loss": 0.0668,
      "step": 4380
    },
    {
      "epoch": 1.0841375897055183,
      "grad_norm": 0.06064191460609436,
      "learning_rate": 0.00021749707856164813,
      "loss": 0.0742,
      "step": 4381
    },
    {
      "epoch": 1.0843850532046524,
      "grad_norm": 0.059376660734415054,
      "learning_rate": 0.00021740060777670144,
      "loss": 0.0901,
      "step": 4382
    },
    {
      "epoch": 1.0846325167037862,
      "grad_norm": 0.07045242190361023,
      "learning_rate": 0.0002173041419297075,
      "loss": 0.0743,
      "step": 4383
    },
    {
      "epoch": 1.08487998020292,
      "grad_norm": 0.05004887282848358,
      "learning_rate": 0.00021720768103527857,
      "loss": 0.0643,
      "step": 4384
    },
    {
      "epoch": 1.085127443702054,
      "grad_norm": 0.04496655985713005,
      "learning_rate": 0.00021711122510802567,
      "loss": 0.0623,
      "step": 4385
    },
    {
      "epoch": 1.0853749072011878,
      "grad_norm": 0.034747324883937836,
      "learning_rate": 0.0002170147741625595,
      "loss": 0.0399,
      "step": 4386
    },
    {
      "epoch": 1.0856223707003216,
      "grad_norm": 0.04563961178064346,
      "learning_rate": 0.0002169183282134899,
      "loss": 0.0545,
      "step": 4387
    },
    {
      "epoch": 1.0858698341994555,
      "grad_norm": 0.034368108958005905,
      "learning_rate": 0.00021682188727542576,
      "loss": 0.0556,
      "step": 4388
    },
    {
      "epoch": 1.0861172976985896,
      "grad_norm": 0.04488589987158775,
      "learning_rate": 0.00021672545136297547,
      "loss": 0.0597,
      "step": 4389
    },
    {
      "epoch": 1.0863647611977234,
      "grad_norm": 0.02554619312286377,
      "learning_rate": 0.00021662902049074644,
      "loss": 0.0333,
      "step": 4390
    },
    {
      "epoch": 1.0866122246968573,
      "grad_norm": 0.036714162677526474,
      "learning_rate": 0.00021653259467334547,
      "loss": 0.0519,
      "step": 4391
    },
    {
      "epoch": 1.086859688195991,
      "grad_norm": 0.07734262943267822,
      "learning_rate": 0.00021643617392537866,
      "loss": 0.0481,
      "step": 4392
    },
    {
      "epoch": 1.087107151695125,
      "grad_norm": 0.03574677184224129,
      "learning_rate": 0.00021633975826145093,
      "loss": 0.0415,
      "step": 4393
    },
    {
      "epoch": 1.0873546151942588,
      "grad_norm": 0.06866567581892014,
      "learning_rate": 0.0002162433476961671,
      "loss": 0.0727,
      "step": 4394
    },
    {
      "epoch": 1.0876020786933926,
      "grad_norm": 0.03303612768650055,
      "learning_rate": 0.00021614694224413047,
      "loss": 0.0558,
      "step": 4395
    },
    {
      "epoch": 1.0878495421925267,
      "grad_norm": 0.046006686985492706,
      "learning_rate": 0.00021605054191994416,
      "loss": 0.046,
      "step": 4396
    },
    {
      "epoch": 1.0880970056916606,
      "grad_norm": 0.045097917318344116,
      "learning_rate": 0.0002159541467382103,
      "loss": 0.0533,
      "step": 4397
    },
    {
      "epoch": 1.0883444691907944,
      "grad_norm": 0.07259654253721237,
      "learning_rate": 0.00021585775671353012,
      "loss": 0.0502,
      "step": 4398
    },
    {
      "epoch": 1.0885919326899283,
      "grad_norm": 0.04137963429093361,
      "learning_rate": 0.0002157613718605043,
      "loss": 0.0542,
      "step": 4399
    },
    {
      "epoch": 1.0888393961890621,
      "grad_norm": 0.07283513993024826,
      "learning_rate": 0.00021566499219373246,
      "loss": 0.026,
      "step": 4400
    },
    {
      "epoch": 1.0888393961890621,
      "eval_loss": 0.29154932498931885,
      "eval_runtime": 202.7834,
      "eval_samples_per_second": 4.931,
      "eval_steps_per_second": 0.311,
      "step": 4400
    },
    {
      "epoch": 1.089086859688196,
      "grad_norm": 0.03996681049466133,
      "learning_rate": 0.00021556861772781364,
      "loss": 0.0617,
      "step": 4401
    },
    {
      "epoch": 1.0893343231873298,
      "grad_norm": 0.04348855838179588,
      "learning_rate": 0.00021547224847734626,
      "loss": 0.0405,
      "step": 4402
    },
    {
      "epoch": 1.0895817866864637,
      "grad_norm": 0.12148407846689224,
      "learning_rate": 0.0002153758844569274,
      "loss": 0.1063,
      "step": 4403
    },
    {
      "epoch": 1.0898292501855975,
      "grad_norm": 0.03382192552089691,
      "learning_rate": 0.00021527952568115393,
      "loss": 0.0514,
      "step": 4404
    },
    {
      "epoch": 1.0900767136847316,
      "grad_norm": 0.0367535762488842,
      "learning_rate": 0.0002151831721646215,
      "loss": 0.0466,
      "step": 4405
    },
    {
      "epoch": 1.0903241771838654,
      "grad_norm": 0.028310256078839302,
      "learning_rate": 0.0002150868239219253,
      "loss": 0.0527,
      "step": 4406
    },
    {
      "epoch": 1.0905716406829993,
      "grad_norm": 0.05069855600595474,
      "learning_rate": 0.00021499048096765953,
      "loss": 0.0418,
      "step": 4407
    },
    {
      "epoch": 1.0908191041821331,
      "grad_norm": 0.051558636128902435,
      "learning_rate": 0.0002148941433164175,
      "loss": 0.0576,
      "step": 4408
    },
    {
      "epoch": 1.091066567681267,
      "grad_norm": 0.027867266908288002,
      "learning_rate": 0.00021479781098279196,
      "loss": 0.0343,
      "step": 4409
    },
    {
      "epoch": 1.0913140311804008,
      "grad_norm": 0.042684298008680344,
      "learning_rate": 0.00021470148398137467,
      "loss": 0.0339,
      "step": 4410
    },
    {
      "epoch": 1.0915614946795347,
      "grad_norm": 0.033125147223472595,
      "learning_rate": 0.0002146051623267566,
      "loss": 0.0366,
      "step": 4411
    },
    {
      "epoch": 1.0918089581786687,
      "grad_norm": 0.03496846929192543,
      "learning_rate": 0.0002145088460335281,
      "loss": 0.0217,
      "step": 4412
    },
    {
      "epoch": 1.0920564216778026,
      "grad_norm": 0.03905082866549492,
      "learning_rate": 0.00021441253511627844,
      "loss": 0.044,
      "step": 4413
    },
    {
      "epoch": 1.0923038851769364,
      "grad_norm": 0.02791721560060978,
      "learning_rate": 0.0002143162295895963,
      "loss": 0.0405,
      "step": 4414
    },
    {
      "epoch": 1.0925513486760703,
      "grad_norm": 0.04140522703528404,
      "learning_rate": 0.00021421992946806927,
      "loss": 0.0387,
      "step": 4415
    },
    {
      "epoch": 1.0927988121752041,
      "grad_norm": 0.0263608917593956,
      "learning_rate": 0.00021412363476628447,
      "loss": 0.0297,
      "step": 4416
    },
    {
      "epoch": 1.093046275674338,
      "grad_norm": 0.046177882701158524,
      "learning_rate": 0.000214027345498828,
      "loss": 0.0395,
      "step": 4417
    },
    {
      "epoch": 1.0932937391734718,
      "grad_norm": 0.02698436565697193,
      "learning_rate": 0.000213931061680285,
      "loss": 0.0424,
      "step": 4418
    },
    {
      "epoch": 1.093541202672606,
      "grad_norm": 0.0454169437289238,
      "learning_rate": 0.00021383478332524013,
      "loss": 0.0508,
      "step": 4419
    },
    {
      "epoch": 1.0937886661717398,
      "grad_norm": 0.05840460583567619,
      "learning_rate": 0.00021373851044827687,
      "loss": 0.0516,
      "step": 4420
    },
    {
      "epoch": 1.0940361296708736,
      "grad_norm": 0.042757295072078705,
      "learning_rate": 0.00021364224306397818,
      "loss": 0.0352,
      "step": 4421
    },
    {
      "epoch": 1.0942835931700075,
      "grad_norm": 0.05513347312808037,
      "learning_rate": 0.00021354598118692602,
      "loss": 0.0892,
      "step": 4422
    },
    {
      "epoch": 1.0945310566691413,
      "grad_norm": 0.05446022376418114,
      "learning_rate": 0.00021344972483170146,
      "loss": 0.0418,
      "step": 4423
    },
    {
      "epoch": 1.0947785201682751,
      "grad_norm": 0.03534473478794098,
      "learning_rate": 0.00021335347401288493,
      "loss": 0.0451,
      "step": 4424
    },
    {
      "epoch": 1.095025983667409,
      "grad_norm": 0.024713722988963127,
      "learning_rate": 0.00021325722874505576,
      "loss": 0.0354,
      "step": 4425
    },
    {
      "epoch": 1.0952734471665428,
      "grad_norm": 0.03159763664007187,
      "learning_rate": 0.00021316098904279265,
      "loss": 0.0327,
      "step": 4426
    },
    {
      "epoch": 1.0955209106656767,
      "grad_norm": 0.03858979418873787,
      "learning_rate": 0.00021306475492067348,
      "loss": 0.0512,
      "step": 4427
    },
    {
      "epoch": 1.0957683741648108,
      "grad_norm": 0.03834148868918419,
      "learning_rate": 0.00021296852639327495,
      "loss": 0.0294,
      "step": 4428
    },
    {
      "epoch": 1.0960158376639446,
      "grad_norm": 0.04703143611550331,
      "learning_rate": 0.00021287230347517356,
      "loss": 0.0288,
      "step": 4429
    },
    {
      "epoch": 1.0962633011630785,
      "grad_norm": 0.046100325882434845,
      "learning_rate": 0.0002127760861809441,
      "loss": 0.0518,
      "step": 4430
    },
    {
      "epoch": 1.0965107646622123,
      "grad_norm": 0.037596724927425385,
      "learning_rate": 0.00021267987452516126,
      "loss": 0.0355,
      "step": 4431
    },
    {
      "epoch": 1.0967582281613462,
      "grad_norm": 0.04682014510035515,
      "learning_rate": 0.00021258366852239857,
      "loss": 0.067,
      "step": 4432
    },
    {
      "epoch": 1.09700569166048,
      "grad_norm": 0.060224588960409164,
      "learning_rate": 0.00021248746818722857,
      "loss": 0.072,
      "step": 4433
    },
    {
      "epoch": 1.0972531551596139,
      "grad_norm": 0.04531696066260338,
      "learning_rate": 0.00021239127353422325,
      "loss": 0.0555,
      "step": 4434
    },
    {
      "epoch": 1.097500618658748,
      "grad_norm": 0.04196810722351074,
      "learning_rate": 0.00021229508457795343,
      "loss": 0.0348,
      "step": 4435
    },
    {
      "epoch": 1.0977480821578818,
      "grad_norm": 0.05493038892745972,
      "learning_rate": 0.00021219890133298926,
      "loss": 0.0811,
      "step": 4436
    },
    {
      "epoch": 1.0979955456570156,
      "grad_norm": 0.07934986799955368,
      "learning_rate": 0.0002121027238139001,
      "loss": 0.0442,
      "step": 4437
    },
    {
      "epoch": 1.0982430091561495,
      "grad_norm": 0.04135345667600632,
      "learning_rate": 0.0002120065520352541,
      "loss": 0.0524,
      "step": 4438
    },
    {
      "epoch": 1.0984904726552833,
      "grad_norm": 0.07164911180734634,
      "learning_rate": 0.00021191038601161904,
      "loss": 0.0553,
      "step": 4439
    },
    {
      "epoch": 1.0987379361544172,
      "grad_norm": 0.035446710884571075,
      "learning_rate": 0.00021181422575756125,
      "loss": 0.0375,
      "step": 4440
    },
    {
      "epoch": 1.098985399653551,
      "grad_norm": 0.04542998969554901,
      "learning_rate": 0.00021171807128764664,
      "loss": 0.0446,
      "step": 4441
    },
    {
      "epoch": 1.099232863152685,
      "grad_norm": 0.07106927782297134,
      "learning_rate": 0.00021162192261644017,
      "loss": 0.0329,
      "step": 4442
    },
    {
      "epoch": 1.099480326651819,
      "grad_norm": 0.07807460427284241,
      "learning_rate": 0.00021152577975850568,
      "loss": 0.1191,
      "step": 4443
    },
    {
      "epoch": 1.0997277901509528,
      "grad_norm": 0.039542242884635925,
      "learning_rate": 0.00021142964272840642,
      "loss": 0.0457,
      "step": 4444
    },
    {
      "epoch": 1.0999752536500866,
      "grad_norm": 0.06650743633508682,
      "learning_rate": 0.0002113335115407045,
      "loss": 0.0436,
      "step": 4445
    },
    {
      "epoch": 1.1002227171492205,
      "grad_norm": 0.03722964972257614,
      "learning_rate": 0.0002112373862099613,
      "loss": 0.0373,
      "step": 4446
    },
    {
      "epoch": 1.1004701806483543,
      "grad_norm": 0.03743334114551544,
      "learning_rate": 0.00021114126675073752,
      "loss": 0.0434,
      "step": 4447
    },
    {
      "epoch": 1.1007176441474882,
      "grad_norm": 0.04332674294710159,
      "learning_rate": 0.00021104515317759234,
      "loss": 0.0478,
      "step": 4448
    },
    {
      "epoch": 1.100965107646622,
      "grad_norm": 0.034713178873062134,
      "learning_rate": 0.00021094904550508485,
      "loss": 0.0306,
      "step": 4449
    },
    {
      "epoch": 1.101212571145756,
      "grad_norm": 0.03314900025725365,
      "learning_rate": 0.00021085294374777248,
      "loss": 0.0525,
      "step": 4450
    },
    {
      "epoch": 1.10146003464489,
      "grad_norm": 0.03497803956270218,
      "learning_rate": 0.00021075684792021233,
      "loss": 0.0393,
      "step": 4451
    },
    {
      "epoch": 1.1017074981440238,
      "grad_norm": 0.047504764050245285,
      "learning_rate": 0.0002106607580369604,
      "loss": 0.0637,
      "step": 4452
    },
    {
      "epoch": 1.1019549616431576,
      "grad_norm": 0.04231096804141998,
      "learning_rate": 0.0002105646741125717,
      "loss": 0.0481,
      "step": 4453
    },
    {
      "epoch": 1.1022024251422915,
      "grad_norm": 0.025989310815930367,
      "learning_rate": 0.00021046859616160053,
      "loss": 0.0415,
      "step": 4454
    },
    {
      "epoch": 1.1024498886414253,
      "grad_norm": 0.029178304597735405,
      "learning_rate": 0.00021037252419860004,
      "loss": 0.0454,
      "step": 4455
    },
    {
      "epoch": 1.1026973521405592,
      "grad_norm": 0.0437069870531559,
      "learning_rate": 0.00021027645823812268,
      "loss": 0.0545,
      "step": 4456
    },
    {
      "epoch": 1.102944815639693,
      "grad_norm": 0.03710179403424263,
      "learning_rate": 0.00021018039829472004,
      "loss": 0.063,
      "step": 4457
    },
    {
      "epoch": 1.1031922791388271,
      "grad_norm": 0.06822751462459564,
      "learning_rate": 0.00021008434438294244,
      "loss": 0.0517,
      "step": 4458
    },
    {
      "epoch": 1.103439742637961,
      "grad_norm": 0.042029622942209244,
      "learning_rate": 0.00020998829651733974,
      "loss": 0.0486,
      "step": 4459
    },
    {
      "epoch": 1.1036872061370948,
      "grad_norm": 0.03526463732123375,
      "learning_rate": 0.00020989225471246058,
      "loss": 0.0337,
      "step": 4460
    },
    {
      "epoch": 1.1039346696362287,
      "grad_norm": 0.05379268899559975,
      "learning_rate": 0.0002097962189828528,
      "loss": 0.0689,
      "step": 4461
    },
    {
      "epoch": 1.1041821331353625,
      "grad_norm": 0.02852252870798111,
      "learning_rate": 0.00020970018934306328,
      "loss": 0.0396,
      "step": 4462
    },
    {
      "epoch": 1.1044295966344964,
      "grad_norm": 0.03769409656524658,
      "learning_rate": 0.00020960416580763797,
      "loss": 0.0268,
      "step": 4463
    },
    {
      "epoch": 1.1046770601336302,
      "grad_norm": 0.06656558066606522,
      "learning_rate": 0.000209508148391122,
      "loss": 0.0709,
      "step": 4464
    },
    {
      "epoch": 1.1049245236327643,
      "grad_norm": 0.056517310440540314,
      "learning_rate": 0.0002094121371080594,
      "loss": 0.083,
      "step": 4465
    },
    {
      "epoch": 1.1051719871318981,
      "grad_norm": 0.04631087929010391,
      "learning_rate": 0.0002093161319729933,
      "loss": 0.063,
      "step": 4466
    },
    {
      "epoch": 1.105419450631032,
      "grad_norm": 0.030212746933102608,
      "learning_rate": 0.00020922013300046617,
      "loss": 0.0257,
      "step": 4467
    },
    {
      "epoch": 1.1056669141301658,
      "grad_norm": 0.029035139828920364,
      "learning_rate": 0.00020912414020501916,
      "loss": 0.0383,
      "step": 4468
    },
    {
      "epoch": 1.1059143776292997,
      "grad_norm": 0.06881083548069,
      "learning_rate": 0.00020902815360119276,
      "loss": 0.0902,
      "step": 4469
    },
    {
      "epoch": 1.1061618411284335,
      "grad_norm": 0.04383227229118347,
      "learning_rate": 0.00020893217320352636,
      "loss": 0.0322,
      "step": 4470
    },
    {
      "epoch": 1.1064093046275674,
      "grad_norm": 0.0515768863260746,
      "learning_rate": 0.00020883619902655848,
      "loss": 0.0268,
      "step": 4471
    },
    {
      "epoch": 1.1066567681267012,
      "grad_norm": 0.042243920266628265,
      "learning_rate": 0.00020874023108482677,
      "loss": 0.0552,
      "step": 4472
    },
    {
      "epoch": 1.1069042316258353,
      "grad_norm": 0.03915639966726303,
      "learning_rate": 0.0002086442693928677,
      "loss": 0.0537,
      "step": 4473
    },
    {
      "epoch": 1.1071516951249691,
      "grad_norm": 0.039409536868333817,
      "learning_rate": 0.00020854831396521713,
      "loss": 0.0542,
      "step": 4474
    },
    {
      "epoch": 1.107399158624103,
      "grad_norm": 0.06456924974918365,
      "learning_rate": 0.00020845236481640956,
      "loss": 0.0493,
      "step": 4475
    },
    {
      "epoch": 1.1076466221232368,
      "grad_norm": 0.05169759690761566,
      "learning_rate": 0.00020835642196097897,
      "loss": 0.0467,
      "step": 4476
    },
    {
      "epoch": 1.1078940856223707,
      "grad_norm": 0.029161320999264717,
      "learning_rate": 0.00020826048541345822,
      "loss": 0.0363,
      "step": 4477
    },
    {
      "epoch": 1.1081415491215045,
      "grad_norm": 0.041298359632492065,
      "learning_rate": 0.000208164555188379,
      "loss": 0.0526,
      "step": 4478
    },
    {
      "epoch": 1.1083890126206384,
      "grad_norm": 0.04779791831970215,
      "learning_rate": 0.00020806863130027234,
      "loss": 0.0876,
      "step": 4479
    },
    {
      "epoch": 1.1086364761197722,
      "grad_norm": 0.028122585266828537,
      "learning_rate": 0.00020797271376366813,
      "loss": 0.0515,
      "step": 4480
    },
    {
      "epoch": 1.1088839396189063,
      "grad_norm": 0.058936331421136856,
      "learning_rate": 0.00020787680259309538,
      "loss": 0.0605,
      "step": 4481
    },
    {
      "epoch": 1.1091314031180401,
      "grad_norm": 0.03977346420288086,
      "learning_rate": 0.0002077808978030822,
      "loss": 0.0453,
      "step": 4482
    },
    {
      "epoch": 1.109378866617174,
      "grad_norm": 0.06180174648761749,
      "learning_rate": 0.00020768499940815546,
      "loss": 0.0592,
      "step": 4483
    },
    {
      "epoch": 1.1096263301163078,
      "grad_norm": 0.033429212868213654,
      "learning_rate": 0.00020758910742284153,
      "loss": 0.022,
      "step": 4484
    },
    {
      "epoch": 1.1098737936154417,
      "grad_norm": 0.04693148285150528,
      "learning_rate": 0.00020749322186166524,
      "loss": 0.0524,
      "step": 4485
    },
    {
      "epoch": 1.1101212571145755,
      "grad_norm": 0.04947527125477791,
      "learning_rate": 0.0002073973427391509,
      "loss": 0.0418,
      "step": 4486
    },
    {
      "epoch": 1.1103687206137094,
      "grad_norm": 0.07250291854143143,
      "learning_rate": 0.0002073014700698217,
      "loss": 0.0472,
      "step": 4487
    },
    {
      "epoch": 1.1106161841128435,
      "grad_norm": 0.03872476518154144,
      "learning_rate": 0.00020720560386819974,
      "loss": 0.0355,
      "step": 4488
    },
    {
      "epoch": 1.1108636476119773,
      "grad_norm": 0.029285358265042305,
      "learning_rate": 0.00020710974414880627,
      "loss": 0.0238,
      "step": 4489
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.0518534779548645,
      "learning_rate": 0.00020701389092616157,
      "loss": 0.0465,
      "step": 4490
    },
    {
      "epoch": 1.111358574610245,
      "grad_norm": 0.03126759082078934,
      "learning_rate": 0.00020691804421478477,
      "loss": 0.0316,
      "step": 4491
    },
    {
      "epoch": 1.1116060381093789,
      "grad_norm": 0.0468544103205204,
      "learning_rate": 0.00020682220402919434,
      "loss": 0.0433,
      "step": 4492
    },
    {
      "epoch": 1.1118535016085127,
      "grad_norm": 0.029826948419213295,
      "learning_rate": 0.0002067263703839073,
      "loss": 0.0337,
      "step": 4493
    },
    {
      "epoch": 1.1121009651076466,
      "grad_norm": 0.02931400015950203,
      "learning_rate": 0.00020663054329344012,
      "loss": 0.0471,
      "step": 4494
    },
    {
      "epoch": 1.1123484286067804,
      "grad_norm": 0.03819018229842186,
      "learning_rate": 0.0002065347227723081,
      "loss": 0.0321,
      "step": 4495
    },
    {
      "epoch": 1.1125958921059145,
      "grad_norm": 0.07129565626382828,
      "learning_rate": 0.00020643890883502536,
      "loss": 0.0655,
      "step": 4496
    },
    {
      "epoch": 1.1128433556050483,
      "grad_norm": 0.03736826777458191,
      "learning_rate": 0.0002063431014961054,
      "loss": 0.0389,
      "step": 4497
    },
    {
      "epoch": 1.1130908191041822,
      "grad_norm": 0.04386865347623825,
      "learning_rate": 0.00020624730077006037,
      "loss": 0.0623,
      "step": 4498
    },
    {
      "epoch": 1.113338282603316,
      "grad_norm": 0.03776480257511139,
      "learning_rate": 0.00020615150667140157,
      "loss": 0.0375,
      "step": 4499
    },
    {
      "epoch": 1.1135857461024499,
      "grad_norm": 0.0289884302765131,
      "learning_rate": 0.00020605571921463947,
      "loss": 0.0451,
      "step": 4500
    },
    {
      "epoch": 1.1138332096015837,
      "grad_norm": 0.026902219280600548,
      "learning_rate": 0.00020595993841428313,
      "loss": 0.0306,
      "step": 4501
    },
    {
      "epoch": 1.1140806731007176,
      "grad_norm": 0.04158712178468704,
      "learning_rate": 0.00020586416428484104,
      "loss": 0.0364,
      "step": 4502
    },
    {
      "epoch": 1.1143281365998514,
      "grad_norm": 0.058018527925014496,
      "learning_rate": 0.00020576839684082021,
      "loss": 0.094,
      "step": 4503
    },
    {
      "epoch": 1.1145756000989855,
      "grad_norm": 0.08549182116985321,
      "learning_rate": 0.00020567263609672714,
      "loss": 0.0876,
      "step": 4504
    },
    {
      "epoch": 1.1148230635981193,
      "grad_norm": 0.055031631141901016,
      "learning_rate": 0.000205576882067067,
      "loss": 0.0421,
      "step": 4505
    },
    {
      "epoch": 1.1150705270972532,
      "grad_norm": 0.04229794070124626,
      "learning_rate": 0.00020548113476634392,
      "loss": 0.053,
      "step": 4506
    },
    {
      "epoch": 1.115317990596387,
      "grad_norm": 0.03143904730677605,
      "learning_rate": 0.00020538539420906124,
      "loss": 0.0418,
      "step": 4507
    },
    {
      "epoch": 1.1155654540955209,
      "grad_norm": 0.04584604874253273,
      "learning_rate": 0.00020528966040972102,
      "loss": 0.0451,
      "step": 4508
    },
    {
      "epoch": 1.1158129175946547,
      "grad_norm": 0.08361193537712097,
      "learning_rate": 0.00020519393338282443,
      "loss": 0.089,
      "step": 4509
    },
    {
      "epoch": 1.1160603810937886,
      "grad_norm": 0.04455215111374855,
      "learning_rate": 0.00020509821314287184,
      "loss": 0.0517,
      "step": 4510
    },
    {
      "epoch": 1.1163078445929226,
      "grad_norm": 0.04435576871037483,
      "learning_rate": 0.00020500249970436195,
      "loss": 0.0676,
      "step": 4511
    },
    {
      "epoch": 1.1165553080920565,
      "grad_norm": 0.0688161700963974,
      "learning_rate": 0.00020490679308179322,
      "loss": 0.0854,
      "step": 4512
    },
    {
      "epoch": 1.1168027715911903,
      "grad_norm": 0.053963299840688705,
      "learning_rate": 0.0002048110932896624,
      "loss": 0.0366,
      "step": 4513
    },
    {
      "epoch": 1.1170502350903242,
      "grad_norm": 0.07740619033575058,
      "learning_rate": 0.0002047154003424657,
      "loss": 0.0531,
      "step": 4514
    },
    {
      "epoch": 1.117297698589458,
      "grad_norm": 0.04001203179359436,
      "learning_rate": 0.00020461971425469802,
      "loss": 0.0378,
      "step": 4515
    },
    {
      "epoch": 1.117545162088592,
      "grad_norm": 0.06733714789152145,
      "learning_rate": 0.00020452403504085323,
      "loss": 0.0773,
      "step": 4516
    },
    {
      "epoch": 1.1177926255877257,
      "grad_norm": 0.08425793796777725,
      "learning_rate": 0.00020442836271542436,
      "loss": 0.1513,
      "step": 4517
    },
    {
      "epoch": 1.1180400890868596,
      "grad_norm": 0.03970044478774071,
      "learning_rate": 0.0002043326972929031,
      "loss": 0.0598,
      "step": 4518
    },
    {
      "epoch": 1.1182875525859937,
      "grad_norm": 0.042693737894296646,
      "learning_rate": 0.0002042370387877803,
      "loss": 0.0618,
      "step": 4519
    },
    {
      "epoch": 1.1185350160851275,
      "grad_norm": 0.0814618319272995,
      "learning_rate": 0.00020414138721454585,
      "loss": 0.06,
      "step": 4520
    },
    {
      "epoch": 1.1187824795842614,
      "grad_norm": 0.048586491495370865,
      "learning_rate": 0.00020404574258768826,
      "loss": 0.0441,
      "step": 4521
    },
    {
      "epoch": 1.1190299430833952,
      "grad_norm": 0.02886531874537468,
      "learning_rate": 0.00020395010492169533,
      "loss": 0.043,
      "step": 4522
    },
    {
      "epoch": 1.119277406582529,
      "grad_norm": 0.05403829738497734,
      "learning_rate": 0.00020385447423105357,
      "loss": 0.085,
      "step": 4523
    },
    {
      "epoch": 1.119524870081663,
      "grad_norm": 0.046098411083221436,
      "learning_rate": 0.0002037588505302485,
      "loss": 0.0466,
      "step": 4524
    },
    {
      "epoch": 1.1197723335807968,
      "grad_norm": 0.07976197451353073,
      "learning_rate": 0.00020366323383376475,
      "loss": 0.0615,
      "step": 4525
    },
    {
      "epoch": 1.1200197970799306,
      "grad_norm": 0.04480612650513649,
      "learning_rate": 0.0002035676241560856,
      "loss": 0.0622,
      "step": 4526
    },
    {
      "epoch": 1.1202672605790647,
      "grad_norm": 0.1331651210784912,
      "learning_rate": 0.00020347202151169347,
      "loss": 0.0497,
      "step": 4527
    },
    {
      "epoch": 1.1205147240781985,
      "grad_norm": 0.05748864635825157,
      "learning_rate": 0.00020337642591506953,
      "loss": 0.0473,
      "step": 4528
    },
    {
      "epoch": 1.1207621875773324,
      "grad_norm": 0.0420476570725441,
      "learning_rate": 0.00020328083738069416,
      "loss": 0.0407,
      "step": 4529
    },
    {
      "epoch": 1.1210096510764662,
      "grad_norm": 0.06281659007072449,
      "learning_rate": 0.00020318525592304652,
      "loss": 0.1404,
      "step": 4530
    },
    {
      "epoch": 1.1212571145756,
      "grad_norm": 0.06205112859606743,
      "learning_rate": 0.0002030896815566046,
      "loss": 0.0911,
      "step": 4531
    },
    {
      "epoch": 1.121504578074734,
      "grad_norm": 0.05067576467990875,
      "learning_rate": 0.00020299411429584545,
      "loss": 0.0717,
      "step": 4532
    },
    {
      "epoch": 1.1217520415738678,
      "grad_norm": 0.02891707234084606,
      "learning_rate": 0.00020289855415524502,
      "loss": 0.0182,
      "step": 4533
    },
    {
      "epoch": 1.1219995050730018,
      "grad_norm": 0.07199250161647797,
      "learning_rate": 0.00020280300114927812,
      "loss": 0.0472,
      "step": 4534
    },
    {
      "epoch": 1.1222469685721357,
      "grad_norm": 0.036743730306625366,
      "learning_rate": 0.00020270745529241856,
      "loss": 0.0429,
      "step": 4535
    },
    {
      "epoch": 1.1224944320712695,
      "grad_norm": 0.04277375712990761,
      "learning_rate": 0.00020261191659913896,
      "loss": 0.041,
      "step": 4536
    },
    {
      "epoch": 1.1227418955704034,
      "grad_norm": 0.03494495898485184,
      "learning_rate": 0.00020251638508391113,
      "loss": 0.0282,
      "step": 4537
    },
    {
      "epoch": 1.1229893590695372,
      "grad_norm": 0.04174942150712013,
      "learning_rate": 0.00020242086076120532,
      "loss": 0.0517,
      "step": 4538
    },
    {
      "epoch": 1.123236822568671,
      "grad_norm": 0.049673549830913544,
      "learning_rate": 0.0002023253436454911,
      "loss": 0.0689,
      "step": 4539
    },
    {
      "epoch": 1.123484286067805,
      "grad_norm": 0.03966744244098663,
      "learning_rate": 0.00020222983375123684,
      "loss": 0.0549,
      "step": 4540
    },
    {
      "epoch": 1.1237317495669388,
      "grad_norm": 0.06639643758535385,
      "learning_rate": 0.00020213433109290968,
      "loss": 0.1052,
      "step": 4541
    },
    {
      "epoch": 1.1239792130660728,
      "grad_norm": 0.040071435272693634,
      "learning_rate": 0.00020203883568497592,
      "loss": 0.0745,
      "step": 4542
    },
    {
      "epoch": 1.1242266765652067,
      "grad_norm": 0.04332282394170761,
      "learning_rate": 0.00020194334754190042,
      "loss": 0.0457,
      "step": 4543
    },
    {
      "epoch": 1.1244741400643405,
      "grad_norm": 0.05598534643650055,
      "learning_rate": 0.00020184786667814717,
      "loss": 0.0673,
      "step": 4544
    },
    {
      "epoch": 1.1247216035634744,
      "grad_norm": 0.03894258663058281,
      "learning_rate": 0.00020175239310817926,
      "loss": 0.0272,
      "step": 4545
    },
    {
      "epoch": 1.1249690670626082,
      "grad_norm": 0.04553671181201935,
      "learning_rate": 0.00020165692684645804,
      "loss": 0.041,
      "step": 4546
    },
    {
      "epoch": 1.125216530561742,
      "grad_norm": 0.04845137521624565,
      "learning_rate": 0.00020156146790744454,
      "loss": 0.0519,
      "step": 4547
    },
    {
      "epoch": 1.125463994060876,
      "grad_norm": 0.0754946917295456,
      "learning_rate": 0.0002014660163055979,
      "loss": 0.076,
      "step": 4548
    },
    {
      "epoch": 1.1257114575600098,
      "grad_norm": 0.05004814267158508,
      "learning_rate": 0.0002013705720553768,
      "loss": 0.0593,
      "step": 4549
    },
    {
      "epoch": 1.1259589210591439,
      "grad_norm": 0.05299367010593414,
      "learning_rate": 0.0002012751351712385,
      "loss": 0.0573,
      "step": 4550
    },
    {
      "epoch": 1.1262063845582777,
      "grad_norm": 0.07483478635549545,
      "learning_rate": 0.0002011797056676391,
      "loss": 0.0636,
      "step": 4551
    },
    {
      "epoch": 1.1264538480574116,
      "grad_norm": 0.1002211943268776,
      "learning_rate": 0.00020108428355903376,
      "loss": 0.1395,
      "step": 4552
    },
    {
      "epoch": 1.1267013115565454,
      "grad_norm": 0.046700369566679,
      "learning_rate": 0.00020098886885987633,
      "loss": 0.0443,
      "step": 4553
    },
    {
      "epoch": 1.1269487750556793,
      "grad_norm": 0.044871214777231216,
      "learning_rate": 0.00020089346158461962,
      "loss": 0.0476,
      "step": 4554
    },
    {
      "epoch": 1.127196238554813,
      "grad_norm": 0.050218451768159866,
      "learning_rate": 0.00020079806174771558,
      "loss": 0.0358,
      "step": 4555
    },
    {
      "epoch": 1.127443702053947,
      "grad_norm": 0.06292817741632462,
      "learning_rate": 0.00020070266936361445,
      "loss": 0.0401,
      "step": 4556
    },
    {
      "epoch": 1.127691165553081,
      "grad_norm": 0.030185479670763016,
      "learning_rate": 0.00020060728444676597,
      "loss": 0.0526,
      "step": 4557
    },
    {
      "epoch": 1.1279386290522149,
      "grad_norm": 0.08767695724964142,
      "learning_rate": 0.00020051190701161813,
      "loss": 0.1114,
      "step": 4558
    },
    {
      "epoch": 1.1281860925513487,
      "grad_norm": 0.05444414168596268,
      "learning_rate": 0.00020041653707261837,
      "loss": 0.0364,
      "step": 4559
    },
    {
      "epoch": 1.1284335560504826,
      "grad_norm": 0.03375967592000961,
      "learning_rate": 0.0002003211746442127,
      "loss": 0.0732,
      "step": 4560
    },
    {
      "epoch": 1.1286810195496164,
      "grad_norm": 0.037852637469768524,
      "learning_rate": 0.0002002258197408459,
      "loss": 0.0442,
      "step": 4561
    },
    {
      "epoch": 1.1289284830487503,
      "grad_norm": 0.04799361154437065,
      "learning_rate": 0.00020013047237696187,
      "loss": 0.0492,
      "step": 4562
    },
    {
      "epoch": 1.1291759465478841,
      "grad_norm": 0.050050582736730576,
      "learning_rate": 0.0002000351325670031,
      "loss": 0.0434,
      "step": 4563
    },
    {
      "epoch": 1.1294234100470182,
      "grad_norm": 0.0255742147564888,
      "learning_rate": 0.0001999398003254111,
      "loss": 0.0245,
      "step": 4564
    },
    {
      "epoch": 1.129670873546152,
      "grad_norm": 0.049513790756464005,
      "learning_rate": 0.00019984447566662644,
      "loss": 0.0638,
      "step": 4565
    },
    {
      "epoch": 1.1299183370452859,
      "grad_norm": 0.04156256467103958,
      "learning_rate": 0.00019974915860508795,
      "loss": 0.0504,
      "step": 4566
    },
    {
      "epoch": 1.1301658005444197,
      "grad_norm": 0.0355750136077404,
      "learning_rate": 0.00019965384915523394,
      "loss": 0.0484,
      "step": 4567
    },
    {
      "epoch": 1.1304132640435536,
      "grad_norm": 0.05283370241522789,
      "learning_rate": 0.0001995585473315011,
      "loss": 0.0534,
      "step": 4568
    },
    {
      "epoch": 1.1306607275426874,
      "grad_norm": 0.12177332490682602,
      "learning_rate": 0.00019946325314832528,
      "loss": 0.0449,
      "step": 4569
    },
    {
      "epoch": 1.1309081910418213,
      "grad_norm": 0.03433535248041153,
      "learning_rate": 0.00019936796662014104,
      "loss": 0.0469,
      "step": 4570
    },
    {
      "epoch": 1.1311556545409551,
      "grad_norm": 0.055724747478961945,
      "learning_rate": 0.0001992726877613817,
      "loss": 0.0592,
      "step": 4571
    },
    {
      "epoch": 1.131403118040089,
      "grad_norm": 0.05518006160855293,
      "learning_rate": 0.00019917741658647965,
      "loss": 0.1013,
      "step": 4572
    },
    {
      "epoch": 1.131650581539223,
      "grad_norm": 0.049887336790561676,
      "learning_rate": 0.0001990821531098658,
      "loss": 0.0313,
      "step": 4573
    },
    {
      "epoch": 1.131898045038357,
      "grad_norm": 0.04908980801701546,
      "learning_rate": 0.0001989868973459701,
      "loss": 0.0523,
      "step": 4574
    },
    {
      "epoch": 1.1321455085374907,
      "grad_norm": 0.04318515211343765,
      "learning_rate": 0.00019889164930922152,
      "loss": 0.0461,
      "step": 4575
    },
    {
      "epoch": 1.1323929720366246,
      "grad_norm": 0.04755444824695587,
      "learning_rate": 0.00019879640901404737,
      "loss": 0.0401,
      "step": 4576
    },
    {
      "epoch": 1.1326404355357584,
      "grad_norm": 0.10299201309680939,
      "learning_rate": 0.00019870117647487423,
      "loss": 0.1094,
      "step": 4577
    },
    {
      "epoch": 1.1328878990348923,
      "grad_norm": 0.06470977514982224,
      "learning_rate": 0.0001986059517061272,
      "loss": 0.0442,
      "step": 4578
    },
    {
      "epoch": 1.1331353625340261,
      "grad_norm": 0.09602106362581253,
      "learning_rate": 0.00019851073472223045,
      "loss": 0.1965,
      "step": 4579
    },
    {
      "epoch": 1.1333828260331602,
      "grad_norm": 0.03375336900353432,
      "learning_rate": 0.0001984155255376068,
      "loss": 0.0433,
      "step": 4580
    },
    {
      "epoch": 1.133630289532294,
      "grad_norm": 0.027513034641742706,
      "learning_rate": 0.00019832032416667794,
      "loss": 0.0474,
      "step": 4581
    },
    {
      "epoch": 1.133877753031428,
      "grad_norm": 0.04895418882369995,
      "learning_rate": 0.0001982251306238644,
      "loss": 0.0345,
      "step": 4582
    },
    {
      "epoch": 1.1341252165305618,
      "grad_norm": 0.07022005319595337,
      "learning_rate": 0.00019812994492358544,
      "loss": 0.0838,
      "step": 4583
    },
    {
      "epoch": 1.1343726800296956,
      "grad_norm": 0.090977244079113,
      "learning_rate": 0.00019803476708025924,
      "loss": 0.1035,
      "step": 4584
    },
    {
      "epoch": 1.1346201435288295,
      "grad_norm": 0.08067085593938828,
      "learning_rate": 0.00019793959710830286,
      "loss": 0.1131,
      "step": 4585
    },
    {
      "epoch": 1.1348676070279633,
      "grad_norm": 0.06750040501356125,
      "learning_rate": 0.0001978444350221319,
      "loss": 0.069,
      "step": 4586
    },
    {
      "epoch": 1.1351150705270974,
      "grad_norm": 0.048253607004880905,
      "learning_rate": 0.00019774928083616101,
      "loss": 0.0742,
      "step": 4587
    },
    {
      "epoch": 1.1353625340262312,
      "grad_norm": 0.025726009160280228,
      "learning_rate": 0.00019765413456480346,
      "loss": 0.0386,
      "step": 4588
    },
    {
      "epoch": 1.135609997525365,
      "grad_norm": 0.049568936228752136,
      "learning_rate": 0.00019755899622247148,
      "loss": 0.0735,
      "step": 4589
    },
    {
      "epoch": 1.135857461024499,
      "grad_norm": 0.04176585376262665,
      "learning_rate": 0.00019746386582357607,
      "loss": 0.0251,
      "step": 4590
    },
    {
      "epoch": 1.1361049245236328,
      "grad_norm": 0.031185321509838104,
      "learning_rate": 0.00019736874338252683,
      "loss": 0.0267,
      "step": 4591
    },
    {
      "epoch": 1.1363523880227666,
      "grad_norm": 0.04861427843570709,
      "learning_rate": 0.0001972736289137326,
      "loss": 0.074,
      "step": 4592
    },
    {
      "epoch": 1.1365998515219005,
      "grad_norm": 0.0256535355001688,
      "learning_rate": 0.0001971785224316004,
      "loss": 0.0413,
      "step": 4593
    },
    {
      "epoch": 1.1368473150210343,
      "grad_norm": 0.046388182789087296,
      "learning_rate": 0.00019708342395053654,
      "loss": 0.0248,
      "step": 4594
    },
    {
      "epoch": 1.1370947785201682,
      "grad_norm": 0.04934437945485115,
      "learning_rate": 0.00019698833348494603,
      "loss": 0.0428,
      "step": 4595
    },
    {
      "epoch": 1.1373422420193022,
      "grad_norm": 0.03363418206572533,
      "learning_rate": 0.0001968932510492324,
      "loss": 0.034,
      "step": 4596
    },
    {
      "epoch": 1.137589705518436,
      "grad_norm": 0.031113049015402794,
      "learning_rate": 0.0001967981766577983,
      "loss": 0.0255,
      "step": 4597
    },
    {
      "epoch": 1.13783716901757,
      "grad_norm": 0.035698194056749344,
      "learning_rate": 0.00019670311032504484,
      "loss": 0.0337,
      "step": 4598
    },
    {
      "epoch": 1.1380846325167038,
      "grad_norm": 0.029765425249934196,
      "learning_rate": 0.00019660805206537214,
      "loss": 0.0298,
      "step": 4599
    },
    {
      "epoch": 1.1383320960158376,
      "grad_norm": 0.026616152375936508,
      "learning_rate": 0.0001965130018931792,
      "loss": 0.0217,
      "step": 4600
    },
    {
      "epoch": 1.1383320960158376,
      "eval_loss": 0.2907043397426605,
      "eval_runtime": 202.534,
      "eval_samples_per_second": 4.937,
      "eval_steps_per_second": 0.311,
      "step": 4600
    },
    {
      "epoch": 1.1385795595149715,
      "grad_norm": 0.04945750534534454,
      "learning_rate": 0.00019641795982286333,
      "loss": 0.048,
      "step": 4601
    },
    {
      "epoch": 1.1388270230141053,
      "grad_norm": 0.04381958022713661,
      "learning_rate": 0.00019632292586882125,
      "loss": 0.049,
      "step": 4602
    },
    {
      "epoch": 1.1390744865132394,
      "grad_norm": 0.042872872203588486,
      "learning_rate": 0.00019622790004544778,
      "loss": 0.0234,
      "step": 4603
    },
    {
      "epoch": 1.1393219500123732,
      "grad_norm": 0.04433960095047951,
      "learning_rate": 0.000196132882367137,
      "loss": 0.0451,
      "step": 4604
    },
    {
      "epoch": 1.139569413511507,
      "grad_norm": 0.03759787231683731,
      "learning_rate": 0.00019603787284828167,
      "loss": 0.0379,
      "step": 4605
    },
    {
      "epoch": 1.139816877010641,
      "grad_norm": 0.049178048968315125,
      "learning_rate": 0.0001959428715032731,
      "loss": 0.0745,
      "step": 4606
    },
    {
      "epoch": 1.1400643405097748,
      "grad_norm": 0.061605680733919144,
      "learning_rate": 0.0001958478783465016,
      "loss": 0.0671,
      "step": 4607
    },
    {
      "epoch": 1.1403118040089086,
      "grad_norm": 0.03470443934202194,
      "learning_rate": 0.00019575289339235602,
      "loss": 0.0422,
      "step": 4608
    },
    {
      "epoch": 1.1405592675080425,
      "grad_norm": 0.03588838875293732,
      "learning_rate": 0.00019565791665522415,
      "loss": 0.0356,
      "step": 4609
    },
    {
      "epoch": 1.1408067310071766,
      "grad_norm": 0.03477580100297928,
      "learning_rate": 0.00019556294814949263,
      "loss": 0.0487,
      "step": 4610
    },
    {
      "epoch": 1.1410541945063104,
      "grad_norm": 0.0320313461124897,
      "learning_rate": 0.00019546798788954643,
      "loss": 0.0195,
      "step": 4611
    },
    {
      "epoch": 1.1413016580054443,
      "grad_norm": 0.06459306180477142,
      "learning_rate": 0.00019537303588976974,
      "loss": 0.0611,
      "step": 4612
    },
    {
      "epoch": 1.141549121504578,
      "grad_norm": 0.08504912257194519,
      "learning_rate": 0.00019527809216454518,
      "loss": 0.0697,
      "step": 4613
    },
    {
      "epoch": 1.141796585003712,
      "grad_norm": 0.031997568905353546,
      "learning_rate": 0.0001951831567282543,
      "loss": 0.0392,
      "step": 4614
    },
    {
      "epoch": 1.1420440485028458,
      "grad_norm": 0.03991636633872986,
      "learning_rate": 0.00019508822959527733,
      "loss": 0.0739,
      "step": 4615
    },
    {
      "epoch": 1.1422915120019796,
      "grad_norm": 0.04036442190408707,
      "learning_rate": 0.00019499331077999318,
      "loss": 0.0267,
      "step": 4616
    },
    {
      "epoch": 1.1425389755011135,
      "grad_norm": 0.03938829526305199,
      "learning_rate": 0.00019489840029677957,
      "loss": 0.044,
      "step": 4617
    },
    {
      "epoch": 1.1427864390002473,
      "grad_norm": 0.04868566617369652,
      "learning_rate": 0.00019480349816001312,
      "loss": 0.0889,
      "step": 4618
    },
    {
      "epoch": 1.1430339024993814,
      "grad_norm": 0.05601023882627487,
      "learning_rate": 0.00019470860438406868,
      "loss": 0.0564,
      "step": 4619
    },
    {
      "epoch": 1.1432813659985153,
      "grad_norm": 0.049532923847436905,
      "learning_rate": 0.0001946137189833205,
      "loss": 0.0538,
      "step": 4620
    },
    {
      "epoch": 1.1435288294976491,
      "grad_norm": 0.037268877029418945,
      "learning_rate": 0.00019451884197214104,
      "loss": 0.0181,
      "step": 4621
    },
    {
      "epoch": 1.143776292996783,
      "grad_norm": 0.05254243686795235,
      "learning_rate": 0.0001944239733649017,
      "loss": 0.0545,
      "step": 4622
    },
    {
      "epoch": 1.1440237564959168,
      "grad_norm": 0.026033151894807816,
      "learning_rate": 0.0001943291131759727,
      "loss": 0.0313,
      "step": 4623
    },
    {
      "epoch": 1.1442712199950507,
      "grad_norm": 0.039616767317056656,
      "learning_rate": 0.00019423426141972273,
      "loss": 0.0569,
      "step": 4624
    },
    {
      "epoch": 1.1445186834941845,
      "grad_norm": 0.04316224157810211,
      "learning_rate": 0.0001941394181105194,
      "loss": 0.0509,
      "step": 4625
    },
    {
      "epoch": 1.1447661469933186,
      "grad_norm": 0.022898029536008835,
      "learning_rate": 0.00019404458326272898,
      "loss": 0.027,
      "step": 4626
    },
    {
      "epoch": 1.1450136104924524,
      "grad_norm": 0.0410505086183548,
      "learning_rate": 0.00019394975689071637,
      "loss": 0.0499,
      "step": 4627
    },
    {
      "epoch": 1.1452610739915863,
      "grad_norm": 0.10042155534029007,
      "learning_rate": 0.0001938549390088455,
      "loss": 0.1474,
      "step": 4628
    },
    {
      "epoch": 1.1455085374907201,
      "grad_norm": 0.05148226022720337,
      "learning_rate": 0.00019376012963147864,
      "loss": 0.079,
      "step": 4629
    },
    {
      "epoch": 1.145756000989854,
      "grad_norm": 0.049374088644981384,
      "learning_rate": 0.00019366532877297692,
      "loss": 0.0534,
      "step": 4630
    },
    {
      "epoch": 1.1460034644889878,
      "grad_norm": 0.059758033603429794,
      "learning_rate": 0.00019357053644770023,
      "loss": 0.0897,
      "step": 4631
    },
    {
      "epoch": 1.1462509279881217,
      "grad_norm": 0.05050446838140488,
      "learning_rate": 0.00019347575267000705,
      "loss": 0.0439,
      "step": 4632
    },
    {
      "epoch": 1.1464983914872557,
      "grad_norm": 0.04572983831167221,
      "learning_rate": 0.0001933809774542548,
      "loss": 0.0434,
      "step": 4633
    },
    {
      "epoch": 1.1467458549863896,
      "grad_norm": 0.03358686342835426,
      "learning_rate": 0.00019328621081479922,
      "loss": 0.0463,
      "step": 4634
    },
    {
      "epoch": 1.1469933184855234,
      "grad_norm": 0.036198846995830536,
      "learning_rate": 0.00019319145276599513,
      "loss": 0.0333,
      "step": 4635
    },
    {
      "epoch": 1.1472407819846573,
      "grad_norm": 0.08542212098836899,
      "learning_rate": 0.00019309670332219575,
      "loss": 0.0702,
      "step": 4636
    },
    {
      "epoch": 1.1474882454837911,
      "grad_norm": 0.04982032626867294,
      "learning_rate": 0.00019300196249775326,
      "loss": 0.051,
      "step": 4637
    },
    {
      "epoch": 1.147735708982925,
      "grad_norm": 0.036969803273677826,
      "learning_rate": 0.00019290723030701844,
      "loss": 0.0442,
      "step": 4638
    },
    {
      "epoch": 1.1479831724820588,
      "grad_norm": 0.05440844967961311,
      "learning_rate": 0.00019281250676434058,
      "loss": 0.0692,
      "step": 4639
    },
    {
      "epoch": 1.1482306359811927,
      "grad_norm": 0.05716933682560921,
      "learning_rate": 0.00019271779188406797,
      "loss": 0.0709,
      "step": 4640
    },
    {
      "epoch": 1.1484780994803265,
      "grad_norm": 0.07126946747303009,
      "learning_rate": 0.0001926230856805473,
      "loss": 0.1105,
      "step": 4641
    },
    {
      "epoch": 1.1487255629794606,
      "grad_norm": 0.04512829706072807,
      "learning_rate": 0.00019252838816812413,
      "loss": 0.0814,
      "step": 4642
    },
    {
      "epoch": 1.1489730264785945,
      "grad_norm": 0.05777391418814659,
      "learning_rate": 0.0001924336993611427,
      "loss": 0.0918,
      "step": 4643
    },
    {
      "epoch": 1.1492204899777283,
      "grad_norm": 0.03182363510131836,
      "learning_rate": 0.0001923390192739457,
      "loss": 0.0304,
      "step": 4644
    },
    {
      "epoch": 1.1494679534768621,
      "grad_norm": 0.0319497287273407,
      "learning_rate": 0.000192244347920875,
      "loss": 0.0436,
      "step": 4645
    },
    {
      "epoch": 1.149715416975996,
      "grad_norm": 0.05322285741567612,
      "learning_rate": 0.00019214968531627044,
      "loss": 0.0533,
      "step": 4646
    },
    {
      "epoch": 1.1499628804751298,
      "grad_norm": 0.03626631200313568,
      "learning_rate": 0.0001920550314744712,
      "loss": 0.0267,
      "step": 4647
    },
    {
      "epoch": 1.1502103439742637,
      "grad_norm": 0.037932321429252625,
      "learning_rate": 0.00019196038640981484,
      "loss": 0.029,
      "step": 4648
    },
    {
      "epoch": 1.1504578074733978,
      "grad_norm": 0.02716047503054142,
      "learning_rate": 0.00019186575013663745,
      "loss": 0.0245,
      "step": 4649
    },
    {
      "epoch": 1.1507052709725316,
      "grad_norm": 0.042502351105213165,
      "learning_rate": 0.0001917711226692741,
      "loss": 0.0552,
      "step": 4650
    },
    {
      "epoch": 1.1509527344716655,
      "grad_norm": 0.05198735371232033,
      "learning_rate": 0.00019167650402205822,
      "loss": 0.0577,
      "step": 4651
    },
    {
      "epoch": 1.1512001979707993,
      "grad_norm": 0.042775992304086685,
      "learning_rate": 0.0001915818942093221,
      "loss": 0.0363,
      "step": 4652
    },
    {
      "epoch": 1.1514476614699332,
      "grad_norm": 0.08238115161657333,
      "learning_rate": 0.00019148729324539686,
      "loss": 0.1051,
      "step": 4653
    },
    {
      "epoch": 1.151695124969067,
      "grad_norm": 0.041593607515096664,
      "learning_rate": 0.0001913927011446117,
      "loss": 0.0824,
      "step": 4654
    },
    {
      "epoch": 1.1519425884682009,
      "grad_norm": 0.03574758768081665,
      "learning_rate": 0.00019129811792129513,
      "loss": 0.0653,
      "step": 4655
    },
    {
      "epoch": 1.152190051967335,
      "grad_norm": 0.04366282746195793,
      "learning_rate": 0.0001912035435897738,
      "loss": 0.0658,
      "step": 4656
    },
    {
      "epoch": 1.1524375154664688,
      "grad_norm": 0.03680557385087013,
      "learning_rate": 0.00019110897816437345,
      "loss": 0.0445,
      "step": 4657
    },
    {
      "epoch": 1.1526849789656026,
      "grad_norm": 0.04456816241145134,
      "learning_rate": 0.0001910144216594182,
      "loss": 0.0542,
      "step": 4658
    },
    {
      "epoch": 1.1529324424647365,
      "grad_norm": 0.033917997032403946,
      "learning_rate": 0.0001909198740892308,
      "loss": 0.0273,
      "step": 4659
    },
    {
      "epoch": 1.1531799059638703,
      "grad_norm": 0.035385534167289734,
      "learning_rate": 0.00019082533546813286,
      "loss": 0.0409,
      "step": 4660
    },
    {
      "epoch": 1.1534273694630042,
      "grad_norm": 0.05152200162410736,
      "learning_rate": 0.00019073080581044434,
      "loss": 0.0842,
      "step": 4661
    },
    {
      "epoch": 1.153674832962138,
      "grad_norm": 0.07119406759738922,
      "learning_rate": 0.000190636285130484,
      "loss": 0.0687,
      "step": 4662
    },
    {
      "epoch": 1.1539222964612719,
      "grad_norm": 0.033078327775001526,
      "learning_rate": 0.00019054177344256953,
      "loss": 0.0429,
      "step": 4663
    },
    {
      "epoch": 1.1541697599604057,
      "grad_norm": 0.06180889904499054,
      "learning_rate": 0.00019044727076101658,
      "loss": 0.0427,
      "step": 4664
    },
    {
      "epoch": 1.1544172234595398,
      "grad_norm": 0.057437289506196976,
      "learning_rate": 0.0001903527771001402,
      "loss": 0.0671,
      "step": 4665
    },
    {
      "epoch": 1.1546646869586736,
      "grad_norm": 0.04584646224975586,
      "learning_rate": 0.00019025829247425335,
      "loss": 0.054,
      "step": 4666
    },
    {
      "epoch": 1.1549121504578075,
      "grad_norm": 0.050851091742515564,
      "learning_rate": 0.00019016381689766822,
      "loss": 0.0949,
      "step": 4667
    },
    {
      "epoch": 1.1551596139569413,
      "grad_norm": 0.03672121465206146,
      "learning_rate": 0.00019006935038469534,
      "loss": 0.0347,
      "step": 4668
    },
    {
      "epoch": 1.1554070774560752,
      "grad_norm": 0.05387810245156288,
      "learning_rate": 0.00018997489294964381,
      "loss": 0.0753,
      "step": 4669
    },
    {
      "epoch": 1.155654540955209,
      "grad_norm": 0.029364299029111862,
      "learning_rate": 0.00018988044460682157,
      "loss": 0.0453,
      "step": 4670
    },
    {
      "epoch": 1.1559020044543429,
      "grad_norm": 0.05268751457333565,
      "learning_rate": 0.00018978600537053494,
      "loss": 0.1203,
      "step": 4671
    },
    {
      "epoch": 1.156149467953477,
      "grad_norm": 0.09011214226484299,
      "learning_rate": 0.00018969157525508903,
      "loss": 0.1488,
      "step": 4672
    },
    {
      "epoch": 1.1563969314526108,
      "grad_norm": 0.05632984638214111,
      "learning_rate": 0.00018959715427478772,
      "loss": 0.0512,
      "step": 4673
    },
    {
      "epoch": 1.1566443949517446,
      "grad_norm": 0.0441199354827404,
      "learning_rate": 0.00018950274244393297,
      "loss": 0.0538,
      "step": 4674
    },
    {
      "epoch": 1.1568918584508785,
      "grad_norm": 0.03831028193235397,
      "learning_rate": 0.00018940833977682596,
      "loss": 0.0254,
      "step": 4675
    },
    {
      "epoch": 1.1571393219500123,
      "grad_norm": 0.07205379009246826,
      "learning_rate": 0.00018931394628776612,
      "loss": 0.0786,
      "step": 4676
    },
    {
      "epoch": 1.1573867854491462,
      "grad_norm": 0.042472124099731445,
      "learning_rate": 0.0001892195619910516,
      "loss": 0.0549,
      "step": 4677
    },
    {
      "epoch": 1.15763424894828,
      "grad_norm": 0.18677012622356415,
      "learning_rate": 0.00018912518690097915,
      "loss": 0.1381,
      "step": 4678
    },
    {
      "epoch": 1.1578817124474141,
      "grad_norm": 0.06726433336734772,
      "learning_rate": 0.0001890308210318441,
      "loss": 0.0764,
      "step": 4679
    },
    {
      "epoch": 1.158129175946548,
      "grad_norm": 0.12812907993793488,
      "learning_rate": 0.00018893646439794047,
      "loss": 0.1332,
      "step": 4680
    },
    {
      "epoch": 1.1583766394456818,
      "grad_norm": 0.037009939551353455,
      "learning_rate": 0.0001888421170135607,
      "loss": 0.0546,
      "step": 4681
    },
    {
      "epoch": 1.1586241029448157,
      "grad_norm": 0.032348569482564926,
      "learning_rate": 0.00018874777889299593,
      "loss": 0.0306,
      "step": 4682
    },
    {
      "epoch": 1.1588715664439495,
      "grad_norm": 0.04522155597805977,
      "learning_rate": 0.00018865345005053612,
      "loss": 0.0225,
      "step": 4683
    },
    {
      "epoch": 1.1591190299430834,
      "grad_norm": 0.051104478538036346,
      "learning_rate": 0.0001885591305004694,
      "loss": 0.0798,
      "step": 4684
    },
    {
      "epoch": 1.1593664934422172,
      "grad_norm": 0.05500112101435661,
      "learning_rate": 0.00018846482025708285,
      "loss": 0.0783,
      "step": 4685
    },
    {
      "epoch": 1.159613956941351,
      "grad_norm": 0.041087523102760315,
      "learning_rate": 0.00018837051933466193,
      "loss": 0.0434,
      "step": 4686
    },
    {
      "epoch": 1.1598614204404851,
      "grad_norm": 0.02739373780786991,
      "learning_rate": 0.0001882762277474907,
      "loss": 0.0465,
      "step": 4687
    },
    {
      "epoch": 1.160108883939619,
      "grad_norm": 0.039168622344732285,
      "learning_rate": 0.00018818194550985202,
      "loss": 0.0412,
      "step": 4688
    },
    {
      "epoch": 1.1603563474387528,
      "grad_norm": 0.06692836433649063,
      "learning_rate": 0.000188087672636027,
      "loss": 0.0621,
      "step": 4689
    },
    {
      "epoch": 1.1606038109378867,
      "grad_norm": 0.02903060056269169,
      "learning_rate": 0.00018799340914029569,
      "loss": 0.0298,
      "step": 4690
    },
    {
      "epoch": 1.1608512744370205,
      "grad_norm": 0.036203041672706604,
      "learning_rate": 0.0001878991550369363,
      "loss": 0.0528,
      "step": 4691
    },
    {
      "epoch": 1.1610987379361544,
      "grad_norm": 0.07342690974473953,
      "learning_rate": 0.00018780491034022607,
      "loss": 0.0833,
      "step": 4692
    },
    {
      "epoch": 1.1613462014352882,
      "grad_norm": 0.04104464501142502,
      "learning_rate": 0.00018771067506444052,
      "loss": 0.0389,
      "step": 4693
    },
    {
      "epoch": 1.161593664934422,
      "grad_norm": 0.06040298193693161,
      "learning_rate": 0.00018761644922385382,
      "loss": 0.0625,
      "step": 4694
    },
    {
      "epoch": 1.1618411284335561,
      "grad_norm": 0.034041859209537506,
      "learning_rate": 0.00018752223283273876,
      "loss": 0.0331,
      "step": 4695
    },
    {
      "epoch": 1.16208859193269,
      "grad_norm": 0.07141388952732086,
      "learning_rate": 0.00018742802590536656,
      "loss": 0.0715,
      "step": 4696
    },
    {
      "epoch": 1.1623360554318238,
      "grad_norm": 0.05629081279039383,
      "learning_rate": 0.00018733382845600711,
      "loss": 0.0661,
      "step": 4697
    },
    {
      "epoch": 1.1625835189309577,
      "grad_norm": 0.04029132053256035,
      "learning_rate": 0.000187239640498929,
      "loss": 0.02,
      "step": 4698
    },
    {
      "epoch": 1.1628309824300915,
      "grad_norm": 0.03639163076877594,
      "learning_rate": 0.00018714546204839897,
      "loss": 0.0351,
      "step": 4699
    },
    {
      "epoch": 1.1630784459292254,
      "grad_norm": 0.04443440958857536,
      "learning_rate": 0.0001870512931186829,
      "loss": 0.0509,
      "step": 4700
    },
    {
      "epoch": 1.1633259094283592,
      "grad_norm": 0.043442655354738235,
      "learning_rate": 0.0001869571337240446,
      "loss": 0.0555,
      "step": 4701
    },
    {
      "epoch": 1.1635733729274933,
      "grad_norm": 0.042177025228738785,
      "learning_rate": 0.000186862983878747,
      "loss": 0.0364,
      "step": 4702
    },
    {
      "epoch": 1.1638208364266271,
      "grad_norm": 0.060515496879816055,
      "learning_rate": 0.00018676884359705123,
      "loss": 0.0867,
      "step": 4703
    },
    {
      "epoch": 1.164068299925761,
      "grad_norm": 0.07770668715238571,
      "learning_rate": 0.00018667471289321702,
      "loss": 0.0923,
      "step": 4704
    },
    {
      "epoch": 1.1643157634248948,
      "grad_norm": 0.04221448302268982,
      "learning_rate": 0.00018658059178150286,
      "loss": 0.0521,
      "step": 4705
    },
    {
      "epoch": 1.1645632269240287,
      "grad_norm": 0.03454802557826042,
      "learning_rate": 0.00018648648027616544,
      "loss": 0.0361,
      "step": 4706
    },
    {
      "epoch": 1.1648106904231625,
      "grad_norm": 0.06772052496671677,
      "learning_rate": 0.00018639237839146021,
      "loss": 0.0825,
      "step": 4707
    },
    {
      "epoch": 1.1650581539222964,
      "grad_norm": 0.037878070026636124,
      "learning_rate": 0.0001862982861416414,
      "loss": 0.0456,
      "step": 4708
    },
    {
      "epoch": 1.1653056174214305,
      "grad_norm": 0.04447943717241287,
      "learning_rate": 0.00018620420354096114,
      "loss": 0.0483,
      "step": 4709
    },
    {
      "epoch": 1.1655530809205643,
      "grad_norm": 0.0696408599615097,
      "learning_rate": 0.00018611013060367083,
      "loss": 0.0793,
      "step": 4710
    },
    {
      "epoch": 1.1658005444196982,
      "grad_norm": 0.05147656053304672,
      "learning_rate": 0.00018601606734401966,
      "loss": 0.06,
      "step": 4711
    },
    {
      "epoch": 1.166048007918832,
      "grad_norm": 0.06858754903078079,
      "learning_rate": 0.00018592201377625606,
      "loss": 0.0623,
      "step": 4712
    },
    {
      "epoch": 1.1662954714179659,
      "grad_norm": 0.05627821385860443,
      "learning_rate": 0.00018582796991462665,
      "loss": 0.1595,
      "step": 4713
    },
    {
      "epoch": 1.1665429349170997,
      "grad_norm": 0.035829488188028336,
      "learning_rate": 0.00018573393577337645,
      "loss": 0.0536,
      "step": 4714
    },
    {
      "epoch": 1.1667903984162336,
      "grad_norm": 0.029866445809602737,
      "learning_rate": 0.0001856399113667493,
      "loss": 0.0401,
      "step": 4715
    },
    {
      "epoch": 1.1670378619153674,
      "grad_norm": 0.03171568363904953,
      "learning_rate": 0.00018554589670898735,
      "loss": 0.0374,
      "step": 4716
    },
    {
      "epoch": 1.1672853254145013,
      "grad_norm": 0.044623639434576035,
      "learning_rate": 0.00018545189181433136,
      "loss": 0.0363,
      "step": 4717
    },
    {
      "epoch": 1.1675327889136353,
      "grad_norm": 0.050945449620485306,
      "learning_rate": 0.00018535789669702076,
      "loss": 0.057,
      "step": 4718
    },
    {
      "epoch": 1.1677802524127692,
      "grad_norm": 0.07551954686641693,
      "learning_rate": 0.0001852639113712931,
      "loss": 0.1095,
      "step": 4719
    },
    {
      "epoch": 1.168027715911903,
      "grad_norm": 0.02639816142618656,
      "learning_rate": 0.0001851699358513849,
      "loss": 0.0262,
      "step": 4720
    },
    {
      "epoch": 1.1682751794110369,
      "grad_norm": 0.051550332456827164,
      "learning_rate": 0.00018507597015153082,
      "loss": 0.0774,
      "step": 4721
    },
    {
      "epoch": 1.1685226429101707,
      "grad_norm": 0.04588992893695831,
      "learning_rate": 0.00018498201428596435,
      "loss": 0.0452,
      "step": 4722
    },
    {
      "epoch": 1.1687701064093046,
      "grad_norm": 0.043636806309223175,
      "learning_rate": 0.00018488806826891723,
      "loss": 0.0283,
      "step": 4723
    },
    {
      "epoch": 1.1690175699084384,
      "grad_norm": 0.036817170679569244,
      "learning_rate": 0.00018479413211461987,
      "loss": 0.0576,
      "step": 4724
    },
    {
      "epoch": 1.1692650334075725,
      "grad_norm": 0.028133107349276543,
      "learning_rate": 0.00018470020583730113,
      "loss": 0.0324,
      "step": 4725
    },
    {
      "epoch": 1.1695124969067063,
      "grad_norm": 0.0317552275955677,
      "learning_rate": 0.00018460628945118836,
      "loss": 0.041,
      "step": 4726
    },
    {
      "epoch": 1.1697599604058402,
      "grad_norm": 0.054141804575920105,
      "learning_rate": 0.00018451238297050731,
      "loss": 0.0557,
      "step": 4727
    },
    {
      "epoch": 1.170007423904974,
      "grad_norm": 0.03971196338534355,
      "learning_rate": 0.00018441848640948267,
      "loss": 0.0432,
      "step": 4728
    },
    {
      "epoch": 1.1702548874041079,
      "grad_norm": 0.041944630444049835,
      "learning_rate": 0.00018432459978233697,
      "loss": 0.0397,
      "step": 4729
    },
    {
      "epoch": 1.1705023509032417,
      "grad_norm": 0.03054271638393402,
      "learning_rate": 0.0001842307231032918,
      "loss": 0.0665,
      "step": 4730
    },
    {
      "epoch": 1.1707498144023756,
      "grad_norm": 0.04199722781777382,
      "learning_rate": 0.00018413685638656689,
      "loss": 0.0475,
      "step": 4731
    },
    {
      "epoch": 1.1709972779015096,
      "grad_norm": 0.08715233951807022,
      "learning_rate": 0.0001840429996463806,
      "loss": 0.0679,
      "step": 4732
    },
    {
      "epoch": 1.1712447414006435,
      "grad_norm": 0.031700439751148224,
      "learning_rate": 0.00018394915289694986,
      "loss": 0.0271,
      "step": 4733
    },
    {
      "epoch": 1.1714922048997773,
      "grad_norm": 0.07352715730667114,
      "learning_rate": 0.00018385531615248982,
      "loss": 0.0577,
      "step": 4734
    },
    {
      "epoch": 1.1717396683989112,
      "grad_norm": 0.07878653705120087,
      "learning_rate": 0.00018376148942721447,
      "loss": 0.0662,
      "step": 4735
    },
    {
      "epoch": 1.171987131898045,
      "grad_norm": 0.025474444031715393,
      "learning_rate": 0.00018366767273533588,
      "loss": 0.03,
      "step": 4736
    },
    {
      "epoch": 1.172234595397179,
      "grad_norm": 0.05246489867568016,
      "learning_rate": 0.00018357386609106502,
      "loss": 0.025,
      "step": 4737
    },
    {
      "epoch": 1.1724820588963127,
      "grad_norm": 0.06953635811805725,
      "learning_rate": 0.0001834800695086111,
      "loss": 0.0855,
      "step": 4738
    },
    {
      "epoch": 1.1727295223954466,
      "grad_norm": 0.033981166779994965,
      "learning_rate": 0.00018338628300218173,
      "loss": 0.0241,
      "step": 4739
    },
    {
      "epoch": 1.1729769858945804,
      "grad_norm": 0.032756101340055466,
      "learning_rate": 0.00018329250658598322,
      "loss": 0.0409,
      "step": 4740
    },
    {
      "epoch": 1.1732244493937145,
      "grad_norm": 0.03064148500561714,
      "learning_rate": 0.00018319874027422024,
      "loss": 0.0252,
      "step": 4741
    },
    {
      "epoch": 1.1734719128928484,
      "grad_norm": 0.06956351548433304,
      "learning_rate": 0.00018310498408109581,
      "loss": 0.0615,
      "step": 4742
    },
    {
      "epoch": 1.1737193763919822,
      "grad_norm": 0.045999448746442795,
      "learning_rate": 0.0001830112380208117,
      "loss": 0.0722,
      "step": 4743
    },
    {
      "epoch": 1.173966839891116,
      "grad_norm": 0.054726723581552505,
      "learning_rate": 0.00018291750210756773,
      "loss": 0.0563,
      "step": 4744
    },
    {
      "epoch": 1.17421430339025,
      "grad_norm": 0.04244540259242058,
      "learning_rate": 0.0001828237763555627,
      "loss": 0.0519,
      "step": 4745
    },
    {
      "epoch": 1.1744617668893838,
      "grad_norm": 0.04324480518698692,
      "learning_rate": 0.0001827300607789935,
      "loss": 0.0984,
      "step": 4746
    },
    {
      "epoch": 1.1747092303885176,
      "grad_norm": 0.05175164341926575,
      "learning_rate": 0.00018263635539205556,
      "loss": 0.0519,
      "step": 4747
    },
    {
      "epoch": 1.1749566938876517,
      "grad_norm": 0.02276894822716713,
      "learning_rate": 0.00018254266020894283,
      "loss": 0.0304,
      "step": 4748
    },
    {
      "epoch": 1.1752041573867855,
      "grad_norm": 0.056086957454681396,
      "learning_rate": 0.0001824489752438476,
      "loss": 0.0861,
      "step": 4749
    },
    {
      "epoch": 1.1754516208859194,
      "grad_norm": 0.06812082976102829,
      "learning_rate": 0.00018235530051096072,
      "loss": 0.0556,
      "step": 4750
    },
    {
      "epoch": 1.1756990843850532,
      "grad_norm": 0.06480088829994202,
      "learning_rate": 0.00018226163602447156,
      "loss": 0.057,
      "step": 4751
    },
    {
      "epoch": 1.175946547884187,
      "grad_norm": 0.027613015845417976,
      "learning_rate": 0.00018216798179856758,
      "loss": 0.0555,
      "step": 4752
    },
    {
      "epoch": 1.176194011383321,
      "grad_norm": 0.044093988835811615,
      "learning_rate": 0.00018207433784743525,
      "loss": 0.0222,
      "step": 4753
    },
    {
      "epoch": 1.1764414748824548,
      "grad_norm": 0.06157384067773819,
      "learning_rate": 0.0001819807041852589,
      "loss": 0.076,
      "step": 4754
    },
    {
      "epoch": 1.1766889383815888,
      "grad_norm": 0.03151073679327965,
      "learning_rate": 0.0001818870808262217,
      "loss": 0.0426,
      "step": 4755
    },
    {
      "epoch": 1.1769364018807227,
      "grad_norm": 0.04665539041161537,
      "learning_rate": 0.00018179346778450522,
      "loss": 0.051,
      "step": 4756
    },
    {
      "epoch": 1.1771838653798565,
      "grad_norm": 0.06911453604698181,
      "learning_rate": 0.0001816998650742892,
      "loss": 0.0904,
      "step": 4757
    },
    {
      "epoch": 1.1774313288789904,
      "grad_norm": 0.03675341233611107,
      "learning_rate": 0.00018160627270975217,
      "loss": 0.0372,
      "step": 4758
    },
    {
      "epoch": 1.1776787923781242,
      "grad_norm": 0.03664668649435043,
      "learning_rate": 0.00018151269070507077,
      "loss": 0.0279,
      "step": 4759
    },
    {
      "epoch": 1.177926255877258,
      "grad_norm": 0.06606080383062363,
      "learning_rate": 0.00018141911907442018,
      "loss": 0.0913,
      "step": 4760
    },
    {
      "epoch": 1.178173719376392,
      "grad_norm": 0.03193075209856033,
      "learning_rate": 0.00018132555783197435,
      "loss": 0.0392,
      "step": 4761
    },
    {
      "epoch": 1.1784211828755258,
      "grad_norm": 0.03156564384698868,
      "learning_rate": 0.00018123200699190494,
      "loss": 0.0505,
      "step": 4762
    },
    {
      "epoch": 1.1786686463746596,
      "grad_norm": 0.04877358675003052,
      "learning_rate": 0.0001811384665683829,
      "loss": 0.0782,
      "step": 4763
    },
    {
      "epoch": 1.1789161098737937,
      "grad_norm": 0.04789732024073601,
      "learning_rate": 0.0001810449365755767,
      "loss": 0.0555,
      "step": 4764
    },
    {
      "epoch": 1.1791635733729275,
      "grad_norm": 0.08553798496723175,
      "learning_rate": 0.00018095141702765394,
      "loss": 0.0528,
      "step": 4765
    },
    {
      "epoch": 1.1794110368720614,
      "grad_norm": 0.035750873386859894,
      "learning_rate": 0.00018085790793878037,
      "loss": 0.0377,
      "step": 4766
    },
    {
      "epoch": 1.1796585003711952,
      "grad_norm": 0.044553712010383606,
      "learning_rate": 0.0001807644093231201,
      "loss": 0.0495,
      "step": 4767
    },
    {
      "epoch": 1.179905963870329,
      "grad_norm": 0.03780220076441765,
      "learning_rate": 0.00018067092119483573,
      "loss": 0.0313,
      "step": 4768
    },
    {
      "epoch": 1.180153427369463,
      "grad_norm": 0.030005773529410362,
      "learning_rate": 0.0001805774435680883,
      "loss": 0.0301,
      "step": 4769
    },
    {
      "epoch": 1.1804008908685968,
      "grad_norm": 0.038183607161045074,
      "learning_rate": 0.0001804839764570371,
      "loss": 0.0421,
      "step": 4770
    },
    {
      "epoch": 1.1806483543677309,
      "grad_norm": 0.03814235329627991,
      "learning_rate": 0.0001803905198758402,
      "loss": 0.0411,
      "step": 4771
    },
    {
      "epoch": 1.1808958178668647,
      "grad_norm": 0.04530668258666992,
      "learning_rate": 0.00018029707383865345,
      "loss": 0.059,
      "step": 4772
    },
    {
      "epoch": 1.1811432813659986,
      "grad_norm": 0.039390627294778824,
      "learning_rate": 0.00018020363835963187,
      "loss": 0.0617,
      "step": 4773
    },
    {
      "epoch": 1.1813907448651324,
      "grad_norm": 0.03454512357711792,
      "learning_rate": 0.00018011021345292817,
      "loss": 0.0282,
      "step": 4774
    },
    {
      "epoch": 1.1816382083642663,
      "grad_norm": 0.08331705629825592,
      "learning_rate": 0.00018001679913269393,
      "loss": 0.0726,
      "step": 4775
    },
    {
      "epoch": 1.1818856718634,
      "grad_norm": 0.09148481488227844,
      "learning_rate": 0.00017992339541307895,
      "loss": 0.0848,
      "step": 4776
    },
    {
      "epoch": 1.182133135362534,
      "grad_norm": 0.023778611794114113,
      "learning_rate": 0.00017983000230823137,
      "loss": 0.034,
      "step": 4777
    },
    {
      "epoch": 1.182380598861668,
      "grad_norm": 0.06579124182462692,
      "learning_rate": 0.00017973661983229793,
      "loss": 0.0614,
      "step": 4778
    },
    {
      "epoch": 1.1826280623608019,
      "grad_norm": 0.03251778334379196,
      "learning_rate": 0.0001796432479994235,
      "loss": 0.0243,
      "step": 4779
    },
    {
      "epoch": 1.1828755258599357,
      "grad_norm": 0.05077553167939186,
      "learning_rate": 0.00017954988682375146,
      "loss": 0.0508,
      "step": 4780
    },
    {
      "epoch": 1.1831229893590696,
      "grad_norm": 0.02728746272623539,
      "learning_rate": 0.00017945653631942378,
      "loss": 0.0492,
      "step": 4781
    },
    {
      "epoch": 1.1833704528582034,
      "grad_norm": 0.03428146615624428,
      "learning_rate": 0.0001793631965005803,
      "loss": 0.0471,
      "step": 4782
    },
    {
      "epoch": 1.1836179163573373,
      "grad_norm": 0.03232710063457489,
      "learning_rate": 0.00017926986738135985,
      "loss": 0.0359,
      "step": 4783
    },
    {
      "epoch": 1.1838653798564711,
      "grad_norm": 0.04344259202480316,
      "learning_rate": 0.00017917654897589913,
      "loss": 0.0439,
      "step": 4784
    },
    {
      "epoch": 1.184112843355605,
      "grad_norm": 0.05993402749300003,
      "learning_rate": 0.00017908324129833352,
      "loss": 0.0399,
      "step": 4785
    },
    {
      "epoch": 1.1843603068547388,
      "grad_norm": 0.03135565295815468,
      "learning_rate": 0.00017898994436279673,
      "loss": 0.0409,
      "step": 4786
    },
    {
      "epoch": 1.1846077703538729,
      "grad_norm": 0.031148230656981468,
      "learning_rate": 0.0001788966581834207,
      "loss": 0.0448,
      "step": 4787
    },
    {
      "epoch": 1.1848552338530067,
      "grad_norm": 0.044913727790117264,
      "learning_rate": 0.00017880338277433594,
      "loss": 0.0566,
      "step": 4788
    },
    {
      "epoch": 1.1851026973521406,
      "grad_norm": 0.07411378622055054,
      "learning_rate": 0.0001787101181496711,
      "loss": 0.1246,
      "step": 4789
    },
    {
      "epoch": 1.1853501608512744,
      "grad_norm": 0.0666709765791893,
      "learning_rate": 0.00017861686432355333,
      "loss": 0.0547,
      "step": 4790
    },
    {
      "epoch": 1.1855976243504083,
      "grad_norm": 0.05955706536769867,
      "learning_rate": 0.00017852362131010835,
      "loss": 0.0429,
      "step": 4791
    },
    {
      "epoch": 1.1858450878495421,
      "grad_norm": 0.039568301290273666,
      "learning_rate": 0.00017843038912345986,
      "loss": 0.0448,
      "step": 4792
    },
    {
      "epoch": 1.186092551348676,
      "grad_norm": 0.05131673812866211,
      "learning_rate": 0.00017833716777773011,
      "loss": 0.0806,
      "step": 4793
    },
    {
      "epoch": 1.18634001484781,
      "grad_norm": 0.07763583958148956,
      "learning_rate": 0.00017824395728703967,
      "loss": 0.0542,
      "step": 4794
    },
    {
      "epoch": 1.186587478346944,
      "grad_norm": 0.06014468893408775,
      "learning_rate": 0.00017815075766550753,
      "loss": 0.0472,
      "step": 4795
    },
    {
      "epoch": 1.1868349418460777,
      "grad_norm": 0.04567505046725273,
      "learning_rate": 0.000178057568927251,
      "loss": 0.0495,
      "step": 4796
    },
    {
      "epoch": 1.1870824053452116,
      "grad_norm": 0.03439575433731079,
      "learning_rate": 0.00017796439108638563,
      "loss": 0.0276,
      "step": 4797
    },
    {
      "epoch": 1.1873298688443454,
      "grad_norm": 0.062400951981544495,
      "learning_rate": 0.00017787122415702556,
      "loss": 0.0677,
      "step": 4798
    },
    {
      "epoch": 1.1875773323434793,
      "grad_norm": 0.03891212120652199,
      "learning_rate": 0.00017777806815328295,
      "loss": 0.0346,
      "step": 4799
    },
    {
      "epoch": 1.1878247958426131,
      "grad_norm": 0.03838904947042465,
      "learning_rate": 0.00017768492308926868,
      "loss": 0.056,
      "step": 4800
    },
    {
      "epoch": 1.1878247958426131,
      "eval_loss": 0.2903873324394226,
      "eval_runtime": 202.6452,
      "eval_samples_per_second": 4.935,
      "eval_steps_per_second": 0.311,
      "step": 4800
    },
    {
      "epoch": 1.1880722593417472,
      "grad_norm": 0.04218283295631409,
      "learning_rate": 0.00017759178897909172,
      "loss": 0.0477,
      "step": 4801
    },
    {
      "epoch": 1.188319722840881,
      "grad_norm": 0.032718975096940994,
      "learning_rate": 0.00017749866583685942,
      "loss": 0.031,
      "step": 4802
    },
    {
      "epoch": 1.188567186340015,
      "grad_norm": 0.06623419374227524,
      "learning_rate": 0.00017740555367667756,
      "loss": 0.0891,
      "step": 4803
    },
    {
      "epoch": 1.1888146498391488,
      "grad_norm": 0.0520026832818985,
      "learning_rate": 0.00017731245251265004,
      "loss": 0.0541,
      "step": 4804
    },
    {
      "epoch": 1.1890621133382826,
      "grad_norm": 0.03904275596141815,
      "learning_rate": 0.00017721936235887936,
      "loss": 0.0308,
      "step": 4805
    },
    {
      "epoch": 1.1893095768374164,
      "grad_norm": 0.08254574984312057,
      "learning_rate": 0.00017712628322946627,
      "loss": 0.1089,
      "step": 4806
    },
    {
      "epoch": 1.1895570403365503,
      "grad_norm": 0.044366504997015,
      "learning_rate": 0.00017703321513850968,
      "loss": 0.0686,
      "step": 4807
    },
    {
      "epoch": 1.1898045038356841,
      "grad_norm": 0.04734957963228226,
      "learning_rate": 0.0001769401581001072,
      "loss": 0.0367,
      "step": 4808
    },
    {
      "epoch": 1.190051967334818,
      "grad_norm": 0.07297579944133759,
      "learning_rate": 0.0001768471121283542,
      "loss": 0.1023,
      "step": 4809
    },
    {
      "epoch": 1.190299430833952,
      "grad_norm": 0.037726327776908875,
      "learning_rate": 0.00017675407723734493,
      "loss": 0.0428,
      "step": 4810
    },
    {
      "epoch": 1.190546894333086,
      "grad_norm": 0.028546281158924103,
      "learning_rate": 0.00017666105344117178,
      "loss": 0.0319,
      "step": 4811
    },
    {
      "epoch": 1.1907943578322198,
      "grad_norm": 0.03419627621769905,
      "learning_rate": 0.00017656804075392524,
      "loss": 0.0308,
      "step": 4812
    },
    {
      "epoch": 1.1910418213313536,
      "grad_norm": 0.044210366904735565,
      "learning_rate": 0.00017647503918969442,
      "loss": 0.045,
      "step": 4813
    },
    {
      "epoch": 1.1912892848304875,
      "grad_norm": 0.03447443246841431,
      "learning_rate": 0.00017638204876256651,
      "loss": 0.0192,
      "step": 4814
    },
    {
      "epoch": 1.1915367483296213,
      "grad_norm": 0.07933064550161362,
      "learning_rate": 0.0001762890694866272,
      "loss": 0.1076,
      "step": 4815
    },
    {
      "epoch": 1.1917842118287552,
      "grad_norm": 0.10977167636156082,
      "learning_rate": 0.0001761961013759605,
      "loss": 0.0732,
      "step": 4816
    },
    {
      "epoch": 1.1920316753278892,
      "grad_norm": 0.027112657204270363,
      "learning_rate": 0.00017610314444464837,
      "loss": 0.0233,
      "step": 4817
    },
    {
      "epoch": 1.192279138827023,
      "grad_norm": 0.040043920278549194,
      "learning_rate": 0.0001760101987067717,
      "loss": 0.0783,
      "step": 4818
    },
    {
      "epoch": 1.192526602326157,
      "grad_norm": 0.0755683034658432,
      "learning_rate": 0.00017591726417640896,
      "loss": 0.0711,
      "step": 4819
    },
    {
      "epoch": 1.1927740658252908,
      "grad_norm": 0.03485504165291786,
      "learning_rate": 0.00017582434086763754,
      "loss": 0.0556,
      "step": 4820
    },
    {
      "epoch": 1.1930215293244246,
      "grad_norm": 0.03710787743330002,
      "learning_rate": 0.00017573142879453286,
      "loss": 0.0479,
      "step": 4821
    },
    {
      "epoch": 1.1932689928235585,
      "grad_norm": 0.06692979484796524,
      "learning_rate": 0.0001756385279711686,
      "loss": 0.0652,
      "step": 4822
    },
    {
      "epoch": 1.1935164563226923,
      "grad_norm": 0.05414562672376633,
      "learning_rate": 0.00017554563841161685,
      "loss": 0.0427,
      "step": 4823
    },
    {
      "epoch": 1.1937639198218264,
      "grad_norm": 0.04119962081313133,
      "learning_rate": 0.00017545276012994788,
      "loss": 0.044,
      "step": 4824
    },
    {
      "epoch": 1.1940113833209602,
      "grad_norm": 0.04087061807513237,
      "learning_rate": 0.00017535989314023027,
      "loss": 0.0423,
      "step": 4825
    },
    {
      "epoch": 1.194258846820094,
      "grad_norm": 0.027742110192775726,
      "learning_rate": 0.00017526703745653118,
      "loss": 0.0187,
      "step": 4826
    },
    {
      "epoch": 1.194506310319228,
      "grad_norm": 0.09693322330713272,
      "learning_rate": 0.0001751741930929155,
      "loss": 0.0909,
      "step": 4827
    },
    {
      "epoch": 1.1947537738183618,
      "grad_norm": 0.04231772944331169,
      "learning_rate": 0.0001750813600634469,
      "loss": 0.0508,
      "step": 4828
    },
    {
      "epoch": 1.1950012373174956,
      "grad_norm": 0.02458026073873043,
      "learning_rate": 0.00017498853838218708,
      "loss": 0.025,
      "step": 4829
    },
    {
      "epoch": 1.1952487008166295,
      "grad_norm": 0.02734312228858471,
      "learning_rate": 0.00017489572806319616,
      "loss": 0.0233,
      "step": 4830
    },
    {
      "epoch": 1.1954961643157633,
      "grad_norm": 0.03246387839317322,
      "learning_rate": 0.0001748029291205324,
      "loss": 0.037,
      "step": 4831
    },
    {
      "epoch": 1.1957436278148972,
      "grad_norm": 0.0535455197095871,
      "learning_rate": 0.00017471014156825244,
      "loss": 0.0574,
      "step": 4832
    },
    {
      "epoch": 1.1959910913140313,
      "grad_norm": 0.025329452008008957,
      "learning_rate": 0.00017461736542041117,
      "loss": 0.0291,
      "step": 4833
    },
    {
      "epoch": 1.196238554813165,
      "grad_norm": 0.05026771128177643,
      "learning_rate": 0.00017452460069106168,
      "loss": 0.0599,
      "step": 4834
    },
    {
      "epoch": 1.196486018312299,
      "grad_norm": 0.04494056478142738,
      "learning_rate": 0.0001744318473942554,
      "loss": 0.0617,
      "step": 4835
    },
    {
      "epoch": 1.1967334818114328,
      "grad_norm": 0.05024411901831627,
      "learning_rate": 0.00017433910554404212,
      "loss": 0.0738,
      "step": 4836
    },
    {
      "epoch": 1.1969809453105666,
      "grad_norm": 0.031702920794487,
      "learning_rate": 0.0001742463751544697,
      "loss": 0.0349,
      "step": 4837
    },
    {
      "epoch": 1.1972284088097005,
      "grad_norm": 0.03544170781970024,
      "learning_rate": 0.00017415365623958445,
      "loss": 0.0329,
      "step": 4838
    },
    {
      "epoch": 1.1974758723088343,
      "grad_norm": 0.04762585461139679,
      "learning_rate": 0.00017406094881343073,
      "loss": 0.0479,
      "step": 4839
    },
    {
      "epoch": 1.1977233358079684,
      "grad_norm": 0.028853347525000572,
      "learning_rate": 0.00017396825289005136,
      "loss": 0.0287,
      "step": 4840
    },
    {
      "epoch": 1.1979707993071023,
      "grad_norm": 0.03212764114141464,
      "learning_rate": 0.0001738755684834874,
      "loss": 0.0673,
      "step": 4841
    },
    {
      "epoch": 1.1982182628062361,
      "grad_norm": 0.04488200321793556,
      "learning_rate": 0.000173782895607778,
      "loss": 0.0657,
      "step": 4842
    },
    {
      "epoch": 1.19846572630537,
      "grad_norm": 0.07044646888971329,
      "learning_rate": 0.00017369023427696073,
      "loss": 0.0788,
      "step": 4843
    },
    {
      "epoch": 1.1987131898045038,
      "grad_norm": 0.053205687552690506,
      "learning_rate": 0.00017359758450507124,
      "loss": 0.0278,
      "step": 4844
    },
    {
      "epoch": 1.1989606533036377,
      "grad_norm": 0.042029641568660736,
      "learning_rate": 0.00017350494630614366,
      "loss": 0.0308,
      "step": 4845
    },
    {
      "epoch": 1.1992081168027715,
      "grad_norm": 0.0946066677570343,
      "learning_rate": 0.00017341231969421033,
      "loss": 0.1053,
      "step": 4846
    },
    {
      "epoch": 1.1994555803019056,
      "grad_norm": 0.06770940124988556,
      "learning_rate": 0.00017331970468330155,
      "loss": 0.1024,
      "step": 4847
    },
    {
      "epoch": 1.1997030438010394,
      "grad_norm": 0.0738147497177124,
      "learning_rate": 0.00017322710128744618,
      "loss": 0.0782,
      "step": 4848
    },
    {
      "epoch": 1.1999505073001733,
      "grad_norm": 0.05889108404517174,
      "learning_rate": 0.00017313450952067118,
      "loss": 0.0763,
      "step": 4849
    },
    {
      "epoch": 1.2001979707993071,
      "grad_norm": 0.025872260332107544,
      "learning_rate": 0.00017304192939700174,
      "loss": 0.0435,
      "step": 4850
    },
    {
      "epoch": 1.200445434298441,
      "grad_norm": 0.03637566417455673,
      "learning_rate": 0.00017294936093046142,
      "loss": 0.0502,
      "step": 4851
    },
    {
      "epoch": 1.2006928977975748,
      "grad_norm": 0.04536012187600136,
      "learning_rate": 0.00017285680413507175,
      "loss": 0.0458,
      "step": 4852
    },
    {
      "epoch": 1.2009403612967087,
      "grad_norm": 0.05208742991089821,
      "learning_rate": 0.0001727642590248529,
      "loss": 0.0409,
      "step": 4853
    },
    {
      "epoch": 1.2011878247958425,
      "grad_norm": 0.029404863715171814,
      "learning_rate": 0.00017267172561382272,
      "loss": 0.0757,
      "step": 4854
    },
    {
      "epoch": 1.2014352882949764,
      "grad_norm": 0.06411872059106827,
      "learning_rate": 0.00017257920391599782,
      "loss": 0.0673,
      "step": 4855
    },
    {
      "epoch": 1.2016827517941104,
      "grad_norm": 0.0377676784992218,
      "learning_rate": 0.00017248669394539278,
      "loss": 0.0347,
      "step": 4856
    },
    {
      "epoch": 1.2019302152932443,
      "grad_norm": 0.07048170268535614,
      "learning_rate": 0.00017239419571602038,
      "loss": 0.0682,
      "step": 4857
    },
    {
      "epoch": 1.2021776787923781,
      "grad_norm": 0.06466948986053467,
      "learning_rate": 0.00017230170924189175,
      "loss": 0.0819,
      "step": 4858
    },
    {
      "epoch": 1.202425142291512,
      "grad_norm": 0.04918549954891205,
      "learning_rate": 0.00017220923453701607,
      "loss": 0.0418,
      "step": 4859
    },
    {
      "epoch": 1.2026726057906458,
      "grad_norm": 0.037679724395275116,
      "learning_rate": 0.0001721167716154008,
      "loss": 0.0424,
      "step": 4860
    },
    {
      "epoch": 1.2029200692897797,
      "grad_norm": 0.03517982363700867,
      "learning_rate": 0.0001720243204910519,
      "loss": 0.0298,
      "step": 4861
    },
    {
      "epoch": 1.2031675327889135,
      "grad_norm": 0.05445001646876335,
      "learning_rate": 0.00017193188117797293,
      "loss": 0.0712,
      "step": 4862
    },
    {
      "epoch": 1.2034149962880476,
      "grad_norm": 0.03908563777804375,
      "learning_rate": 0.00017183945369016635,
      "loss": 0.0524,
      "step": 4863
    },
    {
      "epoch": 1.2036624597871814,
      "grad_norm": 0.05597962439060211,
      "learning_rate": 0.00017174703804163222,
      "loss": 0.0623,
      "step": 4864
    },
    {
      "epoch": 1.2039099232863153,
      "grad_norm": 0.03760549798607826,
      "learning_rate": 0.00017165463424636924,
      "loss": 0.0448,
      "step": 4865
    },
    {
      "epoch": 1.2041573867854491,
      "grad_norm": 0.07307913154363632,
      "learning_rate": 0.0001715622423183742,
      "loss": 0.1028,
      "step": 4866
    },
    {
      "epoch": 1.204404850284583,
      "grad_norm": 0.06804824620485306,
      "learning_rate": 0.00017146986227164197,
      "loss": 0.0619,
      "step": 4867
    },
    {
      "epoch": 1.2046523137837168,
      "grad_norm": 0.03966960683465004,
      "learning_rate": 0.00017137749412016566,
      "loss": 0.0239,
      "step": 4868
    },
    {
      "epoch": 1.2048997772828507,
      "grad_norm": 0.0426851324737072,
      "learning_rate": 0.00017128513787793688,
      "loss": 0.0433,
      "step": 4869
    },
    {
      "epoch": 1.2051472407819848,
      "grad_norm": 0.04366227984428406,
      "learning_rate": 0.0001711927935589448,
      "loss": 0.0601,
      "step": 4870
    },
    {
      "epoch": 1.2053947042811186,
      "grad_norm": 0.035618770867586136,
      "learning_rate": 0.00017110046117717749,
      "loss": 0.0254,
      "step": 4871
    },
    {
      "epoch": 1.2056421677802525,
      "grad_norm": 0.03640471026301384,
      "learning_rate": 0.00017100814074662063,
      "loss": 0.0453,
      "step": 4872
    },
    {
      "epoch": 1.2058896312793863,
      "grad_norm": 0.08442392200231552,
      "learning_rate": 0.00017091583228125845,
      "loss": 0.032,
      "step": 4873
    },
    {
      "epoch": 1.2061370947785202,
      "grad_norm": 0.03872542828321457,
      "learning_rate": 0.00017082353579507336,
      "loss": 0.0452,
      "step": 4874
    },
    {
      "epoch": 1.206384558277654,
      "grad_norm": 0.035317737609148026,
      "learning_rate": 0.00017073125130204574,
      "loss": 0.0378,
      "step": 4875
    },
    {
      "epoch": 1.2066320217767879,
      "grad_norm": 0.059312332421541214,
      "learning_rate": 0.00017063897881615432,
      "loss": 0.0396,
      "step": 4876
    },
    {
      "epoch": 1.206879485275922,
      "grad_norm": 0.06379607319831848,
      "learning_rate": 0.00017054671835137588,
      "loss": 0.0509,
      "step": 4877
    },
    {
      "epoch": 1.2071269487750558,
      "grad_norm": 0.06145734712481499,
      "learning_rate": 0.00017045446992168547,
      "loss": 0.0663,
      "step": 4878
    },
    {
      "epoch": 1.2073744122741896,
      "grad_norm": 0.05295180529356003,
      "learning_rate": 0.00017036223354105652,
      "loss": 0.0612,
      "step": 4879
    },
    {
      "epoch": 1.2076218757733235,
      "grad_norm": 0.06588121503591537,
      "learning_rate": 0.00017027000922346008,
      "loss": 0.0694,
      "step": 4880
    },
    {
      "epoch": 1.2078693392724573,
      "grad_norm": 0.041003450751304626,
      "learning_rate": 0.00017017779698286602,
      "loss": 0.0572,
      "step": 4881
    },
    {
      "epoch": 1.2081168027715912,
      "grad_norm": 0.04641535505652428,
      "learning_rate": 0.0001700855968332419,
      "loss": 0.0541,
      "step": 4882
    },
    {
      "epoch": 1.208364266270725,
      "grad_norm": 0.06709253787994385,
      "learning_rate": 0.0001699934087885537,
      "loss": 0.0582,
      "step": 4883
    },
    {
      "epoch": 1.2086117297698589,
      "grad_norm": 0.03862394019961357,
      "learning_rate": 0.00016990123286276542,
      "loss": 0.0347,
      "step": 4884
    },
    {
      "epoch": 1.2088591932689927,
      "grad_norm": 0.10076002776622772,
      "learning_rate": 0.00016980906906983938,
      "loss": 0.0739,
      "step": 4885
    },
    {
      "epoch": 1.2091066567681268,
      "grad_norm": 0.04356559365987778,
      "learning_rate": 0.00016971691742373592,
      "loss": 0.0632,
      "step": 4886
    },
    {
      "epoch": 1.2093541202672606,
      "grad_norm": 0.07385755330324173,
      "learning_rate": 0.0001696247779384136,
      "loss": 0.0287,
      "step": 4887
    },
    {
      "epoch": 1.2096015837663945,
      "grad_norm": 0.03250264376401901,
      "learning_rate": 0.00016953265062782908,
      "loss": 0.033,
      "step": 4888
    },
    {
      "epoch": 1.2098490472655283,
      "grad_norm": 0.05272957682609558,
      "learning_rate": 0.00016944053550593748,
      "loss": 0.0583,
      "step": 4889
    },
    {
      "epoch": 1.2100965107646622,
      "grad_norm": 0.0561104454100132,
      "learning_rate": 0.00016934843258669148,
      "loss": 0.0904,
      "step": 4890
    },
    {
      "epoch": 1.210343974263796,
      "grad_norm": 0.052672967314720154,
      "learning_rate": 0.00016925634188404249,
      "loss": 0.0582,
      "step": 4891
    },
    {
      "epoch": 1.2105914377629299,
      "grad_norm": 0.09252333641052246,
      "learning_rate": 0.00016916426341193974,
      "loss": 0.1048,
      "step": 4892
    },
    {
      "epoch": 1.210838901262064,
      "grad_norm": 0.04404928907752037,
      "learning_rate": 0.0001690721971843307,
      "loss": 0.0352,
      "step": 4893
    },
    {
      "epoch": 1.2110863647611978,
      "grad_norm": 0.06734181195497513,
      "learning_rate": 0.00016898014321516108,
      "loss": 0.0566,
      "step": 4894
    },
    {
      "epoch": 1.2113338282603316,
      "grad_norm": 0.04306918382644653,
      "learning_rate": 0.00016888810151837458,
      "loss": 0.0531,
      "step": 4895
    },
    {
      "epoch": 1.2115812917594655,
      "grad_norm": 0.047188650816679,
      "learning_rate": 0.0001687960721079131,
      "loss": 0.0652,
      "step": 4896
    },
    {
      "epoch": 1.2118287552585993,
      "grad_norm": 0.07151734083890915,
      "learning_rate": 0.00016870405499771663,
      "loss": 0.0851,
      "step": 4897
    },
    {
      "epoch": 1.2120762187577332,
      "grad_norm": 0.034235630184412,
      "learning_rate": 0.00016861205020172337,
      "loss": 0.0241,
      "step": 4898
    },
    {
      "epoch": 1.212323682256867,
      "grad_norm": 0.06908514350652695,
      "learning_rate": 0.0001685200577338698,
      "loss": 0.0761,
      "step": 4899
    },
    {
      "epoch": 1.2125711457560011,
      "grad_norm": 0.04836157709360123,
      "learning_rate": 0.0001684280776080901,
      "loss": 0.0647,
      "step": 4900
    },
    {
      "epoch": 1.212818609255135,
      "grad_norm": 0.028918417170643806,
      "learning_rate": 0.0001683361098383171,
      "loss": 0.0348,
      "step": 4901
    },
    {
      "epoch": 1.2130660727542688,
      "grad_norm": 0.027828354388475418,
      "learning_rate": 0.0001682441544384813,
      "loss": 0.0303,
      "step": 4902
    },
    {
      "epoch": 1.2133135362534027,
      "grad_norm": 0.04086504504084587,
      "learning_rate": 0.0001681522114225116,
      "loss": 0.0262,
      "step": 4903
    },
    {
      "epoch": 1.2135609997525365,
      "grad_norm": 0.040789563208818436,
      "learning_rate": 0.00016806028080433507,
      "loss": 0.0475,
      "step": 4904
    },
    {
      "epoch": 1.2138084632516704,
      "grad_norm": 0.12575814127922058,
      "learning_rate": 0.00016796836259787656,
      "loss": 0.1078,
      "step": 4905
    },
    {
      "epoch": 1.2140559267508042,
      "grad_norm": 0.09023113548755646,
      "learning_rate": 0.00016787645681705948,
      "loss": 0.172,
      "step": 4906
    },
    {
      "epoch": 1.214303390249938,
      "grad_norm": 0.08731294423341751,
      "learning_rate": 0.00016778456347580497,
      "loss": 0.084,
      "step": 4907
    },
    {
      "epoch": 1.214550853749072,
      "grad_norm": 0.04540436342358589,
      "learning_rate": 0.0001676926825880326,
      "loss": 0.0573,
      "step": 4908
    },
    {
      "epoch": 1.214798317248206,
      "grad_norm": 0.07727266848087311,
      "learning_rate": 0.0001676008141676599,
      "loss": 0.0688,
      "step": 4909
    },
    {
      "epoch": 1.2150457807473398,
      "grad_norm": 0.049948800355196,
      "learning_rate": 0.00016750895822860245,
      "loss": 0.0574,
      "step": 4910
    },
    {
      "epoch": 1.2152932442464737,
      "grad_norm": 0.04007868096232414,
      "learning_rate": 0.0001674171147847741,
      "loss": 0.0544,
      "step": 4911
    },
    {
      "epoch": 1.2155407077456075,
      "grad_norm": 0.17174215614795685,
      "learning_rate": 0.00016732528385008663,
      "loss": 0.0701,
      "step": 4912
    },
    {
      "epoch": 1.2157881712447414,
      "grad_norm": 0.050131697207689285,
      "learning_rate": 0.00016723346543845005,
      "loss": 0.0482,
      "step": 4913
    },
    {
      "epoch": 1.2160356347438752,
      "grad_norm": 0.059727635234594345,
      "learning_rate": 0.00016714165956377254,
      "loss": 0.0644,
      "step": 4914
    },
    {
      "epoch": 1.216283098243009,
      "grad_norm": 0.049177851527929306,
      "learning_rate": 0.00016704986623996006,
      "loss": 0.0662,
      "step": 4915
    },
    {
      "epoch": 1.2165305617421431,
      "grad_norm": 0.02939399890601635,
      "learning_rate": 0.0001669580854809172,
      "loss": 0.0322,
      "step": 4916
    },
    {
      "epoch": 1.216778025241277,
      "grad_norm": 0.05612210929393768,
      "learning_rate": 0.00016686631730054602,
      "loss": 0.0254,
      "step": 4917
    },
    {
      "epoch": 1.2170254887404108,
      "grad_norm": 0.06272626668214798,
      "learning_rate": 0.00016677456171274718,
      "loss": 0.0538,
      "step": 4918
    },
    {
      "epoch": 1.2172729522395447,
      "grad_norm": 0.03735515847802162,
      "learning_rate": 0.00016668281873141925,
      "loss": 0.028,
      "step": 4919
    },
    {
      "epoch": 1.2175204157386785,
      "grad_norm": 0.05322319269180298,
      "learning_rate": 0.00016659108837045877,
      "loss": 0.0439,
      "step": 4920
    },
    {
      "epoch": 1.2177678792378124,
      "grad_norm": 0.08361984044313431,
      "learning_rate": 0.00016649937064376063,
      "loss": 0.0669,
      "step": 4921
    },
    {
      "epoch": 1.2180153427369462,
      "grad_norm": 0.05999821424484253,
      "learning_rate": 0.00016640766556521753,
      "loss": 0.0417,
      "step": 4922
    },
    {
      "epoch": 1.2182628062360803,
      "grad_norm": 0.0326034314930439,
      "learning_rate": 0.00016631597314872038,
      "loss": 0.0252,
      "step": 4923
    },
    {
      "epoch": 1.2185102697352141,
      "grad_norm": 0.05039357393980026,
      "learning_rate": 0.0001662242934081584,
      "loss": 0.0739,
      "step": 4924
    },
    {
      "epoch": 1.218757733234348,
      "grad_norm": 0.06410471349954605,
      "learning_rate": 0.00016613262635741834,
      "loss": 0.028,
      "step": 4925
    },
    {
      "epoch": 1.2190051967334818,
      "grad_norm": 0.043149448931217194,
      "learning_rate": 0.00016604097201038565,
      "loss": 0.0331,
      "step": 4926
    },
    {
      "epoch": 1.2192526602326157,
      "grad_norm": 0.04868866875767708,
      "learning_rate": 0.00016594933038094326,
      "loss": 0.0464,
      "step": 4927
    },
    {
      "epoch": 1.2195001237317495,
      "grad_norm": 0.05052569508552551,
      "learning_rate": 0.0001658577014829727,
      "loss": 0.0531,
      "step": 4928
    },
    {
      "epoch": 1.2197475872308834,
      "grad_norm": 0.04928823933005333,
      "learning_rate": 0.00016576608533035332,
      "loss": 0.0686,
      "step": 4929
    },
    {
      "epoch": 1.2199950507300172,
      "grad_norm": 0.06194544583559036,
      "learning_rate": 0.0001656744819369625,
      "loss": 0.059,
      "step": 4930
    },
    {
      "epoch": 1.220242514229151,
      "grad_norm": 0.04601564630866051,
      "learning_rate": 0.00016558289131667582,
      "loss": 0.0392,
      "step": 4931
    },
    {
      "epoch": 1.2204899777282852,
      "grad_norm": 0.03906140848994255,
      "learning_rate": 0.0001654913134833667,
      "loss": 0.0448,
      "step": 4932
    },
    {
      "epoch": 1.220737441227419,
      "grad_norm": 0.05443553254008293,
      "learning_rate": 0.00016539974845090688,
      "loss": 0.0413,
      "step": 4933
    },
    {
      "epoch": 1.2209849047265529,
      "grad_norm": 0.072829969227314,
      "learning_rate": 0.00016530819623316624,
      "loss": 0.0753,
      "step": 4934
    },
    {
      "epoch": 1.2212323682256867,
      "grad_norm": 0.03778327628970146,
      "learning_rate": 0.00016521665684401222,
      "loss": 0.0268,
      "step": 4935
    },
    {
      "epoch": 1.2214798317248206,
      "grad_norm": 0.05366439372301102,
      "learning_rate": 0.00016512513029731085,
      "loss": 0.0578,
      "step": 4936
    },
    {
      "epoch": 1.2217272952239544,
      "grad_norm": 0.0295648742467165,
      "learning_rate": 0.0001650336166069259,
      "loss": 0.0312,
      "step": 4937
    },
    {
      "epoch": 1.2219747587230883,
      "grad_norm": 0.03662184998393059,
      "learning_rate": 0.00016494211578671936,
      "loss": 0.0354,
      "step": 4938
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.032300084829330444,
      "learning_rate": 0.00016485062785055122,
      "loss": 0.0555,
      "step": 4939
    },
    {
      "epoch": 1.2224696857213562,
      "grad_norm": 0.12662312388420105,
      "learning_rate": 0.00016475915281227938,
      "loss": 0.1557,
      "step": 4940
    },
    {
      "epoch": 1.22271714922049,
      "grad_norm": 0.04165714979171753,
      "learning_rate": 0.00016466769068576004,
      "loss": 0.0322,
      "step": 4941
    },
    {
      "epoch": 1.2229646127196239,
      "grad_norm": 0.055599380284547806,
      "learning_rate": 0.00016457624148484725,
      "loss": 0.0514,
      "step": 4942
    },
    {
      "epoch": 1.2232120762187577,
      "grad_norm": 0.042426977306604385,
      "learning_rate": 0.00016448480522339304,
      "loss": 0.0404,
      "step": 4943
    },
    {
      "epoch": 1.2234595397178916,
      "grad_norm": 0.04961780458688736,
      "learning_rate": 0.00016439338191524792,
      "loss": 0.0229,
      "step": 4944
    },
    {
      "epoch": 1.2237070032170254,
      "grad_norm": 0.06366586685180664,
      "learning_rate": 0.00016430197157425984,
      "loss": 0.0583,
      "step": 4945
    },
    {
      "epoch": 1.2239544667161595,
      "grad_norm": 0.056726567447185516,
      "learning_rate": 0.00016421057421427528,
      "loss": 0.0548,
      "step": 4946
    },
    {
      "epoch": 1.2242019302152933,
      "grad_norm": 0.039294783025979996,
      "learning_rate": 0.00016411918984913834,
      "loss": 0.0411,
      "step": 4947
    },
    {
      "epoch": 1.2244493937144272,
      "grad_norm": 0.04053651914000511,
      "learning_rate": 0.00016402781849269148,
      "loss": 0.046,
      "step": 4948
    },
    {
      "epoch": 1.224696857213561,
      "grad_norm": 0.049827709794044495,
      "learning_rate": 0.00016393646015877507,
      "loss": 0.0223,
      "step": 4949
    },
    {
      "epoch": 1.2249443207126949,
      "grad_norm": 0.04799312725663185,
      "learning_rate": 0.00016384511486122738,
      "loss": 0.0537,
      "step": 4950
    },
    {
      "epoch": 1.2251917842118287,
      "grad_norm": 0.0334172286093235,
      "learning_rate": 0.000163753782613885,
      "loss": 0.0593,
      "step": 4951
    },
    {
      "epoch": 1.2254392477109626,
      "grad_norm": 0.04617660120129585,
      "learning_rate": 0.00016366246343058218,
      "loss": 0.0364,
      "step": 4952
    },
    {
      "epoch": 1.2256867112100964,
      "grad_norm": 0.08655045181512833,
      "learning_rate": 0.00016357115732515155,
      "loss": 0.1336,
      "step": 4953
    },
    {
      "epoch": 1.2259341747092303,
      "grad_norm": 0.054579686373472214,
      "learning_rate": 0.00016347986431142352,
      "loss": 0.0533,
      "step": 4954
    },
    {
      "epoch": 1.2261816382083643,
      "grad_norm": 0.05657845735549927,
      "learning_rate": 0.00016338858440322656,
      "loss": 0.0407,
      "step": 4955
    },
    {
      "epoch": 1.2264291017074982,
      "grad_norm": 0.03763788193464279,
      "learning_rate": 0.00016329731761438726,
      "loss": 0.0411,
      "step": 4956
    },
    {
      "epoch": 1.226676565206632,
      "grad_norm": 0.04924406111240387,
      "learning_rate": 0.00016320606395873004,
      "loss": 0.0305,
      "step": 4957
    },
    {
      "epoch": 1.226924028705766,
      "grad_norm": 0.04596292972564697,
      "learning_rate": 0.00016311482345007753,
      "loss": 0.0477,
      "step": 4958
    },
    {
      "epoch": 1.2271714922048997,
      "grad_norm": 0.059094060212373734,
      "learning_rate": 0.00016302359610225026,
      "loss": 0.0645,
      "step": 4959
    },
    {
      "epoch": 1.2274189557040336,
      "grad_norm": 0.06404171139001846,
      "learning_rate": 0.00016293238192906662,
      "loss": 0.0688,
      "step": 4960
    },
    {
      "epoch": 1.2276664192031674,
      "grad_norm": 0.03511267527937889,
      "learning_rate": 0.00016284118094434346,
      "loss": 0.0416,
      "step": 4961
    },
    {
      "epoch": 1.2279138827023015,
      "grad_norm": 0.07902580499649048,
      "learning_rate": 0.00016274999316189507,
      "loss": 0.1025,
      "step": 4962
    },
    {
      "epoch": 1.2281613462014354,
      "grad_norm": 0.026018112897872925,
      "learning_rate": 0.00016265881859553412,
      "loss": 0.0427,
      "step": 4963
    },
    {
      "epoch": 1.2284088097005692,
      "grad_norm": 0.04095250740647316,
      "learning_rate": 0.00016256765725907124,
      "loss": 0.0564,
      "step": 4964
    },
    {
      "epoch": 1.228656273199703,
      "grad_norm": 0.046700619161129,
      "learning_rate": 0.00016247650916631482,
      "loss": 0.059,
      "step": 4965
    },
    {
      "epoch": 1.228903736698837,
      "grad_norm": 0.05013316124677658,
      "learning_rate": 0.00016238537433107154,
      "loss": 0.0325,
      "step": 4966
    },
    {
      "epoch": 1.2291512001979708,
      "grad_norm": 0.047818176448345184,
      "learning_rate": 0.00016229425276714584,
      "loss": 0.0695,
      "step": 4967
    },
    {
      "epoch": 1.2293986636971046,
      "grad_norm": 0.04442611336708069,
      "learning_rate": 0.00016220314448834022,
      "loss": 0.0283,
      "step": 4968
    },
    {
      "epoch": 1.2296461271962387,
      "grad_norm": 0.07754713296890259,
      "learning_rate": 0.00016211204950845542,
      "loss": 0.0913,
      "step": 4969
    },
    {
      "epoch": 1.2298935906953725,
      "grad_norm": 0.04678765684366226,
      "learning_rate": 0.00016202096784128967,
      "loss": 0.0461,
      "step": 4970
    },
    {
      "epoch": 1.2301410541945064,
      "grad_norm": 0.040116086602211,
      "learning_rate": 0.0001619298995006397,
      "loss": 0.0351,
      "step": 4971
    },
    {
      "epoch": 1.2303885176936402,
      "grad_norm": 0.020977690815925598,
      "learning_rate": 0.00016183884450029966,
      "loss": 0.0241,
      "step": 4972
    },
    {
      "epoch": 1.230635981192774,
      "grad_norm": 0.03455200418829918,
      "learning_rate": 0.00016174780285406224,
      "loss": 0.031,
      "step": 4973
    },
    {
      "epoch": 1.230883444691908,
      "grad_norm": 0.059383466839790344,
      "learning_rate": 0.00016165677457571787,
      "loss": 0.0563,
      "step": 4974
    },
    {
      "epoch": 1.2311309081910418,
      "grad_norm": 0.04071114957332611,
      "learning_rate": 0.00016156575967905479,
      "loss": 0.0366,
      "step": 4975
    },
    {
      "epoch": 1.2313783716901756,
      "grad_norm": 0.04622287303209305,
      "learning_rate": 0.00016147475817785952,
      "loss": 0.0574,
      "step": 4976
    },
    {
      "epoch": 1.2316258351893095,
      "grad_norm": 0.04232104867696762,
      "learning_rate": 0.00016138377008591626,
      "loss": 0.0413,
      "step": 4977
    },
    {
      "epoch": 1.2318732986884435,
      "grad_norm": 0.029714902862906456,
      "learning_rate": 0.00016129279541700735,
      "loss": 0.0212,
      "step": 4978
    },
    {
      "epoch": 1.2321207621875774,
      "grad_norm": 0.04274526983499527,
      "learning_rate": 0.0001612018341849133,
      "loss": 0.0568,
      "step": 4979
    },
    {
      "epoch": 1.2323682256867112,
      "grad_norm": 0.06462285667657852,
      "learning_rate": 0.00016111088640341197,
      "loss": 0.0684,
      "step": 4980
    },
    {
      "epoch": 1.232615689185845,
      "grad_norm": 0.04318297281861305,
      "learning_rate": 0.00016101995208627984,
      "loss": 0.0352,
      "step": 4981
    },
    {
      "epoch": 1.232863152684979,
      "grad_norm": 0.03426792472600937,
      "learning_rate": 0.00016092903124729097,
      "loss": 0.0408,
      "step": 4982
    },
    {
      "epoch": 1.2331106161841128,
      "grad_norm": 0.04089048504829407,
      "learning_rate": 0.00016083812390021746,
      "loss": 0.0486,
      "step": 4983
    },
    {
      "epoch": 1.2333580796832466,
      "grad_norm": 0.03402780368924141,
      "learning_rate": 0.00016074723005882953,
      "loss": 0.0357,
      "step": 4984
    },
    {
      "epoch": 1.2336055431823807,
      "grad_norm": 0.0397716648876667,
      "learning_rate": 0.00016065634973689503,
      "loss": 0.028,
      "step": 4985
    },
    {
      "epoch": 1.2338530066815145,
      "grad_norm": 0.051117368042469025,
      "learning_rate": 0.00016056548294818008,
      "loss": 0.0392,
      "step": 4986
    },
    {
      "epoch": 1.2341004701806484,
      "grad_norm": 0.045531220734119415,
      "learning_rate": 0.0001604746297064485,
      "loss": 0.0356,
      "step": 4987
    },
    {
      "epoch": 1.2343479336797822,
      "grad_norm": 0.056653957813978195,
      "learning_rate": 0.00016038379002546215,
      "loss": 0.054,
      "step": 4988
    },
    {
      "epoch": 1.234595397178916,
      "grad_norm": 0.05142268165946007,
      "learning_rate": 0.0001602929639189811,
      "loss": 0.0853,
      "step": 4989
    },
    {
      "epoch": 1.23484286067805,
      "grad_norm": 0.04166816174983978,
      "learning_rate": 0.00016020215140076288,
      "loss": 0.0616,
      "step": 4990
    },
    {
      "epoch": 1.2350903241771838,
      "grad_norm": 0.06032150611281395,
      "learning_rate": 0.00016011135248456337,
      "loss": 0.0657,
      "step": 4991
    },
    {
      "epoch": 1.2353377876763179,
      "grad_norm": 0.06780453771352768,
      "learning_rate": 0.00016002056718413603,
      "loss": 0.0557,
      "step": 4992
    },
    {
      "epoch": 1.2355852511754517,
      "grad_norm": 0.042078837752342224,
      "learning_rate": 0.00015992979551323263,
      "loss": 0.0755,
      "step": 4993
    },
    {
      "epoch": 1.2358327146745856,
      "grad_norm": 0.04614203795790672,
      "learning_rate": 0.0001598390374856027,
      "loss": 0.0276,
      "step": 4994
    },
    {
      "epoch": 1.2360801781737194,
      "grad_norm": 0.08042636513710022,
      "learning_rate": 0.00015974829311499352,
      "loss": 0.0483,
      "step": 4995
    },
    {
      "epoch": 1.2363276416728533,
      "grad_norm": 0.0381576269865036,
      "learning_rate": 0.00015965756241515056,
      "loss": 0.024,
      "step": 4996
    },
    {
      "epoch": 1.236575105171987,
      "grad_norm": 0.0424196682870388,
      "learning_rate": 0.00015956684539981727,
      "loss": 0.0411,
      "step": 4997
    },
    {
      "epoch": 1.236822568671121,
      "grad_norm": 0.03299926966428757,
      "learning_rate": 0.00015947614208273477,
      "loss": 0.0351,
      "step": 4998
    },
    {
      "epoch": 1.2370700321702548,
      "grad_norm": 0.049174051731824875,
      "learning_rate": 0.0001593854524776423,
      "loss": 0.0183,
      "step": 4999
    },
    {
      "epoch": 1.2373174956693886,
      "grad_norm": 0.03470909968018532,
      "learning_rate": 0.0001592947765982769,
      "loss": 0.0342,
      "step": 5000
    },
    {
      "epoch": 1.2373174956693886,
      "eval_loss": 0.29045113921165466,
      "eval_runtime": 202.5349,
      "eval_samples_per_second": 4.937,
      "eval_steps_per_second": 0.311,
      "step": 5000
    },
    {
      "epoch": 1.2375649591685227,
      "grad_norm": 0.029965773224830627,
      "learning_rate": 0.00015920411445837358,
      "loss": 0.0229,
      "step": 5001
    },
    {
      "epoch": 1.2378124226676566,
      "grad_norm": 0.05355507507920265,
      "learning_rate": 0.00015911346607166537,
      "loss": 0.054,
      "step": 5002
    },
    {
      "epoch": 1.2380598861667904,
      "grad_norm": 0.062138184905052185,
      "learning_rate": 0.00015902283145188302,
      "loss": 0.1162,
      "step": 5003
    },
    {
      "epoch": 1.2383073496659243,
      "grad_norm": 0.04778902605175972,
      "learning_rate": 0.00015893221061275537,
      "loss": 0.0472,
      "step": 5004
    },
    {
      "epoch": 1.238554813165058,
      "grad_norm": 0.07176294177770615,
      "learning_rate": 0.00015884160356800903,
      "loss": 0.0727,
      "step": 5005
    },
    {
      "epoch": 1.238802276664192,
      "grad_norm": 0.074171282351017,
      "learning_rate": 0.00015875101033136857,
      "loss": 0.1006,
      "step": 5006
    },
    {
      "epoch": 1.2390497401633258,
      "grad_norm": 0.10270830988883972,
      "learning_rate": 0.00015866043091655662,
      "loss": 0.0642,
      "step": 5007
    },
    {
      "epoch": 1.2392972036624599,
      "grad_norm": 0.027074923738837242,
      "learning_rate": 0.0001585698653372935,
      "loss": 0.029,
      "step": 5008
    },
    {
      "epoch": 1.2395446671615937,
      "grad_norm": 0.05835619568824768,
      "learning_rate": 0.0001584793136072976,
      "loss": 0.046,
      "step": 5009
    },
    {
      "epoch": 1.2397921306607276,
      "grad_norm": 0.03956585377454758,
      "learning_rate": 0.00015838877574028497,
      "loss": 0.0366,
      "step": 5010
    },
    {
      "epoch": 1.2400395941598614,
      "grad_norm": 0.05862767621874809,
      "learning_rate": 0.00015829825174996985,
      "loss": 0.0582,
      "step": 5011
    },
    {
      "epoch": 1.2402870576589953,
      "grad_norm": 0.04964325577020645,
      "learning_rate": 0.00015820774165006425,
      "loss": 0.0689,
      "step": 5012
    },
    {
      "epoch": 1.2405345211581291,
      "grad_norm": 0.07316127419471741,
      "learning_rate": 0.00015811724545427796,
      "loss": 0.0368,
      "step": 5013
    },
    {
      "epoch": 1.240781984657263,
      "grad_norm": 0.04126601293683052,
      "learning_rate": 0.00015802676317631896,
      "loss": 0.0596,
      "step": 5014
    },
    {
      "epoch": 1.241029448156397,
      "grad_norm": 0.03206999599933624,
      "learning_rate": 0.00015793629482989272,
      "loss": 0.061,
      "step": 5015
    },
    {
      "epoch": 1.241276911655531,
      "grad_norm": 0.07634682208299637,
      "learning_rate": 0.000157845840428703,
      "loss": 0.0675,
      "step": 5016
    },
    {
      "epoch": 1.2415243751546647,
      "grad_norm": 0.033570874482393265,
      "learning_rate": 0.00015775539998645125,
      "loss": 0.0348,
      "step": 5017
    },
    {
      "epoch": 1.2417718386537986,
      "grad_norm": 0.03500396013259888,
      "learning_rate": 0.00015766497351683677,
      "loss": 0.0514,
      "step": 5018
    },
    {
      "epoch": 1.2420193021529324,
      "grad_norm": 0.037922121584415436,
      "learning_rate": 0.00015757456103355683,
      "loss": 0.0278,
      "step": 5019
    },
    {
      "epoch": 1.2422667656520663,
      "grad_norm": 0.030072670429944992,
      "learning_rate": 0.00015748416255030652,
      "loss": 0.0333,
      "step": 5020
    },
    {
      "epoch": 1.2425142291512001,
      "grad_norm": 0.05031346529722214,
      "learning_rate": 0.00015739377808077882,
      "loss": 0.1006,
      "step": 5021
    },
    {
      "epoch": 1.242761692650334,
      "grad_norm": 0.04991089180111885,
      "learning_rate": 0.0001573034076386647,
      "loss": 0.0366,
      "step": 5022
    },
    {
      "epoch": 1.2430091561494678,
      "grad_norm": 0.038683097809553146,
      "learning_rate": 0.00015721305123765278,
      "loss": 0.0287,
      "step": 5023
    },
    {
      "epoch": 1.243256619648602,
      "grad_norm": 0.05305518954992294,
      "learning_rate": 0.00015712270889142987,
      "loss": 0.0436,
      "step": 5024
    },
    {
      "epoch": 1.2435040831477358,
      "grad_norm": 0.06061302125453949,
      "learning_rate": 0.00015703238061368026,
      "loss": 0.0703,
      "step": 5025
    },
    {
      "epoch": 1.2437515466468696,
      "grad_norm": 0.04433878883719444,
      "learning_rate": 0.0001569420664180864,
      "loss": 0.0436,
      "step": 5026
    },
    {
      "epoch": 1.2439990101460034,
      "grad_norm": 0.06849002093076706,
      "learning_rate": 0.00015685176631832862,
      "loss": 0.0317,
      "step": 5027
    },
    {
      "epoch": 1.2442464736451373,
      "grad_norm": 0.055645596235990524,
      "learning_rate": 0.00015676148032808484,
      "loss": 0.0305,
      "step": 5028
    },
    {
      "epoch": 1.2444939371442711,
      "grad_norm": 0.03761979565024376,
      "learning_rate": 0.0001566712084610312,
      "loss": 0.0505,
      "step": 5029
    },
    {
      "epoch": 1.244741400643405,
      "grad_norm": 0.0473669171333313,
      "learning_rate": 0.00015658095073084133,
      "loss": 0.0247,
      "step": 5030
    },
    {
      "epoch": 1.244988864142539,
      "grad_norm": 0.06756094843149185,
      "learning_rate": 0.000156490707151187,
      "loss": 0.0376,
      "step": 5031
    },
    {
      "epoch": 1.245236327641673,
      "grad_norm": 0.0386904738843441,
      "learning_rate": 0.00015640047773573788,
      "loss": 0.0374,
      "step": 5032
    },
    {
      "epoch": 1.2454837911408068,
      "grad_norm": 0.03759795054793358,
      "learning_rate": 0.00015631026249816104,
      "loss": 0.0399,
      "step": 5033
    },
    {
      "epoch": 1.2457312546399406,
      "grad_norm": 0.026068519800901413,
      "learning_rate": 0.00015622006145212214,
      "loss": 0.028,
      "step": 5034
    },
    {
      "epoch": 1.2459787181390745,
      "grad_norm": 0.056899603456258774,
      "learning_rate": 0.00015612987461128384,
      "loss": 0.0659,
      "step": 5035
    },
    {
      "epoch": 1.2462261816382083,
      "grad_norm": 0.03442378342151642,
      "learning_rate": 0.00015603970198930733,
      "loss": 0.0372,
      "step": 5036
    },
    {
      "epoch": 1.2464736451373422,
      "grad_norm": 0.054260920733213425,
      "learning_rate": 0.00015594954359985142,
      "loss": 0.0499,
      "step": 5037
    },
    {
      "epoch": 1.2467211086364762,
      "grad_norm": 0.07920053601264954,
      "learning_rate": 0.00015585939945657264,
      "loss": 0.0628,
      "step": 5038
    },
    {
      "epoch": 1.24696857213561,
      "grad_norm": 0.059059057384729385,
      "learning_rate": 0.00015576926957312553,
      "loss": 0.0678,
      "step": 5039
    },
    {
      "epoch": 1.247216035634744,
      "grad_norm": 0.03828549012541771,
      "learning_rate": 0.0001556791539631623,
      "loss": 0.0409,
      "step": 5040
    },
    {
      "epoch": 1.2474634991338778,
      "grad_norm": 0.0909162387251854,
      "learning_rate": 0.00015558905264033312,
      "loss": 0.0432,
      "step": 5041
    },
    {
      "epoch": 1.2477109626330116,
      "grad_norm": 0.04523778706789017,
      "learning_rate": 0.0001554989656182862,
      "loss": 0.0458,
      "step": 5042
    },
    {
      "epoch": 1.2479584261321455,
      "grad_norm": 0.04622820392251015,
      "learning_rate": 0.00015540889291066706,
      "loss": 0.0463,
      "step": 5043
    },
    {
      "epoch": 1.2482058896312793,
      "grad_norm": 0.04856039211153984,
      "learning_rate": 0.00015531883453111955,
      "loss": 0.0625,
      "step": 5044
    },
    {
      "epoch": 1.2484533531304132,
      "grad_norm": 0.0296563059091568,
      "learning_rate": 0.00015522879049328505,
      "loss": 0.0193,
      "step": 5045
    },
    {
      "epoch": 1.2487008166295472,
      "grad_norm": 0.055617403239011765,
      "learning_rate": 0.00015513876081080287,
      "loss": 0.0546,
      "step": 5046
    },
    {
      "epoch": 1.248948280128681,
      "grad_norm": 0.0348006933927536,
      "learning_rate": 0.00015504874549731023,
      "loss": 0.033,
      "step": 5047
    },
    {
      "epoch": 1.249195743627815,
      "grad_norm": 0.02691786177456379,
      "learning_rate": 0.00015495874456644202,
      "loss": 0.0356,
      "step": 5048
    },
    {
      "epoch": 1.2494432071269488,
      "grad_norm": 0.03651265799999237,
      "learning_rate": 0.00015486875803183108,
      "loss": 0.0313,
      "step": 5049
    },
    {
      "epoch": 1.2496906706260826,
      "grad_norm": 0.08840532600879669,
      "learning_rate": 0.00015477878590710792,
      "loss": 0.0819,
      "step": 5050
    },
    {
      "epoch": 1.2499381341252165,
      "grad_norm": 0.024005379527807236,
      "learning_rate": 0.0001546888282059009,
      "loss": 0.0141,
      "step": 5051
    },
    {
      "epoch": 1.2501855976243503,
      "grad_norm": 0.07772248238325119,
      "learning_rate": 0.0001545988849418365,
      "loss": 0.0759,
      "step": 5052
    },
    {
      "epoch": 1.2504330611234842,
      "grad_norm": 0.06842288374900818,
      "learning_rate": 0.0001545089561285386,
      "loss": 0.0725,
      "step": 5053
    },
    {
      "epoch": 1.2506805246226183,
      "grad_norm": 0.036906030029058456,
      "learning_rate": 0.00015441904177962909,
      "loss": 0.036,
      "step": 5054
    },
    {
      "epoch": 1.250927988121752,
      "grad_norm": 0.048321083188056946,
      "learning_rate": 0.00015432914190872756,
      "loss": 0.0557,
      "step": 5055
    },
    {
      "epoch": 1.251175451620886,
      "grad_norm": 0.0380733348429203,
      "learning_rate": 0.0001542392565294516,
      "loss": 0.0613,
      "step": 5056
    },
    {
      "epoch": 1.2514229151200198,
      "grad_norm": 0.030220450833439827,
      "learning_rate": 0.00015414938565541642,
      "loss": 0.0241,
      "step": 5057
    },
    {
      "epoch": 1.2516703786191536,
      "grad_norm": 0.04539287090301514,
      "learning_rate": 0.00015405952930023508,
      "loss": 0.0379,
      "step": 5058
    },
    {
      "epoch": 1.2519178421182875,
      "grad_norm": 0.03683927655220032,
      "learning_rate": 0.00015396968747751857,
      "loss": 0.0391,
      "step": 5059
    },
    {
      "epoch": 1.2521653056174213,
      "grad_norm": 0.04010390862822533,
      "learning_rate": 0.0001538798602008754,
      "loss": 0.0434,
      "step": 5060
    },
    {
      "epoch": 1.2524127691165554,
      "grad_norm": 0.0502207912504673,
      "learning_rate": 0.00015379004748391215,
      "loss": 0.0612,
      "step": 5061
    },
    {
      "epoch": 1.2526602326156893,
      "grad_norm": 0.062983937561512,
      "learning_rate": 0.00015370024934023313,
      "loss": 0.1035,
      "step": 5062
    },
    {
      "epoch": 1.252907696114823,
      "grad_norm": 0.02436041086912155,
      "learning_rate": 0.00015361046578344034,
      "loss": 0.0224,
      "step": 5063
    },
    {
      "epoch": 1.253155159613957,
      "grad_norm": 0.03024071268737316,
      "learning_rate": 0.00015352069682713365,
      "loss": 0.0216,
      "step": 5064
    },
    {
      "epoch": 1.2534026231130908,
      "grad_norm": 0.045595381408929825,
      "learning_rate": 0.00015343094248491067,
      "loss": 0.0327,
      "step": 5065
    },
    {
      "epoch": 1.2536500866122247,
      "grad_norm": 0.07796434313058853,
      "learning_rate": 0.00015334120277036688,
      "loss": 0.0805,
      "step": 5066
    },
    {
      "epoch": 1.2538975501113585,
      "grad_norm": 0.06039916351437569,
      "learning_rate": 0.00015325147769709553,
      "loss": 0.0528,
      "step": 5067
    },
    {
      "epoch": 1.2541450136104926,
      "grad_norm": 0.046400297433137894,
      "learning_rate": 0.0001531617672786874,
      "loss": 0.0609,
      "step": 5068
    },
    {
      "epoch": 1.2543924771096262,
      "grad_norm": 0.03814898803830147,
      "learning_rate": 0.0001530720715287316,
      "loss": 0.0515,
      "step": 5069
    },
    {
      "epoch": 1.2546399406087603,
      "grad_norm": 0.05067136511206627,
      "learning_rate": 0.00015298239046081436,
      "loss": 0.0587,
      "step": 5070
    },
    {
      "epoch": 1.2548874041078941,
      "grad_norm": 0.04844199866056442,
      "learning_rate": 0.0001528927240885202,
      "loss": 0.0548,
      "step": 5071
    },
    {
      "epoch": 1.255134867607028,
      "grad_norm": 0.058294739574193954,
      "learning_rate": 0.00015280307242543123,
      "loss": 0.0491,
      "step": 5072
    },
    {
      "epoch": 1.2553823311061618,
      "grad_norm": 0.035579584538936615,
      "learning_rate": 0.00015271343548512722,
      "loss": 0.0575,
      "step": 5073
    },
    {
      "epoch": 1.2556297946052957,
      "grad_norm": 0.048240065574645996,
      "learning_rate": 0.00015262381328118592,
      "loss": 0.0431,
      "step": 5074
    },
    {
      "epoch": 1.2558772581044295,
      "grad_norm": 0.04935668408870697,
      "learning_rate": 0.00015253420582718265,
      "loss": 0.0507,
      "step": 5075
    },
    {
      "epoch": 1.2561247216035634,
      "grad_norm": 0.1358107626438141,
      "learning_rate": 0.00015244461313669057,
      "loss": 0.1191,
      "step": 5076
    },
    {
      "epoch": 1.2563721851026974,
      "grad_norm": 0.11486989259719849,
      "learning_rate": 0.00015235503522328082,
      "loss": 0.0856,
      "step": 5077
    },
    {
      "epoch": 1.2566196486018313,
      "grad_norm": 0.061435528099536896,
      "learning_rate": 0.0001522654721005218,
      "loss": 0.063,
      "step": 5078
    },
    {
      "epoch": 1.2568671121009651,
      "grad_norm": 0.04892311990261078,
      "learning_rate": 0.0001521759237819803,
      "loss": 0.0369,
      "step": 5079
    },
    {
      "epoch": 1.257114575600099,
      "grad_norm": 0.06975395977497101,
      "learning_rate": 0.00015208639028122027,
      "loss": 0.0506,
      "step": 5080
    },
    {
      "epoch": 1.2573620390992328,
      "grad_norm": 0.06201363727450371,
      "learning_rate": 0.0001519968716118038,
      "loss": 0.0247,
      "step": 5081
    },
    {
      "epoch": 1.2576095025983667,
      "grad_norm": 0.02947046235203743,
      "learning_rate": 0.0001519073677872907,
      "loss": 0.053,
      "step": 5082
    },
    {
      "epoch": 1.2578569660975005,
      "grad_norm": 0.03833248093724251,
      "learning_rate": 0.00015181787882123833,
      "loss": 0.0542,
      "step": 5083
    },
    {
      "epoch": 1.2581044295966346,
      "grad_norm": 0.034414470195770264,
      "learning_rate": 0.000151728404727202,
      "loss": 0.0512,
      "step": 5084
    },
    {
      "epoch": 1.2583518930957684,
      "grad_norm": 0.03854235261678696,
      "learning_rate": 0.00015163894551873457,
      "loss": 0.038,
      "step": 5085
    },
    {
      "epoch": 1.2585993565949023,
      "grad_norm": 0.05420840531587601,
      "learning_rate": 0.00015154950120938683,
      "loss": 0.0545,
      "step": 5086
    },
    {
      "epoch": 1.2588468200940361,
      "grad_norm": 0.040310077369213104,
      "learning_rate": 0.0001514600718127074,
      "loss": 0.0507,
      "step": 5087
    },
    {
      "epoch": 1.25909428359317,
      "grad_norm": 0.03773089125752449,
      "learning_rate": 0.0001513706573422422,
      "loss": 0.033,
      "step": 5088
    },
    {
      "epoch": 1.2593417470923038,
      "grad_norm": 0.0515572689473629,
      "learning_rate": 0.00015128125781153543,
      "loss": 0.0597,
      "step": 5089
    },
    {
      "epoch": 1.2595892105914377,
      "grad_norm": 0.06616854667663574,
      "learning_rate": 0.00015119187323412863,
      "loss": 0.1583,
      "step": 5090
    },
    {
      "epoch": 1.2598366740905718,
      "grad_norm": 0.049895018339157104,
      "learning_rate": 0.0001511025036235612,
      "loss": 0.0526,
      "step": 5091
    },
    {
      "epoch": 1.2600841375897054,
      "grad_norm": 0.02495904453098774,
      "learning_rate": 0.00015101314899337046,
      "loss": 0.0281,
      "step": 5092
    },
    {
      "epoch": 1.2603316010888395,
      "grad_norm": 0.0703243836760521,
      "learning_rate": 0.0001509238093570911,
      "loss": 0.0364,
      "step": 5093
    },
    {
      "epoch": 1.2605790645879733,
      "grad_norm": 0.03316480666399002,
      "learning_rate": 0.0001508344847282559,
      "loss": 0.0358,
      "step": 5094
    },
    {
      "epoch": 1.2608265280871072,
      "grad_norm": 0.05036662518978119,
      "learning_rate": 0.000150745175120395,
      "loss": 0.031,
      "step": 5095
    },
    {
      "epoch": 1.261073991586241,
      "grad_norm": 0.027054786682128906,
      "learning_rate": 0.00015065588054703657,
      "loss": 0.0405,
      "step": 5096
    },
    {
      "epoch": 1.2613214550853749,
      "grad_norm": 0.04357154294848442,
      "learning_rate": 0.0001505666010217065,
      "loss": 0.0555,
      "step": 5097
    },
    {
      "epoch": 1.2615689185845087,
      "grad_norm": 0.034699369221925735,
      "learning_rate": 0.0001504773365579281,
      "loss": 0.0596,
      "step": 5098
    },
    {
      "epoch": 1.2618163820836426,
      "grad_norm": 0.10881393402814865,
      "learning_rate": 0.00015038808716922282,
      "loss": 0.0649,
      "step": 5099
    },
    {
      "epoch": 1.2620638455827766,
      "grad_norm": 0.04199286177754402,
      "learning_rate": 0.0001502988528691094,
      "loss": 0.029,
      "step": 5100
    },
    {
      "epoch": 1.2623113090819105,
      "grad_norm": 0.03700578585267067,
      "learning_rate": 0.00015020963367110452,
      "loss": 0.0386,
      "step": 5101
    },
    {
      "epoch": 1.2625587725810443,
      "grad_norm": 0.08730915188789368,
      "learning_rate": 0.00015012042958872267,
      "loss": 0.1375,
      "step": 5102
    },
    {
      "epoch": 1.2628062360801782,
      "grad_norm": 0.055141858756542206,
      "learning_rate": 0.0001500312406354758,
      "loss": 0.0693,
      "step": 5103
    },
    {
      "epoch": 1.263053699579312,
      "grad_norm": 0.036859896034002304,
      "learning_rate": 0.00014994206682487384,
      "loss": 0.0345,
      "step": 5104
    },
    {
      "epoch": 1.2633011630784459,
      "grad_norm": 0.13970737159252167,
      "learning_rate": 0.00014985290817042408,
      "loss": 0.0876,
      "step": 5105
    },
    {
      "epoch": 1.2635486265775797,
      "grad_norm": 0.0694906935095787,
      "learning_rate": 0.00014976376468563184,
      "loss": 0.0398,
      "step": 5106
    },
    {
      "epoch": 1.2637960900767138,
      "grad_norm": 0.040347855538129807,
      "learning_rate": 0.0001496746363840001,
      "loss": 0.0322,
      "step": 5107
    },
    {
      "epoch": 1.2640435535758476,
      "grad_norm": 0.05278913304209709,
      "learning_rate": 0.00014958552327902933,
      "loss": 0.044,
      "step": 5108
    },
    {
      "epoch": 1.2642910170749815,
      "grad_norm": 0.043967507779598236,
      "learning_rate": 0.00014949642538421791,
      "loss": 0.0496,
      "step": 5109
    },
    {
      "epoch": 1.2645384805741153,
      "grad_norm": 0.03568309172987938,
      "learning_rate": 0.00014940734271306173,
      "loss": 0.0271,
      "step": 5110
    },
    {
      "epoch": 1.2647859440732492,
      "grad_norm": 0.05875227227807045,
      "learning_rate": 0.00014931827527905455,
      "loss": 0.0678,
      "step": 5111
    },
    {
      "epoch": 1.265033407572383,
      "grad_norm": 0.04596494510769844,
      "learning_rate": 0.00014922922309568783,
      "loss": 0.0531,
      "step": 5112
    },
    {
      "epoch": 1.2652808710715169,
      "grad_norm": 0.09125261008739471,
      "learning_rate": 0.00014914018617645048,
      "loss": 0.0372,
      "step": 5113
    },
    {
      "epoch": 1.265528334570651,
      "grad_norm": 0.04111546277999878,
      "learning_rate": 0.00014905116453482937,
      "loss": 0.0348,
      "step": 5114
    },
    {
      "epoch": 1.2657757980697846,
      "grad_norm": 0.05321396887302399,
      "learning_rate": 0.0001489621581843088,
      "loss": 0.0515,
      "step": 5115
    },
    {
      "epoch": 1.2660232615689186,
      "grad_norm": 0.027403341606259346,
      "learning_rate": 0.00014887316713837104,
      "loss": 0.0371,
      "step": 5116
    },
    {
      "epoch": 1.2662707250680525,
      "grad_norm": 0.04906134679913521,
      "learning_rate": 0.00014878419141049594,
      "loss": 0.0416,
      "step": 5117
    },
    {
      "epoch": 1.2665181885671863,
      "grad_norm": 0.04135892167687416,
      "learning_rate": 0.00014869523101416083,
      "loss": 0.0354,
      "step": 5118
    },
    {
      "epoch": 1.2667656520663202,
      "grad_norm": 0.033441461622714996,
      "learning_rate": 0.000148606285962841,
      "loss": 0.0491,
      "step": 5119
    },
    {
      "epoch": 1.267013115565454,
      "grad_norm": 0.04616604745388031,
      "learning_rate": 0.00014851735627000926,
      "loss": 0.0836,
      "step": 5120
    },
    {
      "epoch": 1.267260579064588,
      "grad_norm": 0.06582791358232498,
      "learning_rate": 0.00014842844194913607,
      "loss": 0.1072,
      "step": 5121
    },
    {
      "epoch": 1.2675080425637217,
      "grad_norm": 0.03244984894990921,
      "learning_rate": 0.00014833954301368972,
      "loss": 0.026,
      "step": 5122
    },
    {
      "epoch": 1.2677555060628558,
      "grad_norm": 0.041058219969272614,
      "learning_rate": 0.0001482506594771359,
      "loss": 0.0394,
      "step": 5123
    },
    {
      "epoch": 1.2680029695619897,
      "grad_norm": 0.16073912382125854,
      "learning_rate": 0.0001481617913529383,
      "loss": 0.0917,
      "step": 5124
    },
    {
      "epoch": 1.2682504330611235,
      "grad_norm": 0.038051869720220566,
      "learning_rate": 0.0001480729386545581,
      "loss": 0.0525,
      "step": 5125
    },
    {
      "epoch": 1.2684978965602574,
      "grad_norm": 0.040539734065532684,
      "learning_rate": 0.00014798410139545403,
      "loss": 0.0619,
      "step": 5126
    },
    {
      "epoch": 1.2687453600593912,
      "grad_norm": 0.06941838562488556,
      "learning_rate": 0.00014789527958908278,
      "loss": 0.068,
      "step": 5127
    },
    {
      "epoch": 1.268992823558525,
      "grad_norm": 0.025339754298329353,
      "learning_rate": 0.00014780647324889835,
      "loss": 0.0219,
      "step": 5128
    },
    {
      "epoch": 1.269240287057659,
      "grad_norm": 0.051689594984054565,
      "learning_rate": 0.00014771768238835265,
      "loss": 0.0476,
      "step": 5129
    },
    {
      "epoch": 1.269487750556793,
      "grad_norm": 0.0847264975309372,
      "learning_rate": 0.0001476289070208952,
      "loss": 0.0972,
      "step": 5130
    },
    {
      "epoch": 1.2697352140559268,
      "grad_norm": 0.03269074857234955,
      "learning_rate": 0.000147540147159973,
      "loss": 0.0499,
      "step": 5131
    },
    {
      "epoch": 1.2699826775550607,
      "grad_norm": 0.03685281053185463,
      "learning_rate": 0.00014745140281903108,
      "loss": 0.044,
      "step": 5132
    },
    {
      "epoch": 1.2702301410541945,
      "grad_norm": 0.03876606747508049,
      "learning_rate": 0.0001473626740115116,
      "loss": 0.0309,
      "step": 5133
    },
    {
      "epoch": 1.2704776045533284,
      "grad_norm": 0.026576191186904907,
      "learning_rate": 0.00014727396075085486,
      "loss": 0.0223,
      "step": 5134
    },
    {
      "epoch": 1.2707250680524622,
      "grad_norm": 0.07759147137403488,
      "learning_rate": 0.00014718526305049857,
      "loss": 0.058,
      "step": 5135
    },
    {
      "epoch": 1.270972531551596,
      "grad_norm": 0.044498663395643234,
      "learning_rate": 0.00014709658092387797,
      "loss": 0.0535,
      "step": 5136
    },
    {
      "epoch": 1.2712199950507301,
      "grad_norm": 0.03475337475538254,
      "learning_rate": 0.00014700791438442625,
      "loss": 0.0527,
      "step": 5137
    },
    {
      "epoch": 1.2714674585498638,
      "grad_norm": 0.032271381467580795,
      "learning_rate": 0.00014691926344557392,
      "loss": 0.0589,
      "step": 5138
    },
    {
      "epoch": 1.2717149220489978,
      "grad_norm": 0.06721896678209305,
      "learning_rate": 0.00014683062812074926,
      "loss": 0.0686,
      "step": 5139
    },
    {
      "epoch": 1.2719623855481317,
      "grad_norm": 0.03236408159136772,
      "learning_rate": 0.00014674200842337842,
      "loss": 0.0377,
      "step": 5140
    },
    {
      "epoch": 1.2722098490472655,
      "grad_norm": 0.030924025923013687,
      "learning_rate": 0.00014665340436688468,
      "loss": 0.0351,
      "step": 5141
    },
    {
      "epoch": 1.2724573125463994,
      "grad_norm": 0.04397120699286461,
      "learning_rate": 0.00014656481596468947,
      "loss": 0.0338,
      "step": 5142
    },
    {
      "epoch": 1.2727047760455332,
      "grad_norm": 0.0450926348567009,
      "learning_rate": 0.00014647624323021135,
      "loss": 0.0542,
      "step": 5143
    },
    {
      "epoch": 1.2729522395446673,
      "grad_norm": 0.08551380038261414,
      "learning_rate": 0.00014638768617686697,
      "loss": 0.0864,
      "step": 5144
    },
    {
      "epoch": 1.273199703043801,
      "grad_norm": 0.06923367828130722,
      "learning_rate": 0.0001462991448180704,
      "loss": 0.0638,
      "step": 5145
    },
    {
      "epoch": 1.273447166542935,
      "grad_norm": 0.058539316058158875,
      "learning_rate": 0.00014621061916723322,
      "loss": 0.0729,
      "step": 5146
    },
    {
      "epoch": 1.2736946300420688,
      "grad_norm": 0.04248901829123497,
      "learning_rate": 0.00014612210923776482,
      "loss": 0.0423,
      "step": 5147
    },
    {
      "epoch": 1.2739420935412027,
      "grad_norm": 0.07774706184864044,
      "learning_rate": 0.00014603361504307208,
      "loss": 0.0837,
      "step": 5148
    },
    {
      "epoch": 1.2741895570403365,
      "grad_norm": 0.04619424790143967,
      "learning_rate": 0.00014594513659655956,
      "loss": 0.04,
      "step": 5149
    },
    {
      "epoch": 1.2744370205394704,
      "grad_norm": 0.04853398725390434,
      "learning_rate": 0.00014585667391162955,
      "loss": 0.0569,
      "step": 5150
    },
    {
      "epoch": 1.2746844840386042,
      "grad_norm": 0.035141587257385254,
      "learning_rate": 0.00014576822700168164,
      "loss": 0.0324,
      "step": 5151
    },
    {
      "epoch": 1.274931947537738,
      "grad_norm": 0.056777890771627426,
      "learning_rate": 0.0001456797958801133,
      "loss": 0.0633,
      "step": 5152
    },
    {
      "epoch": 1.2751794110368722,
      "grad_norm": 0.06098717823624611,
      "learning_rate": 0.0001455913805603195,
      "loss": 0.0577,
      "step": 5153
    },
    {
      "epoch": 1.275426874536006,
      "grad_norm": 0.04409615695476532,
      "learning_rate": 0.00014550298105569286,
      "loss": 0.0458,
      "step": 5154
    },
    {
      "epoch": 1.2756743380351399,
      "grad_norm": 0.03218812495470047,
      "learning_rate": 0.00014541459737962365,
      "loss": 0.03,
      "step": 5155
    },
    {
      "epoch": 1.2759218015342737,
      "grad_norm": 0.03081694431602955,
      "learning_rate": 0.00014532622954549956,
      "loss": 0.0382,
      "step": 5156
    },
    {
      "epoch": 1.2761692650334076,
      "grad_norm": 0.07664093375205994,
      "learning_rate": 0.0001452378775667062,
      "loss": 0.072,
      "step": 5157
    },
    {
      "epoch": 1.2764167285325414,
      "grad_norm": 0.03035585768520832,
      "learning_rate": 0.00014514954145662632,
      "loss": 0.0312,
      "step": 5158
    },
    {
      "epoch": 1.2766641920316752,
      "grad_norm": 0.04130755737423897,
      "learning_rate": 0.00014506122122864062,
      "loss": 0.0468,
      "step": 5159
    },
    {
      "epoch": 1.2769116555308093,
      "grad_norm": 0.031299322843551636,
      "learning_rate": 0.00014497291689612735,
      "loss": 0.0462,
      "step": 5160
    },
    {
      "epoch": 1.277159119029943,
      "grad_norm": 0.034950900822877884,
      "learning_rate": 0.00014488462847246227,
      "loss": 0.035,
      "step": 5161
    },
    {
      "epoch": 1.277406582529077,
      "grad_norm": 0.0797896459698677,
      "learning_rate": 0.00014479635597101882,
      "loss": 0.0571,
      "step": 5162
    },
    {
      "epoch": 1.2776540460282109,
      "grad_norm": 0.05100756138563156,
      "learning_rate": 0.0001447080994051677,
      "loss": 0.0823,
      "step": 5163
    },
    {
      "epoch": 1.2779015095273447,
      "grad_norm": 0.049114495515823364,
      "learning_rate": 0.0001446198587882778,
      "loss": 0.0515,
      "step": 5164
    },
    {
      "epoch": 1.2781489730264786,
      "grad_norm": 0.02958022430539131,
      "learning_rate": 0.00014453163413371523,
      "loss": 0.0322,
      "step": 5165
    },
    {
      "epoch": 1.2783964365256124,
      "grad_norm": 0.037615858018398285,
      "learning_rate": 0.00014444342545484351,
      "loss": 0.0691,
      "step": 5166
    },
    {
      "epoch": 1.2786439000247465,
      "grad_norm": 0.04218167066574097,
      "learning_rate": 0.000144355232765024,
      "loss": 0.0473,
      "step": 5167
    },
    {
      "epoch": 1.27889136352388,
      "grad_norm": 0.04526009038090706,
      "learning_rate": 0.00014426705607761567,
      "loss": 0.0565,
      "step": 5168
    },
    {
      "epoch": 1.2791388270230142,
      "grad_norm": 0.03382951021194458,
      "learning_rate": 0.0001441788954059749,
      "loss": 0.0319,
      "step": 5169
    },
    {
      "epoch": 1.279386290522148,
      "grad_norm": 0.023021195083856583,
      "learning_rate": 0.00014409075076345586,
      "loss": 0.0243,
      "step": 5170
    },
    {
      "epoch": 1.2796337540212819,
      "grad_norm": 0.08928479999303818,
      "learning_rate": 0.00014400262216340996,
      "loss": 0.0605,
      "step": 5171
    },
    {
      "epoch": 1.2798812175204157,
      "grad_norm": 0.035887110978364944,
      "learning_rate": 0.0001439145096191864,
      "loss": 0.039,
      "step": 5172
    },
    {
      "epoch": 1.2801286810195496,
      "grad_norm": 0.06308341026306152,
      "learning_rate": 0.000143826413144132,
      "loss": 0.0247,
      "step": 5173
    },
    {
      "epoch": 1.2803761445186834,
      "grad_norm": 0.03814375028014183,
      "learning_rate": 0.00014373833275159104,
      "loss": 0.0322,
      "step": 5174
    },
    {
      "epoch": 1.2806236080178173,
      "grad_norm": 0.06765224039554596,
      "learning_rate": 0.00014365026845490533,
      "loss": 0.0433,
      "step": 5175
    },
    {
      "epoch": 1.2808710715169513,
      "grad_norm": 0.03360569477081299,
      "learning_rate": 0.00014356222026741438,
      "loss": 0.0639,
      "step": 5176
    },
    {
      "epoch": 1.2811185350160852,
      "grad_norm": 0.09788691997528076,
      "learning_rate": 0.0001434741882024553,
      "loss": 0.0878,
      "step": 5177
    },
    {
      "epoch": 1.281365998515219,
      "grad_norm": 0.04921336844563484,
      "learning_rate": 0.0001433861722733623,
      "loss": 0.0391,
      "step": 5178
    },
    {
      "epoch": 1.2816134620143529,
      "grad_norm": 0.04704204574227333,
      "learning_rate": 0.00014329817249346773,
      "loss": 0.0566,
      "step": 5179
    },
    {
      "epoch": 1.2818609255134867,
      "grad_norm": 0.036053285002708435,
      "learning_rate": 0.0001432101888761012,
      "loss": 0.0238,
      "step": 5180
    },
    {
      "epoch": 1.2821083890126206,
      "grad_norm": 0.05035451799631119,
      "learning_rate": 0.0001431222214345899,
      "loss": 0.0628,
      "step": 5181
    },
    {
      "epoch": 1.2823558525117544,
      "grad_norm": 0.03713909909129143,
      "learning_rate": 0.00014303427018225873,
      "loss": 0.0448,
      "step": 5182
    },
    {
      "epoch": 1.2826033160108885,
      "grad_norm": 0.07863779366016388,
      "learning_rate": 0.00014294633513242967,
      "loss": 0.0854,
      "step": 5183
    },
    {
      "epoch": 1.2828507795100224,
      "grad_norm": 0.042616259306669235,
      "learning_rate": 0.00014285841629842284,
      "loss": 0.0292,
      "step": 5184
    },
    {
      "epoch": 1.2830982430091562,
      "grad_norm": 0.040934789925813675,
      "learning_rate": 0.0001427705136935557,
      "loss": 0.041,
      "step": 5185
    },
    {
      "epoch": 1.28334570650829,
      "grad_norm": 0.06137826293706894,
      "learning_rate": 0.00014268262733114297,
      "loss": 0.0488,
      "step": 5186
    },
    {
      "epoch": 1.283593170007424,
      "grad_norm": 0.07829635590314865,
      "learning_rate": 0.00014259475722449717,
      "loss": 0.0843,
      "step": 5187
    },
    {
      "epoch": 1.2838406335065577,
      "grad_norm": 0.04515010118484497,
      "learning_rate": 0.0001425069033869284,
      "loss": 0.0444,
      "step": 5188
    },
    {
      "epoch": 1.2840880970056916,
      "grad_norm": 0.04830937832593918,
      "learning_rate": 0.00014241906583174418,
      "loss": 0.0292,
      "step": 5189
    },
    {
      "epoch": 1.2843355605048257,
      "grad_norm": 0.030421057716012,
      "learning_rate": 0.00014233124457224973,
      "loss": 0.0283,
      "step": 5190
    },
    {
      "epoch": 1.2845830240039593,
      "grad_norm": 0.05865183845162392,
      "learning_rate": 0.0001422434396217473,
      "loss": 0.0681,
      "step": 5191
    },
    {
      "epoch": 1.2848304875030934,
      "grad_norm": 0.033940669149160385,
      "learning_rate": 0.00014215565099353743,
      "loss": 0.0395,
      "step": 5192
    },
    {
      "epoch": 1.2850779510022272,
      "grad_norm": 0.04680195078253746,
      "learning_rate": 0.0001420678787009176,
      "loss": 0.0491,
      "step": 5193
    },
    {
      "epoch": 1.285325414501361,
      "grad_norm": 0.04056471213698387,
      "learning_rate": 0.000141980122757183,
      "loss": 0.05,
      "step": 5194
    },
    {
      "epoch": 1.285572878000495,
      "grad_norm": 0.041798751801252365,
      "learning_rate": 0.00014189238317562642,
      "loss": 0.0515,
      "step": 5195
    },
    {
      "epoch": 1.2858203414996288,
      "grad_norm": 0.04025767743587494,
      "learning_rate": 0.00014180465996953807,
      "loss": 0.035,
      "step": 5196
    },
    {
      "epoch": 1.2860678049987626,
      "grad_norm": 0.03180670738220215,
      "learning_rate": 0.00014171695315220585,
      "loss": 0.0346,
      "step": 5197
    },
    {
      "epoch": 1.2863152684978965,
      "grad_norm": 0.06436854600906372,
      "learning_rate": 0.0001416292627369148,
      "loss": 0.0934,
      "step": 5198
    },
    {
      "epoch": 1.2865627319970305,
      "grad_norm": 0.11680888384580612,
      "learning_rate": 0.00014154158873694772,
      "loss": 0.104,
      "step": 5199
    },
    {
      "epoch": 1.2868101954961644,
      "grad_norm": 0.042467162013053894,
      "learning_rate": 0.00014145393116558525,
      "loss": 0.0577,
      "step": 5200
    },
    {
      "epoch": 1.2868101954961644,
      "eval_loss": 0.29050353169441223,
      "eval_runtime": 202.4694,
      "eval_samples_per_second": 4.939,
      "eval_steps_per_second": 0.311,
      "step": 5200
    },
    {
      "epoch": 1.2870576589952982,
      "grad_norm": 0.033050213009119034,
      "learning_rate": 0.00014136629003610492,
      "loss": 0.0292,
      "step": 5201
    },
    {
      "epoch": 1.287305122494432,
      "grad_norm": 0.06262718141078949,
      "learning_rate": 0.0001412786653617821,
      "loss": 0.0447,
      "step": 5202
    },
    {
      "epoch": 1.287552585993566,
      "grad_norm": 0.03269502893090248,
      "learning_rate": 0.0001411910571558897,
      "loss": 0.0287,
      "step": 5203
    },
    {
      "epoch": 1.2878000494926998,
      "grad_norm": 0.06066862493753433,
      "learning_rate": 0.00014110346543169805,
      "loss": 0.073,
      "step": 5204
    },
    {
      "epoch": 1.2880475129918336,
      "grad_norm": 0.08232384920120239,
      "learning_rate": 0.00014101589020247508,
      "loss": 0.0704,
      "step": 5205
    },
    {
      "epoch": 1.2882949764909677,
      "grad_norm": 0.06027821823954582,
      "learning_rate": 0.00014092833148148594,
      "loss": 0.0692,
      "step": 5206
    },
    {
      "epoch": 1.2885424399901015,
      "grad_norm": 0.05440792068839073,
      "learning_rate": 0.00014084078928199356,
      "loss": 0.0514,
      "step": 5207
    },
    {
      "epoch": 1.2887899034892354,
      "grad_norm": 0.04125814512372017,
      "learning_rate": 0.00014075326361725837,
      "loss": 0.0359,
      "step": 5208
    },
    {
      "epoch": 1.2890373669883692,
      "grad_norm": 0.1065065860748291,
      "learning_rate": 0.0001406657545005381,
      "loss": 0.1194,
      "step": 5209
    },
    {
      "epoch": 1.289284830487503,
      "grad_norm": 0.06384621560573578,
      "learning_rate": 0.00014057826194508816,
      "loss": 0.0671,
      "step": 5210
    },
    {
      "epoch": 1.289532293986637,
      "grad_norm": 0.08170390874147415,
      "learning_rate": 0.00014049078596416138,
      "loss": 0.0927,
      "step": 5211
    },
    {
      "epoch": 1.2897797574857708,
      "grad_norm": 0.07069490849971771,
      "learning_rate": 0.00014040332657100813,
      "loss": 0.0547,
      "step": 5212
    },
    {
      "epoch": 1.2900272209849049,
      "grad_norm": 0.04044175148010254,
      "learning_rate": 0.00014031588377887605,
      "loss": 0.0416,
      "step": 5213
    },
    {
      "epoch": 1.2902746844840385,
      "grad_norm": 0.05326082184910774,
      "learning_rate": 0.00014022845760101055,
      "loss": 0.0506,
      "step": 5214
    },
    {
      "epoch": 1.2905221479831726,
      "grad_norm": 0.04345010221004486,
      "learning_rate": 0.00014014104805065432,
      "loss": 0.0475,
      "step": 5215
    },
    {
      "epoch": 1.2907696114823064,
      "grad_norm": 0.05711740255355835,
      "learning_rate": 0.00014005365514104768,
      "loss": 0.0632,
      "step": 5216
    },
    {
      "epoch": 1.2910170749814402,
      "grad_norm": 0.028625793755054474,
      "learning_rate": 0.00013996627888542844,
      "loss": 0.0369,
      "step": 5217
    },
    {
      "epoch": 1.291264538480574,
      "grad_norm": 0.09216127544641495,
      "learning_rate": 0.00013987891929703156,
      "loss": 0.086,
      "step": 5218
    },
    {
      "epoch": 1.291512001979708,
      "grad_norm": 0.14564655721187592,
      "learning_rate": 0.00013979157638908997,
      "loss": 0.0673,
      "step": 5219
    },
    {
      "epoch": 1.2917594654788418,
      "grad_norm": 0.04192559793591499,
      "learning_rate": 0.00013970425017483383,
      "loss": 0.0679,
      "step": 5220
    },
    {
      "epoch": 1.2920069289779756,
      "grad_norm": 0.04113984853029251,
      "learning_rate": 0.00013961694066749058,
      "loss": 0.0369,
      "step": 5221
    },
    {
      "epoch": 1.2922543924771097,
      "grad_norm": 0.05373941361904144,
      "learning_rate": 0.00013952964788028544,
      "loss": 0.0549,
      "step": 5222
    },
    {
      "epoch": 1.2925018559762436,
      "grad_norm": 0.03979460895061493,
      "learning_rate": 0.00013944237182644096,
      "loss": 0.0649,
      "step": 5223
    },
    {
      "epoch": 1.2927493194753774,
      "grad_norm": 0.023429540917277336,
      "learning_rate": 0.00013935511251917719,
      "loss": 0.0225,
      "step": 5224
    },
    {
      "epoch": 1.2929967829745113,
      "grad_norm": 0.07762518525123596,
      "learning_rate": 0.00013926786997171172,
      "loss": 0.05,
      "step": 5225
    },
    {
      "epoch": 1.293244246473645,
      "grad_norm": 0.09046844393014908,
      "learning_rate": 0.00013918064419725915,
      "loss": 0.0549,
      "step": 5226
    },
    {
      "epoch": 1.293491709972779,
      "grad_norm": 0.043556299060583115,
      "learning_rate": 0.0001390934352090324,
      "loss": 0.1003,
      "step": 5227
    },
    {
      "epoch": 1.2937391734719128,
      "grad_norm": 0.03372306749224663,
      "learning_rate": 0.00013900624302024095,
      "loss": 0.0449,
      "step": 5228
    },
    {
      "epoch": 1.2939866369710469,
      "grad_norm": 0.19651879370212555,
      "learning_rate": 0.0001389190676440923,
      "loss": 0.1184,
      "step": 5229
    },
    {
      "epoch": 1.2942341004701807,
      "grad_norm": 0.05449612811207771,
      "learning_rate": 0.00013883190909379117,
      "loss": 0.0563,
      "step": 5230
    },
    {
      "epoch": 1.2944815639693146,
      "grad_norm": 0.030325647443532944,
      "learning_rate": 0.00013874476738253986,
      "loss": 0.043,
      "step": 5231
    },
    {
      "epoch": 1.2947290274684484,
      "grad_norm": 0.0537584163248539,
      "learning_rate": 0.00013865764252353813,
      "loss": 0.0748,
      "step": 5232
    },
    {
      "epoch": 1.2949764909675823,
      "grad_norm": 0.05050244554877281,
      "learning_rate": 0.00013857053452998285,
      "loss": 0.0736,
      "step": 5233
    },
    {
      "epoch": 1.2952239544667161,
      "grad_norm": 0.06564081460237503,
      "learning_rate": 0.0001384834434150687,
      "loss": 0.0907,
      "step": 5234
    },
    {
      "epoch": 1.29547141796585,
      "grad_norm": 0.043823760002851486,
      "learning_rate": 0.00013839636919198796,
      "loss": 0.0393,
      "step": 5235
    },
    {
      "epoch": 1.295718881464984,
      "grad_norm": 0.05873279273509979,
      "learning_rate": 0.00013830931187392974,
      "loss": 0.0875,
      "step": 5236
    },
    {
      "epoch": 1.2959663449641177,
      "grad_norm": 0.03106492944061756,
      "learning_rate": 0.00013822227147408116,
      "loss": 0.0252,
      "step": 5237
    },
    {
      "epoch": 1.2962138084632517,
      "grad_norm": 0.07473136484622955,
      "learning_rate": 0.00013813524800562632,
      "loss": 0.0815,
      "step": 5238
    },
    {
      "epoch": 1.2964612719623856,
      "grad_norm": 0.06695710122585297,
      "learning_rate": 0.0001380482414817472,
      "loss": 0.089,
      "step": 5239
    },
    {
      "epoch": 1.2967087354615194,
      "grad_norm": 0.07365640997886658,
      "learning_rate": 0.0001379612519156231,
      "loss": 0.0846,
      "step": 5240
    },
    {
      "epoch": 1.2969561989606533,
      "grad_norm": 0.05246264487504959,
      "learning_rate": 0.00013787427932043043,
      "loss": 0.0403,
      "step": 5241
    },
    {
      "epoch": 1.2972036624597871,
      "grad_norm": 0.0331084169447422,
      "learning_rate": 0.0001377873237093433,
      "loss": 0.0433,
      "step": 5242
    },
    {
      "epoch": 1.297451125958921,
      "grad_norm": 0.04189550131559372,
      "learning_rate": 0.00013770038509553325,
      "loss": 0.05,
      "step": 5243
    },
    {
      "epoch": 1.2976985894580548,
      "grad_norm": 0.03769084811210632,
      "learning_rate": 0.0001376134634921692,
      "loss": 0.0675,
      "step": 5244
    },
    {
      "epoch": 1.297946052957189,
      "grad_norm": 0.06721300631761551,
      "learning_rate": 0.0001375265589124176,
      "loss": 0.0426,
      "step": 5245
    },
    {
      "epoch": 1.2981935164563227,
      "grad_norm": 0.029962461441755295,
      "learning_rate": 0.0001374396713694419,
      "loss": 0.04,
      "step": 5246
    },
    {
      "epoch": 1.2984409799554566,
      "grad_norm": 0.07746747136116028,
      "learning_rate": 0.00013735280087640356,
      "loss": 0.0566,
      "step": 5247
    },
    {
      "epoch": 1.2986884434545904,
      "grad_norm": 0.04480252414941788,
      "learning_rate": 0.0001372659474464612,
      "loss": 0.0333,
      "step": 5248
    },
    {
      "epoch": 1.2989359069537243,
      "grad_norm": 0.04849717393517494,
      "learning_rate": 0.00013717911109277064,
      "loss": 0.0594,
      "step": 5249
    },
    {
      "epoch": 1.2991833704528581,
      "grad_norm": 0.06263783574104309,
      "learning_rate": 0.00013709229182848546,
      "loss": 0.0661,
      "step": 5250
    },
    {
      "epoch": 1.299430833951992,
      "grad_norm": 0.04834999516606331,
      "learning_rate": 0.0001370054896667564,
      "loss": 0.0649,
      "step": 5251
    },
    {
      "epoch": 1.299678297451126,
      "grad_norm": 0.055337585508823395,
      "learning_rate": 0.0001369187046207318,
      "loss": 0.0728,
      "step": 5252
    },
    {
      "epoch": 1.29992576095026,
      "grad_norm": 0.0647328719496727,
      "learning_rate": 0.00013683193670355738,
      "loss": 0.0704,
      "step": 5253
    },
    {
      "epoch": 1.3001732244493938,
      "grad_norm": 0.04505812004208565,
      "learning_rate": 0.00013674518592837588,
      "loss": 0.0684,
      "step": 5254
    },
    {
      "epoch": 1.3004206879485276,
      "grad_norm": 0.044890809804201126,
      "learning_rate": 0.0001366584523083282,
      "loss": 0.0426,
      "step": 5255
    },
    {
      "epoch": 1.3006681514476615,
      "grad_norm": 0.09019352495670319,
      "learning_rate": 0.00013657173585655193,
      "loss": 0.0859,
      "step": 5256
    },
    {
      "epoch": 1.3009156149467953,
      "grad_norm": 0.03784538805484772,
      "learning_rate": 0.00013648503658618237,
      "loss": 0.0399,
      "step": 5257
    },
    {
      "epoch": 1.3011630784459292,
      "grad_norm": 0.03914234787225723,
      "learning_rate": 0.0001363983545103522,
      "loss": 0.0545,
      "step": 5258
    },
    {
      "epoch": 1.3014105419450632,
      "grad_norm": 0.052680108696222305,
      "learning_rate": 0.00013631168964219157,
      "loss": 0.0435,
      "step": 5259
    },
    {
      "epoch": 1.3016580054441969,
      "grad_norm": 0.05402497202157974,
      "learning_rate": 0.00013622504199482793,
      "loss": 0.084,
      "step": 5260
    },
    {
      "epoch": 1.301905468943331,
      "grad_norm": 0.05252275988459587,
      "learning_rate": 0.00013613841158138595,
      "loss": 0.053,
      "step": 5261
    },
    {
      "epoch": 1.3021529324424648,
      "grad_norm": 0.03839736431837082,
      "learning_rate": 0.00013605179841498795,
      "loss": 0.0507,
      "step": 5262
    },
    {
      "epoch": 1.3024003959415986,
      "grad_norm": 0.0934138298034668,
      "learning_rate": 0.0001359652025087537,
      "loss": 0.0484,
      "step": 5263
    },
    {
      "epoch": 1.3026478594407325,
      "grad_norm": 0.06124504283070564,
      "learning_rate": 0.00013587862387580004,
      "loss": 0.0569,
      "step": 5264
    },
    {
      "epoch": 1.3028953229398663,
      "grad_norm": 0.03640132024884224,
      "learning_rate": 0.0001357920625292414,
      "loss": 0.0635,
      "step": 5265
    },
    {
      "epoch": 1.3031427864390002,
      "grad_norm": 0.06801626831293106,
      "learning_rate": 0.00013570551848218954,
      "loss": 0.0559,
      "step": 5266
    },
    {
      "epoch": 1.303390249938134,
      "grad_norm": 0.03628253936767578,
      "learning_rate": 0.00013561899174775366,
      "loss": 0.0401,
      "step": 5267
    },
    {
      "epoch": 1.303637713437268,
      "grad_norm": 0.06837060302495956,
      "learning_rate": 0.0001355324823390404,
      "loss": 0.1831,
      "step": 5268
    },
    {
      "epoch": 1.303885176936402,
      "grad_norm": 0.08378386497497559,
      "learning_rate": 0.0001354459902691534,
      "loss": 0.0676,
      "step": 5269
    },
    {
      "epoch": 1.3041326404355358,
      "grad_norm": 0.056195005774497986,
      "learning_rate": 0.0001353595155511941,
      "loss": 0.09,
      "step": 5270
    },
    {
      "epoch": 1.3043801039346696,
      "grad_norm": 0.05635518953204155,
      "learning_rate": 0.0001352730581982611,
      "loss": 0.1061,
      "step": 5271
    },
    {
      "epoch": 1.3046275674338035,
      "grad_norm": 0.037063103169202805,
      "learning_rate": 0.00013518661822345046,
      "loss": 0.0153,
      "step": 5272
    },
    {
      "epoch": 1.3048750309329373,
      "grad_norm": 0.029541924595832825,
      "learning_rate": 0.00013510019563985553,
      "loss": 0.0314,
      "step": 5273
    },
    {
      "epoch": 1.3051224944320712,
      "grad_norm": 0.042545851320028305,
      "learning_rate": 0.00013501379046056714,
      "loss": 0.0435,
      "step": 5274
    },
    {
      "epoch": 1.3053699579312052,
      "grad_norm": 0.03905407339334488,
      "learning_rate": 0.00013492740269867342,
      "loss": 0.0344,
      "step": 5275
    },
    {
      "epoch": 1.305617421430339,
      "grad_norm": 0.05260597541928291,
      "learning_rate": 0.00013484103236725974,
      "loss": 0.0538,
      "step": 5276
    },
    {
      "epoch": 1.305864884929473,
      "grad_norm": 0.04802992567420006,
      "learning_rate": 0.00013475467947940894,
      "loss": 0.024,
      "step": 5277
    },
    {
      "epoch": 1.3061123484286068,
      "grad_norm": 0.03161156177520752,
      "learning_rate": 0.0001346683440482013,
      "loss": 0.026,
      "step": 5278
    },
    {
      "epoch": 1.3063598119277406,
      "grad_norm": 0.06602253019809723,
      "learning_rate": 0.00013458202608671434,
      "loss": 0.0698,
      "step": 5279
    },
    {
      "epoch": 1.3066072754268745,
      "grad_norm": 0.05143464729189873,
      "learning_rate": 0.00013449572560802304,
      "loss": 0.0334,
      "step": 5280
    },
    {
      "epoch": 1.3068547389260083,
      "grad_norm": 0.043003130704164505,
      "learning_rate": 0.0001344094426251994,
      "loss": 0.0339,
      "step": 5281
    },
    {
      "epoch": 1.3071022024251424,
      "grad_norm": 0.025469493120908737,
      "learning_rate": 0.0001343231771513133,
      "loss": 0.034,
      "step": 5282
    },
    {
      "epoch": 1.307349665924276,
      "grad_norm": 0.05567825213074684,
      "learning_rate": 0.00013423692919943176,
      "loss": 0.0448,
      "step": 5283
    },
    {
      "epoch": 1.30759712942341,
      "grad_norm": 0.043582212179899216,
      "learning_rate": 0.00013415069878261877,
      "loss": 0.0633,
      "step": 5284
    },
    {
      "epoch": 1.307844592922544,
      "grad_norm": 0.05168325453996658,
      "learning_rate": 0.00013406448591393616,
      "loss": 0.0464,
      "step": 5285
    },
    {
      "epoch": 1.3080920564216778,
      "grad_norm": 0.03901650756597519,
      "learning_rate": 0.00013397829060644292,
      "loss": 0.0483,
      "step": 5286
    },
    {
      "epoch": 1.3083395199208117,
      "grad_norm": 0.0533984936773777,
      "learning_rate": 0.00013389211287319537,
      "loss": 0.0597,
      "step": 5287
    },
    {
      "epoch": 1.3085869834199455,
      "grad_norm": 0.03728688135743141,
      "learning_rate": 0.00013380595272724722,
      "loss": 0.03,
      "step": 5288
    },
    {
      "epoch": 1.3088344469190794,
      "grad_norm": 0.0400271862745285,
      "learning_rate": 0.00013371981018164925,
      "loss": 0.0374,
      "step": 5289
    },
    {
      "epoch": 1.3090819104182132,
      "grad_norm": 0.028847703710198402,
      "learning_rate": 0.00013363368524945012,
      "loss": 0.0344,
      "step": 5290
    },
    {
      "epoch": 1.3093293739173473,
      "grad_norm": 0.052214913070201874,
      "learning_rate": 0.00013354757794369526,
      "loss": 0.0633,
      "step": 5291
    },
    {
      "epoch": 1.3095768374164811,
      "grad_norm": 0.041891321539878845,
      "learning_rate": 0.00013346148827742777,
      "loss": 0.0475,
      "step": 5292
    },
    {
      "epoch": 1.309824300915615,
      "grad_norm": 0.032581716775894165,
      "learning_rate": 0.00013337541626368794,
      "loss": 0.0342,
      "step": 5293
    },
    {
      "epoch": 1.3100717644147488,
      "grad_norm": 0.0364476777613163,
      "learning_rate": 0.0001332893619155134,
      "loss": 0.04,
      "step": 5294
    },
    {
      "epoch": 1.3103192279138827,
      "grad_norm": 0.021316759288311005,
      "learning_rate": 0.00013320332524593932,
      "loss": 0.0124,
      "step": 5295
    },
    {
      "epoch": 1.3105666914130165,
      "grad_norm": 0.09205622225999832,
      "learning_rate": 0.00013311730626799768,
      "loss": 0.1243,
      "step": 5296
    },
    {
      "epoch": 1.3108141549121504,
      "grad_norm": 0.04907122254371643,
      "learning_rate": 0.0001330313049947182,
      "loss": 0.0496,
      "step": 5297
    },
    {
      "epoch": 1.3110616184112844,
      "grad_norm": 0.06571707874536514,
      "learning_rate": 0.00013294532143912806,
      "loss": 0.0602,
      "step": 5298
    },
    {
      "epoch": 1.3113090819104183,
      "grad_norm": 0.06657133996486664,
      "learning_rate": 0.00013285935561425125,
      "loss": 0.0569,
      "step": 5299
    },
    {
      "epoch": 1.3115565454095521,
      "grad_norm": 0.033259909600019455,
      "learning_rate": 0.0001327734075331095,
      "loss": 0.0487,
      "step": 5300
    },
    {
      "epoch": 1.311804008908686,
      "grad_norm": 0.026952415704727173,
      "learning_rate": 0.00013268747720872142,
      "loss": 0.0321,
      "step": 5301
    },
    {
      "epoch": 1.3120514724078198,
      "grad_norm": 0.03443644568324089,
      "learning_rate": 0.00013260156465410346,
      "loss": 0.0246,
      "step": 5302
    },
    {
      "epoch": 1.3122989359069537,
      "grad_norm": 0.04880472272634506,
      "learning_rate": 0.0001325156698822692,
      "loss": 0.0593,
      "step": 5303
    },
    {
      "epoch": 1.3125463994060875,
      "grad_norm": 0.0355650819838047,
      "learning_rate": 0.00013242979290622913,
      "loss": 0.0432,
      "step": 5304
    },
    {
      "epoch": 1.3127938629052216,
      "grad_norm": 0.053307559341192245,
      "learning_rate": 0.00013234393373899155,
      "loss": 0.0627,
      "step": 5305
    },
    {
      "epoch": 1.3130413264043552,
      "grad_norm": 0.04281150922179222,
      "learning_rate": 0.00013225809239356186,
      "loss": 0.0817,
      "step": 5306
    },
    {
      "epoch": 1.3132887899034893,
      "grad_norm": 0.06357378512620926,
      "learning_rate": 0.0001321722688829427,
      "loss": 0.1008,
      "step": 5307
    },
    {
      "epoch": 1.3135362534026231,
      "grad_norm": 0.06646349281072617,
      "learning_rate": 0.0001320864632201343,
      "loss": 0.1178,
      "step": 5308
    },
    {
      "epoch": 1.313783716901757,
      "grad_norm": 0.048086266964673996,
      "learning_rate": 0.00013200067541813354,
      "loss": 0.0699,
      "step": 5309
    },
    {
      "epoch": 1.3140311804008908,
      "grad_norm": 0.06864351779222488,
      "learning_rate": 0.0001319149054899355,
      "loss": 0.0614,
      "step": 5310
    },
    {
      "epoch": 1.3142786439000247,
      "grad_norm": 0.039793770760297775,
      "learning_rate": 0.0001318291534485318,
      "loss": 0.0472,
      "step": 5311
    },
    {
      "epoch": 1.3145261073991588,
      "grad_norm": 0.04538174346089363,
      "learning_rate": 0.00013174341930691165,
      "loss": 0.023,
      "step": 5312
    },
    {
      "epoch": 1.3147735708982924,
      "grad_norm": 0.07765276730060577,
      "learning_rate": 0.00013165770307806157,
      "loss": 0.0771,
      "step": 5313
    },
    {
      "epoch": 1.3150210343974265,
      "grad_norm": 0.08179622143507004,
      "learning_rate": 0.00013157200477496527,
      "loss": 0.0534,
      "step": 5314
    },
    {
      "epoch": 1.3152684978965603,
      "grad_norm": 0.027053000405430794,
      "learning_rate": 0.00013148632441060398,
      "loss": 0.0323,
      "step": 5315
    },
    {
      "epoch": 1.3155159613956942,
      "grad_norm": 0.05785463750362396,
      "learning_rate": 0.00013140066199795574,
      "loss": 0.0824,
      "step": 5316
    },
    {
      "epoch": 1.315763424894828,
      "grad_norm": 0.055836305022239685,
      "learning_rate": 0.00013131501754999625,
      "loss": 0.0338,
      "step": 5317
    },
    {
      "epoch": 1.3160108883939619,
      "grad_norm": 0.06998961418867111,
      "learning_rate": 0.00013122939107969861,
      "loss": 0.0708,
      "step": 5318
    },
    {
      "epoch": 1.3162583518930957,
      "grad_norm": 0.02328481711447239,
      "learning_rate": 0.00013114378260003273,
      "loss": 0.0175,
      "step": 5319
    },
    {
      "epoch": 1.3165058153922296,
      "grad_norm": 0.07135280966758728,
      "learning_rate": 0.00013105819212396613,
      "loss": 0.0593,
      "step": 5320
    },
    {
      "epoch": 1.3167532788913636,
      "grad_norm": 0.026541966944932938,
      "learning_rate": 0.00013097261966446352,
      "loss": 0.0443,
      "step": 5321
    },
    {
      "epoch": 1.3170007423904975,
      "grad_norm": 0.05334315448999405,
      "learning_rate": 0.0001308870652344869,
      "loss": 0.087,
      "step": 5322
    },
    {
      "epoch": 1.3172482058896313,
      "grad_norm": 0.07815148681402206,
      "learning_rate": 0.00013080152884699565,
      "loss": 0.0267,
      "step": 5323
    },
    {
      "epoch": 1.3174956693887652,
      "grad_norm": 0.03999089449644089,
      "learning_rate": 0.00013071601051494602,
      "loss": 0.046,
      "step": 5324
    },
    {
      "epoch": 1.317743132887899,
      "grad_norm": 0.044603388756513596,
      "learning_rate": 0.00013063051025129194,
      "loss": 0.0822,
      "step": 5325
    },
    {
      "epoch": 1.3179905963870329,
      "grad_norm": 0.06414931267499924,
      "learning_rate": 0.00013054502806898445,
      "loss": 0.0603,
      "step": 5326
    },
    {
      "epoch": 1.3182380598861667,
      "grad_norm": 0.36575910449028015,
      "learning_rate": 0.00013045956398097187,
      "loss": 0.0861,
      "step": 5327
    },
    {
      "epoch": 1.3184855233853008,
      "grad_norm": 0.02760780043900013,
      "learning_rate": 0.00013037411800019972,
      "loss": 0.0394,
      "step": 5328
    },
    {
      "epoch": 1.3187329868844344,
      "grad_norm": 0.029395900666713715,
      "learning_rate": 0.00013028869013961087,
      "loss": 0.0269,
      "step": 5329
    },
    {
      "epoch": 1.3189804503835685,
      "grad_norm": 0.053118132054805756,
      "learning_rate": 0.00013020328041214548,
      "loss": 0.0529,
      "step": 5330
    },
    {
      "epoch": 1.3192279138827023,
      "grad_norm": 0.03741002827882767,
      "learning_rate": 0.0001301178888307407,
      "loss": 0.0383,
      "step": 5331
    },
    {
      "epoch": 1.3194753773818362,
      "grad_norm": 0.06784311681985855,
      "learning_rate": 0.00013003251540833123,
      "loss": 0.0557,
      "step": 5332
    },
    {
      "epoch": 1.31972284088097,
      "grad_norm": 0.041483961045742035,
      "learning_rate": 0.00012994716015784886,
      "loss": 0.0347,
      "step": 5333
    },
    {
      "epoch": 1.3199703043801039,
      "grad_norm": 0.057760145515203476,
      "learning_rate": 0.00012986182309222266,
      "loss": 0.0626,
      "step": 5334
    },
    {
      "epoch": 1.320217767879238,
      "grad_norm": 0.07550197839736938,
      "learning_rate": 0.0001297765042243791,
      "loss": 0.0339,
      "step": 5335
    },
    {
      "epoch": 1.3204652313783716,
      "grad_norm": 0.04099911078810692,
      "learning_rate": 0.00012969120356724146,
      "loss": 0.0351,
      "step": 5336
    },
    {
      "epoch": 1.3207126948775056,
      "grad_norm": 0.04803892970085144,
      "learning_rate": 0.00012960592113373082,
      "loss": 0.0321,
      "step": 5337
    },
    {
      "epoch": 1.3209601583766395,
      "grad_norm": 0.035308707505464554,
      "learning_rate": 0.00012952065693676523,
      "loss": 0.0205,
      "step": 5338
    },
    {
      "epoch": 1.3212076218757733,
      "grad_norm": 0.05222494900226593,
      "learning_rate": 0.0001294354109892598,
      "loss": 0.0341,
      "step": 5339
    },
    {
      "epoch": 1.3214550853749072,
      "grad_norm": 0.050802480429410934,
      "learning_rate": 0.00012935018330412717,
      "loss": 0.0809,
      "step": 5340
    },
    {
      "epoch": 1.321702548874041,
      "grad_norm": 0.0394151546061039,
      "learning_rate": 0.00012926497389427707,
      "loss": 0.035,
      "step": 5341
    },
    {
      "epoch": 1.3219500123731749,
      "grad_norm": 0.08994358032941818,
      "learning_rate": 0.0001291797827726165,
      "loss": 0.061,
      "step": 5342
    },
    {
      "epoch": 1.3221974758723087,
      "grad_norm": 0.05570347234606743,
      "learning_rate": 0.00012909460995204976,
      "loss": 0.0766,
      "step": 5343
    },
    {
      "epoch": 1.3224449393714428,
      "grad_norm": 0.03961385786533356,
      "learning_rate": 0.00012900945544547801,
      "loss": 0.0364,
      "step": 5344
    },
    {
      "epoch": 1.3226924028705767,
      "grad_norm": 0.039323750883340836,
      "learning_rate": 0.00012892431926580033,
      "loss": 0.0359,
      "step": 5345
    },
    {
      "epoch": 1.3229398663697105,
      "grad_norm": 0.035130880773067474,
      "learning_rate": 0.00012883920142591233,
      "loss": 0.0354,
      "step": 5346
    },
    {
      "epoch": 1.3231873298688444,
      "grad_norm": 0.06259144842624664,
      "learning_rate": 0.0001287541019387072,
      "loss": 0.0415,
      "step": 5347
    },
    {
      "epoch": 1.3234347933679782,
      "grad_norm": 0.054798707365989685,
      "learning_rate": 0.00012866902081707532,
      "loss": 0.0499,
      "step": 5348
    },
    {
      "epoch": 1.323682256867112,
      "grad_norm": 0.043268296867609024,
      "learning_rate": 0.0001285839580739042,
      "loss": 0.0409,
      "step": 5349
    },
    {
      "epoch": 1.323929720366246,
      "grad_norm": 0.03833819180727005,
      "learning_rate": 0.00012849891372207874,
      "loss": 0.0338,
      "step": 5350
    },
    {
      "epoch": 1.32417718386538,
      "grad_norm": 0.08456224203109741,
      "learning_rate": 0.00012841388777448077,
      "loss": 0.0386,
      "step": 5351
    },
    {
      "epoch": 1.3244246473645136,
      "grad_norm": 0.06865504384040833,
      "learning_rate": 0.00012832888024398942,
      "loss": 0.0617,
      "step": 5352
    },
    {
      "epoch": 1.3246721108636477,
      "grad_norm": 0.056327808648347855,
      "learning_rate": 0.0001282438911434814,
      "loss": 0.0631,
      "step": 5353
    },
    {
      "epoch": 1.3249195743627815,
      "grad_norm": 0.07158226519823074,
      "learning_rate": 0.0001281589204858301,
      "loss": 0.0911,
      "step": 5354
    },
    {
      "epoch": 1.3251670378619154,
      "grad_norm": 0.03639157488942146,
      "learning_rate": 0.00012807396828390638,
      "loss": 0.0663,
      "step": 5355
    },
    {
      "epoch": 1.3254145013610492,
      "grad_norm": 0.04991026222705841,
      "learning_rate": 0.0001279890345505783,
      "loss": 0.0662,
      "step": 5356
    },
    {
      "epoch": 1.325661964860183,
      "grad_norm": 0.05949489772319794,
      "learning_rate": 0.0001279041192987111,
      "loss": 0.0603,
      "step": 5357
    },
    {
      "epoch": 1.3259094283593171,
      "grad_norm": 0.03848600387573242,
      "learning_rate": 0.0001278192225411673,
      "loss": 0.0361,
      "step": 5358
    },
    {
      "epoch": 1.3261568918584508,
      "grad_norm": 0.03610343113541603,
      "learning_rate": 0.00012773434429080632,
      "loss": 0.029,
      "step": 5359
    },
    {
      "epoch": 1.3264043553575848,
      "grad_norm": 0.0525781586766243,
      "learning_rate": 0.00012764948456048513,
      "loss": 0.0779,
      "step": 5360
    },
    {
      "epoch": 1.3266518188567187,
      "grad_norm": 0.044540390372276306,
      "learning_rate": 0.00012756464336305772,
      "loss": 0.0627,
      "step": 5361
    },
    {
      "epoch": 1.3268992823558525,
      "grad_norm": 0.034590959548950195,
      "learning_rate": 0.0001274798207113753,
      "loss": 0.0402,
      "step": 5362
    },
    {
      "epoch": 1.3271467458549864,
      "grad_norm": 0.0684560090303421,
      "learning_rate": 0.0001273950166182863,
      "loss": 0.0781,
      "step": 5363
    },
    {
      "epoch": 1.3273942093541202,
      "grad_norm": 0.03873994201421738,
      "learning_rate": 0.00012731023109663627,
      "loss": 0.0348,
      "step": 5364
    },
    {
      "epoch": 1.327641672853254,
      "grad_norm": 0.04684792086482048,
      "learning_rate": 0.00012722546415926817,
      "loss": 0.0552,
      "step": 5365
    },
    {
      "epoch": 1.327889136352388,
      "grad_norm": 0.07134022563695908,
      "learning_rate": 0.00012714071581902168,
      "loss": 0.1076,
      "step": 5366
    },
    {
      "epoch": 1.328136599851522,
      "grad_norm": 0.07121866196393967,
      "learning_rate": 0.00012705598608873413,
      "loss": 0.052,
      "step": 5367
    },
    {
      "epoch": 1.3283840633506558,
      "grad_norm": 0.037222106009721756,
      "learning_rate": 0.0001269712749812398,
      "loss": 0.0525,
      "step": 5368
    },
    {
      "epoch": 1.3286315268497897,
      "grad_norm": 0.15529297292232513,
      "learning_rate": 0.0001268865825093702,
      "loss": 0.0368,
      "step": 5369
    },
    {
      "epoch": 1.3288789903489235,
      "grad_norm": 0.041663896292448044,
      "learning_rate": 0.00012680190868595415,
      "loss": 0.0542,
      "step": 5370
    },
    {
      "epoch": 1.3291264538480574,
      "grad_norm": 0.06799065321683884,
      "learning_rate": 0.00012671725352381721,
      "loss": 0.0628,
      "step": 5371
    },
    {
      "epoch": 1.3293739173471912,
      "grad_norm": 0.0423295795917511,
      "learning_rate": 0.00012663261703578271,
      "loss": 0.0355,
      "step": 5372
    },
    {
      "epoch": 1.329621380846325,
      "grad_norm": 0.05373305454850197,
      "learning_rate": 0.0001265479992346708,
      "loss": 0.0528,
      "step": 5373
    },
    {
      "epoch": 1.3298688443454592,
      "grad_norm": 0.06000014394521713,
      "learning_rate": 0.00012646340013329878,
      "loss": 0.0639,
      "step": 5374
    },
    {
      "epoch": 1.330116307844593,
      "grad_norm": 0.06283700466156006,
      "learning_rate": 0.0001263788197444812,
      "loss": 0.0397,
      "step": 5375
    },
    {
      "epoch": 1.3303637713437269,
      "grad_norm": 0.07781864702701569,
      "learning_rate": 0.00012629425808102977,
      "loss": 0.0898,
      "step": 5376
    },
    {
      "epoch": 1.3306112348428607,
      "grad_norm": 0.04107624664902687,
      "learning_rate": 0.00012620971515575342,
      "loss": 0.0291,
      "step": 5377
    },
    {
      "epoch": 1.3308586983419946,
      "grad_norm": 0.03240489587187767,
      "learning_rate": 0.00012612519098145823,
      "loss": 0.0395,
      "step": 5378
    },
    {
      "epoch": 1.3311061618411284,
      "grad_norm": 0.031036531552672386,
      "learning_rate": 0.0001260406855709471,
      "loss": 0.0266,
      "step": 5379
    },
    {
      "epoch": 1.3313536253402622,
      "grad_norm": 0.06492956727743149,
      "learning_rate": 0.00012595619893702076,
      "loss": 0.0705,
      "step": 5380
    },
    {
      "epoch": 1.3316010888393963,
      "grad_norm": 0.06153404340147972,
      "learning_rate": 0.00012587173109247663,
      "loss": 0.0661,
      "step": 5381
    },
    {
      "epoch": 1.33184855233853,
      "grad_norm": 0.06603457033634186,
      "learning_rate": 0.0001257872820501092,
      "loss": 0.0672,
      "step": 5382
    },
    {
      "epoch": 1.332096015837664,
      "grad_norm": 0.043454185128211975,
      "learning_rate": 0.0001257028518227104,
      "loss": 0.0495,
      "step": 5383
    },
    {
      "epoch": 1.3323434793367979,
      "grad_norm": 0.05507224053144455,
      "learning_rate": 0.0001256184404230692,
      "loss": 0.086,
      "step": 5384
    },
    {
      "epoch": 1.3325909428359317,
      "grad_norm": 0.05325176194310188,
      "learning_rate": 0.00012553404786397166,
      "loss": 0.0382,
      "step": 5385
    },
    {
      "epoch": 1.3328384063350656,
      "grad_norm": 0.05419048294425011,
      "learning_rate": 0.00012544967415820119,
      "loss": 0.0892,
      "step": 5386
    },
    {
      "epoch": 1.3330858698341994,
      "grad_norm": 0.04234888032078743,
      "learning_rate": 0.0001253653193185379,
      "loss": 0.0428,
      "step": 5387
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.031242115423083305,
      "learning_rate": 0.00012528098335775966,
      "loss": 0.0309,
      "step": 5388
    },
    {
      "epoch": 1.333580796832467,
      "grad_norm": 0.046093862503767014,
      "learning_rate": 0.00012519666628864096,
      "loss": 0.0759,
      "step": 5389
    },
    {
      "epoch": 1.3338282603316012,
      "grad_norm": 0.04522981122136116,
      "learning_rate": 0.00012511236812395366,
      "loss": 0.0695,
      "step": 5390
    },
    {
      "epoch": 1.334075723830735,
      "grad_norm": 0.060072727501392365,
      "learning_rate": 0.00012502808887646677,
      "loss": 0.0844,
      "step": 5391
    },
    {
      "epoch": 1.3343231873298689,
      "grad_norm": 0.04237570986151695,
      "learning_rate": 0.0001249438285589463,
      "loss": 0.0411,
      "step": 5392
    },
    {
      "epoch": 1.3345706508290027,
      "grad_norm": 0.06222548335790634,
      "learning_rate": 0.00012485958718415566,
      "loss": 0.0588,
      "step": 5393
    },
    {
      "epoch": 1.3348181143281366,
      "grad_norm": 0.0361146479845047,
      "learning_rate": 0.00012477536476485497,
      "loss": 0.0345,
      "step": 5394
    },
    {
      "epoch": 1.3350655778272704,
      "grad_norm": 0.0631587877869606,
      "learning_rate": 0.00012469116131380172,
      "loss": 0.0405,
      "step": 5395
    },
    {
      "epoch": 1.3353130413264043,
      "grad_norm": 0.04581715539097786,
      "learning_rate": 0.00012460697684375083,
      "loss": 0.0861,
      "step": 5396
    },
    {
      "epoch": 1.3355605048255383,
      "grad_norm": 0.041687797755002975,
      "learning_rate": 0.00012452281136745373,
      "loss": 0.0266,
      "step": 5397
    },
    {
      "epoch": 1.3358079683246722,
      "grad_norm": 0.03937670588493347,
      "learning_rate": 0.00012443866489765952,
      "loss": 0.0576,
      "step": 5398
    },
    {
      "epoch": 1.336055431823806,
      "grad_norm": 0.04905953258275986,
      "learning_rate": 0.0001243545374471138,
      "loss": 0.0322,
      "step": 5399
    },
    {
      "epoch": 1.3363028953229399,
      "grad_norm": 0.04554803669452667,
      "learning_rate": 0.00012427042902856005,
      "loss": 0.0417,
      "step": 5400
    },
    {
      "epoch": 1.3363028953229399,
      "eval_loss": 0.29030925035476685,
      "eval_runtime": 202.6977,
      "eval_samples_per_second": 4.933,
      "eval_steps_per_second": 0.311,
      "step": 5400
    },
    {
      "epoch": 1.3365503588220737,
      "grad_norm": 0.06509803235530853,
      "learning_rate": 0.0001241863396547385,
      "loss": 0.0778,
      "step": 5401
    },
    {
      "epoch": 1.3367978223212076,
      "grad_norm": 0.0541432686150074,
      "learning_rate": 0.0001241022693383862,
      "loss": 0.0477,
      "step": 5402
    },
    {
      "epoch": 1.3370452858203414,
      "grad_norm": 0.033721134066581726,
      "learning_rate": 0.00012401821809223775,
      "loss": 0.0365,
      "step": 5403
    },
    {
      "epoch": 1.3372927493194755,
      "grad_norm": 0.03643227368593216,
      "learning_rate": 0.00012393418592902475,
      "loss": 0.0457,
      "step": 5404
    },
    {
      "epoch": 1.3375402128186091,
      "grad_norm": 0.03139517456293106,
      "learning_rate": 0.00012385017286147579,
      "loss": 0.0398,
      "step": 5405
    },
    {
      "epoch": 1.3377876763177432,
      "grad_norm": 0.04815741628408432,
      "learning_rate": 0.0001237661789023168,
      "loss": 0.0701,
      "step": 5406
    },
    {
      "epoch": 1.338035139816877,
      "grad_norm": 0.037563759833574295,
      "learning_rate": 0.0001236822040642703,
      "loss": 0.0305,
      "step": 5407
    },
    {
      "epoch": 1.338282603316011,
      "grad_norm": 0.03586310148239136,
      "learning_rate": 0.00012359824836005677,
      "loss": 0.0247,
      "step": 5408
    },
    {
      "epoch": 1.3385300668151447,
      "grad_norm": 0.03398579731583595,
      "learning_rate": 0.00012351431180239293,
      "loss": 0.0491,
      "step": 5409
    },
    {
      "epoch": 1.3387775303142786,
      "grad_norm": 0.04270753636956215,
      "learning_rate": 0.00012343039440399303,
      "loss": 0.0234,
      "step": 5410
    },
    {
      "epoch": 1.3390249938134124,
      "grad_norm": 0.06742265820503235,
      "learning_rate": 0.00012334649617756847,
      "loss": 0.0721,
      "step": 5411
    },
    {
      "epoch": 1.3392724573125463,
      "grad_norm": 0.035079095512628555,
      "learning_rate": 0.00012326261713582753,
      "loss": 0.044,
      "step": 5412
    },
    {
      "epoch": 1.3395199208116804,
      "grad_norm": 0.07285121828317642,
      "learning_rate": 0.00012317875729147586,
      "loss": 0.0802,
      "step": 5413
    },
    {
      "epoch": 1.3397673843108142,
      "grad_norm": 0.04605779051780701,
      "learning_rate": 0.00012309491665721573,
      "loss": 0.0514,
      "step": 5414
    },
    {
      "epoch": 1.340014847809948,
      "grad_norm": 0.0334249809384346,
      "learning_rate": 0.00012301109524574688,
      "loss": 0.038,
      "step": 5415
    },
    {
      "epoch": 1.340262311309082,
      "grad_norm": 0.0321323461830616,
      "learning_rate": 0.00012292729306976631,
      "loss": 0.0541,
      "step": 5416
    },
    {
      "epoch": 1.3405097748082158,
      "grad_norm": 0.056508779525756836,
      "learning_rate": 0.00012284351014196763,
      "loss": 0.081,
      "step": 5417
    },
    {
      "epoch": 1.3407572383073496,
      "grad_norm": 0.04547508805990219,
      "learning_rate": 0.00012275974647504175,
      "loss": 0.0521,
      "step": 5418
    },
    {
      "epoch": 1.3410047018064835,
      "grad_norm": 0.03762836009263992,
      "learning_rate": 0.00012267600208167674,
      "loss": 0.0455,
      "step": 5419
    },
    {
      "epoch": 1.3412521653056175,
      "grad_norm": 0.06014961004257202,
      "learning_rate": 0.00012259227697455764,
      "loss": 0.0623,
      "step": 5420
    },
    {
      "epoch": 1.3414996288047514,
      "grad_norm": 0.02471664734184742,
      "learning_rate": 0.0001225085711663667,
      "loss": 0.0329,
      "step": 5421
    },
    {
      "epoch": 1.3417470923038852,
      "grad_norm": 0.05835336819291115,
      "learning_rate": 0.00012242488466978303,
      "loss": 0.0422,
      "step": 5422
    },
    {
      "epoch": 1.341994555803019,
      "grad_norm": 0.03790218383073807,
      "learning_rate": 0.00012234121749748295,
      "loss": 0.044,
      "step": 5423
    },
    {
      "epoch": 1.342242019302153,
      "grad_norm": 0.04139971360564232,
      "learning_rate": 0.00012225756966213985,
      "loss": 0.0345,
      "step": 5424
    },
    {
      "epoch": 1.3424894828012868,
      "grad_norm": 0.04155122861266136,
      "learning_rate": 0.00012217394117642426,
      "loss": 0.0511,
      "step": 5425
    },
    {
      "epoch": 1.3427369463004206,
      "grad_norm": 0.02478598617017269,
      "learning_rate": 0.00012209033205300363,
      "loss": 0.0358,
      "step": 5426
    },
    {
      "epoch": 1.3429844097995547,
      "grad_norm": 0.04337557777762413,
      "learning_rate": 0.00012200674230454259,
      "loss": 0.0598,
      "step": 5427
    },
    {
      "epoch": 1.3432318732986883,
      "grad_norm": 0.06034200266003609,
      "learning_rate": 0.00012192317194370284,
      "loss": 0.0686,
      "step": 5428
    },
    {
      "epoch": 1.3434793367978224,
      "grad_norm": 0.06973240524530411,
      "learning_rate": 0.00012183962098314297,
      "loss": 0.0729,
      "step": 5429
    },
    {
      "epoch": 1.3437268002969562,
      "grad_norm": 0.04051089286804199,
      "learning_rate": 0.0001217560894355188,
      "loss": 0.051,
      "step": 5430
    },
    {
      "epoch": 1.34397426379609,
      "grad_norm": 0.05058569461107254,
      "learning_rate": 0.00012167257731348319,
      "loss": 0.0709,
      "step": 5431
    },
    {
      "epoch": 1.344221727295224,
      "grad_norm": 0.0396595224738121,
      "learning_rate": 0.00012158908462968605,
      "loss": 0.0406,
      "step": 5432
    },
    {
      "epoch": 1.3444691907943578,
      "grad_norm": 0.030794937163591385,
      "learning_rate": 0.0001215056113967744,
      "loss": 0.0385,
      "step": 5433
    },
    {
      "epoch": 1.3447166542934916,
      "grad_norm": 0.027218226343393326,
      "learning_rate": 0.00012142215762739198,
      "loss": 0.0326,
      "step": 5434
    },
    {
      "epoch": 1.3449641177926255,
      "grad_norm": 0.06851941347122192,
      "learning_rate": 0.0001213387233341801,
      "loss": 0.0826,
      "step": 5435
    },
    {
      "epoch": 1.3452115812917596,
      "grad_norm": 0.07618559896945953,
      "learning_rate": 0.00012125530852977695,
      "loss": 0.1093,
      "step": 5436
    },
    {
      "epoch": 1.3454590447908934,
      "grad_norm": 0.038768380880355835,
      "learning_rate": 0.00012117191322681739,
      "loss": 0.046,
      "step": 5437
    },
    {
      "epoch": 1.3457065082900272,
      "grad_norm": 0.04050231724977493,
      "learning_rate": 0.00012108853743793377,
      "loss": 0.0545,
      "step": 5438
    },
    {
      "epoch": 1.345953971789161,
      "grad_norm": 0.04238878935575485,
      "learning_rate": 0.00012100518117575534,
      "loss": 0.0608,
      "step": 5439
    },
    {
      "epoch": 1.346201435288295,
      "grad_norm": 0.029057469218969345,
      "learning_rate": 0.00012092184445290839,
      "loss": 0.026,
      "step": 5440
    },
    {
      "epoch": 1.3464488987874288,
      "grad_norm": 0.044695328921079636,
      "learning_rate": 0.0001208385272820163,
      "loss": 0.0286,
      "step": 5441
    },
    {
      "epoch": 1.3466963622865626,
      "grad_norm": 0.027389831840991974,
      "learning_rate": 0.00012075522967569919,
      "loss": 0.031,
      "step": 5442
    },
    {
      "epoch": 1.3469438257856967,
      "grad_norm": 0.02786421962082386,
      "learning_rate": 0.00012067195164657483,
      "loss": 0.0395,
      "step": 5443
    },
    {
      "epoch": 1.3471912892848306,
      "grad_norm": 0.03623868525028229,
      "learning_rate": 0.00012058869320725735,
      "loss": 0.0343,
      "step": 5444
    },
    {
      "epoch": 1.3474387527839644,
      "grad_norm": 0.043445516377687454,
      "learning_rate": 0.00012050545437035837,
      "loss": 0.0398,
      "step": 5445
    },
    {
      "epoch": 1.3476862162830983,
      "grad_norm": 0.04317672550678253,
      "learning_rate": 0.00012042223514848636,
      "loss": 0.0682,
      "step": 5446
    },
    {
      "epoch": 1.347933679782232,
      "grad_norm": 0.04211984947323799,
      "learning_rate": 0.0001203390355542468,
      "loss": 0.0581,
      "step": 5447
    },
    {
      "epoch": 1.348181143281366,
      "grad_norm": 0.026360822841525078,
      "learning_rate": 0.00012025585560024244,
      "loss": 0.0266,
      "step": 5448
    },
    {
      "epoch": 1.3484286067804998,
      "grad_norm": 0.04001898318529129,
      "learning_rate": 0.0001201726952990726,
      "loss": 0.0281,
      "step": 5449
    },
    {
      "epoch": 1.3486760702796339,
      "grad_norm": 0.05147268995642662,
      "learning_rate": 0.00012008955466333391,
      "loss": 0.0504,
      "step": 5450
    },
    {
      "epoch": 1.3489235337787675,
      "grad_norm": 0.037090737372636795,
      "learning_rate": 0.00012000643370562025,
      "loss": 0.0363,
      "step": 5451
    },
    {
      "epoch": 1.3491709972779016,
      "grad_norm": 0.02467350661754608,
      "learning_rate": 0.00011992333243852205,
      "loss": 0.028,
      "step": 5452
    },
    {
      "epoch": 1.3494184607770354,
      "grad_norm": 0.028826816007494926,
      "learning_rate": 0.00011984025087462708,
      "loss": 0.0362,
      "step": 5453
    },
    {
      "epoch": 1.3496659242761693,
      "grad_norm": 0.04501539468765259,
      "learning_rate": 0.00011975718902651978,
      "loss": 0.0345,
      "step": 5454
    },
    {
      "epoch": 1.3499133877753031,
      "grad_norm": 0.04710189253091812,
      "learning_rate": 0.0001196741469067821,
      "loss": 0.0345,
      "step": 5455
    },
    {
      "epoch": 1.350160851274437,
      "grad_norm": 0.044451553374528885,
      "learning_rate": 0.00011959112452799279,
      "loss": 0.0627,
      "step": 5456
    },
    {
      "epoch": 1.3504083147735708,
      "grad_norm": 0.07578147947788239,
      "learning_rate": 0.00011950812190272734,
      "loss": 0.0588,
      "step": 5457
    },
    {
      "epoch": 1.3506557782727047,
      "grad_norm": 0.07414727658033371,
      "learning_rate": 0.00011942513904355853,
      "loss": 0.0633,
      "step": 5458
    },
    {
      "epoch": 1.3509032417718387,
      "grad_norm": 0.07266145944595337,
      "learning_rate": 0.00011934217596305616,
      "loss": 0.0888,
      "step": 5459
    },
    {
      "epoch": 1.3511507052709726,
      "grad_norm": 0.036080822348594666,
      "learning_rate": 0.00011925923267378691,
      "loss": 0.0312,
      "step": 5460
    },
    {
      "epoch": 1.3513981687701064,
      "grad_norm": 0.03359130769968033,
      "learning_rate": 0.00011917630918831463,
      "loss": 0.0252,
      "step": 5461
    },
    {
      "epoch": 1.3516456322692403,
      "grad_norm": 0.05641964077949524,
      "learning_rate": 0.00011909340551919973,
      "loss": 0.0445,
      "step": 5462
    },
    {
      "epoch": 1.3518930957683741,
      "grad_norm": 0.0459786131978035,
      "learning_rate": 0.00011901052167900037,
      "loss": 0.0433,
      "step": 5463
    },
    {
      "epoch": 1.352140559267508,
      "grad_norm": 0.058091357350349426,
      "learning_rate": 0.00011892765768027097,
      "loss": 0.0384,
      "step": 5464
    },
    {
      "epoch": 1.3523880227666418,
      "grad_norm": 0.08778837323188782,
      "learning_rate": 0.00011884481353556337,
      "loss": 0.0769,
      "step": 5465
    },
    {
      "epoch": 1.352635486265776,
      "grad_norm": 0.07888554781675339,
      "learning_rate": 0.00011876198925742623,
      "loss": 0.0818,
      "step": 5466
    },
    {
      "epoch": 1.3528829497649097,
      "grad_norm": 0.0697663426399231,
      "learning_rate": 0.00011867918485840531,
      "loss": 0.0427,
      "step": 5467
    },
    {
      "epoch": 1.3531304132640436,
      "grad_norm": 0.05889507010579109,
      "learning_rate": 0.00011859640035104339,
      "loss": 0.0956,
      "step": 5468
    },
    {
      "epoch": 1.3533778767631774,
      "grad_norm": 0.058826085180044174,
      "learning_rate": 0.00011851363574787993,
      "loss": 0.0307,
      "step": 5469
    },
    {
      "epoch": 1.3536253402623113,
      "grad_norm": 0.05134214833378792,
      "learning_rate": 0.00011843089106145164,
      "loss": 0.0594,
      "step": 5470
    },
    {
      "epoch": 1.3538728037614451,
      "grad_norm": 0.04748811200261116,
      "learning_rate": 0.00011834816630429243,
      "loss": 0.0606,
      "step": 5471
    },
    {
      "epoch": 1.354120267260579,
      "grad_norm": 0.08861533552408218,
      "learning_rate": 0.00011826546148893263,
      "loss": 0.1082,
      "step": 5472
    },
    {
      "epoch": 1.354367730759713,
      "grad_norm": 0.04882629215717316,
      "learning_rate": 0.00011818277662789997,
      "loss": 0.0428,
      "step": 5473
    },
    {
      "epoch": 1.3546151942588467,
      "grad_norm": 0.053780876100063324,
      "learning_rate": 0.00011810011173371906,
      "loss": 0.0739,
      "step": 5474
    },
    {
      "epoch": 1.3548626577579808,
      "grad_norm": 0.039619576185941696,
      "learning_rate": 0.0001180174668189114,
      "loss": 0.0341,
      "step": 5475
    },
    {
      "epoch": 1.3551101212571146,
      "grad_norm": 0.058248113840818405,
      "learning_rate": 0.00011793484189599568,
      "loss": 0.027,
      "step": 5476
    },
    {
      "epoch": 1.3553575847562485,
      "grad_norm": 0.0744185671210289,
      "learning_rate": 0.00011785223697748718,
      "loss": 0.0978,
      "step": 5477
    },
    {
      "epoch": 1.3556050482553823,
      "grad_norm": 0.04866122454404831,
      "learning_rate": 0.00011776965207589849,
      "loss": 0.0354,
      "step": 5478
    },
    {
      "epoch": 1.3558525117545162,
      "grad_norm": 0.07262765616178513,
      "learning_rate": 0.00011768708720373905,
      "loss": 0.1014,
      "step": 5479
    },
    {
      "epoch": 1.3560999752536502,
      "grad_norm": 0.04251658171415329,
      "learning_rate": 0.00011760454237351529,
      "loss": 0.0601,
      "step": 5480
    },
    {
      "epoch": 1.3563474387527839,
      "grad_norm": 0.037143345922231674,
      "learning_rate": 0.00011752201759773057,
      "loss": 0.0382,
      "step": 5481
    },
    {
      "epoch": 1.356594902251918,
      "grad_norm": 0.036632999777793884,
      "learning_rate": 0.00011743951288888521,
      "loss": 0.037,
      "step": 5482
    },
    {
      "epoch": 1.3568423657510518,
      "grad_norm": 0.0754423514008522,
      "learning_rate": 0.00011735702825947666,
      "loss": 0.0505,
      "step": 5483
    },
    {
      "epoch": 1.3570898292501856,
      "grad_norm": 0.053507354110479355,
      "learning_rate": 0.00011727456372199893,
      "loss": 0.0679,
      "step": 5484
    },
    {
      "epoch": 1.3573372927493195,
      "grad_norm": 0.03915010020136833,
      "learning_rate": 0.00011719211928894339,
      "loss": 0.0325,
      "step": 5485
    },
    {
      "epoch": 1.3575847562484533,
      "grad_norm": 0.048951778560876846,
      "learning_rate": 0.00011710969497279813,
      "loss": 0.0394,
      "step": 5486
    },
    {
      "epoch": 1.3578322197475872,
      "grad_norm": 0.04330416023731232,
      "learning_rate": 0.00011702729078604832,
      "loss": 0.0558,
      "step": 5487
    },
    {
      "epoch": 1.358079683246721,
      "grad_norm": 0.05730779469013214,
      "learning_rate": 0.00011694490674117613,
      "loss": 0.0484,
      "step": 5488
    },
    {
      "epoch": 1.358327146745855,
      "grad_norm": 0.05900117754936218,
      "learning_rate": 0.0001168625428506603,
      "loss": 0.0685,
      "step": 5489
    },
    {
      "epoch": 1.358574610244989,
      "grad_norm": 0.055588629096746445,
      "learning_rate": 0.00011678019912697708,
      "loss": 0.0813,
      "step": 5490
    },
    {
      "epoch": 1.3588220737441228,
      "grad_norm": 0.05002940818667412,
      "learning_rate": 0.00011669787558259937,
      "loss": 0.0635,
      "step": 5491
    },
    {
      "epoch": 1.3590695372432566,
      "grad_norm": 0.041929759085178375,
      "learning_rate": 0.00011661557222999683,
      "loss": 0.0493,
      "step": 5492
    },
    {
      "epoch": 1.3593170007423905,
      "grad_norm": 0.10036183893680573,
      "learning_rate": 0.00011653328908163637,
      "loss": 0.0796,
      "step": 5493
    },
    {
      "epoch": 1.3595644642415243,
      "grad_norm": 0.04505692422389984,
      "learning_rate": 0.00011645102614998174,
      "loss": 0.0441,
      "step": 5494
    },
    {
      "epoch": 1.3598119277406582,
      "grad_norm": 0.09623260051012039,
      "learning_rate": 0.0001163687834474936,
      "loss": 0.1577,
      "step": 5495
    },
    {
      "epoch": 1.3600593912397922,
      "grad_norm": 0.07158873230218887,
      "learning_rate": 0.00011628656098662969,
      "loss": 0.121,
      "step": 5496
    },
    {
      "epoch": 1.3603068547389259,
      "grad_norm": 0.05433182418346405,
      "learning_rate": 0.0001162043587798442,
      "loss": 0.0519,
      "step": 5497
    },
    {
      "epoch": 1.36055431823806,
      "grad_norm": 0.05133894458413124,
      "learning_rate": 0.00011612217683958895,
      "loss": 0.0441,
      "step": 5498
    },
    {
      "epoch": 1.3608017817371938,
      "grad_norm": 0.05904371291399002,
      "learning_rate": 0.00011604001517831234,
      "loss": 0.0493,
      "step": 5499
    },
    {
      "epoch": 1.3610492452363276,
      "grad_norm": 0.04706443473696709,
      "learning_rate": 0.00011595787380845948,
      "loss": 0.0499,
      "step": 5500
    },
    {
      "epoch": 1.3612967087354615,
      "grad_norm": 0.02534019574522972,
      "learning_rate": 0.0001158757527424728,
      "loss": 0.0331,
      "step": 5501
    },
    {
      "epoch": 1.3615441722345953,
      "grad_norm": 0.09352700412273407,
      "learning_rate": 0.00011579365199279143,
      "loss": 0.1126,
      "step": 5502
    },
    {
      "epoch": 1.3617916357337294,
      "grad_norm": 0.029903758317232132,
      "learning_rate": 0.0001157115715718515,
      "loss": 0.0223,
      "step": 5503
    },
    {
      "epoch": 1.362039099232863,
      "grad_norm": 0.07051034271717072,
      "learning_rate": 0.00011562951149208611,
      "loss": 0.0457,
      "step": 5504
    },
    {
      "epoch": 1.362286562731997,
      "grad_norm": 0.07188842445611954,
      "learning_rate": 0.00011554747176592497,
      "loss": 0.0598,
      "step": 5505
    },
    {
      "epoch": 1.362534026231131,
      "grad_norm": 0.03590578958392143,
      "learning_rate": 0.00011546545240579529,
      "loss": 0.0467,
      "step": 5506
    },
    {
      "epoch": 1.3627814897302648,
      "grad_norm": 0.07118403911590576,
      "learning_rate": 0.0001153834534241206,
      "loss": 0.0927,
      "step": 5507
    },
    {
      "epoch": 1.3630289532293987,
      "grad_norm": 0.05449016019701958,
      "learning_rate": 0.00011530147483332165,
      "loss": 0.0511,
      "step": 5508
    },
    {
      "epoch": 1.3632764167285325,
      "grad_norm": 0.12075117230415344,
      "learning_rate": 0.00011521951664581606,
      "loss": 0.0726,
      "step": 5509
    },
    {
      "epoch": 1.3635238802276664,
      "grad_norm": 0.04714864492416382,
      "learning_rate": 0.00011513757887401837,
      "loss": 0.0543,
      "step": 5510
    },
    {
      "epoch": 1.3637713437268002,
      "grad_norm": 0.06784572452306747,
      "learning_rate": 0.00011505566153034008,
      "loss": 0.0303,
      "step": 5511
    },
    {
      "epoch": 1.3640188072259343,
      "grad_norm": 0.05993787199258804,
      "learning_rate": 0.00011497376462718931,
      "loss": 0.0785,
      "step": 5512
    },
    {
      "epoch": 1.3642662707250681,
      "grad_norm": 0.08404140919446945,
      "learning_rate": 0.00011489188817697132,
      "loss": 0.1152,
      "step": 5513
    },
    {
      "epoch": 1.364513734224202,
      "grad_norm": 0.031552329659461975,
      "learning_rate": 0.00011481003219208852,
      "loss": 0.0253,
      "step": 5514
    },
    {
      "epoch": 1.3647611977233358,
      "grad_norm": 0.038275182247161865,
      "learning_rate": 0.00011472819668493967,
      "loss": 0.0475,
      "step": 5515
    },
    {
      "epoch": 1.3650086612224697,
      "grad_norm": 0.05082262307405472,
      "learning_rate": 0.00011464638166792085,
      "loss": 0.05,
      "step": 5516
    },
    {
      "epoch": 1.3652561247216035,
      "grad_norm": 0.03428882360458374,
      "learning_rate": 0.00011456458715342466,
      "loss": 0.0146,
      "step": 5517
    },
    {
      "epoch": 1.3655035882207374,
      "grad_norm": 0.0499894842505455,
      "learning_rate": 0.0001144828131538411,
      "loss": 0.0497,
      "step": 5518
    },
    {
      "epoch": 1.3657510517198714,
      "grad_norm": 0.04824523627758026,
      "learning_rate": 0.00011440105968155675,
      "loss": 0.0543,
      "step": 5519
    },
    {
      "epoch": 1.365998515219005,
      "grad_norm": 0.036681707948446274,
      "learning_rate": 0.00011431932674895496,
      "loss": 0.0474,
      "step": 5520
    },
    {
      "epoch": 1.3662459787181391,
      "grad_norm": 0.04890848323702812,
      "learning_rate": 0.0001142376143684162,
      "loss": 0.0519,
      "step": 5521
    },
    {
      "epoch": 1.366493442217273,
      "grad_norm": 0.0881117433309555,
      "learning_rate": 0.00011415592255231778,
      "loss": 0.0735,
      "step": 5522
    },
    {
      "epoch": 1.3667409057164068,
      "grad_norm": 0.06510563939809799,
      "learning_rate": 0.00011407425131303381,
      "loss": 0.07,
      "step": 5523
    },
    {
      "epoch": 1.3669883692155407,
      "grad_norm": 0.06630337238311768,
      "learning_rate": 0.00011399260066293549,
      "loss": 0.0666,
      "step": 5524
    },
    {
      "epoch": 1.3672358327146745,
      "grad_norm": 0.06405999511480331,
      "learning_rate": 0.00011391097061439045,
      "loss": 0.0602,
      "step": 5525
    },
    {
      "epoch": 1.3674832962138086,
      "grad_norm": 0.07437527179718018,
      "learning_rate": 0.0001138293611797639,
      "loss": 0.0424,
      "step": 5526
    },
    {
      "epoch": 1.3677307597129422,
      "grad_norm": 0.07616601884365082,
      "learning_rate": 0.0001137477723714172,
      "loss": 0.0758,
      "step": 5527
    },
    {
      "epoch": 1.3679782232120763,
      "grad_norm": 0.05338618904352188,
      "learning_rate": 0.00011366620420170903,
      "loss": 0.0511,
      "step": 5528
    },
    {
      "epoch": 1.3682256867112101,
      "grad_norm": 0.056674011051654816,
      "learning_rate": 0.00011358465668299486,
      "loss": 0.041,
      "step": 5529
    },
    {
      "epoch": 1.368473150210344,
      "grad_norm": 0.04302550479769707,
      "learning_rate": 0.00011350312982762695,
      "loss": 0.046,
      "step": 5530
    },
    {
      "epoch": 1.3687206137094778,
      "grad_norm": 0.03976612538099289,
      "learning_rate": 0.0001134216236479546,
      "loss": 0.0482,
      "step": 5531
    },
    {
      "epoch": 1.3689680772086117,
      "grad_norm": 0.03146706894040108,
      "learning_rate": 0.00011334013815632368,
      "loss": 0.0339,
      "step": 5532
    },
    {
      "epoch": 1.3692155407077455,
      "grad_norm": 0.044508811086416245,
      "learning_rate": 0.00011325867336507708,
      "loss": 0.0652,
      "step": 5533
    },
    {
      "epoch": 1.3694630042068794,
      "grad_norm": 0.06709528714418411,
      "learning_rate": 0.00011317722928655489,
      "loss": 0.0452,
      "step": 5534
    },
    {
      "epoch": 1.3697104677060135,
      "grad_norm": 0.03966735675930977,
      "learning_rate": 0.00011309580593309343,
      "loss": 0.0318,
      "step": 5535
    },
    {
      "epoch": 1.3699579312051473,
      "grad_norm": 0.033639755100011826,
      "learning_rate": 0.00011301440331702634,
      "loss": 0.028,
      "step": 5536
    },
    {
      "epoch": 1.3702053947042812,
      "grad_norm": 0.03433779627084732,
      "learning_rate": 0.00011293302145068393,
      "loss": 0.0286,
      "step": 5537
    },
    {
      "epoch": 1.370452858203415,
      "grad_norm": 0.025452321395277977,
      "learning_rate": 0.00011285166034639346,
      "loss": 0.0277,
      "step": 5538
    },
    {
      "epoch": 1.3707003217025489,
      "grad_norm": 0.021286310628056526,
      "learning_rate": 0.0001127703200164791,
      "loss": 0.0177,
      "step": 5539
    },
    {
      "epoch": 1.3709477852016827,
      "grad_norm": 0.06382181495428085,
      "learning_rate": 0.00011268900047326156,
      "loss": 0.0619,
      "step": 5540
    },
    {
      "epoch": 1.3711952487008165,
      "grad_norm": 0.045570455491542816,
      "learning_rate": 0.00011260770172905873,
      "loss": 0.0554,
      "step": 5541
    },
    {
      "epoch": 1.3714427121999506,
      "grad_norm": 0.057115864008665085,
      "learning_rate": 0.00011252642379618522,
      "loss": 0.0494,
      "step": 5542
    },
    {
      "epoch": 1.3716901756990842,
      "grad_norm": 0.052614714950323105,
      "learning_rate": 0.00011244516668695254,
      "loss": 0.0249,
      "step": 5543
    },
    {
      "epoch": 1.3719376391982183,
      "grad_norm": 0.04311619699001312,
      "learning_rate": 0.00011236393041366897,
      "loss": 0.0461,
      "step": 5544
    },
    {
      "epoch": 1.3721851026973522,
      "grad_norm": 0.04191598296165466,
      "learning_rate": 0.00011228271498863968,
      "loss": 0.0433,
      "step": 5545
    },
    {
      "epoch": 1.372432566196486,
      "grad_norm": 0.1563056856393814,
      "learning_rate": 0.00011220152042416682,
      "loss": 0.0421,
      "step": 5546
    },
    {
      "epoch": 1.3726800296956199,
      "grad_norm": 0.07424064725637436,
      "learning_rate": 0.00011212034673254898,
      "loss": 0.0945,
      "step": 5547
    },
    {
      "epoch": 1.3729274931947537,
      "grad_norm": 0.08033425360918045,
      "learning_rate": 0.00011203919392608197,
      "loss": 0.0739,
      "step": 5548
    },
    {
      "epoch": 1.3731749566938878,
      "grad_norm": 0.05504868924617767,
      "learning_rate": 0.00011195806201705833,
      "loss": 0.0512,
      "step": 5549
    },
    {
      "epoch": 1.3734224201930214,
      "grad_norm": 0.030402295291423798,
      "learning_rate": 0.00011187695101776737,
      "loss": 0.0336,
      "step": 5550
    },
    {
      "epoch": 1.3736698836921555,
      "grad_norm": 0.033077143132686615,
      "learning_rate": 0.00011179586094049543,
      "loss": 0.0291,
      "step": 5551
    },
    {
      "epoch": 1.3739173471912893,
      "grad_norm": 0.057796601206064224,
      "learning_rate": 0.00011171479179752522,
      "loss": 0.0713,
      "step": 5552
    },
    {
      "epoch": 1.3741648106904232,
      "grad_norm": 0.037916913628578186,
      "learning_rate": 0.00011163374360113685,
      "loss": 0.0692,
      "step": 5553
    },
    {
      "epoch": 1.374412274189557,
      "grad_norm": 0.07523947954177856,
      "learning_rate": 0.00011155271636360703,
      "loss": 0.0411,
      "step": 5554
    },
    {
      "epoch": 1.3746597376886909,
      "grad_norm": 0.030364127829670906,
      "learning_rate": 0.00011147171009720905,
      "loss": 0.0363,
      "step": 5555
    },
    {
      "epoch": 1.3749072011878247,
      "grad_norm": 0.0367317833006382,
      "learning_rate": 0.00011139072481421333,
      "loss": 0.0387,
      "step": 5556
    },
    {
      "epoch": 1.3751546646869586,
      "grad_norm": 0.04281168803572655,
      "learning_rate": 0.00011130976052688705,
      "loss": 0.0667,
      "step": 5557
    },
    {
      "epoch": 1.3754021281860926,
      "grad_norm": 0.046225406229496,
      "learning_rate": 0.00011122881724749412,
      "loss": 0.0639,
      "step": 5558
    },
    {
      "epoch": 1.3756495916852265,
      "grad_norm": 0.032448552548885345,
      "learning_rate": 0.00011114789498829544,
      "loss": 0.0366,
      "step": 5559
    },
    {
      "epoch": 1.3758970551843603,
      "grad_norm": 0.08332482725381851,
      "learning_rate": 0.00011106699376154836,
      "loss": 0.1042,
      "step": 5560
    },
    {
      "epoch": 1.3761445186834942,
      "grad_norm": 0.05919297784566879,
      "learning_rate": 0.00011098611357950764,
      "loss": 0.0634,
      "step": 5561
    },
    {
      "epoch": 1.376391982182628,
      "grad_norm": 0.04425179958343506,
      "learning_rate": 0.00011090525445442423,
      "loss": 0.0289,
      "step": 5562
    },
    {
      "epoch": 1.3766394456817619,
      "grad_norm": 0.05722076818346977,
      "learning_rate": 0.00011082441639854624,
      "loss": 0.0624,
      "step": 5563
    },
    {
      "epoch": 1.3768869091808957,
      "grad_norm": 0.07742419838905334,
      "learning_rate": 0.00011074359942411854,
      "loss": 0.0469,
      "step": 5564
    },
    {
      "epoch": 1.3771343726800298,
      "grad_norm": 0.055331144481897354,
      "learning_rate": 0.00011066280354338278,
      "loss": 0.0532,
      "step": 5565
    },
    {
      "epoch": 1.3773818361791637,
      "grad_norm": 0.06077628210186958,
      "learning_rate": 0.0001105820287685775,
      "loss": 0.0888,
      "step": 5566
    },
    {
      "epoch": 1.3776292996782975,
      "grad_norm": 0.04264684394001961,
      "learning_rate": 0.00011050127511193778,
      "loss": 0.0423,
      "step": 5567
    },
    {
      "epoch": 1.3778767631774314,
      "grad_norm": 0.08783278614282608,
      "learning_rate": 0.0001104205425856957,
      "loss": 0.0725,
      "step": 5568
    },
    {
      "epoch": 1.3781242266765652,
      "grad_norm": 0.05192285031080246,
      "learning_rate": 0.00011033983120208035,
      "loss": 0.0479,
      "step": 5569
    },
    {
      "epoch": 1.378371690175699,
      "grad_norm": 0.03240986913442612,
      "learning_rate": 0.00011025914097331716,
      "loss": 0.0425,
      "step": 5570
    },
    {
      "epoch": 1.378619153674833,
      "grad_norm": 0.03948235511779785,
      "learning_rate": 0.00011017847191162863,
      "loss": 0.0614,
      "step": 5571
    },
    {
      "epoch": 1.378866617173967,
      "grad_norm": 0.031152335926890373,
      "learning_rate": 0.00011009782402923405,
      "loss": 0.0343,
      "step": 5572
    },
    {
      "epoch": 1.3791140806731006,
      "grad_norm": 0.036060452461242676,
      "learning_rate": 0.00011001719733834941,
      "loss": 0.04,
      "step": 5573
    },
    {
      "epoch": 1.3793615441722347,
      "grad_norm": 0.07652930170297623,
      "learning_rate": 0.00010993659185118767,
      "loss": 0.0458,
      "step": 5574
    },
    {
      "epoch": 1.3796090076713685,
      "grad_norm": 0.05369177460670471,
      "learning_rate": 0.00010985600757995825,
      "loss": 0.0586,
      "step": 5575
    },
    {
      "epoch": 1.3798564711705024,
      "grad_norm": 0.1038048267364502,
      "learning_rate": 0.00010977544453686767,
      "loss": 0.0264,
      "step": 5576
    },
    {
      "epoch": 1.3801039346696362,
      "grad_norm": 0.11060452461242676,
      "learning_rate": 0.00010969490273411909,
      "loss": 0.0693,
      "step": 5577
    },
    {
      "epoch": 1.38035139816877,
      "grad_norm": 0.06318024545907974,
      "learning_rate": 0.00010961438218391249,
      "loss": 0.0504,
      "step": 5578
    },
    {
      "epoch": 1.380598861667904,
      "grad_norm": 0.07618363946676254,
      "learning_rate": 0.0001095338828984446,
      "loss": 0.0852,
      "step": 5579
    },
    {
      "epoch": 1.3808463251670378,
      "grad_norm": 0.03894498944282532,
      "learning_rate": 0.00010945340488990896,
      "loss": 0.0424,
      "step": 5580
    },
    {
      "epoch": 1.3810937886661718,
      "grad_norm": 0.04295189306139946,
      "learning_rate": 0.00010937294817049598,
      "loss": 0.0336,
      "step": 5581
    },
    {
      "epoch": 1.3813412521653057,
      "grad_norm": 0.05330217629671097,
      "learning_rate": 0.00010929251275239257,
      "loss": 0.0486,
      "step": 5582
    },
    {
      "epoch": 1.3815887156644395,
      "grad_norm": 0.1253030151128769,
      "learning_rate": 0.00010921209864778268,
      "loss": 0.1129,
      "step": 5583
    },
    {
      "epoch": 1.3818361791635734,
      "grad_norm": 0.04543597251176834,
      "learning_rate": 0.00010913170586884688,
      "loss": 0.0561,
      "step": 5584
    },
    {
      "epoch": 1.3820836426627072,
      "grad_norm": 0.04338479042053223,
      "learning_rate": 0.00010905133442776263,
      "loss": 0.0385,
      "step": 5585
    },
    {
      "epoch": 1.382331106161841,
      "grad_norm": 0.038171932101249695,
      "learning_rate": 0.00010897098433670416,
      "loss": 0.0273,
      "step": 5586
    },
    {
      "epoch": 1.382578569660975,
      "grad_norm": 0.05148886889219284,
      "learning_rate": 0.00010889065560784211,
      "loss": 0.0202,
      "step": 5587
    },
    {
      "epoch": 1.382826033160109,
      "grad_norm": 0.0365670882165432,
      "learning_rate": 0.00010881034825334452,
      "loss": 0.0398,
      "step": 5588
    },
    {
      "epoch": 1.3830734966592428,
      "grad_norm": 0.06564709544181824,
      "learning_rate": 0.00010873006228537577,
      "loss": 0.0826,
      "step": 5589
    },
    {
      "epoch": 1.3833209601583767,
      "grad_norm": 0.0924377366900444,
      "learning_rate": 0.00010864979771609692,
      "loss": 0.0766,
      "step": 5590
    },
    {
      "epoch": 1.3835684236575105,
      "grad_norm": 0.06109151244163513,
      "learning_rate": 0.00010856955455766603,
      "loss": 0.0714,
      "step": 5591
    },
    {
      "epoch": 1.3838158871566444,
      "grad_norm": 0.04748315364122391,
      "learning_rate": 0.00010848933282223788,
      "loss": 0.0614,
      "step": 5592
    },
    {
      "epoch": 1.3840633506557782,
      "grad_norm": 0.0651017278432846,
      "learning_rate": 0.00010840913252196391,
      "loss": 0.072,
      "step": 5593
    },
    {
      "epoch": 1.384310814154912,
      "grad_norm": 0.04606561362743378,
      "learning_rate": 0.00010832895366899249,
      "loss": 0.0515,
      "step": 5594
    },
    {
      "epoch": 1.3845582776540462,
      "grad_norm": 0.046366166323423386,
      "learning_rate": 0.00010824879627546828,
      "loss": 0.0297,
      "step": 5595
    },
    {
      "epoch": 1.3848057411531798,
      "grad_norm": 0.02364780195057392,
      "learning_rate": 0.00010816866035353345,
      "loss": 0.0358,
      "step": 5596
    },
    {
      "epoch": 1.3850532046523139,
      "grad_norm": 0.08364793658256531,
      "learning_rate": 0.00010808854591532618,
      "loss": 0.0815,
      "step": 5597
    },
    {
      "epoch": 1.3853006681514477,
      "grad_norm": 0.03283119946718216,
      "learning_rate": 0.00010800845297298181,
      "loss": 0.0391,
      "step": 5598
    },
    {
      "epoch": 1.3855481316505815,
      "grad_norm": 0.03036554716527462,
      "learning_rate": 0.00010792838153863233,
      "loss": 0.0371,
      "step": 5599
    },
    {
      "epoch": 1.3857955951497154,
      "grad_norm": 0.0319659523665905,
      "learning_rate": 0.00010784833162440643,
      "loss": 0.0494,
      "step": 5600
    },
    {
      "epoch": 1.3857955951497154,
      "eval_loss": 0.28987136483192444,
      "eval_runtime": 202.535,
      "eval_samples_per_second": 4.937,
      "eval_steps_per_second": 0.311,
      "step": 5600
    },
    {
      "epoch": 1.3860430586488492,
      "grad_norm": 0.03577957674860954,
      "learning_rate": 0.0001077683032424297,
      "loss": 0.0331,
      "step": 5601
    },
    {
      "epoch": 1.386290522147983,
      "grad_norm": 0.0598144456744194,
      "learning_rate": 0.00010768829640482414,
      "loss": 0.037,
      "step": 5602
    },
    {
      "epoch": 1.386537985647117,
      "grad_norm": 0.044585149735212326,
      "learning_rate": 0.00010760831112370867,
      "loss": 0.0396,
      "step": 5603
    },
    {
      "epoch": 1.386785449146251,
      "grad_norm": 0.037921834737062454,
      "learning_rate": 0.00010752834741119927,
      "loss": 0.0466,
      "step": 5604
    },
    {
      "epoch": 1.3870329126453849,
      "grad_norm": 0.05414198711514473,
      "learning_rate": 0.00010744840527940805,
      "loss": 0.0417,
      "step": 5605
    },
    {
      "epoch": 1.3872803761445187,
      "grad_norm": 0.04092556610703468,
      "learning_rate": 0.00010736848474044436,
      "loss": 0.0442,
      "step": 5606
    },
    {
      "epoch": 1.3875278396436526,
      "grad_norm": 0.05824263021349907,
      "learning_rate": 0.00010728858580641373,
      "loss": 0.0753,
      "step": 5607
    },
    {
      "epoch": 1.3877753031427864,
      "grad_norm": 0.03307591378688812,
      "learning_rate": 0.0001072087084894191,
      "loss": 0.0378,
      "step": 5608
    },
    {
      "epoch": 1.3880227666419203,
      "grad_norm": 0.04557172581553459,
      "learning_rate": 0.00010712885280155974,
      "loss": 0.0542,
      "step": 5609
    },
    {
      "epoch": 1.388270230141054,
      "grad_norm": 0.045296214520931244,
      "learning_rate": 0.0001070490187549315,
      "loss": 0.0203,
      "step": 5610
    },
    {
      "epoch": 1.3885176936401882,
      "grad_norm": 0.1044088751077652,
      "learning_rate": 0.00010696920636162727,
      "loss": 0.0628,
      "step": 5611
    },
    {
      "epoch": 1.388765157139322,
      "grad_norm": 0.04291404411196709,
      "learning_rate": 0.00010688941563373652,
      "loss": 0.0378,
      "step": 5612
    },
    {
      "epoch": 1.3890126206384559,
      "grad_norm": 0.037347983568906784,
      "learning_rate": 0.00010680964658334547,
      "loss": 0.0428,
      "step": 5613
    },
    {
      "epoch": 1.3892600841375897,
      "grad_norm": 0.04260803386569023,
      "learning_rate": 0.00010672989922253709,
      "loss": 0.0449,
      "step": 5614
    },
    {
      "epoch": 1.3895075476367236,
      "grad_norm": 0.1357743889093399,
      "learning_rate": 0.00010665017356339077,
      "loss": 0.066,
      "step": 5615
    },
    {
      "epoch": 1.3897550111358574,
      "grad_norm": 0.05352574586868286,
      "learning_rate": 0.00010657046961798319,
      "loss": 0.0368,
      "step": 5616
    },
    {
      "epoch": 1.3900024746349913,
      "grad_norm": 0.026954708620905876,
      "learning_rate": 0.00010649078739838719,
      "loss": 0.0266,
      "step": 5617
    },
    {
      "epoch": 1.3902499381341253,
      "grad_norm": 0.055388517677783966,
      "learning_rate": 0.00010641112691667257,
      "loss": 0.0701,
      "step": 5618
    },
    {
      "epoch": 1.390497401633259,
      "grad_norm": 0.06057140231132507,
      "learning_rate": 0.00010633148818490584,
      "loss": 0.0463,
      "step": 5619
    },
    {
      "epoch": 1.390744865132393,
      "grad_norm": 0.08072030544281006,
      "learning_rate": 0.00010625187121515018,
      "loss": 0.0711,
      "step": 5620
    },
    {
      "epoch": 1.3909923286315269,
      "grad_norm": 0.04990595951676369,
      "learning_rate": 0.00010617227601946553,
      "loss": 0.0816,
      "step": 5621
    },
    {
      "epoch": 1.3912397921306607,
      "grad_norm": 0.061036400496959686,
      "learning_rate": 0.00010609270260990831,
      "loss": 0.0344,
      "step": 5622
    },
    {
      "epoch": 1.3914872556297946,
      "grad_norm": 0.06123458221554756,
      "learning_rate": 0.00010601315099853181,
      "loss": 0.0311,
      "step": 5623
    },
    {
      "epoch": 1.3917347191289284,
      "grad_norm": 0.034947387874126434,
      "learning_rate": 0.00010593362119738629,
      "loss": 0.0387,
      "step": 5624
    },
    {
      "epoch": 1.3919821826280623,
      "grad_norm": 0.07271230965852737,
      "learning_rate": 0.00010585411321851814,
      "loss": 0.0629,
      "step": 5625
    },
    {
      "epoch": 1.3922296461271961,
      "grad_norm": 0.040757372975349426,
      "learning_rate": 0.00010577462707397084,
      "loss": 0.0652,
      "step": 5626
    },
    {
      "epoch": 1.3924771096263302,
      "grad_norm": 0.042200371623039246,
      "learning_rate": 0.00010569516277578445,
      "loss": 0.0587,
      "step": 5627
    },
    {
      "epoch": 1.392724573125464,
      "grad_norm": 0.04420427605509758,
      "learning_rate": 0.0001056157203359957,
      "loss": 0.0476,
      "step": 5628
    },
    {
      "epoch": 1.392972036624598,
      "grad_norm": 0.07815005630254745,
      "learning_rate": 0.00010553629976663818,
      "loss": 0.0869,
      "step": 5629
    },
    {
      "epoch": 1.3932195001237317,
      "grad_norm": 0.034557074308395386,
      "learning_rate": 0.00010545690107974182,
      "loss": 0.0335,
      "step": 5630
    },
    {
      "epoch": 1.3934669636228656,
      "grad_norm": 0.03300250694155693,
      "learning_rate": 0.00010537752428733341,
      "loss": 0.0335,
      "step": 5631
    },
    {
      "epoch": 1.3937144271219994,
      "grad_norm": 0.04091711342334747,
      "learning_rate": 0.00010529816940143675,
      "loss": 0.0327,
      "step": 5632
    },
    {
      "epoch": 1.3939618906211333,
      "grad_norm": 0.037600088864564896,
      "learning_rate": 0.00010521883643407174,
      "loss": 0.057,
      "step": 5633
    },
    {
      "epoch": 1.3942093541202674,
      "grad_norm": 0.05170384794473648,
      "learning_rate": 0.00010513952539725535,
      "loss": 0.0667,
      "step": 5634
    },
    {
      "epoch": 1.3944568176194012,
      "grad_norm": 0.04991934821009636,
      "learning_rate": 0.0001050602363030011,
      "loss": 0.0462,
      "step": 5635
    },
    {
      "epoch": 1.394704281118535,
      "grad_norm": 0.04273790121078491,
      "learning_rate": 0.00010498096916331923,
      "loss": 0.0511,
      "step": 5636
    },
    {
      "epoch": 1.394951744617669,
      "grad_norm": 0.04181879758834839,
      "learning_rate": 0.00010490172399021671,
      "loss": 0.0544,
      "step": 5637
    },
    {
      "epoch": 1.3951992081168028,
      "grad_norm": 0.04136990010738373,
      "learning_rate": 0.00010482250079569694,
      "loss": 0.0435,
      "step": 5638
    },
    {
      "epoch": 1.3954466716159366,
      "grad_norm": 0.04834943637251854,
      "learning_rate": 0.00010474329959176024,
      "loss": 0.0392,
      "step": 5639
    },
    {
      "epoch": 1.3956941351150705,
      "grad_norm": 0.05122128501534462,
      "learning_rate": 0.00010466412039040348,
      "loss": 0.1135,
      "step": 5640
    },
    {
      "epoch": 1.3959415986142045,
      "grad_norm": 0.04932200536131859,
      "learning_rate": 0.0001045849632036203,
      "loss": 0.0638,
      "step": 5641
    },
    {
      "epoch": 1.3961890621133382,
      "grad_norm": 0.051482174545526505,
      "learning_rate": 0.00010450582804340086,
      "loss": 0.0631,
      "step": 5642
    },
    {
      "epoch": 1.3964365256124722,
      "grad_norm": 0.09536559879779816,
      "learning_rate": 0.00010442671492173215,
      "loss": 0.0417,
      "step": 5643
    },
    {
      "epoch": 1.396683989111606,
      "grad_norm": 0.041668228805065155,
      "learning_rate": 0.00010434762385059774,
      "loss": 0.0427,
      "step": 5644
    },
    {
      "epoch": 1.39693145261074,
      "grad_norm": 0.04308345168828964,
      "learning_rate": 0.0001042685548419777,
      "loss": 0.0594,
      "step": 5645
    },
    {
      "epoch": 1.3971789161098738,
      "grad_norm": 0.06109192967414856,
      "learning_rate": 0.00010418950790784901,
      "loss": 0.0625,
      "step": 5646
    },
    {
      "epoch": 1.3974263796090076,
      "grad_norm": 0.07346806675195694,
      "learning_rate": 0.0001041104830601852,
      "loss": 0.0482,
      "step": 5647
    },
    {
      "epoch": 1.3976738431081415,
      "grad_norm": 0.07460900396108627,
      "learning_rate": 0.00010403148031095646,
      "loss": 0.0687,
      "step": 5648
    },
    {
      "epoch": 1.3979213066072753,
      "grad_norm": 0.06298752874135971,
      "learning_rate": 0.00010395249967212975,
      "loss": 0.051,
      "step": 5649
    },
    {
      "epoch": 1.3981687701064094,
      "grad_norm": 0.048497408628463745,
      "learning_rate": 0.00010387354115566828,
      "loss": 0.0564,
      "step": 5650
    },
    {
      "epoch": 1.3984162336055432,
      "grad_norm": 0.06286751478910446,
      "learning_rate": 0.00010379460477353242,
      "loss": 0.047,
      "step": 5651
    },
    {
      "epoch": 1.398663697104677,
      "grad_norm": 0.058625053614377975,
      "learning_rate": 0.00010371569053767902,
      "loss": 0.0554,
      "step": 5652
    },
    {
      "epoch": 1.398911160603811,
      "grad_norm": 0.031632471829652786,
      "learning_rate": 0.00010363679846006133,
      "loss": 0.0324,
      "step": 5653
    },
    {
      "epoch": 1.3991586241029448,
      "grad_norm": 0.07557020336389542,
      "learning_rate": 0.00010355792855262949,
      "loss": 0.0417,
      "step": 5654
    },
    {
      "epoch": 1.3994060876020786,
      "grad_norm": 0.03263535350561142,
      "learning_rate": 0.00010347908082733024,
      "loss": 0.0306,
      "step": 5655
    },
    {
      "epoch": 1.3996535511012125,
      "grad_norm": 0.039387285709381104,
      "learning_rate": 0.00010340025529610695,
      "loss": 0.0535,
      "step": 5656
    },
    {
      "epoch": 1.3999010146003465,
      "grad_norm": 0.07266858220100403,
      "learning_rate": 0.00010332145197089971,
      "loss": 0.084,
      "step": 5657
    },
    {
      "epoch": 1.4001484780994804,
      "grad_norm": 0.08914852887392044,
      "learning_rate": 0.00010324267086364487,
      "loss": 0.0417,
      "step": 5658
    },
    {
      "epoch": 1.4003959415986142,
      "grad_norm": 0.09260272234678268,
      "learning_rate": 0.00010316391198627606,
      "loss": 0.0669,
      "step": 5659
    },
    {
      "epoch": 1.400643405097748,
      "grad_norm": 0.04415550455451012,
      "learning_rate": 0.00010308517535072292,
      "loss": 0.0366,
      "step": 5660
    },
    {
      "epoch": 1.400890868596882,
      "grad_norm": 0.08831813186407089,
      "learning_rate": 0.00010300646096891209,
      "loss": 0.0537,
      "step": 5661
    },
    {
      "epoch": 1.4011383320960158,
      "grad_norm": 0.05599195137619972,
      "learning_rate": 0.00010292776885276673,
      "loss": 0.0375,
      "step": 5662
    },
    {
      "epoch": 1.4013857955951496,
      "grad_norm": 0.04217744246125221,
      "learning_rate": 0.00010284909901420661,
      "loss": 0.0442,
      "step": 5663
    },
    {
      "epoch": 1.4016332590942837,
      "grad_norm": 0.041783157736063004,
      "learning_rate": 0.00010277045146514825,
      "loss": 0.0456,
      "step": 5664
    },
    {
      "epoch": 1.4018807225934173,
      "grad_norm": 0.03834189847111702,
      "learning_rate": 0.0001026918262175045,
      "loss": 0.051,
      "step": 5665
    },
    {
      "epoch": 1.4021281860925514,
      "grad_norm": 0.058954790234565735,
      "learning_rate": 0.00010261322328318506,
      "loss": 0.0637,
      "step": 5666
    },
    {
      "epoch": 1.4023756495916853,
      "grad_norm": 0.061140675097703934,
      "learning_rate": 0.00010253464267409645,
      "loss": 0.06,
      "step": 5667
    },
    {
      "epoch": 1.402623113090819,
      "grad_norm": 0.050337113440036774,
      "learning_rate": 0.00010245608440214132,
      "loss": 0.071,
      "step": 5668
    },
    {
      "epoch": 1.402870576589953,
      "grad_norm": 0.039666712284088135,
      "learning_rate": 0.00010237754847921934,
      "loss": 0.0412,
      "step": 5669
    },
    {
      "epoch": 1.4031180400890868,
      "grad_norm": 0.07672358304262161,
      "learning_rate": 0.0001022990349172264,
      "loss": 0.0629,
      "step": 5670
    },
    {
      "epoch": 1.4033655035882209,
      "grad_norm": 0.030626917257905006,
      "learning_rate": 0.00010222054372805548,
      "loss": 0.0236,
      "step": 5671
    },
    {
      "epoch": 1.4036129670873545,
      "grad_norm": 0.04189913347363472,
      "learning_rate": 0.00010214207492359599,
      "loss": 0.0563,
      "step": 5672
    },
    {
      "epoch": 1.4038604305864886,
      "grad_norm": 0.09088270366191864,
      "learning_rate": 0.00010206362851573367,
      "loss": 0.0839,
      "step": 5673
    },
    {
      "epoch": 1.4041078940856224,
      "grad_norm": 0.05502702295780182,
      "learning_rate": 0.00010198520451635121,
      "loss": 0.0375,
      "step": 5674
    },
    {
      "epoch": 1.4043553575847563,
      "grad_norm": 0.04087952896952629,
      "learning_rate": 0.00010190680293732776,
      "loss": 0.0325,
      "step": 5675
    },
    {
      "epoch": 1.4046028210838901,
      "grad_norm": 0.02746904082596302,
      "learning_rate": 0.00010182842379053911,
      "loss": 0.0343,
      "step": 5676
    },
    {
      "epoch": 1.404850284583024,
      "grad_norm": 0.04790722206234932,
      "learning_rate": 0.00010175006708785778,
      "loss": 0.0568,
      "step": 5677
    },
    {
      "epoch": 1.4050977480821578,
      "grad_norm": 0.06785671412944794,
      "learning_rate": 0.00010167173284115241,
      "loss": 0.0419,
      "step": 5678
    },
    {
      "epoch": 1.4053452115812917,
      "grad_norm": 0.025680730119347572,
      "learning_rate": 0.00010159342106228903,
      "loss": 0.0294,
      "step": 5679
    },
    {
      "epoch": 1.4055926750804257,
      "grad_norm": 0.04209616407752037,
      "learning_rate": 0.0001015151317631295,
      "loss": 0.027,
      "step": 5680
    },
    {
      "epoch": 1.4058401385795596,
      "grad_norm": 0.046112239360809326,
      "learning_rate": 0.00010143686495553265,
      "loss": 0.0773,
      "step": 5681
    },
    {
      "epoch": 1.4060876020786934,
      "grad_norm": 0.03626177832484245,
      "learning_rate": 0.0001013586206513539,
      "loss": 0.0289,
      "step": 5682
    },
    {
      "epoch": 1.4063350655778273,
      "grad_norm": 0.046784158796072006,
      "learning_rate": 0.0001012803988624452,
      "loss": 0.04,
      "step": 5683
    },
    {
      "epoch": 1.4065825290769611,
      "grad_norm": 0.028416305780410767,
      "learning_rate": 0.00010120219960065519,
      "loss": 0.0425,
      "step": 5684
    },
    {
      "epoch": 1.406829992576095,
      "grad_norm": 0.031395625323057175,
      "learning_rate": 0.00010112402287782883,
      "loss": 0.0277,
      "step": 5685
    },
    {
      "epoch": 1.4070774560752288,
      "grad_norm": 0.06318426877260208,
      "learning_rate": 0.00010104586870580784,
      "loss": 0.0513,
      "step": 5686
    },
    {
      "epoch": 1.407324919574363,
      "grad_norm": 0.04580789804458618,
      "learning_rate": 0.00010096773709643078,
      "loss": 0.0545,
      "step": 5687
    },
    {
      "epoch": 1.4075723830734965,
      "grad_norm": 0.04932422563433647,
      "learning_rate": 0.0001008896280615323,
      "loss": 0.0575,
      "step": 5688
    },
    {
      "epoch": 1.4078198465726306,
      "grad_norm": 0.04894449934363365,
      "learning_rate": 0.00010081154161294393,
      "loss": 0.0567,
      "step": 5689
    },
    {
      "epoch": 1.4080673100717644,
      "grad_norm": 0.04879224672913551,
      "learning_rate": 0.00010073347776249372,
      "loss": 0.0517,
      "step": 5690
    },
    {
      "epoch": 1.4083147735708983,
      "grad_norm": 0.04575997218489647,
      "learning_rate": 0.00010065543652200632,
      "loss": 0.0177,
      "step": 5691
    },
    {
      "epoch": 1.4085622370700321,
      "grad_norm": 0.04142985865473747,
      "learning_rate": 0.000100577417903303,
      "loss": 0.0489,
      "step": 5692
    },
    {
      "epoch": 1.408809700569166,
      "grad_norm": 0.06417491286993027,
      "learning_rate": 0.00010049942191820139,
      "loss": 0.0589,
      "step": 5693
    },
    {
      "epoch": 1.4090571640683,
      "grad_norm": 0.12177037447690964,
      "learning_rate": 0.00010042144857851585,
      "loss": 0.0878,
      "step": 5694
    },
    {
      "epoch": 1.4093046275674337,
      "grad_norm": 0.04297288879752159,
      "learning_rate": 0.00010034349789605738,
      "loss": 0.042,
      "step": 5695
    },
    {
      "epoch": 1.4095520910665678,
      "grad_norm": 0.040779732167720795,
      "learning_rate": 0.00010026556988263341,
      "loss": 0.0498,
      "step": 5696
    },
    {
      "epoch": 1.4097995545657016,
      "grad_norm": 0.06056895852088928,
      "learning_rate": 0.00010018766455004801,
      "loss": 0.0729,
      "step": 5697
    },
    {
      "epoch": 1.4100470180648355,
      "grad_norm": 0.04280349239706993,
      "learning_rate": 0.00010010978191010181,
      "loss": 0.0349,
      "step": 5698
    },
    {
      "epoch": 1.4102944815639693,
      "grad_norm": 0.037837959825992584,
      "learning_rate": 0.00010003192197459204,
      "loss": 0.0325,
      "step": 5699
    },
    {
      "epoch": 1.4105419450631032,
      "grad_norm": 0.038654886186122894,
      "learning_rate": 9.995408475531232e-05,
      "loss": 0.0408,
      "step": 5700
    },
    {
      "epoch": 1.410789408562237,
      "grad_norm": 0.0488717295229435,
      "learning_rate": 9.987627026405294e-05,
      "loss": 0.057,
      "step": 5701
    },
    {
      "epoch": 1.4110368720613709,
      "grad_norm": 0.05438666790723801,
      "learning_rate": 9.979847851260082e-05,
      "loss": 0.0311,
      "step": 5702
    },
    {
      "epoch": 1.411284335560505,
      "grad_norm": 0.04285971820354462,
      "learning_rate": 9.972070951273937e-05,
      "loss": 0.0363,
      "step": 5703
    },
    {
      "epoch": 1.4115317990596388,
      "grad_norm": 0.028698351234197617,
      "learning_rate": 9.964296327624864e-05,
      "loss": 0.028,
      "step": 5704
    },
    {
      "epoch": 1.4117792625587726,
      "grad_norm": 0.06956777721643448,
      "learning_rate": 9.956523981490485e-05,
      "loss": 0.1081,
      "step": 5705
    },
    {
      "epoch": 1.4120267260579065,
      "grad_norm": 0.04196851700544357,
      "learning_rate": 9.948753914048137e-05,
      "loss": 0.0878,
      "step": 5706
    },
    {
      "epoch": 1.4122741895570403,
      "grad_norm": 0.031001416966319084,
      "learning_rate": 9.940986126474778e-05,
      "loss": 0.0275,
      "step": 5707
    },
    {
      "epoch": 1.4125216530561742,
      "grad_norm": 0.06506326794624329,
      "learning_rate": 9.933220619947006e-05,
      "loss": 0.0649,
      "step": 5708
    },
    {
      "epoch": 1.412769116555308,
      "grad_norm": 0.06320016831159592,
      "learning_rate": 9.925457395641105e-05,
      "loss": 0.131,
      "step": 5709
    },
    {
      "epoch": 1.413016580054442,
      "grad_norm": 0.06502219289541245,
      "learning_rate": 9.917696454732997e-05,
      "loss": 0.0571,
      "step": 5710
    },
    {
      "epoch": 1.4132640435535757,
      "grad_norm": 0.10448039323091507,
      "learning_rate": 9.909937798398261e-05,
      "loss": 0.0758,
      "step": 5711
    },
    {
      "epoch": 1.4135115070527098,
      "grad_norm": 0.042657528072595596,
      "learning_rate": 9.90218142781214e-05,
      "loss": 0.0246,
      "step": 5712
    },
    {
      "epoch": 1.4137589705518436,
      "grad_norm": 0.04528823122382164,
      "learning_rate": 9.894427344149493e-05,
      "loss": 0.0543,
      "step": 5713
    },
    {
      "epoch": 1.4140064340509775,
      "grad_norm": 0.03199978172779083,
      "learning_rate": 9.886675548584891e-05,
      "loss": 0.0301,
      "step": 5714
    },
    {
      "epoch": 1.4142538975501113,
      "grad_norm": 0.05061500146985054,
      "learning_rate": 9.87892604229251e-05,
      "loss": 0.059,
      "step": 5715
    },
    {
      "epoch": 1.4145013610492452,
      "grad_norm": 0.042138658463954926,
      "learning_rate": 9.871178826446203e-05,
      "loss": 0.0501,
      "step": 5716
    },
    {
      "epoch": 1.4147488245483792,
      "grad_norm": 0.04864427447319031,
      "learning_rate": 9.863433902219465e-05,
      "loss": 0.0434,
      "step": 5717
    },
    {
      "epoch": 1.4149962880475129,
      "grad_norm": 0.05533737689256668,
      "learning_rate": 9.855691270785455e-05,
      "loss": 0.0631,
      "step": 5718
    },
    {
      "epoch": 1.415243751546647,
      "grad_norm": 0.06296393275260925,
      "learning_rate": 9.847950933316984e-05,
      "loss": 0.0689,
      "step": 5719
    },
    {
      "epoch": 1.4154912150457808,
      "grad_norm": 0.043744053691625595,
      "learning_rate": 9.840212890986494e-05,
      "loss": 0.0534,
      "step": 5720
    },
    {
      "epoch": 1.4157386785449146,
      "grad_norm": 0.046508993953466415,
      "learning_rate": 9.832477144966095e-05,
      "loss": 0.0567,
      "step": 5721
    },
    {
      "epoch": 1.4159861420440485,
      "grad_norm": 0.11679413914680481,
      "learning_rate": 9.824743696427574e-05,
      "loss": 0.0677,
      "step": 5722
    },
    {
      "epoch": 1.4162336055431823,
      "grad_norm": 0.07248811423778534,
      "learning_rate": 9.817012546542322e-05,
      "loss": 0.0481,
      "step": 5723
    },
    {
      "epoch": 1.4164810690423162,
      "grad_norm": 0.06414492428302765,
      "learning_rate": 9.809283696481425e-05,
      "loss": 0.079,
      "step": 5724
    },
    {
      "epoch": 1.41672853254145,
      "grad_norm": 0.048319004476070404,
      "learning_rate": 9.801557147415568e-05,
      "loss": 0.0629,
      "step": 5725
    },
    {
      "epoch": 1.416975996040584,
      "grad_norm": 0.04277569428086281,
      "learning_rate": 9.793832900515152e-05,
      "loss": 0.0614,
      "step": 5726
    },
    {
      "epoch": 1.417223459539718,
      "grad_norm": 0.05377096310257912,
      "learning_rate": 9.786110956950195e-05,
      "loss": 0.0596,
      "step": 5727
    },
    {
      "epoch": 1.4174709230388518,
      "grad_norm": 0.06557311117649078,
      "learning_rate": 9.778391317890353e-05,
      "loss": 0.0535,
      "step": 5728
    },
    {
      "epoch": 1.4177183865379857,
      "grad_norm": 0.05854113772511482,
      "learning_rate": 9.770673984504961e-05,
      "loss": 0.0922,
      "step": 5729
    },
    {
      "epoch": 1.4179658500371195,
      "grad_norm": 0.045709602534770966,
      "learning_rate": 9.762958957962984e-05,
      "loss": 0.0624,
      "step": 5730
    },
    {
      "epoch": 1.4182133135362534,
      "grad_norm": 0.04584483802318573,
      "learning_rate": 9.755246239433052e-05,
      "loss": 0.0438,
      "step": 5731
    },
    {
      "epoch": 1.4184607770353872,
      "grad_norm": 0.054667022079229355,
      "learning_rate": 9.747535830083452e-05,
      "loss": 0.0648,
      "step": 5732
    },
    {
      "epoch": 1.4187082405345213,
      "grad_norm": 0.0498664453625679,
      "learning_rate": 9.739827731082071e-05,
      "loss": 0.0386,
      "step": 5733
    },
    {
      "epoch": 1.4189557040336551,
      "grad_norm": 0.0467672161757946,
      "learning_rate": 9.732121943596531e-05,
      "loss": 0.0583,
      "step": 5734
    },
    {
      "epoch": 1.419203167532789,
      "grad_norm": 0.021370550617575645,
      "learning_rate": 9.724418468794021e-05,
      "loss": 0.0271,
      "step": 5735
    },
    {
      "epoch": 1.4194506310319228,
      "grad_norm": 0.041222941130399704,
      "learning_rate": 9.716717307841433e-05,
      "loss": 0.0588,
      "step": 5736
    },
    {
      "epoch": 1.4196980945310567,
      "grad_norm": 0.03168243169784546,
      "learning_rate": 9.709018461905284e-05,
      "loss": 0.0313,
      "step": 5737
    },
    {
      "epoch": 1.4199455580301905,
      "grad_norm": 0.046953752636909485,
      "learning_rate": 9.701321932151746e-05,
      "loss": 0.0455,
      "step": 5738
    },
    {
      "epoch": 1.4201930215293244,
      "grad_norm": 0.08350121974945068,
      "learning_rate": 9.69362771974666e-05,
      "loss": 0.0469,
      "step": 5739
    },
    {
      "epoch": 1.4204404850284584,
      "grad_norm": 0.02454843930900097,
      "learning_rate": 9.685935825855469e-05,
      "loss": 0.0261,
      "step": 5740
    },
    {
      "epoch": 1.420687948527592,
      "grad_norm": 0.07126964628696442,
      "learning_rate": 9.678246251643297e-05,
      "loss": 0.0498,
      "step": 5741
    },
    {
      "epoch": 1.4209354120267261,
      "grad_norm": 0.033327408134937286,
      "learning_rate": 9.67055899827494e-05,
      "loss": 0.0555,
      "step": 5742
    },
    {
      "epoch": 1.42118287552586,
      "grad_norm": 0.04428049176931381,
      "learning_rate": 9.662874066914789e-05,
      "loss": 0.0483,
      "step": 5743
    },
    {
      "epoch": 1.4214303390249938,
      "grad_norm": 0.02357470989227295,
      "learning_rate": 9.655191458726919e-05,
      "loss": 0.0313,
      "step": 5744
    },
    {
      "epoch": 1.4216778025241277,
      "grad_norm": 0.032139308750629425,
      "learning_rate": 9.647511174875043e-05,
      "loss": 0.0276,
      "step": 5745
    },
    {
      "epoch": 1.4219252660232615,
      "grad_norm": 0.08776938915252686,
      "learning_rate": 9.63983321652252e-05,
      "loss": 0.068,
      "step": 5746
    },
    {
      "epoch": 1.4221727295223954,
      "grad_norm": 0.03362530842423439,
      "learning_rate": 9.632157584832368e-05,
      "loss": 0.0298,
      "step": 5747
    },
    {
      "epoch": 1.4224201930215292,
      "grad_norm": 0.0696205273270607,
      "learning_rate": 9.624484280967232e-05,
      "loss": 0.0801,
      "step": 5748
    },
    {
      "epoch": 1.4226676565206633,
      "grad_norm": 0.04173947870731354,
      "learning_rate": 9.616813306089411e-05,
      "loss": 0.0262,
      "step": 5749
    },
    {
      "epoch": 1.4229151200197971,
      "grad_norm": 0.04830100014805794,
      "learning_rate": 9.609144661360886e-05,
      "loss": 0.0615,
      "step": 5750
    },
    {
      "epoch": 1.423162583518931,
      "grad_norm": 0.09950267523527145,
      "learning_rate": 9.601478347943227e-05,
      "loss": 0.0537,
      "step": 5751
    },
    {
      "epoch": 1.4234100470180648,
      "grad_norm": 0.0452725887298584,
      "learning_rate": 9.593814366997689e-05,
      "loss": 0.0472,
      "step": 5752
    },
    {
      "epoch": 1.4236575105171987,
      "grad_norm": 0.031396377831697464,
      "learning_rate": 9.586152719685163e-05,
      "loss": 0.0367,
      "step": 5753
    },
    {
      "epoch": 1.4239049740163325,
      "grad_norm": 0.05622806027531624,
      "learning_rate": 9.578493407166186e-05,
      "loss": 0.0757,
      "step": 5754
    },
    {
      "epoch": 1.4241524375154664,
      "grad_norm": 0.05265364050865173,
      "learning_rate": 9.570836430600957e-05,
      "loss": 0.0709,
      "step": 5755
    },
    {
      "epoch": 1.4243999010146005,
      "grad_norm": 0.051300834864377975,
      "learning_rate": 9.563181791149287e-05,
      "loss": 0.0442,
      "step": 5756
    },
    {
      "epoch": 1.4246473645137343,
      "grad_norm": 0.04406313970685005,
      "learning_rate": 9.55552948997066e-05,
      "loss": 0.0553,
      "step": 5757
    },
    {
      "epoch": 1.4248948280128682,
      "grad_norm": 0.03934081643819809,
      "learning_rate": 9.547879528224205e-05,
      "loss": 0.0295,
      "step": 5758
    },
    {
      "epoch": 1.425142291512002,
      "grad_norm": 0.040686942636966705,
      "learning_rate": 9.54023190706868e-05,
      "loss": 0.039,
      "step": 5759
    },
    {
      "epoch": 1.4253897550111359,
      "grad_norm": 0.0350935161113739,
      "learning_rate": 9.53258662766251e-05,
      "loss": 0.0384,
      "step": 5760
    },
    {
      "epoch": 1.4256372185102697,
      "grad_norm": 0.06657138466835022,
      "learning_rate": 9.524943691163745e-05,
      "loss": 0.0454,
      "step": 5761
    },
    {
      "epoch": 1.4258846820094035,
      "grad_norm": 0.12277901917695999,
      "learning_rate": 9.517303098730107e-05,
      "loss": 0.3762,
      "step": 5762
    },
    {
      "epoch": 1.4261321455085376,
      "grad_norm": 0.034870557487010956,
      "learning_rate": 9.509664851518921e-05,
      "loss": 0.057,
      "step": 5763
    },
    {
      "epoch": 1.4263796090076712,
      "grad_norm": 0.04379396140575409,
      "learning_rate": 9.502028950687192e-05,
      "loss": 0.0537,
      "step": 5764
    },
    {
      "epoch": 1.4266270725068053,
      "grad_norm": 0.05021362379193306,
      "learning_rate": 9.494395397391558e-05,
      "loss": 0.0409,
      "step": 5765
    },
    {
      "epoch": 1.4268745360059392,
      "grad_norm": 0.0760718584060669,
      "learning_rate": 9.486764192788303e-05,
      "loss": 0.0801,
      "step": 5766
    },
    {
      "epoch": 1.427121999505073,
      "grad_norm": 0.03143659234046936,
      "learning_rate": 9.479135338033366e-05,
      "loss": 0.0389,
      "step": 5767
    },
    {
      "epoch": 1.4273694630042069,
      "grad_norm": 0.03595203533768654,
      "learning_rate": 9.471508834282286e-05,
      "loss": 0.0379,
      "step": 5768
    },
    {
      "epoch": 1.4276169265033407,
      "grad_norm": 0.07282069325447083,
      "learning_rate": 9.46388468269031e-05,
      "loss": 0.0709,
      "step": 5769
    },
    {
      "epoch": 1.4278643900024746,
      "grad_norm": 0.041821155697107315,
      "learning_rate": 9.456262884412293e-05,
      "loss": 0.0363,
      "step": 5770
    },
    {
      "epoch": 1.4281118535016084,
      "grad_norm": 0.04029979184269905,
      "learning_rate": 9.448643440602723e-05,
      "loss": 0.028,
      "step": 5771
    },
    {
      "epoch": 1.4283593170007425,
      "grad_norm": 0.05521854758262634,
      "learning_rate": 9.441026352415752e-05,
      "loss": 0.0776,
      "step": 5772
    },
    {
      "epoch": 1.4286067804998763,
      "grad_norm": 0.030375685542821884,
      "learning_rate": 9.43341162100517e-05,
      "loss": 0.0213,
      "step": 5773
    },
    {
      "epoch": 1.4288542439990102,
      "grad_norm": 0.07046777755022049,
      "learning_rate": 9.425799247524411e-05,
      "loss": 0.0627,
      "step": 5774
    },
    {
      "epoch": 1.429101707498144,
      "grad_norm": 0.02390517294406891,
      "learning_rate": 9.418189233126556e-05,
      "loss": 0.0306,
      "step": 5775
    },
    {
      "epoch": 1.4293491709972779,
      "grad_norm": 0.06654335558414459,
      "learning_rate": 9.4105815789643e-05,
      "loss": 0.0418,
      "step": 5776
    },
    {
      "epoch": 1.4295966344964117,
      "grad_norm": 0.045961685478687286,
      "learning_rate": 9.402976286190032e-05,
      "loss": 0.0444,
      "step": 5777
    },
    {
      "epoch": 1.4298440979955456,
      "grad_norm": 0.06366511434316635,
      "learning_rate": 9.395373355955734e-05,
      "loss": 0.063,
      "step": 5778
    },
    {
      "epoch": 1.4300915614946796,
      "grad_norm": 0.06933405995368958,
      "learning_rate": 9.387772789413055e-05,
      "loss": 0.0598,
      "step": 5779
    },
    {
      "epoch": 1.4303390249938135,
      "grad_norm": 0.08310485631227493,
      "learning_rate": 9.380174587713286e-05,
      "loss": 0.0471,
      "step": 5780
    },
    {
      "epoch": 1.4305864884929473,
      "grad_norm": 0.049205150455236435,
      "learning_rate": 9.372578752007349e-05,
      "loss": 0.0631,
      "step": 5781
    },
    {
      "epoch": 1.4308339519920812,
      "grad_norm": 0.0544523261487484,
      "learning_rate": 9.364985283445828e-05,
      "loss": 0.0587,
      "step": 5782
    },
    {
      "epoch": 1.431081415491215,
      "grad_norm": 0.08969871699810028,
      "learning_rate": 9.357394183178916e-05,
      "loss": 0.0381,
      "step": 5783
    },
    {
      "epoch": 1.4313288789903489,
      "grad_norm": 0.07488834857940674,
      "learning_rate": 9.349805452356464e-05,
      "loss": 0.0867,
      "step": 5784
    },
    {
      "epoch": 1.4315763424894827,
      "grad_norm": 0.04591885581612587,
      "learning_rate": 9.34221909212799e-05,
      "loss": 0.0529,
      "step": 5785
    },
    {
      "epoch": 1.4318238059886168,
      "grad_norm": 0.07303676754236221,
      "learning_rate": 9.334635103642608e-05,
      "loss": 0.0574,
      "step": 5786
    },
    {
      "epoch": 1.4320712694877504,
      "grad_norm": 0.054722316563129425,
      "learning_rate": 9.327053488049098e-05,
      "loss": 0.0714,
      "step": 5787
    },
    {
      "epoch": 1.4323187329868845,
      "grad_norm": 0.03718351200222969,
      "learning_rate": 9.319474246495877e-05,
      "loss": 0.059,
      "step": 5788
    },
    {
      "epoch": 1.4325661964860184,
      "grad_norm": 0.05663614720106125,
      "learning_rate": 9.311897380131001e-05,
      "loss": 0.048,
      "step": 5789
    },
    {
      "epoch": 1.4328136599851522,
      "grad_norm": 0.08916888386011124,
      "learning_rate": 9.304322890102174e-05,
      "loss": 0.0841,
      "step": 5790
    },
    {
      "epoch": 1.433061123484286,
      "grad_norm": 0.05234827846288681,
      "learning_rate": 9.296750777556718e-05,
      "loss": 0.0648,
      "step": 5791
    },
    {
      "epoch": 1.43330858698342,
      "grad_norm": 0.057680293917655945,
      "learning_rate": 9.289181043641618e-05,
      "loss": 0.0744,
      "step": 5792
    },
    {
      "epoch": 1.4335560504825537,
      "grad_norm": 0.04001415893435478,
      "learning_rate": 9.281613689503485e-05,
      "loss": 0.0504,
      "step": 5793
    },
    {
      "epoch": 1.4338035139816876,
      "grad_norm": 0.06279749423265457,
      "learning_rate": 9.27404871628858e-05,
      "loss": 0.0866,
      "step": 5794
    },
    {
      "epoch": 1.4340509774808217,
      "grad_norm": 0.057340167462825775,
      "learning_rate": 9.266486125142798e-05,
      "loss": 0.0608,
      "step": 5795
    },
    {
      "epoch": 1.4342984409799555,
      "grad_norm": 0.06683310866355896,
      "learning_rate": 9.258925917211669e-05,
      "loss": 0.0743,
      "step": 5796
    },
    {
      "epoch": 1.4345459044790894,
      "grad_norm": 0.058430496603250504,
      "learning_rate": 9.251368093640378e-05,
      "loss": 0.0615,
      "step": 5797
    },
    {
      "epoch": 1.4347933679782232,
      "grad_norm": 0.039567604660987854,
      "learning_rate": 9.243812655573719e-05,
      "loss": 0.065,
      "step": 5798
    },
    {
      "epoch": 1.435040831477357,
      "grad_norm": 0.03410305455327034,
      "learning_rate": 9.236259604156152e-05,
      "loss": 0.0305,
      "step": 5799
    },
    {
      "epoch": 1.435288294976491,
      "grad_norm": 0.42219749093055725,
      "learning_rate": 9.228708940531765e-05,
      "loss": 0.0423,
      "step": 5800
    },
    {
      "epoch": 1.435288294976491,
      "eval_loss": 0.2897258698940277,
      "eval_runtime": 202.4491,
      "eval_samples_per_second": 4.94,
      "eval_steps_per_second": 0.311,
      "step": 5800
    },
    {
      "epoch": 1.4355357584756248,
      "grad_norm": 0.09225787222385406,
      "learning_rate": 9.221160665844288e-05,
      "loss": 0.1168,
      "step": 5801
    },
    {
      "epoch": 1.4357832219747588,
      "grad_norm": 0.056411609053611755,
      "learning_rate": 9.213614781237092e-05,
      "loss": 0.0409,
      "step": 5802
    },
    {
      "epoch": 1.4360306854738927,
      "grad_norm": 0.03867628797888756,
      "learning_rate": 9.206071287853157e-05,
      "loss": 0.033,
      "step": 5803
    },
    {
      "epoch": 1.4362781489730265,
      "grad_norm": 0.0376577191054821,
      "learning_rate": 9.198530186835149e-05,
      "loss": 0.0546,
      "step": 5804
    },
    {
      "epoch": 1.4365256124721604,
      "grad_norm": 0.057015031576156616,
      "learning_rate": 9.190991479325347e-05,
      "loss": 0.069,
      "step": 5805
    },
    {
      "epoch": 1.4367730759712942,
      "grad_norm": 0.035637613385915756,
      "learning_rate": 9.183455166465649e-05,
      "loss": 0.032,
      "step": 5806
    },
    {
      "epoch": 1.437020539470428,
      "grad_norm": 0.05326538532972336,
      "learning_rate": 9.17592124939762e-05,
      "loss": 0.0946,
      "step": 5807
    },
    {
      "epoch": 1.437268002969562,
      "grad_norm": 0.06425497680902481,
      "learning_rate": 9.16838972926245e-05,
      "loss": 0.0493,
      "step": 5808
    },
    {
      "epoch": 1.437515466468696,
      "grad_norm": 0.04765572026371956,
      "learning_rate": 9.160860607200966e-05,
      "loss": 0.045,
      "step": 5809
    },
    {
      "epoch": 1.4377629299678296,
      "grad_norm": 0.041049543768167496,
      "learning_rate": 9.153333884353643e-05,
      "loss": 0.0593,
      "step": 5810
    },
    {
      "epoch": 1.4380103934669637,
      "grad_norm": 0.047586411237716675,
      "learning_rate": 9.145809561860555e-05,
      "loss": 0.0471,
      "step": 5811
    },
    {
      "epoch": 1.4382578569660975,
      "grad_norm": 0.07292349636554718,
      "learning_rate": 9.138287640861471e-05,
      "loss": 0.0763,
      "step": 5812
    },
    {
      "epoch": 1.4385053204652314,
      "grad_norm": 0.06150539964437485,
      "learning_rate": 9.130768122495742e-05,
      "loss": 0.0679,
      "step": 5813
    },
    {
      "epoch": 1.4387527839643652,
      "grad_norm": 0.05246211960911751,
      "learning_rate": 9.123251007902388e-05,
      "loss": 0.0631,
      "step": 5814
    },
    {
      "epoch": 1.439000247463499,
      "grad_norm": 0.041016120463609695,
      "learning_rate": 9.115736298220051e-05,
      "loss": 0.0503,
      "step": 5815
    },
    {
      "epoch": 1.439247710962633,
      "grad_norm": 0.03447817265987396,
      "learning_rate": 9.108223994587017e-05,
      "loss": 0.0248,
      "step": 5816
    },
    {
      "epoch": 1.4394951744617668,
      "grad_norm": 0.04710856452584267,
      "learning_rate": 9.100714098141206e-05,
      "loss": 0.0522,
      "step": 5817
    },
    {
      "epoch": 1.4397426379609009,
      "grad_norm": 0.057321224361658096,
      "learning_rate": 9.093206610020155e-05,
      "loss": 0.0477,
      "step": 5818
    },
    {
      "epoch": 1.4399901014600347,
      "grad_norm": 0.05918201431632042,
      "learning_rate": 9.085701531361054e-05,
      "loss": 0.0221,
      "step": 5819
    },
    {
      "epoch": 1.4402375649591685,
      "grad_norm": 0.0525660514831543,
      "learning_rate": 9.078198863300749e-05,
      "loss": 0.0446,
      "step": 5820
    },
    {
      "epoch": 1.4404850284583024,
      "grad_norm": 0.06140849366784096,
      "learning_rate": 9.070698606975671e-05,
      "loss": 0.058,
      "step": 5821
    },
    {
      "epoch": 1.4407324919574362,
      "grad_norm": 0.041326504200696945,
      "learning_rate": 9.063200763521934e-05,
      "loss": 0.0585,
      "step": 5822
    },
    {
      "epoch": 1.44097995545657,
      "grad_norm": 0.059764564037323,
      "learning_rate": 9.055705334075234e-05,
      "loss": 0.088,
      "step": 5823
    },
    {
      "epoch": 1.441227418955704,
      "grad_norm": 0.0417499840259552,
      "learning_rate": 9.048212319770957e-05,
      "loss": 0.0575,
      "step": 5824
    },
    {
      "epoch": 1.441474882454838,
      "grad_norm": 0.0791192278265953,
      "learning_rate": 9.040721721744102e-05,
      "loss": 0.1042,
      "step": 5825
    },
    {
      "epoch": 1.4417223459539719,
      "grad_norm": 0.06599419564008713,
      "learning_rate": 9.033233541129279e-05,
      "loss": 0.0664,
      "step": 5826
    },
    {
      "epoch": 1.4419698094531057,
      "grad_norm": 0.11769826710224152,
      "learning_rate": 9.025747779060762e-05,
      "loss": 0.096,
      "step": 5827
    },
    {
      "epoch": 1.4422172729522396,
      "grad_norm": 0.05407380312681198,
      "learning_rate": 9.018264436672443e-05,
      "loss": 0.0421,
      "step": 5828
    },
    {
      "epoch": 1.4424647364513734,
      "grad_norm": 0.039119232445955276,
      "learning_rate": 9.010783515097853e-05,
      "loss": 0.0386,
      "step": 5829
    },
    {
      "epoch": 1.4427121999505073,
      "grad_norm": 0.042866602540016174,
      "learning_rate": 9.003305015470168e-05,
      "loss": 0.0507,
      "step": 5830
    },
    {
      "epoch": 1.442959663449641,
      "grad_norm": 0.07212499529123306,
      "learning_rate": 8.995828938922154e-05,
      "loss": 0.0497,
      "step": 5831
    },
    {
      "epoch": 1.4432071269487752,
      "grad_norm": 0.04982727766036987,
      "learning_rate": 8.988355286586278e-05,
      "loss": 0.0597,
      "step": 5832
    },
    {
      "epoch": 1.4434545904479088,
      "grad_norm": 0.05683659762144089,
      "learning_rate": 8.980884059594577e-05,
      "loss": 0.0594,
      "step": 5833
    },
    {
      "epoch": 1.4437020539470429,
      "grad_norm": 0.049306076020002365,
      "learning_rate": 8.973415259078752e-05,
      "loss": 0.0493,
      "step": 5834
    },
    {
      "epoch": 1.4439495174461767,
      "grad_norm": 0.04797326400876045,
      "learning_rate": 8.96594888617013e-05,
      "loss": 0.0687,
      "step": 5835
    },
    {
      "epoch": 1.4441969809453106,
      "grad_norm": 0.04394793510437012,
      "learning_rate": 8.958484941999673e-05,
      "loss": 0.0537,
      "step": 5836
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.08511777222156525,
      "learning_rate": 8.951023427697983e-05,
      "loss": 0.0406,
      "step": 5837
    },
    {
      "epoch": 1.4446919079435783,
      "grad_norm": 0.07728926837444305,
      "learning_rate": 8.94356434439526e-05,
      "loss": 0.1125,
      "step": 5838
    },
    {
      "epoch": 1.4449393714427121,
      "grad_norm": 0.03176780790090561,
      "learning_rate": 8.936107693221365e-05,
      "loss": 0.0221,
      "step": 5839
    },
    {
      "epoch": 1.445186834941846,
      "grad_norm": 0.060984186828136444,
      "learning_rate": 8.928653475305809e-05,
      "loss": 0.0674,
      "step": 5840
    },
    {
      "epoch": 1.44543429844098,
      "grad_norm": 0.026024378836154938,
      "learning_rate": 8.921201691777683e-05,
      "loss": 0.0329,
      "step": 5841
    },
    {
      "epoch": 1.4456817619401139,
      "grad_norm": 0.08295390754938126,
      "learning_rate": 8.913752343765751e-05,
      "loss": 0.0964,
      "step": 5842
    },
    {
      "epoch": 1.4459292254392477,
      "grad_norm": 0.07860131561756134,
      "learning_rate": 8.906305432398387e-05,
      "loss": 0.1113,
      "step": 5843
    },
    {
      "epoch": 1.4461766889383816,
      "grad_norm": 0.026851164177060127,
      "learning_rate": 8.898860958803609e-05,
      "loss": 0.0154,
      "step": 5844
    },
    {
      "epoch": 1.4464241524375154,
      "grad_norm": 0.044097986072301865,
      "learning_rate": 8.891418924109062e-05,
      "loss": 0.0501,
      "step": 5845
    },
    {
      "epoch": 1.4466716159366493,
      "grad_norm": 0.09763012081384659,
      "learning_rate": 8.883979329442004e-05,
      "loss": 0.1241,
      "step": 5846
    },
    {
      "epoch": 1.4469190794357831,
      "grad_norm": 0.0478777177631855,
      "learning_rate": 8.87654217592935e-05,
      "loss": 0.0352,
      "step": 5847
    },
    {
      "epoch": 1.4471665429349172,
      "grad_norm": 0.03356609493494034,
      "learning_rate": 8.869107464697631e-05,
      "loss": 0.0347,
      "step": 5848
    },
    {
      "epoch": 1.447414006434051,
      "grad_norm": 0.03140329197049141,
      "learning_rate": 8.861675196873012e-05,
      "loss": 0.0489,
      "step": 5849
    },
    {
      "epoch": 1.447661469933185,
      "grad_norm": 0.03308486565947533,
      "learning_rate": 8.854245373581285e-05,
      "loss": 0.0235,
      "step": 5850
    },
    {
      "epoch": 1.4479089334323187,
      "grad_norm": 0.04171391949057579,
      "learning_rate": 8.846817995947875e-05,
      "loss": 0.0626,
      "step": 5851
    },
    {
      "epoch": 1.4481563969314526,
      "grad_norm": 0.040249306708574295,
      "learning_rate": 8.839393065097842e-05,
      "loss": 0.056,
      "step": 5852
    },
    {
      "epoch": 1.4484038604305864,
      "grad_norm": 0.042648814618587494,
      "learning_rate": 8.831970582155851e-05,
      "loss": 0.0494,
      "step": 5853
    },
    {
      "epoch": 1.4486513239297203,
      "grad_norm": 0.029802938923239708,
      "learning_rate": 8.824550548246224e-05,
      "loss": 0.0256,
      "step": 5854
    },
    {
      "epoch": 1.4488987874288544,
      "grad_norm": 0.05055912211537361,
      "learning_rate": 8.8171329644929e-05,
      "loss": 0.03,
      "step": 5855
    },
    {
      "epoch": 1.449146250927988,
      "grad_norm": 0.02852315828204155,
      "learning_rate": 8.809717832019449e-05,
      "loss": 0.0274,
      "step": 5856
    },
    {
      "epoch": 1.449393714427122,
      "grad_norm": 0.07405896484851837,
      "learning_rate": 8.802305151949075e-05,
      "loss": 0.0666,
      "step": 5857
    },
    {
      "epoch": 1.449641177926256,
      "grad_norm": 0.04439757019281387,
      "learning_rate": 8.794894925404584e-05,
      "loss": 0.0359,
      "step": 5858
    },
    {
      "epoch": 1.4498886414253898,
      "grad_norm": 0.04497593268752098,
      "learning_rate": 8.787487153508453e-05,
      "loss": 0.0245,
      "step": 5859
    },
    {
      "epoch": 1.4501361049245236,
      "grad_norm": 0.048240307718515396,
      "learning_rate": 8.780081837382767e-05,
      "loss": 0.0313,
      "step": 5860
    },
    {
      "epoch": 1.4503835684236575,
      "grad_norm": 0.02734631672501564,
      "learning_rate": 8.772678978149218e-05,
      "loss": 0.0517,
      "step": 5861
    },
    {
      "epoch": 1.4506310319227915,
      "grad_norm": 0.05891573429107666,
      "learning_rate": 8.765278576929153e-05,
      "loss": 0.0862,
      "step": 5862
    },
    {
      "epoch": 1.4508784954219252,
      "grad_norm": 0.03319574147462845,
      "learning_rate": 8.757880634843543e-05,
      "loss": 0.0328,
      "step": 5863
    },
    {
      "epoch": 1.4511259589210592,
      "grad_norm": 0.05742700770497322,
      "learning_rate": 8.750485153012977e-05,
      "loss": 0.0359,
      "step": 5864
    },
    {
      "epoch": 1.451373422420193,
      "grad_norm": 0.056037504225969315,
      "learning_rate": 8.74309213255769e-05,
      "loss": 0.0423,
      "step": 5865
    },
    {
      "epoch": 1.451620885919327,
      "grad_norm": 0.04401080682873726,
      "learning_rate": 8.735701574597498e-05,
      "loss": 0.0258,
      "step": 5866
    },
    {
      "epoch": 1.4518683494184608,
      "grad_norm": 0.038621243089437485,
      "learning_rate": 8.72831348025192e-05,
      "loss": 0.0392,
      "step": 5867
    },
    {
      "epoch": 1.4521158129175946,
      "grad_norm": 0.03921034559607506,
      "learning_rate": 8.720927850640026e-05,
      "loss": 0.0415,
      "step": 5868
    },
    {
      "epoch": 1.4523632764167285,
      "grad_norm": 0.058905452489852905,
      "learning_rate": 8.713544686880556e-05,
      "loss": 0.0593,
      "step": 5869
    },
    {
      "epoch": 1.4526107399158623,
      "grad_norm": 0.04013796150684357,
      "learning_rate": 8.706163990091867e-05,
      "loss": 0.0606,
      "step": 5870
    },
    {
      "epoch": 1.4528582034149964,
      "grad_norm": 0.036358799785375595,
      "learning_rate": 8.698785761391939e-05,
      "loss": 0.0335,
      "step": 5871
    },
    {
      "epoch": 1.4531056669141302,
      "grad_norm": 0.07128867506980896,
      "learning_rate": 8.691410001898389e-05,
      "loss": 0.0447,
      "step": 5872
    },
    {
      "epoch": 1.453353130413264,
      "grad_norm": 0.05848633125424385,
      "learning_rate": 8.684036712728436e-05,
      "loss": 0.0823,
      "step": 5873
    },
    {
      "epoch": 1.453600593912398,
      "grad_norm": 0.07792896032333374,
      "learning_rate": 8.676665894998933e-05,
      "loss": 0.111,
      "step": 5874
    },
    {
      "epoch": 1.4538480574115318,
      "grad_norm": 0.024348903447389603,
      "learning_rate": 8.669297549826402e-05,
      "loss": 0.0314,
      "step": 5875
    },
    {
      "epoch": 1.4540955209106656,
      "grad_norm": 0.03533080220222473,
      "learning_rate": 8.661931678326918e-05,
      "loss": 0.0246,
      "step": 5876
    },
    {
      "epoch": 1.4543429844097995,
      "grad_norm": 0.050402265042066574,
      "learning_rate": 8.654568281616234e-05,
      "loss": 0.0604,
      "step": 5877
    },
    {
      "epoch": 1.4545904479089335,
      "grad_norm": 0.03549191355705261,
      "learning_rate": 8.647207360809706e-05,
      "loss": 0.0343,
      "step": 5878
    },
    {
      "epoch": 1.4548379114080672,
      "grad_norm": 0.04523354023694992,
      "learning_rate": 8.639848917022322e-05,
      "loss": 0.0628,
      "step": 5879
    },
    {
      "epoch": 1.4550853749072012,
      "grad_norm": 0.04326197877526283,
      "learning_rate": 8.632492951368703e-05,
      "loss": 0.0512,
      "step": 5880
    },
    {
      "epoch": 1.455332838406335,
      "grad_norm": 0.048087481409311295,
      "learning_rate": 8.62513946496307e-05,
      "loss": 0.0419,
      "step": 5881
    },
    {
      "epoch": 1.455580301905469,
      "grad_norm": 0.04785024747252464,
      "learning_rate": 8.617788458919276e-05,
      "loss": 0.0415,
      "step": 5882
    },
    {
      "epoch": 1.4558277654046028,
      "grad_norm": 0.06118125468492508,
      "learning_rate": 8.610439934350839e-05,
      "loss": 0.0884,
      "step": 5883
    },
    {
      "epoch": 1.4560752289037366,
      "grad_norm": 0.04416326805949211,
      "learning_rate": 8.603093892370837e-05,
      "loss": 0.0456,
      "step": 5884
    },
    {
      "epoch": 1.4563226924028707,
      "grad_norm": 0.06642314791679382,
      "learning_rate": 8.59575033409202e-05,
      "loss": 0.0546,
      "step": 5885
    },
    {
      "epoch": 1.4565701559020043,
      "grad_norm": 0.06472226977348328,
      "learning_rate": 8.58840926062672e-05,
      "loss": 0.0363,
      "step": 5886
    },
    {
      "epoch": 1.4568176194011384,
      "grad_norm": 0.05361562967300415,
      "learning_rate": 8.581070673086943e-05,
      "loss": 0.0749,
      "step": 5887
    },
    {
      "epoch": 1.4570650829002723,
      "grad_norm": 0.04402173310518265,
      "learning_rate": 8.573734572584294e-05,
      "loss": 0.0356,
      "step": 5888
    },
    {
      "epoch": 1.457312546399406,
      "grad_norm": 0.051470398902893066,
      "learning_rate": 8.56640096022998e-05,
      "loss": 0.069,
      "step": 5889
    },
    {
      "epoch": 1.45756000989854,
      "grad_norm": 0.05930348113179207,
      "learning_rate": 8.559069837134861e-05,
      "loss": 0.0671,
      "step": 5890
    },
    {
      "epoch": 1.4578074733976738,
      "grad_norm": 0.06956518441438675,
      "learning_rate": 8.551741204409408e-05,
      "loss": 0.0988,
      "step": 5891
    },
    {
      "epoch": 1.4580549368968077,
      "grad_norm": 0.05184526368975639,
      "learning_rate": 8.544415063163721e-05,
      "loss": 0.0423,
      "step": 5892
    },
    {
      "epoch": 1.4583024003959415,
      "grad_norm": 0.043161287903785706,
      "learning_rate": 8.537091414507522e-05,
      "loss": 0.0382,
      "step": 5893
    },
    {
      "epoch": 1.4585498638950756,
      "grad_norm": 0.05225403606891632,
      "learning_rate": 8.529770259550126e-05,
      "loss": 0.0587,
      "step": 5894
    },
    {
      "epoch": 1.4587973273942094,
      "grad_norm": 0.06936334818601608,
      "learning_rate": 8.522451599400536e-05,
      "loss": 0.0324,
      "step": 5895
    },
    {
      "epoch": 1.4590447908933433,
      "grad_norm": 0.027919506654143333,
      "learning_rate": 8.515135435167304e-05,
      "loss": 0.0262,
      "step": 5896
    },
    {
      "epoch": 1.4592922543924771,
      "grad_norm": 0.08041483163833618,
      "learning_rate": 8.507821767958649e-05,
      "loss": 0.0637,
      "step": 5897
    },
    {
      "epoch": 1.459539717891611,
      "grad_norm": 0.04655269905924797,
      "learning_rate": 8.500510598882402e-05,
      "loss": 0.0384,
      "step": 5898
    },
    {
      "epoch": 1.4597871813907448,
      "grad_norm": 0.04167724400758743,
      "learning_rate": 8.49320192904601e-05,
      "loss": 0.0383,
      "step": 5899
    },
    {
      "epoch": 1.4600346448898787,
      "grad_norm": 0.06179342418909073,
      "learning_rate": 8.485895759556553e-05,
      "loss": 0.0556,
      "step": 5900
    },
    {
      "epoch": 1.4602821083890127,
      "grad_norm": 0.05345407500863075,
      "learning_rate": 8.47859209152071e-05,
      "loss": 0.0765,
      "step": 5901
    },
    {
      "epoch": 1.4605295718881464,
      "grad_norm": 0.08771425485610962,
      "learning_rate": 8.471290926044791e-05,
      "loss": 0.0365,
      "step": 5902
    },
    {
      "epoch": 1.4607770353872804,
      "grad_norm": 0.03370782732963562,
      "learning_rate": 8.46399226423476e-05,
      "loss": 0.0265,
      "step": 5903
    },
    {
      "epoch": 1.4610244988864143,
      "grad_norm": 0.037130486220121384,
      "learning_rate": 8.456696107196146e-05,
      "loss": 0.0542,
      "step": 5904
    },
    {
      "epoch": 1.4612719623855481,
      "grad_norm": 0.08604244142770767,
      "learning_rate": 8.449402456034136e-05,
      "loss": 0.0845,
      "step": 5905
    },
    {
      "epoch": 1.461519425884682,
      "grad_norm": 0.04548018053174019,
      "learning_rate": 8.442111311853523e-05,
      "loss": 0.0385,
      "step": 5906
    },
    {
      "epoch": 1.4617668893838158,
      "grad_norm": 0.06950812041759491,
      "learning_rate": 8.434822675758728e-05,
      "loss": 0.0683,
      "step": 5907
    },
    {
      "epoch": 1.46201435288295,
      "grad_norm": 0.036467138677835464,
      "learning_rate": 8.427536548853793e-05,
      "loss": 0.0294,
      "step": 5908
    },
    {
      "epoch": 1.4622618163820835,
      "grad_norm": 0.03615594282746315,
      "learning_rate": 8.420252932242358e-05,
      "loss": 0.0305,
      "step": 5909
    },
    {
      "epoch": 1.4625092798812176,
      "grad_norm": 0.08332764357328415,
      "learning_rate": 8.412971827027713e-05,
      "loss": 0.0669,
      "step": 5910
    },
    {
      "epoch": 1.4627567433803514,
      "grad_norm": 0.07875923067331314,
      "learning_rate": 8.40569323431275e-05,
      "loss": 0.0714,
      "step": 5911
    },
    {
      "epoch": 1.4630042068794853,
      "grad_norm": 0.03463079407811165,
      "learning_rate": 8.398417155199989e-05,
      "loss": 0.0566,
      "step": 5912
    },
    {
      "epoch": 1.4632516703786191,
      "grad_norm": 0.04946238920092583,
      "learning_rate": 8.391143590791562e-05,
      "loss": 0.0622,
      "step": 5913
    },
    {
      "epoch": 1.463499133877753,
      "grad_norm": 0.08922021090984344,
      "learning_rate": 8.383872542189223e-05,
      "loss": 0.0722,
      "step": 5914
    },
    {
      "epoch": 1.4637465973768868,
      "grad_norm": 0.0416703075170517,
      "learning_rate": 8.376604010494355e-05,
      "loss": 0.0409,
      "step": 5915
    },
    {
      "epoch": 1.4639940608760207,
      "grad_norm": 0.05869417265057564,
      "learning_rate": 8.369337996807933e-05,
      "loss": 0.0654,
      "step": 5916
    },
    {
      "epoch": 1.4642415243751548,
      "grad_norm": 0.052578095346689224,
      "learning_rate": 8.362074502230573e-05,
      "loss": 0.0456,
      "step": 5917
    },
    {
      "epoch": 1.4644889878742886,
      "grad_norm": 0.03991209343075752,
      "learning_rate": 8.354813527862509e-05,
      "loss": 0.0292,
      "step": 5918
    },
    {
      "epoch": 1.4647364513734225,
      "grad_norm": 0.02873394452035427,
      "learning_rate": 8.347555074803586e-05,
      "loss": 0.0221,
      "step": 5919
    },
    {
      "epoch": 1.4649839148725563,
      "grad_norm": 0.05095941573381424,
      "learning_rate": 8.340299144153277e-05,
      "loss": 0.0572,
      "step": 5920
    },
    {
      "epoch": 1.4652313783716902,
      "grad_norm": 0.04179859906435013,
      "learning_rate": 8.333045737010639e-05,
      "loss": 0.0461,
      "step": 5921
    },
    {
      "epoch": 1.465478841870824,
      "grad_norm": 0.048795633018016815,
      "learning_rate": 8.325794854474398e-05,
      "loss": 0.0404,
      "step": 5922
    },
    {
      "epoch": 1.4657263053699578,
      "grad_norm": 0.07574541866779327,
      "learning_rate": 8.318546497642876e-05,
      "loss": 0.0377,
      "step": 5923
    },
    {
      "epoch": 1.465973768869092,
      "grad_norm": 0.07902669906616211,
      "learning_rate": 8.311300667613988e-05,
      "loss": 0.0558,
      "step": 5924
    },
    {
      "epoch": 1.4662212323682258,
      "grad_norm": 0.043424010276794434,
      "learning_rate": 8.304057365485298e-05,
      "loss": 0.0469,
      "step": 5925
    },
    {
      "epoch": 1.4664686958673596,
      "grad_norm": 0.052526526153087616,
      "learning_rate": 8.296816592353976e-05,
      "loss": 0.0774,
      "step": 5926
    },
    {
      "epoch": 1.4667161593664935,
      "grad_norm": 0.04586854577064514,
      "learning_rate": 8.289578349316809e-05,
      "loss": 0.0499,
      "step": 5927
    },
    {
      "epoch": 1.4669636228656273,
      "grad_norm": 0.04950470104813576,
      "learning_rate": 8.282342637470208e-05,
      "loss": 0.032,
      "step": 5928
    },
    {
      "epoch": 1.4672110863647612,
      "grad_norm": 0.044230516999959946,
      "learning_rate": 8.275109457910173e-05,
      "loss": 0.0479,
      "step": 5929
    },
    {
      "epoch": 1.467458549863895,
      "grad_norm": 0.05993663892149925,
      "learning_rate": 8.267878811732368e-05,
      "loss": 0.0484,
      "step": 5930
    },
    {
      "epoch": 1.467706013363029,
      "grad_norm": 0.06165599077939987,
      "learning_rate": 8.260650700032027e-05,
      "loss": 0.0649,
      "step": 5931
    },
    {
      "epoch": 1.4679534768621627,
      "grad_norm": 0.04233788326382637,
      "learning_rate": 8.253425123904024e-05,
      "loss": 0.035,
      "step": 5932
    },
    {
      "epoch": 1.4682009403612968,
      "grad_norm": 0.033750519156455994,
      "learning_rate": 8.246202084442845e-05,
      "loss": 0.0352,
      "step": 5933
    },
    {
      "epoch": 1.4684484038604306,
      "grad_norm": 0.07245254516601562,
      "learning_rate": 8.238981582742588e-05,
      "loss": 0.0436,
      "step": 5934
    },
    {
      "epoch": 1.4686958673595645,
      "grad_norm": 0.03715815767645836,
      "learning_rate": 8.231763619896987e-05,
      "loss": 0.037,
      "step": 5935
    },
    {
      "epoch": 1.4689433308586983,
      "grad_norm": 0.048866577446460724,
      "learning_rate": 8.224548196999348e-05,
      "loss": 0.0647,
      "step": 5936
    },
    {
      "epoch": 1.4691907943578322,
      "grad_norm": 0.04970557615160942,
      "learning_rate": 8.217335315142623e-05,
      "loss": 0.0448,
      "step": 5937
    },
    {
      "epoch": 1.469438257856966,
      "grad_norm": 0.033294398337602615,
      "learning_rate": 8.210124975419398e-05,
      "loss": 0.0391,
      "step": 5938
    },
    {
      "epoch": 1.4696857213560999,
      "grad_norm": 0.07124008238315582,
      "learning_rate": 8.202917178921826e-05,
      "loss": 0.0802,
      "step": 5939
    },
    {
      "epoch": 1.469933184855234,
      "grad_norm": 0.04270152002573013,
      "learning_rate": 8.195711926741706e-05,
      "loss": 0.0557,
      "step": 5940
    },
    {
      "epoch": 1.4701806483543678,
      "grad_norm": 0.046946145594120026,
      "learning_rate": 8.188509219970447e-05,
      "loss": 0.0631,
      "step": 5941
    },
    {
      "epoch": 1.4704281118535016,
      "grad_norm": 0.08838962018489838,
      "learning_rate": 8.181309059699068e-05,
      "loss": 0.0787,
      "step": 5942
    },
    {
      "epoch": 1.4706755753526355,
      "grad_norm": 0.03203403949737549,
      "learning_rate": 8.174111447018214e-05,
      "loss": 0.0304,
      "step": 5943
    },
    {
      "epoch": 1.4709230388517693,
      "grad_norm": 0.04233788326382637,
      "learning_rate": 8.166916383018116e-05,
      "loss": 0.0437,
      "step": 5944
    },
    {
      "epoch": 1.4711705023509032,
      "grad_norm": 0.02417769655585289,
      "learning_rate": 8.159723868788648e-05,
      "loss": 0.0321,
      "step": 5945
    },
    {
      "epoch": 1.471417965850037,
      "grad_norm": 0.0581987090408802,
      "learning_rate": 8.15253390541929e-05,
      "loss": 0.0512,
      "step": 5946
    },
    {
      "epoch": 1.471665429349171,
      "grad_norm": 0.049114882946014404,
      "learning_rate": 8.145346493999126e-05,
      "loss": 0.0468,
      "step": 5947
    },
    {
      "epoch": 1.471912892848305,
      "grad_norm": 0.06511839479207993,
      "learning_rate": 8.138161635616864e-05,
      "loss": 0.0555,
      "step": 5948
    },
    {
      "epoch": 1.4721603563474388,
      "grad_norm": 0.06942222267389297,
      "learning_rate": 8.130979331360825e-05,
      "loss": 0.0419,
      "step": 5949
    },
    {
      "epoch": 1.4724078198465727,
      "grad_norm": 0.03490835428237915,
      "learning_rate": 8.12379958231894e-05,
      "loss": 0.0443,
      "step": 5950
    },
    {
      "epoch": 1.4726552833457065,
      "grad_norm": 0.042233169078826904,
      "learning_rate": 8.11662238957874e-05,
      "loss": 0.0252,
      "step": 5951
    },
    {
      "epoch": 1.4729027468448403,
      "grad_norm": 0.043890904635190964,
      "learning_rate": 8.109447754227392e-05,
      "loss": 0.0489,
      "step": 5952
    },
    {
      "epoch": 1.4731502103439742,
      "grad_norm": 0.052699014544487,
      "learning_rate": 8.102275677351664e-05,
      "loss": 0.0414,
      "step": 5953
    },
    {
      "epoch": 1.4733976738431083,
      "grad_norm": 0.051451075822114944,
      "learning_rate": 8.095106160037934e-05,
      "loss": 0.0343,
      "step": 5954
    },
    {
      "epoch": 1.473645137342242,
      "grad_norm": 0.045263759791851044,
      "learning_rate": 8.08793920337221e-05,
      "loss": 0.0554,
      "step": 5955
    },
    {
      "epoch": 1.473892600841376,
      "grad_norm": 0.06417465955018997,
      "learning_rate": 8.080774808440076e-05,
      "loss": 0.0711,
      "step": 5956
    },
    {
      "epoch": 1.4741400643405098,
      "grad_norm": 0.04237886145710945,
      "learning_rate": 8.073612976326752e-05,
      "loss": 0.052,
      "step": 5957
    },
    {
      "epoch": 1.4743875278396437,
      "grad_norm": 0.044653765857219696,
      "learning_rate": 8.066453708117094e-05,
      "loss": 0.0467,
      "step": 5958
    },
    {
      "epoch": 1.4746349913387775,
      "grad_norm": 0.03927355632185936,
      "learning_rate": 8.059297004895516e-05,
      "loss": 0.0349,
      "step": 5959
    },
    {
      "epoch": 1.4748824548379114,
      "grad_norm": 0.06646864116191864,
      "learning_rate": 8.052142867746082e-05,
      "loss": 0.0692,
      "step": 5960
    },
    {
      "epoch": 1.4751299183370452,
      "grad_norm": 0.03892139717936516,
      "learning_rate": 8.044991297752452e-05,
      "loss": 0.0385,
      "step": 5961
    },
    {
      "epoch": 1.475377381836179,
      "grad_norm": 0.04611712321639061,
      "learning_rate": 8.037842295997905e-05,
      "loss": 0.0337,
      "step": 5962
    },
    {
      "epoch": 1.4756248453353131,
      "grad_norm": 0.04540398344397545,
      "learning_rate": 8.030695863565332e-05,
      "loss": 0.0745,
      "step": 5963
    },
    {
      "epoch": 1.475872308834447,
      "grad_norm": 0.043829295784235,
      "learning_rate": 8.023552001537219e-05,
      "loss": 0.0282,
      "step": 5964
    },
    {
      "epoch": 1.4761197723335808,
      "grad_norm": 0.027493150904774666,
      "learning_rate": 8.016410710995675e-05,
      "loss": 0.0332,
      "step": 5965
    },
    {
      "epoch": 1.4763672358327147,
      "grad_norm": 0.03932255506515503,
      "learning_rate": 8.009271993022425e-05,
      "loss": 0.0405,
      "step": 5966
    },
    {
      "epoch": 1.4766146993318485,
      "grad_norm": 0.05652246251702309,
      "learning_rate": 8.002135848698791e-05,
      "loss": 0.069,
      "step": 5967
    },
    {
      "epoch": 1.4768621628309824,
      "grad_norm": 0.03675328567624092,
      "learning_rate": 7.995002279105717e-05,
      "loss": 0.034,
      "step": 5968
    },
    {
      "epoch": 1.4771096263301162,
      "grad_norm": 0.06781159341335297,
      "learning_rate": 7.98787128532375e-05,
      "loss": 0.0336,
      "step": 5969
    },
    {
      "epoch": 1.4773570898292503,
      "grad_norm": 0.04910282418131828,
      "learning_rate": 7.980742868433058e-05,
      "loss": 0.0504,
      "step": 5970
    },
    {
      "epoch": 1.4776045533283841,
      "grad_norm": 0.0713559165596962,
      "learning_rate": 7.973617029513388e-05,
      "loss": 0.0558,
      "step": 5971
    },
    {
      "epoch": 1.477852016827518,
      "grad_norm": 0.05326416343450546,
      "learning_rate": 7.966493769644132e-05,
      "loss": 0.0332,
      "step": 5972
    },
    {
      "epoch": 1.4780994803266518,
      "grad_norm": 0.07777436077594757,
      "learning_rate": 7.95937308990427e-05,
      "loss": 0.1927,
      "step": 5973
    },
    {
      "epoch": 1.4783469438257857,
      "grad_norm": 0.08638141304254532,
      "learning_rate": 7.952254991372407e-05,
      "loss": 0.0618,
      "step": 5974
    },
    {
      "epoch": 1.4785944073249195,
      "grad_norm": 0.04432016983628273,
      "learning_rate": 7.945139475126748e-05,
      "loss": 0.0622,
      "step": 5975
    },
    {
      "epoch": 1.4788418708240534,
      "grad_norm": 0.04173301160335541,
      "learning_rate": 7.938026542245089e-05,
      "loss": 0.052,
      "step": 5976
    },
    {
      "epoch": 1.4790893343231875,
      "grad_norm": 0.037710271775722504,
      "learning_rate": 7.930916193804871e-05,
      "loss": 0.0552,
      "step": 5977
    },
    {
      "epoch": 1.479336797822321,
      "grad_norm": 0.07660498470067978,
      "learning_rate": 7.923808430883131e-05,
      "loss": 0.1125,
      "step": 5978
    },
    {
      "epoch": 1.4795842613214552,
      "grad_norm": 0.04439149051904678,
      "learning_rate": 7.916703254556493e-05,
      "loss": 0.0428,
      "step": 5979
    },
    {
      "epoch": 1.479831724820589,
      "grad_norm": 0.08584576100111008,
      "learning_rate": 7.909600665901206e-05,
      "loss": 0.0946,
      "step": 5980
    },
    {
      "epoch": 1.4800791883197228,
      "grad_norm": 0.07679354399442673,
      "learning_rate": 7.902500665993131e-05,
      "loss": 0.0707,
      "step": 5981
    },
    {
      "epoch": 1.4803266518188567,
      "grad_norm": 0.03921739012002945,
      "learning_rate": 7.895403255907729e-05,
      "loss": 0.0247,
      "step": 5982
    },
    {
      "epoch": 1.4805741153179905,
      "grad_norm": 0.04864402860403061,
      "learning_rate": 7.888308436720084e-05,
      "loss": 0.0443,
      "step": 5983
    },
    {
      "epoch": 1.4808215788171244,
      "grad_norm": 0.06172841787338257,
      "learning_rate": 7.881216209504849e-05,
      "loss": 0.0751,
      "step": 5984
    },
    {
      "epoch": 1.4810690423162582,
      "grad_norm": 0.11168278008699417,
      "learning_rate": 7.874126575336338e-05,
      "loss": 0.0789,
      "step": 5985
    },
    {
      "epoch": 1.4813165058153923,
      "grad_norm": 0.05152327939867973,
      "learning_rate": 7.867039535288425e-05,
      "loss": 0.0487,
      "step": 5986
    },
    {
      "epoch": 1.4815639693145262,
      "grad_norm": 0.03875844553112984,
      "learning_rate": 7.85995509043462e-05,
      "loss": 0.0316,
      "step": 5987
    },
    {
      "epoch": 1.48181143281366,
      "grad_norm": 0.04561183229088783,
      "learning_rate": 7.852873241848027e-05,
      "loss": 0.0363,
      "step": 5988
    },
    {
      "epoch": 1.4820588963127939,
      "grad_norm": 0.03775745630264282,
      "learning_rate": 7.845793990601363e-05,
      "loss": 0.0443,
      "step": 5989
    },
    {
      "epoch": 1.4823063598119277,
      "grad_norm": 0.03720967471599579,
      "learning_rate": 7.838717337766954e-05,
      "loss": 0.0378,
      "step": 5990
    },
    {
      "epoch": 1.4825538233110616,
      "grad_norm": 0.046502482146024704,
      "learning_rate": 7.831643284416715e-05,
      "loss": 0.0564,
      "step": 5991
    },
    {
      "epoch": 1.4828012868101954,
      "grad_norm": 0.03959258273243904,
      "learning_rate": 7.824571831622174e-05,
      "loss": 0.0786,
      "step": 5992
    },
    {
      "epoch": 1.4830487503093295,
      "grad_norm": 0.037849195301532745,
      "learning_rate": 7.817502980454502e-05,
      "loss": 0.0388,
      "step": 5993
    },
    {
      "epoch": 1.4832962138084633,
      "grad_norm": 0.04293964058160782,
      "learning_rate": 7.810436731984413e-05,
      "loss": 0.0275,
      "step": 5994
    },
    {
      "epoch": 1.4835436773075972,
      "grad_norm": 0.062461335211992264,
      "learning_rate": 7.803373087282273e-05,
      "loss": 0.061,
      "step": 5995
    },
    {
      "epoch": 1.483791140806731,
      "grad_norm": 0.02596208266913891,
      "learning_rate": 7.796312047418033e-05,
      "loss": 0.0277,
      "step": 5996
    },
    {
      "epoch": 1.4840386043058649,
      "grad_norm": 0.03895159438252449,
      "learning_rate": 7.789253613461258e-05,
      "loss": 0.045,
      "step": 5997
    },
    {
      "epoch": 1.4842860678049987,
      "grad_norm": 0.057971302419900894,
      "learning_rate": 7.782197786481127e-05,
      "loss": 0.0319,
      "step": 5998
    },
    {
      "epoch": 1.4845335313041326,
      "grad_norm": 0.03410806134343147,
      "learning_rate": 7.775144567546391e-05,
      "loss": 0.0321,
      "step": 5999
    },
    {
      "epoch": 1.4847809948032666,
      "grad_norm": 0.03711363673210144,
      "learning_rate": 7.76809395772544e-05,
      "loss": 0.0359,
      "step": 6000
    },
    {
      "epoch": 1.4847809948032666,
      "eval_loss": 0.28913912177085876,
      "eval_runtime": 202.7667,
      "eval_samples_per_second": 4.932,
      "eval_steps_per_second": 0.311,
      "step": 6000
    },
    {
      "epoch": 1.4850284583024003,
      "grad_norm": 0.06324601918458939,
      "learning_rate": 7.761045958086251e-05,
      "loss": 0.0685,
      "step": 6001
    },
    {
      "epoch": 1.4852759218015343,
      "grad_norm": 0.04738413915038109,
      "learning_rate": 7.754000569696415e-05,
      "loss": 0.0314,
      "step": 6002
    },
    {
      "epoch": 1.4855233853006682,
      "grad_norm": 0.05083950608968735,
      "learning_rate": 7.746957793623125e-05,
      "loss": 0.0562,
      "step": 6003
    },
    {
      "epoch": 1.485770848799802,
      "grad_norm": 0.03717720881104469,
      "learning_rate": 7.739917630933174e-05,
      "loss": 0.0344,
      "step": 6004
    },
    {
      "epoch": 1.4860183122989359,
      "grad_norm": 0.08409979194402695,
      "learning_rate": 7.732880082692963e-05,
      "loss": 0.0749,
      "step": 6005
    },
    {
      "epoch": 1.4862657757980697,
      "grad_norm": 0.060304831713438034,
      "learning_rate": 7.725845149968505e-05,
      "loss": 0.0972,
      "step": 6006
    },
    {
      "epoch": 1.4865132392972036,
      "grad_norm": 0.0575689896941185,
      "learning_rate": 7.718812833825389e-05,
      "loss": 0.0656,
      "step": 6007
    },
    {
      "epoch": 1.4867607027963374,
      "grad_norm": 0.09281434863805771,
      "learning_rate": 7.711783135328838e-05,
      "loss": 0.0864,
      "step": 6008
    },
    {
      "epoch": 1.4870081662954715,
      "grad_norm": 0.09145950525999069,
      "learning_rate": 7.704756055543663e-05,
      "loss": 0.0843,
      "step": 6009
    },
    {
      "epoch": 1.4872556297946053,
      "grad_norm": 0.09451606124639511,
      "learning_rate": 7.697731595534285e-05,
      "loss": 0.0655,
      "step": 6010
    },
    {
      "epoch": 1.4875030932937392,
      "grad_norm": 0.04820843040943146,
      "learning_rate": 7.690709756364725e-05,
      "loss": 0.0545,
      "step": 6011
    },
    {
      "epoch": 1.487750556792873,
      "grad_norm": 0.03297419473528862,
      "learning_rate": 7.68369053909861e-05,
      "loss": 0.037,
      "step": 6012
    },
    {
      "epoch": 1.487998020292007,
      "grad_norm": 0.05596598982810974,
      "learning_rate": 7.676673944799168e-05,
      "loss": 0.025,
      "step": 6013
    },
    {
      "epoch": 1.4882454837911407,
      "grad_norm": 0.05486016720533371,
      "learning_rate": 7.669659974529219e-05,
      "loss": 0.051,
      "step": 6014
    },
    {
      "epoch": 1.4884929472902746,
      "grad_norm": 0.06984926015138626,
      "learning_rate": 7.662648629351202e-05,
      "loss": 0.0384,
      "step": 6015
    },
    {
      "epoch": 1.4887404107894087,
      "grad_norm": 0.07920587807893753,
      "learning_rate": 7.655639910327153e-05,
      "loss": 0.0825,
      "step": 6016
    },
    {
      "epoch": 1.4889878742885425,
      "grad_norm": 0.08342967927455902,
      "learning_rate": 7.648633818518705e-05,
      "loss": 0.0988,
      "step": 6017
    },
    {
      "epoch": 1.4892353377876764,
      "grad_norm": 0.05529160425066948,
      "learning_rate": 7.641630354987112e-05,
      "loss": 0.0435,
      "step": 6018
    },
    {
      "epoch": 1.4894828012868102,
      "grad_norm": 0.09350709617137909,
      "learning_rate": 7.634629520793185e-05,
      "loss": 0.1408,
      "step": 6019
    },
    {
      "epoch": 1.489730264785944,
      "grad_norm": 0.05042104795575142,
      "learning_rate": 7.627631316997391e-05,
      "loss": 0.0356,
      "step": 6020
    },
    {
      "epoch": 1.489977728285078,
      "grad_norm": 0.04661165550351143,
      "learning_rate": 7.620635744659779e-05,
      "loss": 0.0551,
      "step": 6021
    },
    {
      "epoch": 1.4902251917842118,
      "grad_norm": 0.03793492168188095,
      "learning_rate": 7.613642804839974e-05,
      "loss": 0.0329,
      "step": 6022
    },
    {
      "epoch": 1.4904726552833458,
      "grad_norm": 0.0476459339261055,
      "learning_rate": 7.606652498597235e-05,
      "loss": 0.0652,
      "step": 6023
    },
    {
      "epoch": 1.4907201187824795,
      "grad_norm": 0.07164245843887329,
      "learning_rate": 7.599664826990408e-05,
      "loss": 0.0658,
      "step": 6024
    },
    {
      "epoch": 1.4909675822816135,
      "grad_norm": 0.06076033413410187,
      "learning_rate": 7.592679791077939e-05,
      "loss": 0.0675,
      "step": 6025
    },
    {
      "epoch": 1.4912150457807474,
      "grad_norm": 0.04587205499410629,
      "learning_rate": 7.585697391917889e-05,
      "loss": 0.0366,
      "step": 6026
    },
    {
      "epoch": 1.4914625092798812,
      "grad_norm": 0.05540701374411583,
      "learning_rate": 7.578717630567886e-05,
      "loss": 0.0578,
      "step": 6027
    },
    {
      "epoch": 1.491709972779015,
      "grad_norm": 0.04565703123807907,
      "learning_rate": 7.571740508085209e-05,
      "loss": 0.0542,
      "step": 6028
    },
    {
      "epoch": 1.491957436278149,
      "grad_norm": 0.03948420658707619,
      "learning_rate": 7.564766025526687e-05,
      "loss": 0.0349,
      "step": 6029
    },
    {
      "epoch": 1.492204899777283,
      "grad_norm": 0.07014131546020508,
      "learning_rate": 7.55779418394878e-05,
      "loss": 0.0997,
      "step": 6030
    },
    {
      "epoch": 1.4924523632764166,
      "grad_norm": 0.04836355522274971,
      "learning_rate": 7.550824984407537e-05,
      "loss": 0.0546,
      "step": 6031
    },
    {
      "epoch": 1.4926998267755507,
      "grad_norm": 0.04854803904891014,
      "learning_rate": 7.543858427958608e-05,
      "loss": 0.0513,
      "step": 6032
    },
    {
      "epoch": 1.4929472902746845,
      "grad_norm": 0.03863470256328583,
      "learning_rate": 7.536894515657256e-05,
      "loss": 0.0449,
      "step": 6033
    },
    {
      "epoch": 1.4931947537738184,
      "grad_norm": 0.04387514293193817,
      "learning_rate": 7.529933248558313e-05,
      "loss": 0.0297,
      "step": 6034
    },
    {
      "epoch": 1.4934422172729522,
      "grad_norm": 0.08465781807899475,
      "learning_rate": 7.522974627716225e-05,
      "loss": 0.0653,
      "step": 6035
    },
    {
      "epoch": 1.493689680772086,
      "grad_norm": 0.036284614354372025,
      "learning_rate": 7.51601865418507e-05,
      "loss": 0.0263,
      "step": 6036
    },
    {
      "epoch": 1.49393714427122,
      "grad_norm": 0.03878509998321533,
      "learning_rate": 7.509065329018469e-05,
      "loss": 0.0469,
      "step": 6037
    },
    {
      "epoch": 1.4941846077703538,
      "grad_norm": 0.07208845764398575,
      "learning_rate": 7.502114653269681e-05,
      "loss": 0.069,
      "step": 6038
    },
    {
      "epoch": 1.4944320712694878,
      "grad_norm": 0.033428043127059937,
      "learning_rate": 7.495166627991535e-05,
      "loss": 0.0366,
      "step": 6039
    },
    {
      "epoch": 1.4946795347686217,
      "grad_norm": 0.05089344456791878,
      "learning_rate": 7.488221254236491e-05,
      "loss": 0.0249,
      "step": 6040
    },
    {
      "epoch": 1.4949269982677555,
      "grad_norm": 0.04259486496448517,
      "learning_rate": 7.481278533056596e-05,
      "loss": 0.0399,
      "step": 6041
    },
    {
      "epoch": 1.4951744617668894,
      "grad_norm": 0.05184916406869888,
      "learning_rate": 7.474338465503468e-05,
      "loss": 0.0427,
      "step": 6042
    },
    {
      "epoch": 1.4954219252660232,
      "grad_norm": 0.039199911057949066,
      "learning_rate": 7.467401052628362e-05,
      "loss": 0.0228,
      "step": 6043
    },
    {
      "epoch": 1.495669388765157,
      "grad_norm": 0.0392322801053524,
      "learning_rate": 7.460466295482108e-05,
      "loss": 0.0403,
      "step": 6044
    },
    {
      "epoch": 1.495916852264291,
      "grad_norm": 0.023977655917406082,
      "learning_rate": 7.453534195115139e-05,
      "loss": 0.0162,
      "step": 6045
    },
    {
      "epoch": 1.496164315763425,
      "grad_norm": 0.045661553740501404,
      "learning_rate": 7.446604752577498e-05,
      "loss": 0.0434,
      "step": 6046
    },
    {
      "epoch": 1.4964117792625586,
      "grad_norm": 0.03478386998176575,
      "learning_rate": 7.439677968918787e-05,
      "loss": 0.0263,
      "step": 6047
    },
    {
      "epoch": 1.4966592427616927,
      "grad_norm": 0.031074564903974533,
      "learning_rate": 7.432753845188267e-05,
      "loss": 0.0209,
      "step": 6048
    },
    {
      "epoch": 1.4969067062608266,
      "grad_norm": 0.03920417279005051,
      "learning_rate": 7.425832382434736e-05,
      "loss": 0.0204,
      "step": 6049
    },
    {
      "epoch": 1.4971541697599604,
      "grad_norm": 0.04447096958756447,
      "learning_rate": 7.418913581706619e-05,
      "loss": 0.0375,
      "step": 6050
    },
    {
      "epoch": 1.4974016332590943,
      "grad_norm": 0.04248691722750664,
      "learning_rate": 7.411997444051938e-05,
      "loss": 0.0328,
      "step": 6051
    },
    {
      "epoch": 1.497649096758228,
      "grad_norm": 0.04006117209792137,
      "learning_rate": 7.4050839705183e-05,
      "loss": 0.0584,
      "step": 6052
    },
    {
      "epoch": 1.4978965602573622,
      "grad_norm": 0.03738217055797577,
      "learning_rate": 7.398173162152929e-05,
      "loss": 0.0608,
      "step": 6053
    },
    {
      "epoch": 1.4981440237564958,
      "grad_norm": 0.045193541795015335,
      "learning_rate": 7.391265020002614e-05,
      "loss": 0.0487,
      "step": 6054
    },
    {
      "epoch": 1.4983914872556299,
      "grad_norm": 0.026621989905834198,
      "learning_rate": 7.384359545113755e-05,
      "loss": 0.0271,
      "step": 6055
    },
    {
      "epoch": 1.4986389507547637,
      "grad_norm": 0.03399418666958809,
      "learning_rate": 7.377456738532376e-05,
      "loss": 0.0512,
      "step": 6056
    },
    {
      "epoch": 1.4988864142538976,
      "grad_norm": 0.048796702176332474,
      "learning_rate": 7.370556601304046e-05,
      "loss": 0.0521,
      "step": 6057
    },
    {
      "epoch": 1.4991338777530314,
      "grad_norm": 0.10888675600290298,
      "learning_rate": 7.363659134473963e-05,
      "loss": 0.1157,
      "step": 6058
    },
    {
      "epoch": 1.4993813412521653,
      "grad_norm": 0.0685541182756424,
      "learning_rate": 7.356764339086913e-05,
      "loss": 0.0589,
      "step": 6059
    },
    {
      "epoch": 1.4996288047512991,
      "grad_norm": 0.0535910539329052,
      "learning_rate": 7.349872216187276e-05,
      "loss": 0.0334,
      "step": 6060
    },
    {
      "epoch": 1.499876268250433,
      "grad_norm": 0.041977256536483765,
      "learning_rate": 7.342982766819036e-05,
      "loss": 0.0348,
      "step": 6061
    },
    {
      "epoch": 1.500123731749567,
      "grad_norm": 0.06398007273674011,
      "learning_rate": 7.336095992025751e-05,
      "loss": 0.0482,
      "step": 6062
    },
    {
      "epoch": 1.5003711952487007,
      "grad_norm": 0.04661167412996292,
      "learning_rate": 7.329211892850588e-05,
      "loss": 0.0548,
      "step": 6063
    },
    {
      "epoch": 1.5006186587478347,
      "grad_norm": 0.05125531926751137,
      "learning_rate": 7.322330470336314e-05,
      "loss": 0.0675,
      "step": 6064
    },
    {
      "epoch": 1.5008661222469686,
      "grad_norm": 0.06654147803783417,
      "learning_rate": 7.31545172552528e-05,
      "loss": 0.0471,
      "step": 6065
    },
    {
      "epoch": 1.5011135857461024,
      "grad_norm": 0.036635272204875946,
      "learning_rate": 7.308575659459437e-05,
      "loss": 0.0363,
      "step": 6066
    },
    {
      "epoch": 1.5013610492452363,
      "grad_norm": 0.04791272431612015,
      "learning_rate": 7.301702273180328e-05,
      "loss": 0.0559,
      "step": 6067
    },
    {
      "epoch": 1.5016085127443701,
      "grad_norm": 0.07439320534467697,
      "learning_rate": 7.294831567729098e-05,
      "loss": 0.067,
      "step": 6068
    },
    {
      "epoch": 1.5018559762435042,
      "grad_norm": 0.05419318377971649,
      "learning_rate": 7.287963544146467e-05,
      "loss": 0.0438,
      "step": 6069
    },
    {
      "epoch": 1.5021034397426378,
      "grad_norm": 0.0596187487244606,
      "learning_rate": 7.28109820347276e-05,
      "loss": 0.0841,
      "step": 6070
    },
    {
      "epoch": 1.502350903241772,
      "grad_norm": 0.04425403103232384,
      "learning_rate": 7.274235546747906e-05,
      "loss": 0.0506,
      "step": 6071
    },
    {
      "epoch": 1.5025983667409057,
      "grad_norm": 0.041169989854097366,
      "learning_rate": 7.267375575011411e-05,
      "loss": 0.0399,
      "step": 6072
    },
    {
      "epoch": 1.5028458302400396,
      "grad_norm": 0.04564789682626724,
      "learning_rate": 7.26051828930239e-05,
      "loss": 0.0678,
      "step": 6073
    },
    {
      "epoch": 1.5030932937391734,
      "grad_norm": 0.06274881958961487,
      "learning_rate": 7.253663690659521e-05,
      "loss": 0.0399,
      "step": 6074
    },
    {
      "epoch": 1.5033407572383073,
      "grad_norm": 0.0585634745657444,
      "learning_rate": 7.246811780121113e-05,
      "loss": 0.0718,
      "step": 6075
    },
    {
      "epoch": 1.5035882207374414,
      "grad_norm": 0.05089801922440529,
      "learning_rate": 7.239962558725058e-05,
      "loss": 0.0521,
      "step": 6076
    },
    {
      "epoch": 1.503835684236575,
      "grad_norm": 0.057573553174734116,
      "learning_rate": 7.233116027508813e-05,
      "loss": 0.0512,
      "step": 6077
    },
    {
      "epoch": 1.504083147735709,
      "grad_norm": 0.06502363830804825,
      "learning_rate": 7.226272187509455e-05,
      "loss": 0.0661,
      "step": 6078
    },
    {
      "epoch": 1.504330611234843,
      "grad_norm": 0.059335727244615555,
      "learning_rate": 7.219431039763652e-05,
      "loss": 0.0899,
      "step": 6079
    },
    {
      "epoch": 1.5045780747339768,
      "grad_norm": 0.04010754078626633,
      "learning_rate": 7.212592585307653e-05,
      "loss": 0.0307,
      "step": 6080
    },
    {
      "epoch": 1.5048255382331106,
      "grad_norm": 0.06748628616333008,
      "learning_rate": 7.205756825177314e-05,
      "loss": 0.0995,
      "step": 6081
    },
    {
      "epoch": 1.5050730017322445,
      "grad_norm": 0.028437530621886253,
      "learning_rate": 7.198923760408053e-05,
      "loss": 0.037,
      "step": 6082
    },
    {
      "epoch": 1.5053204652313785,
      "grad_norm": 0.03334260359406471,
      "learning_rate": 7.192093392034926e-05,
      "loss": 0.0599,
      "step": 6083
    },
    {
      "epoch": 1.5055679287305122,
      "grad_norm": 0.05256495252251625,
      "learning_rate": 7.185265721092535e-05,
      "loss": 0.0623,
      "step": 6084
    },
    {
      "epoch": 1.5058153922296462,
      "grad_norm": 0.043132632970809937,
      "learning_rate": 7.178440748615097e-05,
      "loss": 0.0552,
      "step": 6085
    },
    {
      "epoch": 1.5060628557287798,
      "grad_norm": 0.048918720334768295,
      "learning_rate": 7.171618475636421e-05,
      "loss": 0.0412,
      "step": 6086
    },
    {
      "epoch": 1.506310319227914,
      "grad_norm": 0.039412721991539,
      "learning_rate": 7.164798903189901e-05,
      "loss": 0.0413,
      "step": 6087
    },
    {
      "epoch": 1.5065577827270478,
      "grad_norm": 0.03415364399552345,
      "learning_rate": 7.157982032308527e-05,
      "loss": 0.0358,
      "step": 6088
    },
    {
      "epoch": 1.5068052462261816,
      "grad_norm": 0.049716562032699585,
      "learning_rate": 7.151167864024868e-05,
      "loss": 0.0533,
      "step": 6089
    },
    {
      "epoch": 1.5070527097253155,
      "grad_norm": 0.04847480356693268,
      "learning_rate": 7.144356399371085e-05,
      "loss": 0.0627,
      "step": 6090
    },
    {
      "epoch": 1.5073001732244493,
      "grad_norm": 0.09173381328582764,
      "learning_rate": 7.137547639378964e-05,
      "loss": 0.0804,
      "step": 6091
    },
    {
      "epoch": 1.5075476367235834,
      "grad_norm": 0.05573881417512894,
      "learning_rate": 7.130741585079825e-05,
      "loss": 0.0569,
      "step": 6092
    },
    {
      "epoch": 1.507795100222717,
      "grad_norm": 0.0695001631975174,
      "learning_rate": 7.123938237504629e-05,
      "loss": 0.0491,
      "step": 6093
    },
    {
      "epoch": 1.508042563721851,
      "grad_norm": 0.09157132357358932,
      "learning_rate": 7.117137597683879e-05,
      "loss": 0.1297,
      "step": 6094
    },
    {
      "epoch": 1.508290027220985,
      "grad_norm": 0.05885021761059761,
      "learning_rate": 7.110339666647714e-05,
      "loss": 0.0896,
      "step": 6095
    },
    {
      "epoch": 1.5085374907201188,
      "grad_norm": 0.046950940042734146,
      "learning_rate": 7.103544445425844e-05,
      "loss": 0.0759,
      "step": 6096
    },
    {
      "epoch": 1.5087849542192526,
      "grad_norm": 0.04087582975625992,
      "learning_rate": 7.096751935047552e-05,
      "loss": 0.0461,
      "step": 6097
    },
    {
      "epoch": 1.5090324177183865,
      "grad_norm": 0.05654139816761017,
      "learning_rate": 7.08996213654173e-05,
      "loss": 0.0283,
      "step": 6098
    },
    {
      "epoch": 1.5092798812175205,
      "grad_norm": 0.0824519619345665,
      "learning_rate": 7.08317505093686e-05,
      "loss": 0.0391,
      "step": 6099
    },
    {
      "epoch": 1.5095273447166542,
      "grad_norm": 0.08469977974891663,
      "learning_rate": 7.076390679261002e-05,
      "loss": 0.0575,
      "step": 6100
    },
    {
      "epoch": 1.5097748082157882,
      "grad_norm": 0.05129362642765045,
      "learning_rate": 7.06960902254182e-05,
      "loss": 0.0555,
      "step": 6101
    },
    {
      "epoch": 1.510022271714922,
      "grad_norm": 0.04861169308423996,
      "learning_rate": 7.062830081806538e-05,
      "loss": 0.0343,
      "step": 6102
    },
    {
      "epoch": 1.510269735214056,
      "grad_norm": 0.08420783281326294,
      "learning_rate": 7.056053858082012e-05,
      "loss": 0.136,
      "step": 6103
    },
    {
      "epoch": 1.5105171987131898,
      "grad_norm": 0.06194361671805382,
      "learning_rate": 7.049280352394641e-05,
      "loss": 0.0404,
      "step": 6104
    },
    {
      "epoch": 1.5107646622123236,
      "grad_norm": 0.03921705484390259,
      "learning_rate": 7.042509565770442e-05,
      "loss": 0.0411,
      "step": 6105
    },
    {
      "epoch": 1.5110121257114577,
      "grad_norm": 0.04623841494321823,
      "learning_rate": 7.035741499235013e-05,
      "loss": 0.0484,
      "step": 6106
    },
    {
      "epoch": 1.5112595892105913,
      "grad_norm": 0.05638821795582771,
      "learning_rate": 7.028976153813537e-05,
      "loss": 0.0695,
      "step": 6107
    },
    {
      "epoch": 1.5115070527097254,
      "grad_norm": 0.046864598989486694,
      "learning_rate": 7.022213530530794e-05,
      "loss": 0.0613,
      "step": 6108
    },
    {
      "epoch": 1.511754516208859,
      "grad_norm": 0.07259447127580643,
      "learning_rate": 7.015453630411129e-05,
      "loss": 0.0941,
      "step": 6109
    },
    {
      "epoch": 1.512001979707993,
      "grad_norm": 0.06642723083496094,
      "learning_rate": 7.008696454478486e-05,
      "loss": 0.0447,
      "step": 6110
    },
    {
      "epoch": 1.512249443207127,
      "grad_norm": 0.036840975284576416,
      "learning_rate": 7.001942003756428e-05,
      "loss": 0.0542,
      "step": 6111
    },
    {
      "epoch": 1.5124969067062608,
      "grad_norm": 0.07493634521961212,
      "learning_rate": 6.995190279268055e-05,
      "loss": 0.0967,
      "step": 6112
    },
    {
      "epoch": 1.5127443702053947,
      "grad_norm": 0.034126054495573044,
      "learning_rate": 6.988441282036079e-05,
      "loss": 0.0304,
      "step": 6113
    },
    {
      "epoch": 1.5129918337045285,
      "grad_norm": 0.07174995541572571,
      "learning_rate": 6.981695013082797e-05,
      "loss": 0.0664,
      "step": 6114
    },
    {
      "epoch": 1.5132392972036626,
      "grad_norm": 0.027312325313687325,
      "learning_rate": 6.974951473430097e-05,
      "loss": 0.0289,
      "step": 6115
    },
    {
      "epoch": 1.5134867607027962,
      "grad_norm": 0.039685141295194626,
      "learning_rate": 6.96821066409945e-05,
      "loss": 0.0304,
      "step": 6116
    },
    {
      "epoch": 1.5137342242019303,
      "grad_norm": 0.06892185658216476,
      "learning_rate": 6.961472586111897e-05,
      "loss": 0.0551,
      "step": 6117
    },
    {
      "epoch": 1.5139816877010641,
      "grad_norm": 0.05121446028351784,
      "learning_rate": 6.95473724048809e-05,
      "loss": 0.0541,
      "step": 6118
    },
    {
      "epoch": 1.514229151200198,
      "grad_norm": 0.04329204559326172,
      "learning_rate": 6.948004628248259e-05,
      "loss": 0.0381,
      "step": 6119
    },
    {
      "epoch": 1.5144766146993318,
      "grad_norm": 0.0750872790813446,
      "learning_rate": 6.941274750412214e-05,
      "loss": 0.0708,
      "step": 6120
    },
    {
      "epoch": 1.5147240781984657,
      "grad_norm": 0.04183303192257881,
      "learning_rate": 6.934547607999356e-05,
      "loss": 0.0378,
      "step": 6121
    },
    {
      "epoch": 1.5149715416975997,
      "grad_norm": 0.06615152955055237,
      "learning_rate": 6.927823202028671e-05,
      "loss": 0.1021,
      "step": 6122
    },
    {
      "epoch": 1.5152190051967334,
      "grad_norm": 0.032995786517858505,
      "learning_rate": 6.921101533518737e-05,
      "loss": 0.0352,
      "step": 6123
    },
    {
      "epoch": 1.5154664686958674,
      "grad_norm": 0.08518154919147491,
      "learning_rate": 6.914382603487699e-05,
      "loss": 0.0926,
      "step": 6124
    },
    {
      "epoch": 1.5157139321950013,
      "grad_norm": 0.05927992984652519,
      "learning_rate": 6.907666412953301e-05,
      "loss": 0.0478,
      "step": 6125
    },
    {
      "epoch": 1.5159613956941351,
      "grad_norm": 0.05645228177309036,
      "learning_rate": 6.900952962932875e-05,
      "loss": 0.0695,
      "step": 6126
    },
    {
      "epoch": 1.516208859193269,
      "grad_norm": 0.0484560951590538,
      "learning_rate": 6.894242254443326e-05,
      "loss": 0.0467,
      "step": 6127
    },
    {
      "epoch": 1.5164563226924028,
      "grad_norm": 0.14308881759643555,
      "learning_rate": 6.887534288501157e-05,
      "loss": 0.1119,
      "step": 6128
    },
    {
      "epoch": 1.516703786191537,
      "grad_norm": 0.06109621375799179,
      "learning_rate": 6.880829066122443e-05,
      "loss": 0.0717,
      "step": 6129
    },
    {
      "epoch": 1.5169512496906705,
      "grad_norm": 0.03936575725674629,
      "learning_rate": 6.874126588322855e-05,
      "loss": 0.0404,
      "step": 6130
    },
    {
      "epoch": 1.5171987131898046,
      "grad_norm": 0.059807777404785156,
      "learning_rate": 6.867426856117648e-05,
      "loss": 0.1016,
      "step": 6131
    },
    {
      "epoch": 1.5174461766889382,
      "grad_norm": 0.06881716847419739,
      "learning_rate": 6.860729870521637e-05,
      "loss": 0.0837,
      "step": 6132
    },
    {
      "epoch": 1.5176936401880723,
      "grad_norm": 0.05335088074207306,
      "learning_rate": 6.854035632549251e-05,
      "loss": 0.0437,
      "step": 6133
    },
    {
      "epoch": 1.5179411036872061,
      "grad_norm": 0.033876679837703705,
      "learning_rate": 6.847344143214493e-05,
      "loss": 0.0354,
      "step": 6134
    },
    {
      "epoch": 1.51818856718634,
      "grad_norm": 0.04191598668694496,
      "learning_rate": 6.840655403530943e-05,
      "loss": 0.031,
      "step": 6135
    },
    {
      "epoch": 1.518436030685474,
      "grad_norm": 0.0450158417224884,
      "learning_rate": 6.833969414511782e-05,
      "loss": 0.0891,
      "step": 6136
    },
    {
      "epoch": 1.5186834941846077,
      "grad_norm": 0.0551404133439064,
      "learning_rate": 6.827286177169736e-05,
      "loss": 0.1143,
      "step": 6137
    },
    {
      "epoch": 1.5189309576837418,
      "grad_norm": 0.06272713094949722,
      "learning_rate": 6.820605692517165e-05,
      "loss": 0.0752,
      "step": 6138
    },
    {
      "epoch": 1.5191784211828754,
      "grad_norm": 0.03450735658407211,
      "learning_rate": 6.813927961565986e-05,
      "loss": 0.0369,
      "step": 6139
    },
    {
      "epoch": 1.5194258846820095,
      "grad_norm": 0.04026179760694504,
      "learning_rate": 6.807252985327684e-05,
      "loss": 0.0477,
      "step": 6140
    },
    {
      "epoch": 1.5196733481811433,
      "grad_norm": 0.04916049540042877,
      "learning_rate": 6.800580764813352e-05,
      "loss": 0.0655,
      "step": 6141
    },
    {
      "epoch": 1.5199208116802772,
      "grad_norm": 0.058388758450746536,
      "learning_rate": 6.793911301033656e-05,
      "loss": 0.0882,
      "step": 6142
    },
    {
      "epoch": 1.520168275179411,
      "grad_norm": 0.03124573454260826,
      "learning_rate": 6.787244594998842e-05,
      "loss": 0.0318,
      "step": 6143
    },
    {
      "epoch": 1.5204157386785448,
      "grad_norm": 0.06457079946994781,
      "learning_rate": 6.780580647718757e-05,
      "loss": 0.0532,
      "step": 6144
    },
    {
      "epoch": 1.520663202177679,
      "grad_norm": 0.04025779664516449,
      "learning_rate": 6.773919460202782e-05,
      "loss": 0.036,
      "step": 6145
    },
    {
      "epoch": 1.5209106656768125,
      "grad_norm": 0.04749266803264618,
      "learning_rate": 6.767261033459948e-05,
      "loss": 0.0306,
      "step": 6146
    },
    {
      "epoch": 1.5211581291759466,
      "grad_norm": 0.049920789897441864,
      "learning_rate": 6.760605368498807e-05,
      "loss": 0.0524,
      "step": 6147
    },
    {
      "epoch": 1.5214055926750805,
      "grad_norm": 0.0382666178047657,
      "learning_rate": 6.753952466327526e-05,
      "loss": 0.0448,
      "step": 6148
    },
    {
      "epoch": 1.5216530561742143,
      "grad_norm": 0.11228523403406143,
      "learning_rate": 6.747302327953848e-05,
      "loss": 0.0752,
      "step": 6149
    },
    {
      "epoch": 1.5219005196733482,
      "grad_norm": 0.04086665064096451,
      "learning_rate": 6.740654954385087e-05,
      "loss": 0.0537,
      "step": 6150
    },
    {
      "epoch": 1.522147983172482,
      "grad_norm": 0.029631486162543297,
      "learning_rate": 6.734010346628158e-05,
      "loss": 0.0397,
      "step": 6151
    },
    {
      "epoch": 1.522395446671616,
      "grad_norm": 0.06559759378433228,
      "learning_rate": 6.727368505689532e-05,
      "loss": 0.0641,
      "step": 6152
    },
    {
      "epoch": 1.5226429101707497,
      "grad_norm": 0.05062495172023773,
      "learning_rate": 6.72072943257527e-05,
      "loss": 0.0814,
      "step": 6153
    },
    {
      "epoch": 1.5228903736698838,
      "grad_norm": 0.040495943278074265,
      "learning_rate": 6.714093128291043e-05,
      "loss": 0.0264,
      "step": 6154
    },
    {
      "epoch": 1.5231378371690174,
      "grad_norm": 0.03538087010383606,
      "learning_rate": 6.707459593842052e-05,
      "loss": 0.0403,
      "step": 6155
    },
    {
      "epoch": 1.5233853006681515,
      "grad_norm": 0.039999231696128845,
      "learning_rate": 6.700828830233111e-05,
      "loss": 0.041,
      "step": 6156
    },
    {
      "epoch": 1.5236327641672853,
      "grad_norm": 0.05202455446124077,
      "learning_rate": 6.694200838468609e-05,
      "loss": 0.0458,
      "step": 6157
    },
    {
      "epoch": 1.5238802276664192,
      "grad_norm": 0.035775866359472275,
      "learning_rate": 6.68757561955251e-05,
      "loss": 0.0314,
      "step": 6158
    },
    {
      "epoch": 1.5241276911655532,
      "grad_norm": 0.08430182188749313,
      "learning_rate": 6.680953174488371e-05,
      "loss": 0.1094,
      "step": 6159
    },
    {
      "epoch": 1.5243751546646869,
      "grad_norm": 0.03320382907986641,
      "learning_rate": 6.674333504279301e-05,
      "loss": 0.0534,
      "step": 6160
    },
    {
      "epoch": 1.524622618163821,
      "grad_norm": 0.09877540916204453,
      "learning_rate": 6.667716609928015e-05,
      "loss": 0.048,
      "step": 6161
    },
    {
      "epoch": 1.5248700816629546,
      "grad_norm": 0.0422397144138813,
      "learning_rate": 6.661102492436802e-05,
      "loss": 0.0252,
      "step": 6162
    },
    {
      "epoch": 1.5251175451620886,
      "grad_norm": 0.0553835965692997,
      "learning_rate": 6.654491152807523e-05,
      "loss": 0.084,
      "step": 6163
    },
    {
      "epoch": 1.5253650086612225,
      "grad_norm": 0.04597197845578194,
      "learning_rate": 6.647882592041624e-05,
      "loss": 0.0368,
      "step": 6164
    },
    {
      "epoch": 1.5256124721603563,
      "grad_norm": 0.05505363643169403,
      "learning_rate": 6.64127681114013e-05,
      "loss": 0.0452,
      "step": 6165
    },
    {
      "epoch": 1.5258599356594902,
      "grad_norm": 0.06195782497525215,
      "learning_rate": 6.634673811103648e-05,
      "loss": 0.0685,
      "step": 6166
    },
    {
      "epoch": 1.526107399158624,
      "grad_norm": 0.09840060770511627,
      "learning_rate": 6.628073592932346e-05,
      "loss": 0.1097,
      "step": 6167
    },
    {
      "epoch": 1.526354862657758,
      "grad_norm": 0.08013329654932022,
      "learning_rate": 6.621476157625994e-05,
      "loss": 0.1096,
      "step": 6168
    },
    {
      "epoch": 1.5266023261568917,
      "grad_norm": 0.05092934891581535,
      "learning_rate": 6.614881506183926e-05,
      "loss": 0.0845,
      "step": 6169
    },
    {
      "epoch": 1.5268497896560258,
      "grad_norm": 0.05486540123820305,
      "learning_rate": 6.608289639605062e-05,
      "loss": 0.0407,
      "step": 6170
    },
    {
      "epoch": 1.5270972531551597,
      "grad_norm": 0.08107840269804001,
      "learning_rate": 6.601700558887902e-05,
      "loss": 0.0607,
      "step": 6171
    },
    {
      "epoch": 1.5273447166542935,
      "grad_norm": 0.04779692366719246,
      "learning_rate": 6.595114265030497e-05,
      "loss": 0.0438,
      "step": 6172
    },
    {
      "epoch": 1.5275921801534273,
      "grad_norm": 0.04423518478870392,
      "learning_rate": 6.588530759030519e-05,
      "loss": 0.0523,
      "step": 6173
    },
    {
      "epoch": 1.5278396436525612,
      "grad_norm": 0.08043601363897324,
      "learning_rate": 6.581950041885201e-05,
      "loss": 0.0302,
      "step": 6174
    },
    {
      "epoch": 1.5280871071516953,
      "grad_norm": 0.03929181024432182,
      "learning_rate": 6.575372114591332e-05,
      "loss": 0.0446,
      "step": 6175
    },
    {
      "epoch": 1.528334570650829,
      "grad_norm": 0.025294994935393333,
      "learning_rate": 6.5687969781453e-05,
      "loss": 0.019,
      "step": 6176
    },
    {
      "epoch": 1.528582034149963,
      "grad_norm": 0.0562208853662014,
      "learning_rate": 6.562224633543068e-05,
      "loss": 0.0545,
      "step": 6177
    },
    {
      "epoch": 1.5288294976490968,
      "grad_norm": 0.0301671139895916,
      "learning_rate": 6.555655081780176e-05,
      "loss": 0.0543,
      "step": 6178
    },
    {
      "epoch": 1.5290769611482307,
      "grad_norm": 0.04586511105298996,
      "learning_rate": 6.549088323851742e-05,
      "loss": 0.0441,
      "step": 6179
    },
    {
      "epoch": 1.5293244246473645,
      "grad_norm": 0.035885319113731384,
      "learning_rate": 6.542524360752444e-05,
      "loss": 0.0579,
      "step": 6180
    },
    {
      "epoch": 1.5295718881464984,
      "grad_norm": 0.04437617212533951,
      "learning_rate": 6.53596319347656e-05,
      "loss": 0.0623,
      "step": 6181
    },
    {
      "epoch": 1.5298193516456324,
      "grad_norm": 0.08546876162290573,
      "learning_rate": 6.529404823017932e-05,
      "loss": 0.123,
      "step": 6182
    },
    {
      "epoch": 1.530066815144766,
      "grad_norm": 0.05868299677968025,
      "learning_rate": 6.52284925036998e-05,
      "loss": 0.0651,
      "step": 6183
    },
    {
      "epoch": 1.5303142786439001,
      "grad_norm": 0.0884551852941513,
      "learning_rate": 6.516296476525707e-05,
      "loss": 0.0371,
      "step": 6184
    },
    {
      "epoch": 1.5305617421430338,
      "grad_norm": 0.04651038348674774,
      "learning_rate": 6.509746502477681e-05,
      "loss": 0.0311,
      "step": 6185
    },
    {
      "epoch": 1.5308092056421678,
      "grad_norm": 0.053685884922742844,
      "learning_rate": 6.50319932921806e-05,
      "loss": 0.047,
      "step": 6186
    },
    {
      "epoch": 1.5310566691413017,
      "grad_norm": 0.06782086193561554,
      "learning_rate": 6.496654957738554e-05,
      "loss": 0.0607,
      "step": 6187
    },
    {
      "epoch": 1.5313041326404355,
      "grad_norm": 0.04141989350318909,
      "learning_rate": 6.490113389030475e-05,
      "loss": 0.0303,
      "step": 6188
    },
    {
      "epoch": 1.5315515961395694,
      "grad_norm": 0.07485037297010422,
      "learning_rate": 6.483574624084693e-05,
      "loss": 0.0645,
      "step": 6189
    },
    {
      "epoch": 1.5317990596387032,
      "grad_norm": 0.03968179598450661,
      "learning_rate": 6.477038663891663e-05,
      "loss": 0.0758,
      "step": 6190
    },
    {
      "epoch": 1.5320465231378373,
      "grad_norm": 0.054234448820352554,
      "learning_rate": 6.470505509441415e-05,
      "loss": 0.0355,
      "step": 6191
    },
    {
      "epoch": 1.532293986636971,
      "grad_norm": 0.03222769498825073,
      "learning_rate": 6.463975161723534e-05,
      "loss": 0.0337,
      "step": 6192
    },
    {
      "epoch": 1.532541450136105,
      "grad_norm": 0.030419792979955673,
      "learning_rate": 6.457447621727217e-05,
      "loss": 0.0162,
      "step": 6193
    },
    {
      "epoch": 1.5327889136352388,
      "grad_norm": 0.059413403272628784,
      "learning_rate": 6.450922890441214e-05,
      "loss": 0.0513,
      "step": 6194
    },
    {
      "epoch": 1.5330363771343727,
      "grad_norm": 0.03407684713602066,
      "learning_rate": 6.444400968853834e-05,
      "loss": 0.0395,
      "step": 6195
    },
    {
      "epoch": 1.5332838406335065,
      "grad_norm": 0.06390004605054855,
      "learning_rate": 6.437881857952987e-05,
      "loss": 0.038,
      "step": 6196
    },
    {
      "epoch": 1.5335313041326404,
      "grad_norm": 0.04281725361943245,
      "learning_rate": 6.431365558726146e-05,
      "loss": 0.0365,
      "step": 6197
    },
    {
      "epoch": 1.5337787676317745,
      "grad_norm": 0.05902910977602005,
      "learning_rate": 6.424852072160358e-05,
      "loss": 0.0349,
      "step": 6198
    },
    {
      "epoch": 1.534026231130908,
      "grad_norm": 0.0660879984498024,
      "learning_rate": 6.418341399242258e-05,
      "loss": 0.0605,
      "step": 6199
    },
    {
      "epoch": 1.5342736946300422,
      "grad_norm": 0.05164504051208496,
      "learning_rate": 6.411833540958018e-05,
      "loss": 0.0381,
      "step": 6200
    },
    {
      "epoch": 1.5342736946300422,
      "eval_loss": 0.2893665134906769,
      "eval_runtime": 202.6585,
      "eval_samples_per_second": 4.934,
      "eval_steps_per_second": 0.311,
      "step": 6200
    },
    {
      "epoch": 1.534521158129176,
      "grad_norm": 0.03565135970711708,
      "learning_rate": 6.405328498293434e-05,
      "loss": 0.0225,
      "step": 6201
    },
    {
      "epoch": 1.5347686216283098,
      "grad_norm": 0.05124526470899582,
      "learning_rate": 6.39882627223383e-05,
      "loss": 0.0607,
      "step": 6202
    },
    {
      "epoch": 1.5350160851274437,
      "grad_norm": 0.03711917996406555,
      "learning_rate": 6.392326863764128e-05,
      "loss": 0.0449,
      "step": 6203
    },
    {
      "epoch": 1.5352635486265775,
      "grad_norm": 0.08823700249195099,
      "learning_rate": 6.38583027386882e-05,
      "loss": 0.0813,
      "step": 6204
    },
    {
      "epoch": 1.5355110121257116,
      "grad_norm": 0.03583386540412903,
      "learning_rate": 6.379336503531968e-05,
      "loss": 0.0276,
      "step": 6205
    },
    {
      "epoch": 1.5357584756248452,
      "grad_norm": 0.04381144419312477,
      "learning_rate": 6.372845553737217e-05,
      "loss": 0.0392,
      "step": 6206
    },
    {
      "epoch": 1.5360059391239793,
      "grad_norm": 0.043932829052209854,
      "learning_rate": 6.366357425467756e-05,
      "loss": 0.0443,
      "step": 6207
    },
    {
      "epoch": 1.536253402623113,
      "grad_norm": 0.08283314853906631,
      "learning_rate": 6.359872119706372e-05,
      "loss": 0.0733,
      "step": 6208
    },
    {
      "epoch": 1.536500866122247,
      "grad_norm": 0.028992556035518646,
      "learning_rate": 6.353389637435438e-05,
      "loss": 0.0274,
      "step": 6209
    },
    {
      "epoch": 1.5367483296213809,
      "grad_norm": 0.04798435792326927,
      "learning_rate": 6.346909979636861e-05,
      "loss": 0.0928,
      "step": 6210
    },
    {
      "epoch": 1.5369957931205147,
      "grad_norm": 0.03127294033765793,
      "learning_rate": 6.340433147292141e-05,
      "loss": 0.0339,
      "step": 6211
    },
    {
      "epoch": 1.5372432566196486,
      "grad_norm": 0.0548178106546402,
      "learning_rate": 6.333959141382351e-05,
      "loss": 0.0707,
      "step": 6212
    },
    {
      "epoch": 1.5374907201187824,
      "grad_norm": 0.067827507853508,
      "learning_rate": 6.327487962888135e-05,
      "loss": 0.0397,
      "step": 6213
    },
    {
      "epoch": 1.5377381836179165,
      "grad_norm": 0.02814667299389839,
      "learning_rate": 6.321019612789711e-05,
      "loss": 0.0257,
      "step": 6214
    },
    {
      "epoch": 1.53798564711705,
      "grad_norm": 0.05747886374592781,
      "learning_rate": 6.314554092066852e-05,
      "loss": 0.0567,
      "step": 6215
    },
    {
      "epoch": 1.5382331106161842,
      "grad_norm": 0.06381011754274368,
      "learning_rate": 6.308091401698921e-05,
      "loss": 0.1029,
      "step": 6216
    },
    {
      "epoch": 1.538480574115318,
      "grad_norm": 0.05939636752009392,
      "learning_rate": 6.301631542664846e-05,
      "loss": 0.0444,
      "step": 6217
    },
    {
      "epoch": 1.5387280376144519,
      "grad_norm": 0.032571349292993546,
      "learning_rate": 6.295174515943126e-05,
      "loss": 0.0414,
      "step": 6218
    },
    {
      "epoch": 1.5389755011135857,
      "grad_norm": 0.05064936727285385,
      "learning_rate": 6.288720322511832e-05,
      "loss": 0.0557,
      "step": 6219
    },
    {
      "epoch": 1.5392229646127196,
      "grad_norm": 0.0640707015991211,
      "learning_rate": 6.282268963348604e-05,
      "loss": 0.0475,
      "step": 6220
    },
    {
      "epoch": 1.5394704281118536,
      "grad_norm": 0.04557857662439346,
      "learning_rate": 6.275820439430663e-05,
      "loss": 0.0504,
      "step": 6221
    },
    {
      "epoch": 1.5397178916109873,
      "grad_norm": 0.04208024591207504,
      "learning_rate": 6.269374751734775e-05,
      "loss": 0.0396,
      "step": 6222
    },
    {
      "epoch": 1.5399653551101213,
      "grad_norm": 0.029655637219548225,
      "learning_rate": 6.262931901237298e-05,
      "loss": 0.0385,
      "step": 6223
    },
    {
      "epoch": 1.5402128186092552,
      "grad_norm": 0.04237678274512291,
      "learning_rate": 6.25649188891416e-05,
      "loss": 0.0472,
      "step": 6224
    },
    {
      "epoch": 1.540460282108389,
      "grad_norm": 0.08462632447481155,
      "learning_rate": 6.25005471574085e-05,
      "loss": 0.0816,
      "step": 6225
    },
    {
      "epoch": 1.5407077456075229,
      "grad_norm": 0.05174017697572708,
      "learning_rate": 6.243620382692436e-05,
      "loss": 0.0423,
      "step": 6226
    },
    {
      "epoch": 1.5409552091066567,
      "grad_norm": 0.0520513691008091,
      "learning_rate": 6.237188890743536e-05,
      "loss": 0.0302,
      "step": 6227
    },
    {
      "epoch": 1.5412026726057908,
      "grad_norm": 0.040194690227508545,
      "learning_rate": 6.230760240868369e-05,
      "loss": 0.0573,
      "step": 6228
    },
    {
      "epoch": 1.5414501361049244,
      "grad_norm": 0.04757240042090416,
      "learning_rate": 6.224334434040707e-05,
      "loss": 0.065,
      "step": 6229
    },
    {
      "epoch": 1.5416975996040585,
      "grad_norm": 0.03950703144073486,
      "learning_rate": 6.217911471233878e-05,
      "loss": 0.0286,
      "step": 6230
    },
    {
      "epoch": 1.5419450631031921,
      "grad_norm": 0.04047326743602753,
      "learning_rate": 6.2114913534208e-05,
      "loss": 0.039,
      "step": 6231
    },
    {
      "epoch": 1.5421925266023262,
      "grad_norm": 0.04894595593214035,
      "learning_rate": 6.205074081573953e-05,
      "loss": 0.0524,
      "step": 6232
    },
    {
      "epoch": 1.54243999010146,
      "grad_norm": 0.052474550902843475,
      "learning_rate": 6.198659656665381e-05,
      "loss": 0.0754,
      "step": 6233
    },
    {
      "epoch": 1.542687453600594,
      "grad_norm": 0.07928493618965149,
      "learning_rate": 6.192248079666715e-05,
      "loss": 0.0994,
      "step": 6234
    },
    {
      "epoch": 1.5429349170997277,
      "grad_norm": 0.054609477519989014,
      "learning_rate": 6.185839351549114e-05,
      "loss": 0.0522,
      "step": 6235
    },
    {
      "epoch": 1.5431823805988616,
      "grad_norm": 0.034814413636922836,
      "learning_rate": 6.179433473283361e-05,
      "loss": 0.0342,
      "step": 6236
    },
    {
      "epoch": 1.5434298440979957,
      "grad_norm": 0.06841511279344559,
      "learning_rate": 6.173030445839759e-05,
      "loss": 0.0857,
      "step": 6237
    },
    {
      "epoch": 1.5436773075971293,
      "grad_norm": 0.04530462995171547,
      "learning_rate": 6.166630270188206e-05,
      "loss": 0.0655,
      "step": 6238
    },
    {
      "epoch": 1.5439247710962634,
      "grad_norm": 0.032139021903276443,
      "learning_rate": 6.160232947298161e-05,
      "loss": 0.0304,
      "step": 6239
    },
    {
      "epoch": 1.5441722345953972,
      "grad_norm": 0.04643739014863968,
      "learning_rate": 6.153838478138649e-05,
      "loss": 0.0602,
      "step": 6240
    },
    {
      "epoch": 1.544419698094531,
      "grad_norm": 0.033124227076768875,
      "learning_rate": 6.147446863678272e-05,
      "loss": 0.0215,
      "step": 6241
    },
    {
      "epoch": 1.544667161593665,
      "grad_norm": 0.05333932116627693,
      "learning_rate": 6.141058104885177e-05,
      "loss": 0.0737,
      "step": 6242
    },
    {
      "epoch": 1.5449146250927988,
      "grad_norm": 0.060184549540281296,
      "learning_rate": 6.134672202727096e-05,
      "loss": 0.0737,
      "step": 6243
    },
    {
      "epoch": 1.5451620885919328,
      "grad_norm": 0.05252666398882866,
      "learning_rate": 6.128289158171343e-05,
      "loss": 0.0386,
      "step": 6244
    },
    {
      "epoch": 1.5454095520910665,
      "grad_norm": 0.07365041971206665,
      "learning_rate": 6.121908972184764e-05,
      "loss": 0.0811,
      "step": 6245
    },
    {
      "epoch": 1.5456570155902005,
      "grad_norm": 0.03283493593335152,
      "learning_rate": 6.115531645733804e-05,
      "loss": 0.0257,
      "step": 6246
    },
    {
      "epoch": 1.5459044790893344,
      "grad_norm": 0.06823597848415375,
      "learning_rate": 6.109157179784436e-05,
      "loss": 0.0554,
      "step": 6247
    },
    {
      "epoch": 1.5461519425884682,
      "grad_norm": 0.036583010107278824,
      "learning_rate": 6.10278557530225e-05,
      "loss": 0.0228,
      "step": 6248
    },
    {
      "epoch": 1.546399406087602,
      "grad_norm": 0.04462367296218872,
      "learning_rate": 6.096416833252374e-05,
      "loss": 0.0516,
      "step": 6249
    },
    {
      "epoch": 1.546646869586736,
      "grad_norm": 0.04047641158103943,
      "learning_rate": 6.0900509545994906e-05,
      "loss": 0.0401,
      "step": 6250
    },
    {
      "epoch": 1.54689433308587,
      "grad_norm": 0.0406242199242115,
      "learning_rate": 6.0836879403078723e-05,
      "loss": 0.1245,
      "step": 6251
    },
    {
      "epoch": 1.5471417965850036,
      "grad_norm": 0.04954958334565163,
      "learning_rate": 6.0773277913413496e-05,
      "loss": 0.0615,
      "step": 6252
    },
    {
      "epoch": 1.5473892600841377,
      "grad_norm": 0.05328573286533356,
      "learning_rate": 6.0709705086633174e-05,
      "loss": 0.0426,
      "step": 6253
    },
    {
      "epoch": 1.5476367235832713,
      "grad_norm": 0.05165610462427139,
      "learning_rate": 6.064616093236744e-05,
      "loss": 0.0579,
      "step": 6254
    },
    {
      "epoch": 1.5478841870824054,
      "grad_norm": 0.03948519006371498,
      "learning_rate": 6.058264546024134e-05,
      "loss": 0.0403,
      "step": 6255
    },
    {
      "epoch": 1.5481316505815392,
      "grad_norm": 0.05107185244560242,
      "learning_rate": 6.051915867987604e-05,
      "loss": 0.0443,
      "step": 6256
    },
    {
      "epoch": 1.548379114080673,
      "grad_norm": 0.03657807409763336,
      "learning_rate": 6.045570060088812e-05,
      "loss": 0.0377,
      "step": 6257
    },
    {
      "epoch": 1.548626577579807,
      "grad_norm": 0.07366225123405457,
      "learning_rate": 6.039227123288965e-05,
      "loss": 0.0715,
      "step": 6258
    },
    {
      "epoch": 1.5488740410789408,
      "grad_norm": 0.04936957359313965,
      "learning_rate": 6.032887058548858e-05,
      "loss": 0.0483,
      "step": 6259
    },
    {
      "epoch": 1.5491215045780748,
      "grad_norm": 0.03879129886627197,
      "learning_rate": 6.026549866828851e-05,
      "loss": 0.0231,
      "step": 6260
    },
    {
      "epoch": 1.5493689680772085,
      "grad_norm": 0.07558637112379074,
      "learning_rate": 6.0202155490888534e-05,
      "loss": 0.0688,
      "step": 6261
    },
    {
      "epoch": 1.5496164315763425,
      "grad_norm": 0.02595660649240017,
      "learning_rate": 6.0138841062883644e-05,
      "loss": 0.0209,
      "step": 6262
    },
    {
      "epoch": 1.5498638950754764,
      "grad_norm": 0.03849336877465248,
      "learning_rate": 6.007555539386403e-05,
      "loss": 0.0505,
      "step": 6263
    },
    {
      "epoch": 1.5501113585746102,
      "grad_norm": 0.04433850198984146,
      "learning_rate": 6.001229849341611e-05,
      "loss": 0.0524,
      "step": 6264
    },
    {
      "epoch": 1.550358822073744,
      "grad_norm": 0.0428004190325737,
      "learning_rate": 5.994907037112146e-05,
      "loss": 0.0465,
      "step": 6265
    },
    {
      "epoch": 1.550606285572878,
      "grad_norm": 0.03992962837219238,
      "learning_rate": 5.988587103655752e-05,
      "loss": 0.0548,
      "step": 6266
    },
    {
      "epoch": 1.550853749072012,
      "grad_norm": 0.06676306575536728,
      "learning_rate": 5.982270049929736e-05,
      "loss": 0.0483,
      "step": 6267
    },
    {
      "epoch": 1.5511012125711456,
      "grad_norm": 0.0507594496011734,
      "learning_rate": 5.975955876890965e-05,
      "loss": 0.0556,
      "step": 6268
    },
    {
      "epoch": 1.5513486760702797,
      "grad_norm": 0.032027438282966614,
      "learning_rate": 5.969644585495876e-05,
      "loss": 0.0239,
      "step": 6269
    },
    {
      "epoch": 1.5515961395694136,
      "grad_norm": 0.07289358973503113,
      "learning_rate": 5.963336176700451e-05,
      "loss": 0.0836,
      "step": 6270
    },
    {
      "epoch": 1.5518436030685474,
      "grad_norm": 0.05244412273168564,
      "learning_rate": 5.957030651460249e-05,
      "loss": 0.0729,
      "step": 6271
    },
    {
      "epoch": 1.5520910665676813,
      "grad_norm": 0.05848413705825806,
      "learning_rate": 5.950728010730414e-05,
      "loss": 0.0462,
      "step": 6272
    },
    {
      "epoch": 1.552338530066815,
      "grad_norm": 0.04748905077576637,
      "learning_rate": 5.944428255465606e-05,
      "loss": 0.045,
      "step": 6273
    },
    {
      "epoch": 1.5525859935659492,
      "grad_norm": 0.03632133826613426,
      "learning_rate": 5.938131386620085e-05,
      "loss": 0.0352,
      "step": 6274
    },
    {
      "epoch": 1.5528334570650828,
      "grad_norm": 0.04488831385970116,
      "learning_rate": 5.931837405147655e-05,
      "loss": 0.0368,
      "step": 6275
    },
    {
      "epoch": 1.5530809205642169,
      "grad_norm": 0.06388821452856064,
      "learning_rate": 5.925546312001695e-05,
      "loss": 0.0413,
      "step": 6276
    },
    {
      "epoch": 1.5533283840633505,
      "grad_norm": 0.06136944517493248,
      "learning_rate": 5.919258108135145e-05,
      "loss": 0.0876,
      "step": 6277
    },
    {
      "epoch": 1.5535758475624846,
      "grad_norm": 0.05205303058028221,
      "learning_rate": 5.9129727945004875e-05,
      "loss": 0.0433,
      "step": 6278
    },
    {
      "epoch": 1.5538233110616184,
      "grad_norm": 0.06847517192363739,
      "learning_rate": 5.906690372049792e-05,
      "loss": 0.0762,
      "step": 6279
    },
    {
      "epoch": 1.5540707745607523,
      "grad_norm": 0.056303828954696655,
      "learning_rate": 5.900410841734677e-05,
      "loss": 0.0546,
      "step": 6280
    },
    {
      "epoch": 1.5543182380598861,
      "grad_norm": 0.03612431883811951,
      "learning_rate": 5.8941342045063324e-05,
      "loss": 0.0589,
      "step": 6281
    },
    {
      "epoch": 1.55456570155902,
      "grad_norm": 0.03236893564462662,
      "learning_rate": 5.8878604613155e-05,
      "loss": 0.0284,
      "step": 6282
    },
    {
      "epoch": 1.554813165058154,
      "grad_norm": 0.04541772976517677,
      "learning_rate": 5.88158961311249e-05,
      "loss": 0.0462,
      "step": 6283
    },
    {
      "epoch": 1.5550606285572877,
      "grad_norm": 0.051038485020399094,
      "learning_rate": 5.875321660847177e-05,
      "loss": 0.0528,
      "step": 6284
    },
    {
      "epoch": 1.5553080920564217,
      "grad_norm": 0.05469609424471855,
      "learning_rate": 5.869056605468976e-05,
      "loss": 0.0369,
      "step": 6285
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.10228849202394485,
      "learning_rate": 5.862794447926886e-05,
      "loss": 0.066,
      "step": 6286
    },
    {
      "epoch": 1.5558030190546894,
      "grad_norm": 0.056753918528556824,
      "learning_rate": 5.856535189169462e-05,
      "loss": 0.0459,
      "step": 6287
    },
    {
      "epoch": 1.5560504825538233,
      "grad_norm": 0.07273046672344208,
      "learning_rate": 5.850278830144814e-05,
      "loss": 0.0937,
      "step": 6288
    },
    {
      "epoch": 1.5562979460529571,
      "grad_norm": 0.05415806919336319,
      "learning_rate": 5.844025371800629e-05,
      "loss": 0.0602,
      "step": 6289
    },
    {
      "epoch": 1.5565454095520912,
      "grad_norm": 0.12560532987117767,
      "learning_rate": 5.837774815084115e-05,
      "loss": 0.0401,
      "step": 6290
    },
    {
      "epoch": 1.5567928730512248,
      "grad_norm": 0.06632815301418304,
      "learning_rate": 5.831527160942091e-05,
      "loss": 0.096,
      "step": 6291
    },
    {
      "epoch": 1.557040336550359,
      "grad_norm": 0.05775758996605873,
      "learning_rate": 5.825282410320912e-05,
      "loss": 0.0497,
      "step": 6292
    },
    {
      "epoch": 1.5572878000494927,
      "grad_norm": 0.05137607455253601,
      "learning_rate": 5.819040564166478e-05,
      "loss": 0.0558,
      "step": 6293
    },
    {
      "epoch": 1.5575352635486266,
      "grad_norm": 0.03809189423918724,
      "learning_rate": 5.812801623424277e-05,
      "loss": 0.0433,
      "step": 6294
    },
    {
      "epoch": 1.5577827270477604,
      "grad_norm": 0.039270464330911636,
      "learning_rate": 5.806565589039342e-05,
      "loss": 0.0307,
      "step": 6295
    },
    {
      "epoch": 1.5580301905468943,
      "grad_norm": 0.04110952839255333,
      "learning_rate": 5.8003324619562676e-05,
      "loss": 0.0295,
      "step": 6296
    },
    {
      "epoch": 1.5582776540460284,
      "grad_norm": 0.060246940702199936,
      "learning_rate": 5.794102243119218e-05,
      "loss": 0.0374,
      "step": 6297
    },
    {
      "epoch": 1.558525117545162,
      "grad_norm": 0.03952200338244438,
      "learning_rate": 5.7878749334718845e-05,
      "loss": 0.0534,
      "step": 6298
    },
    {
      "epoch": 1.558772581044296,
      "grad_norm": 0.0348871648311615,
      "learning_rate": 5.781650533957569e-05,
      "loss": 0.0327,
      "step": 6299
    },
    {
      "epoch": 1.5590200445434297,
      "grad_norm": 0.06012878566980362,
      "learning_rate": 5.77542904551909e-05,
      "loss": 0.0576,
      "step": 6300
    },
    {
      "epoch": 1.5592675080425638,
      "grad_norm": 0.06534680724143982,
      "learning_rate": 5.76921046909884e-05,
      "loss": 0.0434,
      "step": 6301
    },
    {
      "epoch": 1.5595149715416976,
      "grad_norm": 0.045117978006601334,
      "learning_rate": 5.7629948056387715e-05,
      "loss": 0.077,
      "step": 6302
    },
    {
      "epoch": 1.5597624350408315,
      "grad_norm": 0.04587077349424362,
      "learning_rate": 5.7567820560803976e-05,
      "loss": 0.0728,
      "step": 6303
    },
    {
      "epoch": 1.5600098985399653,
      "grad_norm": 0.029215844348073006,
      "learning_rate": 5.7505722213647914e-05,
      "loss": 0.0292,
      "step": 6304
    },
    {
      "epoch": 1.5602573620390991,
      "grad_norm": 0.07120227813720703,
      "learning_rate": 5.744365302432564e-05,
      "loss": 0.0634,
      "step": 6305
    },
    {
      "epoch": 1.5605048255382332,
      "grad_norm": 0.06271106749773026,
      "learning_rate": 5.738161300223907e-05,
      "loss": 0.0868,
      "step": 6306
    },
    {
      "epoch": 1.5607522890373668,
      "grad_norm": 0.052143070846796036,
      "learning_rate": 5.7319602156785776e-05,
      "loss": 0.0295,
      "step": 6307
    },
    {
      "epoch": 1.560999752536501,
      "grad_norm": 0.034576673060655594,
      "learning_rate": 5.725762049735861e-05,
      "loss": 0.0451,
      "step": 6308
    },
    {
      "epoch": 1.5612472160356348,
      "grad_norm": 0.03891311213374138,
      "learning_rate": 5.71956680333463e-05,
      "loss": 0.0485,
      "step": 6309
    },
    {
      "epoch": 1.5614946795347686,
      "grad_norm": 0.06030182167887688,
      "learning_rate": 5.713374477413283e-05,
      "loss": 0.042,
      "step": 6310
    },
    {
      "epoch": 1.5617421430339025,
      "grad_norm": 0.03853452950716019,
      "learning_rate": 5.70718507290981e-05,
      "loss": 0.0488,
      "step": 6311
    },
    {
      "epoch": 1.5619896065330363,
      "grad_norm": 0.04653659835457802,
      "learning_rate": 5.70099859076175e-05,
      "loss": 0.0218,
      "step": 6312
    },
    {
      "epoch": 1.5622370700321704,
      "grad_norm": 0.057099245488643646,
      "learning_rate": 5.694815031906173e-05,
      "loss": 0.061,
      "step": 6313
    },
    {
      "epoch": 1.562484533531304,
      "grad_norm": 0.05744897201657295,
      "learning_rate": 5.688634397279738e-05,
      "loss": 0.0542,
      "step": 6314
    },
    {
      "epoch": 1.562731997030438,
      "grad_norm": 0.04808294028043747,
      "learning_rate": 5.6824566878186465e-05,
      "loss": 0.0313,
      "step": 6315
    },
    {
      "epoch": 1.562979460529572,
      "grad_norm": 0.03183269873261452,
      "learning_rate": 5.676281904458658e-05,
      "loss": 0.0269,
      "step": 6316
    },
    {
      "epoch": 1.5632269240287058,
      "grad_norm": 0.04875203222036362,
      "learning_rate": 5.6701100481350994e-05,
      "loss": 0.0456,
      "step": 6317
    },
    {
      "epoch": 1.5634743875278396,
      "grad_norm": 0.03154250606894493,
      "learning_rate": 5.6639411197828253e-05,
      "loss": 0.0289,
      "step": 6318
    },
    {
      "epoch": 1.5637218510269735,
      "grad_norm": 0.05505100265145302,
      "learning_rate": 5.657775120336292e-05,
      "loss": 0.0636,
      "step": 6319
    },
    {
      "epoch": 1.5639693145261075,
      "grad_norm": 0.05637155473232269,
      "learning_rate": 5.651612050729465e-05,
      "loss": 0.0373,
      "step": 6320
    },
    {
      "epoch": 1.5642167780252412,
      "grad_norm": 0.1099383682012558,
      "learning_rate": 5.645451911895899e-05,
      "loss": 0.1082,
      "step": 6321
    },
    {
      "epoch": 1.5644642415243752,
      "grad_norm": 0.10769969969987869,
      "learning_rate": 5.639294704768688e-05,
      "loss": 0.1248,
      "step": 6322
    },
    {
      "epoch": 1.5647117050235089,
      "grad_norm": 0.039036430418491364,
      "learning_rate": 5.6331404302804926e-05,
      "loss": 0.0462,
      "step": 6323
    },
    {
      "epoch": 1.564959168522643,
      "grad_norm": 0.04299192130565643,
      "learning_rate": 5.6269890893635264e-05,
      "loss": 0.0321,
      "step": 6324
    },
    {
      "epoch": 1.5652066320217768,
      "grad_norm": 0.052238959819078445,
      "learning_rate": 5.620840682949546e-05,
      "loss": 0.0375,
      "step": 6325
    },
    {
      "epoch": 1.5654540955209106,
      "grad_norm": 0.038543812930583954,
      "learning_rate": 5.6146952119698696e-05,
      "loss": 0.0256,
      "step": 6326
    },
    {
      "epoch": 1.5657015590200447,
      "grad_norm": 0.07302254438400269,
      "learning_rate": 5.6085526773554e-05,
      "loss": 0.0683,
      "step": 6327
    },
    {
      "epoch": 1.5659490225191783,
      "grad_norm": 0.04813823103904724,
      "learning_rate": 5.602413080036548e-05,
      "loss": 0.0493,
      "step": 6328
    },
    {
      "epoch": 1.5661964860183124,
      "grad_norm": 0.04940261319279671,
      "learning_rate": 5.596276420943305e-05,
      "loss": 0.0621,
      "step": 6329
    },
    {
      "epoch": 1.566443949517446,
      "grad_norm": 0.05786071717739105,
      "learning_rate": 5.590142701005219e-05,
      "loss": 0.0797,
      "step": 6330
    },
    {
      "epoch": 1.56669141301658,
      "grad_norm": 0.04669344425201416,
      "learning_rate": 5.584011921151383e-05,
      "loss": 0.0278,
      "step": 6331
    },
    {
      "epoch": 1.566938876515714,
      "grad_norm": 0.0439441092312336,
      "learning_rate": 5.577884082310461e-05,
      "loss": 0.0409,
      "step": 6332
    },
    {
      "epoch": 1.5671863400148478,
      "grad_norm": 0.04517100378870964,
      "learning_rate": 5.571759185410641e-05,
      "loss": 0.0436,
      "step": 6333
    },
    {
      "epoch": 1.5674338035139816,
      "grad_norm": 0.0418580137193203,
      "learning_rate": 5.565637231379697e-05,
      "loss": 0.0357,
      "step": 6334
    },
    {
      "epoch": 1.5676812670131155,
      "grad_norm": 0.03325836360454559,
      "learning_rate": 5.559518221144938e-05,
      "loss": 0.0419,
      "step": 6335
    },
    {
      "epoch": 1.5679287305122496,
      "grad_norm": 0.05697363615036011,
      "learning_rate": 5.553402155633239e-05,
      "loss": 0.0325,
      "step": 6336
    },
    {
      "epoch": 1.5681761940113832,
      "grad_norm": 0.08340711146593094,
      "learning_rate": 5.5472890357710194e-05,
      "loss": 0.0581,
      "step": 6337
    },
    {
      "epoch": 1.5684236575105173,
      "grad_norm": 0.036250364035367966,
      "learning_rate": 5.5411788624842616e-05,
      "loss": 0.0389,
      "step": 6338
    },
    {
      "epoch": 1.5686711210096511,
      "grad_norm": 0.12131493538618088,
      "learning_rate": 5.535071636698497e-05,
      "loss": 0.0539,
      "step": 6339
    },
    {
      "epoch": 1.568918584508785,
      "grad_norm": 0.047912467271089554,
      "learning_rate": 5.5289673593388016e-05,
      "loss": 0.0564,
      "step": 6340
    },
    {
      "epoch": 1.5691660480079188,
      "grad_norm": 0.03668263554573059,
      "learning_rate": 5.5228660313298154e-05,
      "loss": 0.0275,
      "step": 6341
    },
    {
      "epoch": 1.5694135115070527,
      "grad_norm": 0.055288165807724,
      "learning_rate": 5.516767653595733e-05,
      "loss": 0.0333,
      "step": 6342
    },
    {
      "epoch": 1.5696609750061867,
      "grad_norm": 0.040627263486385345,
      "learning_rate": 5.510672227060301e-05,
      "loss": 0.0383,
      "step": 6343
    },
    {
      "epoch": 1.5699084385053204,
      "grad_norm": 0.04871433600783348,
      "learning_rate": 5.5045797526468184e-05,
      "loss": 0.0564,
      "step": 6344
    },
    {
      "epoch": 1.5701559020044544,
      "grad_norm": 0.02686498500406742,
      "learning_rate": 5.498490231278117e-05,
      "loss": 0.0231,
      "step": 6345
    },
    {
      "epoch": 1.570403365503588,
      "grad_norm": 0.0377611443400383,
      "learning_rate": 5.492403663876619e-05,
      "loss": 0.0339,
      "step": 6346
    },
    {
      "epoch": 1.5706508290027221,
      "grad_norm": 0.04638972878456116,
      "learning_rate": 5.48632005136428e-05,
      "loss": 0.0298,
      "step": 6347
    },
    {
      "epoch": 1.570898292501856,
      "grad_norm": 0.032488055527210236,
      "learning_rate": 5.480239394662598e-05,
      "loss": 0.0471,
      "step": 6348
    },
    {
      "epoch": 1.5711457560009898,
      "grad_norm": 0.05725137144327164,
      "learning_rate": 5.4741616946926336e-05,
      "loss": 0.0445,
      "step": 6349
    },
    {
      "epoch": 1.571393219500124,
      "grad_norm": 0.056125298142433167,
      "learning_rate": 5.468086952375001e-05,
      "loss": 0.0297,
      "step": 6350
    },
    {
      "epoch": 1.5716406829992575,
      "grad_norm": 0.037761978805065155,
      "learning_rate": 5.462015168629866e-05,
      "loss": 0.0419,
      "step": 6351
    },
    {
      "epoch": 1.5718881464983916,
      "grad_norm": 0.07742145657539368,
      "learning_rate": 5.455946344376952e-05,
      "loss": 0.0827,
      "step": 6352
    },
    {
      "epoch": 1.5721356099975252,
      "grad_norm": 0.045351333916187286,
      "learning_rate": 5.449880480535505e-05,
      "loss": 0.0617,
      "step": 6353
    },
    {
      "epoch": 1.5723830734966593,
      "grad_norm": 0.03983963280916214,
      "learning_rate": 5.443817578024368e-05,
      "loss": 0.056,
      "step": 6354
    },
    {
      "epoch": 1.5726305369957931,
      "grad_norm": 0.06178911030292511,
      "learning_rate": 5.437757637761898e-05,
      "loss": 0.0724,
      "step": 6355
    },
    {
      "epoch": 1.572878000494927,
      "grad_norm": 0.033403731882572174,
      "learning_rate": 5.431700660666017e-05,
      "loss": 0.0311,
      "step": 6356
    },
    {
      "epoch": 1.5731254639940608,
      "grad_norm": 0.047856565564870834,
      "learning_rate": 5.4256466476542035e-05,
      "loss": 0.0286,
      "step": 6357
    },
    {
      "epoch": 1.5733729274931947,
      "grad_norm": 0.04450106993317604,
      "learning_rate": 5.4195955996434785e-05,
      "loss": 0.0481,
      "step": 6358
    },
    {
      "epoch": 1.5736203909923288,
      "grad_norm": 0.047242943197488785,
      "learning_rate": 5.4135475175504266e-05,
      "loss": 0.0715,
      "step": 6359
    },
    {
      "epoch": 1.5738678544914624,
      "grad_norm": 0.06691356748342514,
      "learning_rate": 5.407502402291156e-05,
      "loss": 0.0572,
      "step": 6360
    },
    {
      "epoch": 1.5741153179905965,
      "grad_norm": 0.0464785173535347,
      "learning_rate": 5.4014602547813485e-05,
      "loss": 0.0526,
      "step": 6361
    },
    {
      "epoch": 1.5743627814897303,
      "grad_norm": 0.0332389771938324,
      "learning_rate": 5.395421075936246e-05,
      "loss": 0.0309,
      "step": 6362
    },
    {
      "epoch": 1.5746102449888641,
      "grad_norm": 0.056544095277786255,
      "learning_rate": 5.389384866670608e-05,
      "loss": 0.0511,
      "step": 6363
    },
    {
      "epoch": 1.574857708487998,
      "grad_norm": 0.04677435755729675,
      "learning_rate": 5.383351627898769e-05,
      "loss": 0.0701,
      "step": 6364
    },
    {
      "epoch": 1.5751051719871318,
      "grad_norm": 0.051162201911211014,
      "learning_rate": 5.377321360534604e-05,
      "loss": 0.0556,
      "step": 6365
    },
    {
      "epoch": 1.575352635486266,
      "grad_norm": 0.10454455763101578,
      "learning_rate": 5.371294065491544e-05,
      "loss": 0.0822,
      "step": 6366
    },
    {
      "epoch": 1.5756000989853995,
      "grad_norm": 0.1046413704752922,
      "learning_rate": 5.365269743682571e-05,
      "loss": 0.0666,
      "step": 6367
    },
    {
      "epoch": 1.5758475624845336,
      "grad_norm": 0.04649369791150093,
      "learning_rate": 5.359248396020197e-05,
      "loss": 0.0714,
      "step": 6368
    },
    {
      "epoch": 1.5760950259836675,
      "grad_norm": 0.04108842462301254,
      "learning_rate": 5.353230023416505e-05,
      "loss": 0.0419,
      "step": 6369
    },
    {
      "epoch": 1.5763424894828013,
      "grad_norm": 0.05488280579447746,
      "learning_rate": 5.347214626783123e-05,
      "loss": 0.0385,
      "step": 6370
    },
    {
      "epoch": 1.5765899529819352,
      "grad_norm": 0.12186132371425629,
      "learning_rate": 5.341202207031226e-05,
      "loss": 0.0996,
      "step": 6371
    },
    {
      "epoch": 1.576837416481069,
      "grad_norm": 0.06128103286027908,
      "learning_rate": 5.3351927650715335e-05,
      "loss": 0.0611,
      "step": 6372
    },
    {
      "epoch": 1.577084879980203,
      "grad_norm": 0.04819326475262642,
      "learning_rate": 5.329186301814323e-05,
      "loss": 0.0543,
      "step": 6373
    },
    {
      "epoch": 1.5773323434793367,
      "grad_norm": 0.063410684466362,
      "learning_rate": 5.3231828181694196e-05,
      "loss": 0.0682,
      "step": 6374
    },
    {
      "epoch": 1.5775798069784708,
      "grad_norm": 0.04167063534259796,
      "learning_rate": 5.317182315046182e-05,
      "loss": 0.0291,
      "step": 6375
    },
    {
      "epoch": 1.5778272704776044,
      "grad_norm": 0.07004855573177338,
      "learning_rate": 5.311184793353538e-05,
      "loss": 0.0797,
      "step": 6376
    },
    {
      "epoch": 1.5780747339767385,
      "grad_norm": 0.05925198644399643,
      "learning_rate": 5.30519025399995e-05,
      "loss": 0.0676,
      "step": 6377
    },
    {
      "epoch": 1.5783221974758723,
      "grad_norm": 0.03732339292764664,
      "learning_rate": 5.299198697893434e-05,
      "loss": 0.0349,
      "step": 6378
    },
    {
      "epoch": 1.5785696609750062,
      "grad_norm": 0.0790368914604187,
      "learning_rate": 5.293210125941564e-05,
      "loss": 0.1045,
      "step": 6379
    },
    {
      "epoch": 1.57881712447414,
      "grad_norm": 0.05830717831850052,
      "learning_rate": 5.28722453905143e-05,
      "loss": 0.0782,
      "step": 6380
    },
    {
      "epoch": 1.5790645879732739,
      "grad_norm": 0.03193299099802971,
      "learning_rate": 5.281241938129708e-05,
      "loss": 0.0384,
      "step": 6381
    },
    {
      "epoch": 1.579312051472408,
      "grad_norm": 0.05087364464998245,
      "learning_rate": 5.275262324082611e-05,
      "loss": 0.0462,
      "step": 6382
    },
    {
      "epoch": 1.5795595149715416,
      "grad_norm": 0.08842398226261139,
      "learning_rate": 5.269285697815876e-05,
      "loss": 0.0676,
      "step": 6383
    },
    {
      "epoch": 1.5798069784706756,
      "grad_norm": 0.06432738900184631,
      "learning_rate": 5.2633120602348125e-05,
      "loss": 0.0348,
      "step": 6384
    },
    {
      "epoch": 1.5800544419698095,
      "grad_norm": 0.05157701298594475,
      "learning_rate": 5.257341412244271e-05,
      "loss": 0.0826,
      "step": 6385
    },
    {
      "epoch": 1.5803019054689433,
      "grad_norm": 0.04114959388971329,
      "learning_rate": 5.251373754748648e-05,
      "loss": 0.0575,
      "step": 6386
    },
    {
      "epoch": 1.5805493689680772,
      "grad_norm": 0.047299593687057495,
      "learning_rate": 5.245409088651895e-05,
      "loss": 0.0552,
      "step": 6387
    },
    {
      "epoch": 1.580796832467211,
      "grad_norm": 0.04747884348034859,
      "learning_rate": 5.2394474148574787e-05,
      "loss": 0.0659,
      "step": 6388
    },
    {
      "epoch": 1.581044295966345,
      "grad_norm": 0.058455221354961395,
      "learning_rate": 5.233488734268457e-05,
      "loss": 0.0642,
      "step": 6389
    },
    {
      "epoch": 1.5812917594654787,
      "grad_norm": 0.05674989894032478,
      "learning_rate": 5.227533047787417e-05,
      "loss": 0.0605,
      "step": 6390
    },
    {
      "epoch": 1.5815392229646128,
      "grad_norm": 0.03333340585231781,
      "learning_rate": 5.2215803563164746e-05,
      "loss": 0.0423,
      "step": 6391
    },
    {
      "epoch": 1.5817866864637466,
      "grad_norm": 0.07094866782426834,
      "learning_rate": 5.215630660757309e-05,
      "loss": 0.055,
      "step": 6392
    },
    {
      "epoch": 1.5820341499628805,
      "grad_norm": 0.07426991313695908,
      "learning_rate": 5.20968396201115e-05,
      "loss": 0.0394,
      "step": 6393
    },
    {
      "epoch": 1.5822816134620143,
      "grad_norm": 0.053502053022384644,
      "learning_rate": 5.2037402609787594e-05,
      "loss": 0.0659,
      "step": 6394
    },
    {
      "epoch": 1.5825290769611482,
      "grad_norm": 0.07648194581270218,
      "learning_rate": 5.197799558560465e-05,
      "loss": 0.0642,
      "step": 6395
    },
    {
      "epoch": 1.5827765404602823,
      "grad_norm": 0.049948904663324356,
      "learning_rate": 5.191861855656102e-05,
      "loss": 0.0382,
      "step": 6396
    },
    {
      "epoch": 1.583024003959416,
      "grad_norm": 0.043776653707027435,
      "learning_rate": 5.185927153165107e-05,
      "loss": 0.0426,
      "step": 6397
    },
    {
      "epoch": 1.58327146745855,
      "grad_norm": 0.050457511097192764,
      "learning_rate": 5.1799954519864106e-05,
      "loss": 0.0834,
      "step": 6398
    },
    {
      "epoch": 1.5835189309576836,
      "grad_norm": 0.027463693171739578,
      "learning_rate": 5.1740667530185136e-05,
      "loss": 0.03,
      "step": 6399
    },
    {
      "epoch": 1.5837663944568177,
      "grad_norm": 0.04297826439142227,
      "learning_rate": 5.168141057159464e-05,
      "loss": 0.0469,
      "step": 6400
    },
    {
      "epoch": 1.5837663944568177,
      "eval_loss": 0.28937751054763794,
      "eval_runtime": 202.5148,
      "eval_samples_per_second": 4.938,
      "eval_steps_per_second": 0.311,
      "step": 6400
    },
    {
      "epoch": 1.5840138579559515,
      "grad_norm": 0.05869597569108009,
      "learning_rate": 5.162218365306845e-05,
      "loss": 0.0582,
      "step": 6401
    },
    {
      "epoch": 1.5842613214550854,
      "grad_norm": 0.04819411411881447,
      "learning_rate": 5.1562986783577985e-05,
      "loss": 0.044,
      "step": 6402
    },
    {
      "epoch": 1.5845087849542192,
      "grad_norm": 0.05486248433589935,
      "learning_rate": 5.1503819972089886e-05,
      "loss": 0.0565,
      "step": 6403
    },
    {
      "epoch": 1.584756248453353,
      "grad_norm": 0.06887231767177582,
      "learning_rate": 5.144468322756632e-05,
      "loss": 0.0476,
      "step": 6404
    },
    {
      "epoch": 1.5850037119524871,
      "grad_norm": 0.03640463948249817,
      "learning_rate": 5.138557655896523e-05,
      "loss": 0.0574,
      "step": 6405
    },
    {
      "epoch": 1.5852511754516208,
      "grad_norm": 0.04632986709475517,
      "learning_rate": 5.132649997523947e-05,
      "loss": 0.034,
      "step": 6406
    },
    {
      "epoch": 1.5854986389507548,
      "grad_norm": 0.055072005838155746,
      "learning_rate": 5.126745348533774e-05,
      "loss": 0.0588,
      "step": 6407
    },
    {
      "epoch": 1.5857461024498887,
      "grad_norm": 0.0564136803150177,
      "learning_rate": 5.120843709820383e-05,
      "loss": 0.0596,
      "step": 6408
    },
    {
      "epoch": 1.5859935659490225,
      "grad_norm": 0.03448577597737312,
      "learning_rate": 5.1149450822777385e-05,
      "loss": 0.0497,
      "step": 6409
    },
    {
      "epoch": 1.5862410294481564,
      "grad_norm": 0.054055988788604736,
      "learning_rate": 5.109049466799329e-05,
      "loss": 0.0531,
      "step": 6410
    },
    {
      "epoch": 1.5864884929472902,
      "grad_norm": 0.03506968170404434,
      "learning_rate": 5.103156864278172e-05,
      "loss": 0.0548,
      "step": 6411
    },
    {
      "epoch": 1.5867359564464243,
      "grad_norm": 0.045792046934366226,
      "learning_rate": 5.097267275606846e-05,
      "loss": 0.0384,
      "step": 6412
    },
    {
      "epoch": 1.586983419945558,
      "grad_norm": 0.04975161701440811,
      "learning_rate": 5.09138070167747e-05,
      "loss": 0.0459,
      "step": 6413
    },
    {
      "epoch": 1.587230883444692,
      "grad_norm": 0.07512819766998291,
      "learning_rate": 5.0854971433817066e-05,
      "loss": 0.0829,
      "step": 6414
    },
    {
      "epoch": 1.5874783469438258,
      "grad_norm": 0.05608374625444412,
      "learning_rate": 5.07961660161077e-05,
      "loss": 0.0388,
      "step": 6415
    },
    {
      "epoch": 1.5877258104429597,
      "grad_norm": 0.04916608706116676,
      "learning_rate": 5.073739077255382e-05,
      "loss": 0.0726,
      "step": 6416
    },
    {
      "epoch": 1.5879732739420935,
      "grad_norm": 0.057826586067676544,
      "learning_rate": 5.0678645712058656e-05,
      "loss": 0.0573,
      "step": 6417
    },
    {
      "epoch": 1.5882207374412274,
      "grad_norm": 0.03215041384100914,
      "learning_rate": 5.061993084352029e-05,
      "loss": 0.0218,
      "step": 6418
    },
    {
      "epoch": 1.5884682009403615,
      "grad_norm": 0.06087294965982437,
      "learning_rate": 5.05612461758326e-05,
      "loss": 0.1188,
      "step": 6419
    },
    {
      "epoch": 1.588715664439495,
      "grad_norm": 0.0507148839533329,
      "learning_rate": 5.0502591717884765e-05,
      "loss": 0.0407,
      "step": 6420
    },
    {
      "epoch": 1.5889631279386291,
      "grad_norm": 0.027425719425082207,
      "learning_rate": 5.044396747856137e-05,
      "loss": 0.0311,
      "step": 6421
    },
    {
      "epoch": 1.5892105914377628,
      "grad_norm": 0.03968415781855583,
      "learning_rate": 5.0385373466742543e-05,
      "loss": 0.0435,
      "step": 6422
    },
    {
      "epoch": 1.5894580549368968,
      "grad_norm": 0.05784020572900772,
      "learning_rate": 5.0326809691303574e-05,
      "loss": 0.0488,
      "step": 6423
    },
    {
      "epoch": 1.5897055184360307,
      "grad_norm": 0.08828511089086533,
      "learning_rate": 5.026827616111537e-05,
      "loss": 0.0393,
      "step": 6424
    },
    {
      "epoch": 1.5899529819351645,
      "grad_norm": 0.09058962762355804,
      "learning_rate": 5.0209772885044444e-05,
      "loss": 0.0519,
      "step": 6425
    },
    {
      "epoch": 1.5902004454342984,
      "grad_norm": 0.06542567908763885,
      "learning_rate": 5.015129987195224e-05,
      "loss": 0.0982,
      "step": 6426
    },
    {
      "epoch": 1.5904479089334322,
      "grad_norm": 0.04907025396823883,
      "learning_rate": 5.009285713069603e-05,
      "loss": 0.062,
      "step": 6427
    },
    {
      "epoch": 1.5906953724325663,
      "grad_norm": 0.0401948057115078,
      "learning_rate": 5.00344446701283e-05,
      "loss": 0.0422,
      "step": 6428
    },
    {
      "epoch": 1.5909428359317,
      "grad_norm": 0.050735726952552795,
      "learning_rate": 4.997606249909703e-05,
      "loss": 0.0486,
      "step": 6429
    },
    {
      "epoch": 1.591190299430834,
      "grad_norm": 0.029990404844284058,
      "learning_rate": 4.9917710626445624e-05,
      "loss": 0.0304,
      "step": 6430
    },
    {
      "epoch": 1.5914377629299679,
      "grad_norm": 0.07203558832406998,
      "learning_rate": 4.985938906101278e-05,
      "loss": 0.0586,
      "step": 6431
    },
    {
      "epoch": 1.5916852264291017,
      "grad_norm": 0.09829603880643845,
      "learning_rate": 4.980109781163272e-05,
      "loss": 0.0735,
      "step": 6432
    },
    {
      "epoch": 1.5919326899282356,
      "grad_norm": 0.05366899445652962,
      "learning_rate": 4.974283688713502e-05,
      "loss": 0.0837,
      "step": 6433
    },
    {
      "epoch": 1.5921801534273694,
      "grad_norm": 0.047332823276519775,
      "learning_rate": 4.968460629634472e-05,
      "loss": 0.0423,
      "step": 6434
    },
    {
      "epoch": 1.5924276169265035,
      "grad_norm": 0.05494235083460808,
      "learning_rate": 4.962640604808222e-05,
      "loss": 0.0508,
      "step": 6435
    },
    {
      "epoch": 1.592675080425637,
      "grad_norm": 0.0445222407579422,
      "learning_rate": 4.956823615116332e-05,
      "loss": 0.0264,
      "step": 6436
    },
    {
      "epoch": 1.5929225439247712,
      "grad_norm": 0.04520057141780853,
      "learning_rate": 4.9510096614399314e-05,
      "loss": 0.0506,
      "step": 6437
    },
    {
      "epoch": 1.593170007423905,
      "grad_norm": 0.0602785162627697,
      "learning_rate": 4.9451987446596656e-05,
      "loss": 0.0683,
      "step": 6438
    },
    {
      "epoch": 1.5934174709230389,
      "grad_norm": 0.029591402038931847,
      "learning_rate": 4.939390865655743e-05,
      "loss": 0.028,
      "step": 6439
    },
    {
      "epoch": 1.5936649344221727,
      "grad_norm": 0.03700678050518036,
      "learning_rate": 4.933586025307909e-05,
      "loss": 0.0413,
      "step": 6440
    },
    {
      "epoch": 1.5939123979213066,
      "grad_norm": 0.036897316575050354,
      "learning_rate": 4.9277842244954406e-05,
      "loss": 0.0333,
      "step": 6441
    },
    {
      "epoch": 1.5941598614204406,
      "grad_norm": 0.032553914934396744,
      "learning_rate": 4.921985464097167e-05,
      "loss": 0.042,
      "step": 6442
    },
    {
      "epoch": 1.5944073249195743,
      "grad_norm": 0.038429755717515945,
      "learning_rate": 4.9161897449914255e-05,
      "loss": 0.0379,
      "step": 6443
    },
    {
      "epoch": 1.5946547884187083,
      "grad_norm": 0.04142691567540169,
      "learning_rate": 4.910397068056141e-05,
      "loss": 0.0436,
      "step": 6444
    },
    {
      "epoch": 1.594902251917842,
      "grad_norm": 0.05344612896442413,
      "learning_rate": 4.904607434168748e-05,
      "loss": 0.0349,
      "step": 6445
    },
    {
      "epoch": 1.595149715416976,
      "grad_norm": 0.035986512899398804,
      "learning_rate": 4.898820844206211e-05,
      "loss": 0.0248,
      "step": 6446
    },
    {
      "epoch": 1.5953971789161099,
      "grad_norm": 0.051353130489587784,
      "learning_rate": 4.8930372990450525e-05,
      "loss": 0.0851,
      "step": 6447
    },
    {
      "epoch": 1.5956446424152437,
      "grad_norm": 0.029629046097397804,
      "learning_rate": 4.887256799561332e-05,
      "loss": 0.0274,
      "step": 6448
    },
    {
      "epoch": 1.5958921059143776,
      "grad_norm": 0.039936553686857224,
      "learning_rate": 4.881479346630641e-05,
      "loss": 0.0593,
      "step": 6449
    },
    {
      "epoch": 1.5961395694135114,
      "grad_norm": 0.0691123828291893,
      "learning_rate": 4.875704941128117e-05,
      "loss": 0.0824,
      "step": 6450
    },
    {
      "epoch": 1.5963870329126455,
      "grad_norm": 0.051267120987176895,
      "learning_rate": 4.8699335839284126e-05,
      "loss": 0.0304,
      "step": 6451
    },
    {
      "epoch": 1.5966344964117791,
      "grad_norm": 0.07729931175708771,
      "learning_rate": 4.864165275905766e-05,
      "loss": 0.0497,
      "step": 6452
    },
    {
      "epoch": 1.5968819599109132,
      "grad_norm": 0.07076773792505264,
      "learning_rate": 4.858400017933901e-05,
      "loss": 0.0801,
      "step": 6453
    },
    {
      "epoch": 1.597129423410047,
      "grad_norm": 0.04478340595960617,
      "learning_rate": 4.85263781088611e-05,
      "loss": 0.04,
      "step": 6454
    },
    {
      "epoch": 1.597376886909181,
      "grad_norm": 0.07820313423871994,
      "learning_rate": 4.846878655635217e-05,
      "loss": 0.074,
      "step": 6455
    },
    {
      "epoch": 1.5976243504083147,
      "grad_norm": 0.07617034763097763,
      "learning_rate": 4.8411225530535844e-05,
      "loss": 0.0692,
      "step": 6456
    },
    {
      "epoch": 1.5978718139074486,
      "grad_norm": 0.0496337004005909,
      "learning_rate": 4.835369504013115e-05,
      "loss": 0.0583,
      "step": 6457
    },
    {
      "epoch": 1.5981192774065827,
      "grad_norm": 0.06568174064159393,
      "learning_rate": 4.829619509385233e-05,
      "loss": 0.0579,
      "step": 6458
    },
    {
      "epoch": 1.5983667409057163,
      "grad_norm": 0.08658340573310852,
      "learning_rate": 4.823872570040913e-05,
      "loss": 0.0409,
      "step": 6459
    },
    {
      "epoch": 1.5986142044048504,
      "grad_norm": 0.03331736475229263,
      "learning_rate": 4.818128686850681e-05,
      "loss": 0.0584,
      "step": 6460
    },
    {
      "epoch": 1.5988616679039842,
      "grad_norm": 0.04143007844686508,
      "learning_rate": 4.81238786068457e-05,
      "loss": 0.0326,
      "step": 6461
    },
    {
      "epoch": 1.599109131403118,
      "grad_norm": 0.08486054837703705,
      "learning_rate": 4.806650092412174e-05,
      "loss": 0.0652,
      "step": 6462
    },
    {
      "epoch": 1.599356594902252,
      "grad_norm": 0.04631611332297325,
      "learning_rate": 4.800915382902596e-05,
      "loss": 0.0545,
      "step": 6463
    },
    {
      "epoch": 1.5996040584013858,
      "grad_norm": 0.040497202426195145,
      "learning_rate": 4.7951837330245134e-05,
      "loss": 0.0378,
      "step": 6464
    },
    {
      "epoch": 1.5998515219005198,
      "grad_norm": 0.059426676481962204,
      "learning_rate": 4.789455143646124e-05,
      "loss": 0.0445,
      "step": 6465
    },
    {
      "epoch": 1.6000989853996535,
      "grad_norm": 0.040549080818891525,
      "learning_rate": 4.7837296156351405e-05,
      "loss": 0.042,
      "step": 6466
    },
    {
      "epoch": 1.6003464488987875,
      "grad_norm": 0.03350580111145973,
      "learning_rate": 4.77800714985884e-05,
      "loss": 0.0495,
      "step": 6467
    },
    {
      "epoch": 1.6005939123979211,
      "grad_norm": 0.06321720033884048,
      "learning_rate": 4.772287747184023e-05,
      "loss": 0.0333,
      "step": 6468
    },
    {
      "epoch": 1.6008413758970552,
      "grad_norm": 0.06916873157024384,
      "learning_rate": 4.766571408477036e-05,
      "loss": 0.082,
      "step": 6469
    },
    {
      "epoch": 1.601088839396189,
      "grad_norm": 0.07770723104476929,
      "learning_rate": 4.7608581346037514e-05,
      "loss": 0.1202,
      "step": 6470
    },
    {
      "epoch": 1.601336302895323,
      "grad_norm": 0.06755437701940536,
      "learning_rate": 4.7551479264295675e-05,
      "loss": 0.0606,
      "step": 6471
    },
    {
      "epoch": 1.6015837663944568,
      "grad_norm": 0.05597005784511566,
      "learning_rate": 4.749440784819456e-05,
      "loss": 0.06,
      "step": 6472
    },
    {
      "epoch": 1.6018312298935906,
      "grad_norm": 0.05533256009221077,
      "learning_rate": 4.743736710637878e-05,
      "loss": 0.0665,
      "step": 6473
    },
    {
      "epoch": 1.6020786933927247,
      "grad_norm": 0.09820996969938278,
      "learning_rate": 4.738035704748856e-05,
      "loss": 0.0467,
      "step": 6474
    },
    {
      "epoch": 1.6023261568918583,
      "grad_norm": 0.043059270828962326,
      "learning_rate": 4.732337768015946e-05,
      "loss": 0.0402,
      "step": 6475
    },
    {
      "epoch": 1.6025736203909924,
      "grad_norm": 0.04194004833698273,
      "learning_rate": 4.7266429013022364e-05,
      "loss": 0.0519,
      "step": 6476
    },
    {
      "epoch": 1.6028210838901262,
      "grad_norm": 0.04092307388782501,
      "learning_rate": 4.7209511054703555e-05,
      "loss": 0.0222,
      "step": 6477
    },
    {
      "epoch": 1.60306854738926,
      "grad_norm": 0.045859940350055695,
      "learning_rate": 4.715262381382449e-05,
      "loss": 0.0627,
      "step": 6478
    },
    {
      "epoch": 1.603316010888394,
      "grad_norm": 0.07152751088142395,
      "learning_rate": 4.7095767299002055e-05,
      "loss": 0.071,
      "step": 6479
    },
    {
      "epoch": 1.6035634743875278,
      "grad_norm": 0.030460886657238007,
      "learning_rate": 4.7038941518848744e-05,
      "loss": 0.0279,
      "step": 6480
    },
    {
      "epoch": 1.6038109378866618,
      "grad_norm": 0.0646015927195549,
      "learning_rate": 4.698214648197199e-05,
      "loss": 0.0615,
      "step": 6481
    },
    {
      "epoch": 1.6040584013857955,
      "grad_norm": 0.03400237858295441,
      "learning_rate": 4.6925382196974815e-05,
      "loss": 0.0222,
      "step": 6482
    },
    {
      "epoch": 1.6043058648849295,
      "grad_norm": 0.02796163596212864,
      "learning_rate": 4.6868648672455464e-05,
      "loss": 0.0235,
      "step": 6483
    },
    {
      "epoch": 1.6045533283840634,
      "grad_norm": 0.045588329434394836,
      "learning_rate": 4.681194591700766e-05,
      "loss": 0.0434,
      "step": 6484
    },
    {
      "epoch": 1.6048007918831972,
      "grad_norm": 0.0368436798453331,
      "learning_rate": 4.675527393922041e-05,
      "loss": 0.0402,
      "step": 6485
    },
    {
      "epoch": 1.605048255382331,
      "grad_norm": 0.031868692487478256,
      "learning_rate": 4.669863274767788e-05,
      "loss": 0.0334,
      "step": 6486
    },
    {
      "epoch": 1.605295718881465,
      "grad_norm": 0.054462045431137085,
      "learning_rate": 4.664202235095982e-05,
      "loss": 0.0826,
      "step": 6487
    },
    {
      "epoch": 1.605543182380599,
      "grad_norm": 0.039172034710645676,
      "learning_rate": 4.658544275764121e-05,
      "loss": 0.0426,
      "step": 6488
    },
    {
      "epoch": 1.6057906458797326,
      "grad_norm": 0.07106718420982361,
      "learning_rate": 4.65288939762924e-05,
      "loss": 0.0565,
      "step": 6489
    },
    {
      "epoch": 1.6060381093788667,
      "grad_norm": 0.0602906160056591,
      "learning_rate": 4.647237601547899e-05,
      "loss": 0.0817,
      "step": 6490
    },
    {
      "epoch": 1.6062855728780003,
      "grad_norm": 0.03774029016494751,
      "learning_rate": 4.6415888883761994e-05,
      "loss": 0.0389,
      "step": 6491
    },
    {
      "epoch": 1.6065330363771344,
      "grad_norm": 0.037432778626680374,
      "learning_rate": 4.635943258969782e-05,
      "loss": 0.0425,
      "step": 6492
    },
    {
      "epoch": 1.6067804998762683,
      "grad_norm": 0.03894377127289772,
      "learning_rate": 4.6303007141837964e-05,
      "loss": 0.0407,
      "step": 6493
    },
    {
      "epoch": 1.607027963375402,
      "grad_norm": 0.029901865869760513,
      "learning_rate": 4.62466125487295e-05,
      "loss": 0.0229,
      "step": 6494
    },
    {
      "epoch": 1.607275426874536,
      "grad_norm": 0.03398726135492325,
      "learning_rate": 4.6190248818914674e-05,
      "loss": 0.0518,
      "step": 6495
    },
    {
      "epoch": 1.6075228903736698,
      "grad_norm": 0.05937594175338745,
      "learning_rate": 4.613391596093114e-05,
      "loss": 0.0835,
      "step": 6496
    },
    {
      "epoch": 1.6077703538728039,
      "grad_norm": 0.02708585001528263,
      "learning_rate": 4.607761398331189e-05,
      "loss": 0.0465,
      "step": 6497
    },
    {
      "epoch": 1.6080178173719375,
      "grad_norm": 0.04561131075024605,
      "learning_rate": 4.6021342894585066e-05,
      "loss": 0.0448,
      "step": 6498
    },
    {
      "epoch": 1.6082652808710716,
      "grad_norm": 0.04271158576011658,
      "learning_rate": 4.596510270327439e-05,
      "loss": 0.0318,
      "step": 6499
    },
    {
      "epoch": 1.6085127443702054,
      "grad_norm": 0.09724654257297516,
      "learning_rate": 4.5908893417898806e-05,
      "loss": 0.0651,
      "step": 6500
    },
    {
      "epoch": 1.6087602078693393,
      "grad_norm": 0.03933132812380791,
      "learning_rate": 4.585271504697241e-05,
      "loss": 0.0515,
      "step": 6501
    },
    {
      "epoch": 1.6090076713684731,
      "grad_norm": 0.07756851613521576,
      "learning_rate": 4.579656759900483e-05,
      "loss": 0.0494,
      "step": 6502
    },
    {
      "epoch": 1.609255134867607,
      "grad_norm": 0.03462347015738487,
      "learning_rate": 4.5740451082500914e-05,
      "loss": 0.0341,
      "step": 6503
    },
    {
      "epoch": 1.609502598366741,
      "grad_norm": 0.04559062793850899,
      "learning_rate": 4.568436550596084e-05,
      "loss": 0.0426,
      "step": 6504
    },
    {
      "epoch": 1.6097500618658747,
      "grad_norm": 0.04617048799991608,
      "learning_rate": 4.562831087788019e-05,
      "loss": 0.0237,
      "step": 6505
    },
    {
      "epoch": 1.6099975253650087,
      "grad_norm": 0.05986397713422775,
      "learning_rate": 4.557228720674955e-05,
      "loss": 0.0412,
      "step": 6506
    },
    {
      "epoch": 1.6102449888641426,
      "grad_norm": 0.05230450630187988,
      "learning_rate": 4.551629450105524e-05,
      "loss": 0.0562,
      "step": 6507
    },
    {
      "epoch": 1.6104924523632764,
      "grad_norm": 0.12706750631332397,
      "learning_rate": 4.546033276927866e-05,
      "loss": 0.0916,
      "step": 6508
    },
    {
      "epoch": 1.6107399158624103,
      "grad_norm": 0.058157000690698624,
      "learning_rate": 4.5404402019896486e-05,
      "loss": 0.0722,
      "step": 6509
    },
    {
      "epoch": 1.6109873793615441,
      "grad_norm": 0.0953020229935646,
      "learning_rate": 4.534850226138076e-05,
      "loss": 0.0515,
      "step": 6510
    },
    {
      "epoch": 1.6112348428606782,
      "grad_norm": 0.042757079005241394,
      "learning_rate": 4.529263350219884e-05,
      "loss": 0.04,
      "step": 6511
    },
    {
      "epoch": 1.6114823063598118,
      "grad_norm": 0.06796585023403168,
      "learning_rate": 4.5236795750813406e-05,
      "loss": 0.0599,
      "step": 6512
    },
    {
      "epoch": 1.611729769858946,
      "grad_norm": 0.04003555700182915,
      "learning_rate": 4.518098901568246e-05,
      "loss": 0.0209,
      "step": 6513
    },
    {
      "epoch": 1.6119772333580795,
      "grad_norm": 0.0458502396941185,
      "learning_rate": 4.512521330525904e-05,
      "loss": 0.0483,
      "step": 6514
    },
    {
      "epoch": 1.6122246968572136,
      "grad_norm": 0.046575553715229034,
      "learning_rate": 4.5069468627991994e-05,
      "loss": 0.0586,
      "step": 6515
    },
    {
      "epoch": 1.6124721603563474,
      "grad_norm": 0.06649736315011978,
      "learning_rate": 4.501375499232499e-05,
      "loss": 0.0747,
      "step": 6516
    },
    {
      "epoch": 1.6127196238554813,
      "grad_norm": 0.05401819571852684,
      "learning_rate": 4.495807240669722e-05,
      "loss": 0.0515,
      "step": 6517
    },
    {
      "epoch": 1.6129670873546154,
      "grad_norm": 0.03712766617536545,
      "learning_rate": 4.490242087954313e-05,
      "loss": 0.0471,
      "step": 6518
    },
    {
      "epoch": 1.613214550853749,
      "grad_norm": 0.0409206822514534,
      "learning_rate": 4.484680041929251e-05,
      "loss": 0.0634,
      "step": 6519
    },
    {
      "epoch": 1.613462014352883,
      "grad_norm": 0.07372374832630157,
      "learning_rate": 4.4791211034370424e-05,
      "loss": 0.0938,
      "step": 6520
    },
    {
      "epoch": 1.6137094778520167,
      "grad_norm": 0.03681036829948425,
      "learning_rate": 4.47356527331971e-05,
      "loss": 0.0253,
      "step": 6521
    },
    {
      "epoch": 1.6139569413511508,
      "grad_norm": 0.08455869555473328,
      "learning_rate": 4.4680125524188157e-05,
      "loss": 0.0707,
      "step": 6522
    },
    {
      "epoch": 1.6142044048502846,
      "grad_norm": 0.02337377704679966,
      "learning_rate": 4.462462941575468e-05,
      "loss": 0.0213,
      "step": 6523
    },
    {
      "epoch": 1.6144518683494185,
      "grad_norm": 0.06828010082244873,
      "learning_rate": 4.4569164416302736e-05,
      "loss": 0.0617,
      "step": 6524
    },
    {
      "epoch": 1.6146993318485523,
      "grad_norm": 0.05248326063156128,
      "learning_rate": 4.451373053423388e-05,
      "loss": 0.0628,
      "step": 6525
    },
    {
      "epoch": 1.6149467953476861,
      "grad_norm": 0.0474696010351181,
      "learning_rate": 4.4458327777944754e-05,
      "loss": 0.0497,
      "step": 6526
    },
    {
      "epoch": 1.6151942588468202,
      "grad_norm": 0.030491231009364128,
      "learning_rate": 4.4402956155827595e-05,
      "loss": 0.0333,
      "step": 6527
    },
    {
      "epoch": 1.6154417223459538,
      "grad_norm": 0.02903576008975506,
      "learning_rate": 4.434761567626974e-05,
      "loss": 0.0445,
      "step": 6528
    },
    {
      "epoch": 1.615689185845088,
      "grad_norm": 0.03366469219326973,
      "learning_rate": 4.429230634765369e-05,
      "loss": 0.0393,
      "step": 6529
    },
    {
      "epoch": 1.6159366493442218,
      "grad_norm": 0.07281064987182617,
      "learning_rate": 4.423702817835745e-05,
      "loss": 0.0959,
      "step": 6530
    },
    {
      "epoch": 1.6161841128433556,
      "grad_norm": 0.06913837790489197,
      "learning_rate": 4.418178117675417e-05,
      "loss": 0.0961,
      "step": 6531
    },
    {
      "epoch": 1.6164315763424895,
      "grad_norm": 0.042179543524980545,
      "learning_rate": 4.4126565351212375e-05,
      "loss": 0.0787,
      "step": 6532
    },
    {
      "epoch": 1.6166790398416233,
      "grad_norm": 0.05128949508070946,
      "learning_rate": 4.407138071009584e-05,
      "loss": 0.0531,
      "step": 6533
    },
    {
      "epoch": 1.6169265033407574,
      "grad_norm": 0.053254157304763794,
      "learning_rate": 4.4016227261763384e-05,
      "loss": 0.0556,
      "step": 6534
    },
    {
      "epoch": 1.617173966839891,
      "grad_norm": 0.03276795521378517,
      "learning_rate": 4.39611050145696e-05,
      "loss": 0.0302,
      "step": 6535
    },
    {
      "epoch": 1.617421430339025,
      "grad_norm": 0.047225359827280045,
      "learning_rate": 4.3906013976863865e-05,
      "loss": 0.0459,
      "step": 6536
    },
    {
      "epoch": 1.6176688938381587,
      "grad_norm": 0.044306106865406036,
      "learning_rate": 4.3850954156991084e-05,
      "loss": 0.0507,
      "step": 6537
    },
    {
      "epoch": 1.6179163573372928,
      "grad_norm": 0.03382483497262001,
      "learning_rate": 4.379592556329137e-05,
      "loss": 0.0529,
      "step": 6538
    },
    {
      "epoch": 1.6181638208364266,
      "grad_norm": 0.06607766449451447,
      "learning_rate": 4.374092820410014e-05,
      "loss": 0.0738,
      "step": 6539
    },
    {
      "epoch": 1.6184112843355605,
      "grad_norm": 0.02524210698902607,
      "learning_rate": 4.368596208774806e-05,
      "loss": 0.0211,
      "step": 6540
    },
    {
      "epoch": 1.6186587478346945,
      "grad_norm": 0.03593963384628296,
      "learning_rate": 4.363102722256096e-05,
      "loss": 0.0255,
      "step": 6541
    },
    {
      "epoch": 1.6189062113338282,
      "grad_norm": 0.03533758968114853,
      "learning_rate": 4.357612361686006e-05,
      "loss": 0.0538,
      "step": 6542
    },
    {
      "epoch": 1.6191536748329622,
      "grad_norm": 0.050820961594581604,
      "learning_rate": 4.352125127896195e-05,
      "loss": 0.0742,
      "step": 6543
    },
    {
      "epoch": 1.6194011383320959,
      "grad_norm": 0.10784624516963959,
      "learning_rate": 4.346641021717818e-05,
      "loss": 0.0514,
      "step": 6544
    },
    {
      "epoch": 1.61964860183123,
      "grad_norm": 0.08746341615915298,
      "learning_rate": 4.3411600439815816e-05,
      "loss": 0.1076,
      "step": 6545
    },
    {
      "epoch": 1.6198960653303638,
      "grad_norm": 0.04775438830256462,
      "learning_rate": 4.3356821955177095e-05,
      "loss": 0.0815,
      "step": 6546
    },
    {
      "epoch": 1.6201435288294976,
      "grad_norm": 0.07043007761240005,
      "learning_rate": 4.330207477155951e-05,
      "loss": 0.0678,
      "step": 6547
    },
    {
      "epoch": 1.6203909923286315,
      "grad_norm": 0.04521695896983147,
      "learning_rate": 4.324735889725587e-05,
      "loss": 0.0855,
      "step": 6548
    },
    {
      "epoch": 1.6206384558277653,
      "grad_norm": 0.03805140405893326,
      "learning_rate": 4.31926743405541e-05,
      "loss": 0.0449,
      "step": 6549
    },
    {
      "epoch": 1.6208859193268994,
      "grad_norm": 0.049505461007356644,
      "learning_rate": 4.3138021109737526e-05,
      "loss": 0.0407,
      "step": 6550
    },
    {
      "epoch": 1.621133382826033,
      "grad_norm": 0.051874782890081406,
      "learning_rate": 4.308339921308468e-05,
      "loss": 0.0674,
      "step": 6551
    },
    {
      "epoch": 1.621380846325167,
      "grad_norm": 0.05144697427749634,
      "learning_rate": 4.302880865886935e-05,
      "loss": 0.0414,
      "step": 6552
    },
    {
      "epoch": 1.621628309824301,
      "grad_norm": 0.05509133264422417,
      "learning_rate": 4.2974249455360576e-05,
      "loss": 0.0397,
      "step": 6553
    },
    {
      "epoch": 1.6218757733234348,
      "grad_norm": 0.049770113080739975,
      "learning_rate": 4.291972161082264e-05,
      "loss": 0.0383,
      "step": 6554
    },
    {
      "epoch": 1.6221232368225686,
      "grad_norm": 0.05897802487015724,
      "learning_rate": 4.2865225133515126e-05,
      "loss": 0.0905,
      "step": 6555
    },
    {
      "epoch": 1.6223707003217025,
      "grad_norm": 0.049265988171100616,
      "learning_rate": 4.281076003169271e-05,
      "loss": 0.0479,
      "step": 6556
    },
    {
      "epoch": 1.6226181638208366,
      "grad_norm": 0.05905303731560707,
      "learning_rate": 4.275632631360551e-05,
      "loss": 0.0388,
      "step": 6557
    },
    {
      "epoch": 1.6228656273199702,
      "grad_norm": 0.038266170769929886,
      "learning_rate": 4.270192398749878e-05,
      "loss": 0.0293,
      "step": 6558
    },
    {
      "epoch": 1.6231130908191043,
      "grad_norm": 0.07455338537693024,
      "learning_rate": 4.2647553061613054e-05,
      "loss": 0.0661,
      "step": 6559
    },
    {
      "epoch": 1.6233605543182381,
      "grad_norm": 0.054903317242860794,
      "learning_rate": 4.2593213544184136e-05,
      "loss": 0.0617,
      "step": 6560
    },
    {
      "epoch": 1.623608017817372,
      "grad_norm": 0.060722481459379196,
      "learning_rate": 4.253890544344291e-05,
      "loss": 0.0749,
      "step": 6561
    },
    {
      "epoch": 1.6238554813165058,
      "grad_norm": 0.04893605411052704,
      "learning_rate": 4.2484628767615775e-05,
      "loss": 0.0486,
      "step": 6562
    },
    {
      "epoch": 1.6241029448156397,
      "grad_norm": 0.09385795146226883,
      "learning_rate": 4.2430383524924234e-05,
      "loss": 0.14,
      "step": 6563
    },
    {
      "epoch": 1.6243504083147737,
      "grad_norm": 0.026463216170668602,
      "learning_rate": 4.237616972358488e-05,
      "loss": 0.0316,
      "step": 6564
    },
    {
      "epoch": 1.6245978718139074,
      "grad_norm": 0.10478322952985764,
      "learning_rate": 4.232198737180976e-05,
      "loss": 0.0785,
      "step": 6565
    },
    {
      "epoch": 1.6248453353130414,
      "grad_norm": 0.05379176884889603,
      "learning_rate": 4.2267836477806075e-05,
      "loss": 0.0495,
      "step": 6566
    },
    {
      "epoch": 1.625092798812175,
      "grad_norm": 0.03717457875609398,
      "learning_rate": 4.2213717049776276e-05,
      "loss": 0.0485,
      "step": 6567
    },
    {
      "epoch": 1.6253402623113091,
      "grad_norm": 0.050231434404850006,
      "learning_rate": 4.215962909591805e-05,
      "loss": 0.0415,
      "step": 6568
    },
    {
      "epoch": 1.625587725810443,
      "grad_norm": 0.044002465903759,
      "learning_rate": 4.210557262442419e-05,
      "loss": 0.0696,
      "step": 6569
    },
    {
      "epoch": 1.6258351893095768,
      "grad_norm": 0.06224672123789787,
      "learning_rate": 4.205154764348304e-05,
      "loss": 0.0713,
      "step": 6570
    },
    {
      "epoch": 1.6260826528087107,
      "grad_norm": 0.054448094218969345,
      "learning_rate": 4.199755416127779e-05,
      "loss": 0.061,
      "step": 6571
    },
    {
      "epoch": 1.6263301163078445,
      "grad_norm": 0.07670470327138901,
      "learning_rate": 4.194359218598709e-05,
      "loss": 0.0291,
      "step": 6572
    },
    {
      "epoch": 1.6265775798069786,
      "grad_norm": 0.08378197997808456,
      "learning_rate": 4.188966172578476e-05,
      "loss": 0.1134,
      "step": 6573
    },
    {
      "epoch": 1.6268250433061122,
      "grad_norm": 0.04313862696290016,
      "learning_rate": 4.183576278883988e-05,
      "loss": 0.0544,
      "step": 6574
    },
    {
      "epoch": 1.6270725068052463,
      "grad_norm": 0.04051653668284416,
      "learning_rate": 4.178189538331675e-05,
      "loss": 0.0409,
      "step": 6575
    },
    {
      "epoch": 1.6273199703043801,
      "grad_norm": 0.029457418248057365,
      "learning_rate": 4.1728059517374764e-05,
      "loss": 0.0342,
      "step": 6576
    },
    {
      "epoch": 1.627567433803514,
      "grad_norm": 0.054886940866708755,
      "learning_rate": 4.167425519916865e-05,
      "loss": 0.0455,
      "step": 6577
    },
    {
      "epoch": 1.6278148973026478,
      "grad_norm": 0.03707605227828026,
      "learning_rate": 4.162048243684852e-05,
      "loss": 0.0256,
      "step": 6578
    },
    {
      "epoch": 1.6280623608017817,
      "grad_norm": 0.037990059703588486,
      "learning_rate": 4.156674123855936e-05,
      "loss": 0.0297,
      "step": 6579
    },
    {
      "epoch": 1.6283098243009158,
      "grad_norm": 0.04431607201695442,
      "learning_rate": 4.15130316124416e-05,
      "loss": 0.0391,
      "step": 6580
    },
    {
      "epoch": 1.6285572878000494,
      "grad_norm": 0.0528082512319088,
      "learning_rate": 4.1459353566630886e-05,
      "loss": 0.0624,
      "step": 6581
    },
    {
      "epoch": 1.6288047512991835,
      "grad_norm": 0.03131912648677826,
      "learning_rate": 4.140570710925798e-05,
      "loss": 0.0434,
      "step": 6582
    },
    {
      "epoch": 1.6290522147983173,
      "grad_norm": 0.08088701963424683,
      "learning_rate": 4.135209224844899e-05,
      "loss": 0.051,
      "step": 6583
    },
    {
      "epoch": 1.6292996782974511,
      "grad_norm": 0.04981067404150963,
      "learning_rate": 4.129850899232504e-05,
      "loss": 0.0505,
      "step": 6584
    },
    {
      "epoch": 1.629547141796585,
      "grad_norm": 0.06895941495895386,
      "learning_rate": 4.124495734900266e-05,
      "loss": 0.0589,
      "step": 6585
    },
    {
      "epoch": 1.6297946052957188,
      "grad_norm": 0.028980353847146034,
      "learning_rate": 4.119143732659353e-05,
      "loss": 0.0201,
      "step": 6586
    },
    {
      "epoch": 1.630042068794853,
      "grad_norm": 0.05091924965381622,
      "learning_rate": 4.113794893320449e-05,
      "loss": 0.0627,
      "step": 6587
    },
    {
      "epoch": 1.6302895322939865,
      "grad_norm": 0.05065377801656723,
      "learning_rate": 4.108449217693766e-05,
      "loss": 0.048,
      "step": 6588
    },
    {
      "epoch": 1.6305369957931206,
      "grad_norm": 0.05548103153705597,
      "learning_rate": 4.1031067065890324e-05,
      "loss": 0.0367,
      "step": 6589
    },
    {
      "epoch": 1.6307844592922542,
      "grad_norm": 0.036664608865976334,
      "learning_rate": 4.0977673608155045e-05,
      "loss": 0.0292,
      "step": 6590
    },
    {
      "epoch": 1.6310319227913883,
      "grad_norm": 0.05372342839837074,
      "learning_rate": 4.092431181181941e-05,
      "loss": 0.0763,
      "step": 6591
    },
    {
      "epoch": 1.6312793862905222,
      "grad_norm": 0.036396320909261703,
      "learning_rate": 4.087098168496639e-05,
      "loss": 0.0387,
      "step": 6592
    },
    {
      "epoch": 1.631526849789656,
      "grad_norm": 0.05774219334125519,
      "learning_rate": 4.081768323567411e-05,
      "loss": 0.0633,
      "step": 6593
    },
    {
      "epoch": 1.6317743132887899,
      "grad_norm": 0.0677989274263382,
      "learning_rate": 4.0764416472015896e-05,
      "loss": 0.0338,
      "step": 6594
    },
    {
      "epoch": 1.6320217767879237,
      "grad_norm": 0.054777201265096664,
      "learning_rate": 4.071118140206029e-05,
      "loss": 0.0569,
      "step": 6595
    },
    {
      "epoch": 1.6322692402870578,
      "grad_norm": 0.06435147672891617,
      "learning_rate": 4.0657978033870865e-05,
      "loss": 0.0646,
      "step": 6596
    },
    {
      "epoch": 1.6325167037861914,
      "grad_norm": 0.031186724081635475,
      "learning_rate": 4.0604806375506705e-05,
      "loss": 0.0306,
      "step": 6597
    },
    {
      "epoch": 1.6327641672853255,
      "grad_norm": 0.048157624900341034,
      "learning_rate": 4.055166643502192e-05,
      "loss": 0.0536,
      "step": 6598
    },
    {
      "epoch": 1.6330116307844593,
      "grad_norm": 0.032748933881521225,
      "learning_rate": 4.0498558220465665e-05,
      "loss": 0.033,
      "step": 6599
    },
    {
      "epoch": 1.6332590942835932,
      "grad_norm": 0.044157616794109344,
      "learning_rate": 4.044548173988255e-05,
      "loss": 0.0515,
      "step": 6600
    },
    {
      "epoch": 1.6332590942835932,
      "eval_loss": 0.2892835736274719,
      "eval_runtime": 202.9551,
      "eval_samples_per_second": 4.927,
      "eval_steps_per_second": 0.31,
      "step": 6600
    },
    {
      "epoch": 1.633506557782727,
      "grad_norm": 0.03774965927004814,
      "learning_rate": 4.039243700131226e-05,
      "loss": 0.0359,
      "step": 6601
    },
    {
      "epoch": 1.6337540212818609,
      "grad_norm": 0.04438995569944382,
      "learning_rate": 4.033942401278967e-05,
      "loss": 0.0947,
      "step": 6602
    },
    {
      "epoch": 1.634001484780995,
      "grad_norm": 0.0724688172340393,
      "learning_rate": 4.0286442782344915e-05,
      "loss": 0.0649,
      "step": 6603
    },
    {
      "epoch": 1.6342489482801286,
      "grad_norm": 0.05335994064807892,
      "learning_rate": 4.023349331800308e-05,
      "loss": 0.0684,
      "step": 6604
    },
    {
      "epoch": 1.6344964117792626,
      "grad_norm": 0.09189518541097641,
      "learning_rate": 4.018057562778488e-05,
      "loss": 0.0856,
      "step": 6605
    },
    {
      "epoch": 1.6347438752783965,
      "grad_norm": 0.036202918738126755,
      "learning_rate": 4.0127689719705764e-05,
      "loss": 0.0445,
      "step": 6606
    },
    {
      "epoch": 1.6349913387775303,
      "grad_norm": 0.08388820290565491,
      "learning_rate": 4.0074835601776604e-05,
      "loss": 0.0954,
      "step": 6607
    },
    {
      "epoch": 1.6352388022766642,
      "grad_norm": 0.049806784838438034,
      "learning_rate": 4.0022013282003444e-05,
      "loss": 0.0806,
      "step": 6608
    },
    {
      "epoch": 1.635486265775798,
      "grad_norm": 0.05012001469731331,
      "learning_rate": 3.9969222768387466e-05,
      "loss": 0.1533,
      "step": 6609
    },
    {
      "epoch": 1.635733729274932,
      "grad_norm": 0.06685467064380646,
      "learning_rate": 3.9916464068925075e-05,
      "loss": 0.0955,
      "step": 6610
    },
    {
      "epoch": 1.6359811927740657,
      "grad_norm": 0.06502167135477066,
      "learning_rate": 3.986373719160777e-05,
      "loss": 0.0549,
      "step": 6611
    },
    {
      "epoch": 1.6362286562731998,
      "grad_norm": 0.07408590614795685,
      "learning_rate": 3.9811042144422245e-05,
      "loss": 0.1053,
      "step": 6612
    },
    {
      "epoch": 1.6364761197723334,
      "grad_norm": 0.02925516478717327,
      "learning_rate": 3.9758378935350615e-05,
      "loss": 0.0273,
      "step": 6613
    },
    {
      "epoch": 1.6367235832714675,
      "grad_norm": 0.0728304460644722,
      "learning_rate": 3.9705747572369805e-05,
      "loss": 0.0958,
      "step": 6614
    },
    {
      "epoch": 1.6369710467706013,
      "grad_norm": 0.05231383815407753,
      "learning_rate": 3.9653148063452186e-05,
      "loss": 0.0392,
      "step": 6615
    },
    {
      "epoch": 1.6372185102697352,
      "grad_norm": 0.04326940327882767,
      "learning_rate": 3.960058041656503e-05,
      "loss": 0.0761,
      "step": 6616
    },
    {
      "epoch": 1.637465973768869,
      "grad_norm": 0.15485180914402008,
      "learning_rate": 3.954804463967115e-05,
      "loss": 0.0522,
      "step": 6617
    },
    {
      "epoch": 1.637713437268003,
      "grad_norm": 0.03303027153015137,
      "learning_rate": 3.949554074072831e-05,
      "loss": 0.0277,
      "step": 6618
    },
    {
      "epoch": 1.637960900767137,
      "grad_norm": 0.1013914942741394,
      "learning_rate": 3.944306872768938e-05,
      "loss": 0.0882,
      "step": 6619
    },
    {
      "epoch": 1.6382083642662706,
      "grad_norm": 0.0541493184864521,
      "learning_rate": 3.939062860850254e-05,
      "loss": 0.0381,
      "step": 6620
    },
    {
      "epoch": 1.6384558277654047,
      "grad_norm": 0.03993235155940056,
      "learning_rate": 3.93382203911111e-05,
      "loss": 0.018,
      "step": 6621
    },
    {
      "epoch": 1.6387032912645385,
      "grad_norm": 0.04707474634051323,
      "learning_rate": 3.928584408345354e-05,
      "loss": 0.05,
      "step": 6622
    },
    {
      "epoch": 1.6389507547636724,
      "grad_norm": 0.07267845422029495,
      "learning_rate": 3.9233499693463516e-05,
      "loss": 0.1,
      "step": 6623
    },
    {
      "epoch": 1.6391982182628062,
      "grad_norm": 0.049191027879714966,
      "learning_rate": 3.918118722906969e-05,
      "loss": 0.0499,
      "step": 6624
    },
    {
      "epoch": 1.63944568176194,
      "grad_norm": 0.028283488005399704,
      "learning_rate": 3.912890669819624e-05,
      "loss": 0.029,
      "step": 6625
    },
    {
      "epoch": 1.6396931452610741,
      "grad_norm": 0.04240769147872925,
      "learning_rate": 3.907665810876215e-05,
      "loss": 0.0395,
      "step": 6626
    },
    {
      "epoch": 1.6399406087602078,
      "grad_norm": 0.05812377855181694,
      "learning_rate": 3.902444146868175e-05,
      "loss": 0.0884,
      "step": 6627
    },
    {
      "epoch": 1.6401880722593418,
      "grad_norm": 0.08040573447942734,
      "learning_rate": 3.897225678586447e-05,
      "loss": 0.0959,
      "step": 6628
    },
    {
      "epoch": 1.6404355357584757,
      "grad_norm": 0.08781979233026505,
      "learning_rate": 3.8920104068214976e-05,
      "loss": 0.0536,
      "step": 6629
    },
    {
      "epoch": 1.6406829992576095,
      "grad_norm": 0.032913122326135635,
      "learning_rate": 3.8867983323633035e-05,
      "loss": 0.0582,
      "step": 6630
    },
    {
      "epoch": 1.6409304627567434,
      "grad_norm": 0.04871819168329239,
      "learning_rate": 3.88158945600135e-05,
      "loss": 0.0489,
      "step": 6631
    },
    {
      "epoch": 1.6411779262558772,
      "grad_norm": 0.04446795955300331,
      "learning_rate": 3.8763837785246445e-05,
      "loss": 0.0289,
      "step": 6632
    },
    {
      "epoch": 1.6414253897550113,
      "grad_norm": 0.062457967549562454,
      "learning_rate": 3.871181300721724e-05,
      "loss": 0.0661,
      "step": 6633
    },
    {
      "epoch": 1.641672853254145,
      "grad_norm": 0.06819909811019897,
      "learning_rate": 3.865982023380615e-05,
      "loss": 0.037,
      "step": 6634
    },
    {
      "epoch": 1.641920316753279,
      "grad_norm": 0.030024481937289238,
      "learning_rate": 3.860785947288875e-05,
      "loss": 0.0195,
      "step": 6635
    },
    {
      "epoch": 1.6421677802524126,
      "grad_norm": 0.0674772784113884,
      "learning_rate": 3.8555930732335765e-05,
      "loss": 0.0755,
      "step": 6636
    },
    {
      "epoch": 1.6424152437515467,
      "grad_norm": 0.04177461564540863,
      "learning_rate": 3.8504034020012985e-05,
      "loss": 0.0616,
      "step": 6637
    },
    {
      "epoch": 1.6426627072506805,
      "grad_norm": 0.03194670006632805,
      "learning_rate": 3.845216934378149e-05,
      "loss": 0.0343,
      "step": 6638
    },
    {
      "epoch": 1.6429101707498144,
      "grad_norm": 0.05520382151007652,
      "learning_rate": 3.8400336711497285e-05,
      "loss": 0.0475,
      "step": 6639
    },
    {
      "epoch": 1.6431576342489482,
      "grad_norm": 0.03404974192380905,
      "learning_rate": 3.834853613101166e-05,
      "loss": 0.0346,
      "step": 6640
    },
    {
      "epoch": 1.643405097748082,
      "grad_norm": 0.028693052008748055,
      "learning_rate": 3.829676761017123e-05,
      "loss": 0.0299,
      "step": 6641
    },
    {
      "epoch": 1.6436525612472161,
      "grad_norm": 0.04705554246902466,
      "learning_rate": 3.8245031156817405e-05,
      "loss": 0.0738,
      "step": 6642
    },
    {
      "epoch": 1.6439000247463498,
      "grad_norm": 0.047714147716760635,
      "learning_rate": 3.8193326778786924e-05,
      "loss": 0.0741,
      "step": 6643
    },
    {
      "epoch": 1.6441474882454838,
      "grad_norm": 0.06219065189361572,
      "learning_rate": 3.814165448391166e-05,
      "loss": 0.0811,
      "step": 6644
    },
    {
      "epoch": 1.6443949517446177,
      "grad_norm": 0.06605440378189087,
      "learning_rate": 3.8090014280018624e-05,
      "loss": 0.0244,
      "step": 6645
    },
    {
      "epoch": 1.6446424152437515,
      "grad_norm": 0.07628555595874786,
      "learning_rate": 3.8038406174929984e-05,
      "loss": 0.0552,
      "step": 6646
    },
    {
      "epoch": 1.6448898787428854,
      "grad_norm": 0.04176587983965874,
      "learning_rate": 3.79868301764629e-05,
      "loss": 0.0506,
      "step": 6647
    },
    {
      "epoch": 1.6451373422420192,
      "grad_norm": 0.05974309518933296,
      "learning_rate": 3.793528629242987e-05,
      "loss": 0.0735,
      "step": 6648
    },
    {
      "epoch": 1.6453848057411533,
      "grad_norm": 0.041471242904663086,
      "learning_rate": 3.7883774530638434e-05,
      "loss": 0.0479,
      "step": 6649
    },
    {
      "epoch": 1.645632269240287,
      "grad_norm": 0.06173662096261978,
      "learning_rate": 3.783229489889123e-05,
      "loss": 0.0539,
      "step": 6650
    },
    {
      "epoch": 1.645879732739421,
      "grad_norm": 0.04519385099411011,
      "learning_rate": 3.7780847404986135e-05,
      "loss": 0.0378,
      "step": 6651
    },
    {
      "epoch": 1.6461271962385549,
      "grad_norm": 0.031749267131090164,
      "learning_rate": 3.772943205671606e-05,
      "loss": 0.0311,
      "step": 6652
    },
    {
      "epoch": 1.6463746597376887,
      "grad_norm": 0.04890705272555351,
      "learning_rate": 3.767804886186912e-05,
      "loss": 0.0362,
      "step": 6653
    },
    {
      "epoch": 1.6466221232368226,
      "grad_norm": 0.05710186809301376,
      "learning_rate": 3.7626697828228445e-05,
      "loss": 0.0592,
      "step": 6654
    },
    {
      "epoch": 1.6468695867359564,
      "grad_norm": 0.05271594226360321,
      "learning_rate": 3.757537896357241e-05,
      "loss": 0.057,
      "step": 6655
    },
    {
      "epoch": 1.6471170502350905,
      "grad_norm": 0.041713032871484756,
      "learning_rate": 3.752409227567446e-05,
      "loss": 0.0465,
      "step": 6656
    },
    {
      "epoch": 1.647364513734224,
      "grad_norm": 0.19345806539058685,
      "learning_rate": 3.747283777230323e-05,
      "loss": 0.0974,
      "step": 6657
    },
    {
      "epoch": 1.6476119772333582,
      "grad_norm": 0.07029149681329727,
      "learning_rate": 3.742161546122244e-05,
      "loss": 0.0862,
      "step": 6658
    },
    {
      "epoch": 1.6478594407324918,
      "grad_norm": 0.0609745979309082,
      "learning_rate": 3.737042535019078e-05,
      "loss": 0.0565,
      "step": 6659
    },
    {
      "epoch": 1.6481069042316259,
      "grad_norm": 0.04500579088926315,
      "learning_rate": 3.7319267446962376e-05,
      "loss": 0.0535,
      "step": 6660
    },
    {
      "epoch": 1.6483543677307597,
      "grad_norm": 0.06686858832836151,
      "learning_rate": 3.7268141759286306e-05,
      "loss": 0.0517,
      "step": 6661
    },
    {
      "epoch": 1.6486018312298936,
      "grad_norm": 0.028079643845558167,
      "learning_rate": 3.721704829490666e-05,
      "loss": 0.0713,
      "step": 6662
    },
    {
      "epoch": 1.6488492947290274,
      "grad_norm": 0.08308176696300507,
      "learning_rate": 3.716598706156282e-05,
      "loss": 0.0894,
      "step": 6663
    },
    {
      "epoch": 1.6490967582281613,
      "grad_norm": 0.04125003516674042,
      "learning_rate": 3.7114958066989225e-05,
      "loss": 0.0359,
      "step": 6664
    },
    {
      "epoch": 1.6493442217272953,
      "grad_norm": 0.03732982650399208,
      "learning_rate": 3.70639613189154e-05,
      "loss": 0.0333,
      "step": 6665
    },
    {
      "epoch": 1.649591685226429,
      "grad_norm": 0.05849163234233856,
      "learning_rate": 3.701299682506609e-05,
      "loss": 0.0267,
      "step": 6666
    },
    {
      "epoch": 1.649839148725563,
      "grad_norm": 0.08753392845392227,
      "learning_rate": 3.696206459316093e-05,
      "loss": 0.0449,
      "step": 6667
    },
    {
      "epoch": 1.6500866122246969,
      "grad_norm": 0.06189608946442604,
      "learning_rate": 3.691116463091504e-05,
      "loss": 0.0689,
      "step": 6668
    },
    {
      "epoch": 1.6503340757238307,
      "grad_norm": 0.036929886788129807,
      "learning_rate": 3.686029694603821e-05,
      "loss": 0.0359,
      "step": 6669
    },
    {
      "epoch": 1.6505815392229646,
      "grad_norm": 0.04713793471455574,
      "learning_rate": 3.680946154623566e-05,
      "loss": 0.05,
      "step": 6670
    },
    {
      "epoch": 1.6508290027220984,
      "grad_norm": 0.08750469237565994,
      "learning_rate": 3.675865843920761e-05,
      "loss": 0.0613,
      "step": 6671
    },
    {
      "epoch": 1.6510764662212325,
      "grad_norm": 0.05950356274843216,
      "learning_rate": 3.670788763264943e-05,
      "loss": 0.0553,
      "step": 6672
    },
    {
      "epoch": 1.6513239297203661,
      "grad_norm": 0.07537312060594559,
      "learning_rate": 3.665714913425156e-05,
      "loss": 0.068,
      "step": 6673
    },
    {
      "epoch": 1.6515713932195002,
      "grad_norm": 0.050239767879247665,
      "learning_rate": 3.6606442951699484e-05,
      "loss": 0.0633,
      "step": 6674
    },
    {
      "epoch": 1.651818856718634,
      "grad_norm": 0.032977111637592316,
      "learning_rate": 3.655576909267383e-05,
      "loss": 0.0275,
      "step": 6675
    },
    {
      "epoch": 1.652066320217768,
      "grad_norm": 0.05779402703046799,
      "learning_rate": 3.650512756485055e-05,
      "loss": 0.0461,
      "step": 6676
    },
    {
      "epoch": 1.6523137837169017,
      "grad_norm": 0.08607861399650574,
      "learning_rate": 3.645451837590033e-05,
      "loss": 0.053,
      "step": 6677
    },
    {
      "epoch": 1.6525612472160356,
      "grad_norm": 0.0849384292960167,
      "learning_rate": 3.6403941533489246e-05,
      "loss": 0.0495,
      "step": 6678
    },
    {
      "epoch": 1.6528087107151697,
      "grad_norm": 0.035738375037908554,
      "learning_rate": 3.635339704527821e-05,
      "loss": 0.0449,
      "step": 6679
    },
    {
      "epoch": 1.6530561742143033,
      "grad_norm": 0.03518638387322426,
      "learning_rate": 3.630288491892353e-05,
      "loss": 0.0266,
      "step": 6680
    },
    {
      "epoch": 1.6533036377134374,
      "grad_norm": 0.04768478870391846,
      "learning_rate": 3.6252405162076453e-05,
      "loss": 0.0483,
      "step": 6681
    },
    {
      "epoch": 1.653551101212571,
      "grad_norm": 0.059932079166173935,
      "learning_rate": 3.620195778238328e-05,
      "loss": 0.0698,
      "step": 6682
    },
    {
      "epoch": 1.653798564711705,
      "grad_norm": 0.07386033982038498,
      "learning_rate": 3.6151542787485504e-05,
      "loss": 0.0922,
      "step": 6683
    },
    {
      "epoch": 1.654046028210839,
      "grad_norm": 0.07383481413125992,
      "learning_rate": 3.610116018501963e-05,
      "loss": 0.0718,
      "step": 6684
    },
    {
      "epoch": 1.6542934917099728,
      "grad_norm": 0.16408678889274597,
      "learning_rate": 3.6050809982617364e-05,
      "loss": 0.0898,
      "step": 6685
    },
    {
      "epoch": 1.6545409552091068,
      "grad_norm": 0.05247936025261879,
      "learning_rate": 3.600049218790544e-05,
      "loss": 0.0675,
      "step": 6686
    },
    {
      "epoch": 1.6547884187082404,
      "grad_norm": 0.04109179228544235,
      "learning_rate": 3.5950206808505555e-05,
      "loss": 0.0536,
      "step": 6687
    },
    {
      "epoch": 1.6550358822073745,
      "grad_norm": 0.11356163024902344,
      "learning_rate": 3.589995385203484e-05,
      "loss": 0.1088,
      "step": 6688
    },
    {
      "epoch": 1.6552833457065081,
      "grad_norm": 0.05326602980494499,
      "learning_rate": 3.584973332610514e-05,
      "loss": 0.047,
      "step": 6689
    },
    {
      "epoch": 1.6555308092056422,
      "grad_norm": 0.08830110728740692,
      "learning_rate": 3.579954523832357e-05,
      "loss": 0.0666,
      "step": 6690
    },
    {
      "epoch": 1.655778272704776,
      "grad_norm": 0.04465453326702118,
      "learning_rate": 3.574938959629234e-05,
      "loss": 0.0411,
      "step": 6691
    },
    {
      "epoch": 1.65602573620391,
      "grad_norm": 0.04137469455599785,
      "learning_rate": 3.5699266407608706e-05,
      "loss": 0.0655,
      "step": 6692
    },
    {
      "epoch": 1.6562731997030438,
      "grad_norm": 0.03325793519616127,
      "learning_rate": 3.564917567986509e-05,
      "loss": 0.0322,
      "step": 6693
    },
    {
      "epoch": 1.6565206632021776,
      "grad_norm": 0.04563054069876671,
      "learning_rate": 3.559911742064878e-05,
      "loss": 0.0348,
      "step": 6694
    },
    {
      "epoch": 1.6567681267013117,
      "grad_norm": 0.05047808587551117,
      "learning_rate": 3.5549091637542275e-05,
      "loss": 0.0442,
      "step": 6695
    },
    {
      "epoch": 1.6570155902004453,
      "grad_norm": 0.042195774614810944,
      "learning_rate": 3.54990983381234e-05,
      "loss": 0.0462,
      "step": 6696
    },
    {
      "epoch": 1.6572630536995794,
      "grad_norm": 0.09481102973222733,
      "learning_rate": 3.544913752996462e-05,
      "loss": 0.0825,
      "step": 6697
    },
    {
      "epoch": 1.6575105171987132,
      "grad_norm": 0.04332541674375534,
      "learning_rate": 3.539920922063372e-05,
      "loss": 0.0537,
      "step": 6698
    },
    {
      "epoch": 1.657757980697847,
      "grad_norm": 0.027087993919849396,
      "learning_rate": 3.5349313417693596e-05,
      "loss": 0.0254,
      "step": 6699
    },
    {
      "epoch": 1.658005444196981,
      "grad_norm": 0.03769322857260704,
      "learning_rate": 3.529945012870209e-05,
      "loss": 0.0444,
      "step": 6700
    },
    {
      "epoch": 1.6582529076961148,
      "grad_norm": 0.06380438059568405,
      "learning_rate": 3.524961936121227e-05,
      "loss": 0.047,
      "step": 6701
    },
    {
      "epoch": 1.6585003711952488,
      "grad_norm": 0.09245080500841141,
      "learning_rate": 3.5199821122772064e-05,
      "loss": 0.0985,
      "step": 6702
    },
    {
      "epoch": 1.6587478346943825,
      "grad_norm": 0.08873416483402252,
      "learning_rate": 3.515005542092467e-05,
      "loss": 0.0457,
      "step": 6703
    },
    {
      "epoch": 1.6589952981935165,
      "grad_norm": 0.05445464327931404,
      "learning_rate": 3.510032226320828e-05,
      "loss": 0.0431,
      "step": 6704
    },
    {
      "epoch": 1.6592427616926502,
      "grad_norm": 0.12132816761732101,
      "learning_rate": 3.5050621657156137e-05,
      "loss": 0.076,
      "step": 6705
    },
    {
      "epoch": 1.6594902251917842,
      "grad_norm": 0.030469948425889015,
      "learning_rate": 3.500095361029662e-05,
      "loss": 0.0261,
      "step": 6706
    },
    {
      "epoch": 1.659737688690918,
      "grad_norm": 0.06359454244375229,
      "learning_rate": 3.495131813015312e-05,
      "loss": 0.0377,
      "step": 6707
    },
    {
      "epoch": 1.659985152190052,
      "grad_norm": 0.034893449395895004,
      "learning_rate": 3.490171522424418e-05,
      "loss": 0.0364,
      "step": 6708
    },
    {
      "epoch": 1.660232615689186,
      "grad_norm": 0.03368882089853287,
      "learning_rate": 3.4852144900083175e-05,
      "loss": 0.0272,
      "step": 6709
    },
    {
      "epoch": 1.6604800791883196,
      "grad_norm": 0.053338948637247086,
      "learning_rate": 3.480260716517883e-05,
      "loss": 0.0803,
      "step": 6710
    },
    {
      "epoch": 1.6607275426874537,
      "grad_norm": 0.037508368492126465,
      "learning_rate": 3.4753102027034774e-05,
      "loss": 0.047,
      "step": 6711
    },
    {
      "epoch": 1.6609750061865873,
      "grad_norm": 0.10105855017900467,
      "learning_rate": 3.4703629493149755e-05,
      "loss": 0.0713,
      "step": 6712
    },
    {
      "epoch": 1.6612224696857214,
      "grad_norm": 0.05538327991962433,
      "learning_rate": 3.46541895710176e-05,
      "loss": 0.0584,
      "step": 6713
    },
    {
      "epoch": 1.6614699331848553,
      "grad_norm": 0.06181484833359718,
      "learning_rate": 3.460478226812702e-05,
      "loss": 0.056,
      "step": 6714
    },
    {
      "epoch": 1.661717396683989,
      "grad_norm": 0.05543319880962372,
      "learning_rate": 3.455540759196207e-05,
      "loss": 0.0943,
      "step": 6715
    },
    {
      "epoch": 1.661964860183123,
      "grad_norm": 0.05033660680055618,
      "learning_rate": 3.450606555000171e-05,
      "loss": 0.0546,
      "step": 6716
    },
    {
      "epoch": 1.6622123236822568,
      "grad_norm": 0.05589986592531204,
      "learning_rate": 3.445675614971991e-05,
      "loss": 0.0711,
      "step": 6717
    },
    {
      "epoch": 1.6624597871813909,
      "grad_norm": 0.07181724905967712,
      "learning_rate": 3.440747939858574e-05,
      "loss": 0.0684,
      "step": 6718
    },
    {
      "epoch": 1.6627072506805245,
      "grad_norm": 0.021236665546894073,
      "learning_rate": 3.435823530406335e-05,
      "loss": 0.0188,
      "step": 6719
    },
    {
      "epoch": 1.6629547141796586,
      "grad_norm": 0.052839599549770355,
      "learning_rate": 3.4309023873611956e-05,
      "loss": 0.071,
      "step": 6720
    },
    {
      "epoch": 1.6632021776787924,
      "grad_norm": 0.042889636009931564,
      "learning_rate": 3.4259845114685825e-05,
      "loss": 0.055,
      "step": 6721
    },
    {
      "epoch": 1.6634496411779263,
      "grad_norm": 0.08478341996669769,
      "learning_rate": 3.4210699034734116e-05,
      "loss": 0.0757,
      "step": 6722
    },
    {
      "epoch": 1.6636971046770601,
      "grad_norm": 0.10751847922801971,
      "learning_rate": 3.416158564120137e-05,
      "loss": 0.1597,
      "step": 6723
    },
    {
      "epoch": 1.663944568176194,
      "grad_norm": 0.061630573123693466,
      "learning_rate": 3.411250494152682e-05,
      "loss": 0.04,
      "step": 6724
    },
    {
      "epoch": 1.664192031675328,
      "grad_norm": 0.03265944495797157,
      "learning_rate": 3.4063456943144945e-05,
      "loss": 0.0303,
      "step": 6725
    },
    {
      "epoch": 1.6644394951744617,
      "grad_norm": 0.09451256692409515,
      "learning_rate": 3.401444165348522e-05,
      "loss": 0.0885,
      "step": 6726
    },
    {
      "epoch": 1.6646869586735957,
      "grad_norm": 0.1354270577430725,
      "learning_rate": 3.396545907997223e-05,
      "loss": 0.0863,
      "step": 6727
    },
    {
      "epoch": 1.6649344221727296,
      "grad_norm": 0.040791917592287064,
      "learning_rate": 3.3916509230025537e-05,
      "loss": 0.0358,
      "step": 6728
    },
    {
      "epoch": 1.6651818856718634,
      "grad_norm": 0.05282068997621536,
      "learning_rate": 3.386759211105969e-05,
      "loss": 0.0589,
      "step": 6729
    },
    {
      "epoch": 1.6654293491709973,
      "grad_norm": 0.059426408261060715,
      "learning_rate": 3.381870773048437e-05,
      "loss": 0.0723,
      "step": 6730
    },
    {
      "epoch": 1.6656768126701311,
      "grad_norm": 0.07109498232603073,
      "learning_rate": 3.376985609570438e-05,
      "loss": 0.0663,
      "step": 6731
    },
    {
      "epoch": 1.6659242761692652,
      "grad_norm": 0.029479363933205605,
      "learning_rate": 3.3721037214119344e-05,
      "loss": 0.032,
      "step": 6732
    },
    {
      "epoch": 1.6661717396683988,
      "grad_norm": 0.0657561868429184,
      "learning_rate": 3.367225109312416e-05,
      "loss": 0.0834,
      "step": 6733
    },
    {
      "epoch": 1.666419203167533,
      "grad_norm": 0.04031025618314743,
      "learning_rate": 3.362349774010842e-05,
      "loss": 0.0576,
      "step": 6734
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.04611341282725334,
      "learning_rate": 3.357477716245721e-05,
      "loss": 0.034,
      "step": 6735
    },
    {
      "epoch": 1.6669141301658006,
      "grad_norm": 0.04080250486731529,
      "learning_rate": 3.352608936755039e-05,
      "loss": 0.0339,
      "step": 6736
    },
    {
      "epoch": 1.6671615936649344,
      "grad_norm": 0.03845033794641495,
      "learning_rate": 3.347743436276279e-05,
      "loss": 0.0356,
      "step": 6737
    },
    {
      "epoch": 1.6674090571640683,
      "grad_norm": 0.07235272973775864,
      "learning_rate": 3.3428812155464394e-05,
      "loss": 0.0499,
      "step": 6738
    },
    {
      "epoch": 1.6676565206632021,
      "grad_norm": 0.037950821220874786,
      "learning_rate": 3.3380222753020214e-05,
      "loss": 0.0351,
      "step": 6739
    },
    {
      "epoch": 1.667903984162336,
      "grad_norm": 0.06289931386709213,
      "learning_rate": 3.333166616279024e-05,
      "loss": 0.1003,
      "step": 6740
    },
    {
      "epoch": 1.66815144766147,
      "grad_norm": 0.042550962418317795,
      "learning_rate": 3.328314239212962e-05,
      "loss": 0.0582,
      "step": 6741
    },
    {
      "epoch": 1.6683989111606037,
      "grad_norm": 0.13881617784500122,
      "learning_rate": 3.323465144838825e-05,
      "loss": 0.1335,
      "step": 6742
    },
    {
      "epoch": 1.6686463746597378,
      "grad_norm": 0.024481886997818947,
      "learning_rate": 3.318619333891143e-05,
      "loss": 0.0276,
      "step": 6743
    },
    {
      "epoch": 1.6688938381588716,
      "grad_norm": 0.07495854794979095,
      "learning_rate": 3.313776807103916e-05,
      "loss": 0.0542,
      "step": 6744
    },
    {
      "epoch": 1.6691413016580054,
      "grad_norm": 0.07320889830589294,
      "learning_rate": 3.308937565210665e-05,
      "loss": 0.0509,
      "step": 6745
    },
    {
      "epoch": 1.6693887651571393,
      "grad_norm": 0.06885549426078796,
      "learning_rate": 3.304101608944407e-05,
      "loss": 0.0592,
      "step": 6746
    },
    {
      "epoch": 1.6696362286562731,
      "grad_norm": 0.06676658987998962,
      "learning_rate": 3.299268939037664e-05,
      "loss": 0.0568,
      "step": 6747
    },
    {
      "epoch": 1.6698836921554072,
      "grad_norm": 0.0409809872508049,
      "learning_rate": 3.294439556222462e-05,
      "loss": 0.0487,
      "step": 6748
    },
    {
      "epoch": 1.6701311556545408,
      "grad_norm": 0.0352691188454628,
      "learning_rate": 3.289613461230315e-05,
      "loss": 0.045,
      "step": 6749
    },
    {
      "epoch": 1.670378619153675,
      "grad_norm": 0.04478913173079491,
      "learning_rate": 3.2847906547922524e-05,
      "loss": 0.0477,
      "step": 6750
    },
    {
      "epoch": 1.6706260826528088,
      "grad_norm": 0.07242922484874725,
      "learning_rate": 3.279971137638818e-05,
      "loss": 0.072,
      "step": 6751
    },
    {
      "epoch": 1.6708735461519426,
      "grad_norm": 0.0462966226041317,
      "learning_rate": 3.275154910500025e-05,
      "loss": 0.033,
      "step": 6752
    },
    {
      "epoch": 1.6711210096510765,
      "grad_norm": 0.053778015077114105,
      "learning_rate": 3.270341974105412e-05,
      "loss": 0.0367,
      "step": 6753
    },
    {
      "epoch": 1.6713684731502103,
      "grad_norm": 0.04077433794736862,
      "learning_rate": 3.2655323291840095e-05,
      "loss": 0.0407,
      "step": 6754
    },
    {
      "epoch": 1.6716159366493444,
      "grad_norm": 0.04524191468954086,
      "learning_rate": 3.2607259764643574e-05,
      "loss": 0.0454,
      "step": 6755
    },
    {
      "epoch": 1.671863400148478,
      "grad_norm": 0.046111464500427246,
      "learning_rate": 3.2559229166744945e-05,
      "loss": 0.0364,
      "step": 6756
    },
    {
      "epoch": 1.672110863647612,
      "grad_norm": 0.037440214306116104,
      "learning_rate": 3.251123150541951e-05,
      "loss": 0.0462,
      "step": 6757
    },
    {
      "epoch": 1.6723583271467457,
      "grad_norm": 0.019893670454621315,
      "learning_rate": 3.24632667879376e-05,
      "loss": 0.0202,
      "step": 6758
    },
    {
      "epoch": 1.6726057906458798,
      "grad_norm": 0.03464503586292267,
      "learning_rate": 3.2415335021564825e-05,
      "loss": 0.0412,
      "step": 6759
    },
    {
      "epoch": 1.6728532541450136,
      "grad_norm": 0.050015419721603394,
      "learning_rate": 3.236743621356139e-05,
      "loss": 0.051,
      "step": 6760
    },
    {
      "epoch": 1.6731007176441475,
      "grad_norm": 0.0314834825694561,
      "learning_rate": 3.231957037118277e-05,
      "loss": 0.0494,
      "step": 6761
    },
    {
      "epoch": 1.6733481811432813,
      "grad_norm": 0.03317028284072876,
      "learning_rate": 3.227173750167941e-05,
      "loss": 0.0289,
      "step": 6762
    },
    {
      "epoch": 1.6735956446424152,
      "grad_norm": 0.06057979539036751,
      "learning_rate": 3.2223937612296695e-05,
      "loss": 0.0445,
      "step": 6763
    },
    {
      "epoch": 1.6738431081415492,
      "grad_norm": 0.03784561529755592,
      "learning_rate": 3.217617071027515e-05,
      "loss": 0.0379,
      "step": 6764
    },
    {
      "epoch": 1.6740905716406829,
      "grad_norm": 0.03136752173304558,
      "learning_rate": 3.2128436802850076e-05,
      "loss": 0.0188,
      "step": 6765
    },
    {
      "epoch": 1.674338035139817,
      "grad_norm": 0.040902044624090195,
      "learning_rate": 3.208073589725197e-05,
      "loss": 0.0374,
      "step": 6766
    },
    {
      "epoch": 1.6745854986389508,
      "grad_norm": 0.057696305215358734,
      "learning_rate": 3.2033068000706265e-05,
      "loss": 0.0399,
      "step": 6767
    },
    {
      "epoch": 1.6748329621380846,
      "grad_norm": 0.036810074001550674,
      "learning_rate": 3.198543312043339e-05,
      "loss": 0.0436,
      "step": 6768
    },
    {
      "epoch": 1.6750804256372185,
      "grad_norm": 0.037576694041490555,
      "learning_rate": 3.19378312636488e-05,
      "loss": 0.0484,
      "step": 6769
    },
    {
      "epoch": 1.6753278891363523,
      "grad_norm": 0.05973590165376663,
      "learning_rate": 3.189026243756288e-05,
      "loss": 0.0563,
      "step": 6770
    },
    {
      "epoch": 1.6755753526354864,
      "grad_norm": 0.04189782217144966,
      "learning_rate": 3.1842726649381174e-05,
      "loss": 0.0357,
      "step": 6771
    },
    {
      "epoch": 1.67582281613462,
      "grad_norm": 0.0552583709359169,
      "learning_rate": 3.179522390630396e-05,
      "loss": 0.0532,
      "step": 6772
    },
    {
      "epoch": 1.676070279633754,
      "grad_norm": 0.0705549493432045,
      "learning_rate": 3.1747754215526756e-05,
      "loss": 0.0671,
      "step": 6773
    },
    {
      "epoch": 1.676317743132888,
      "grad_norm": 0.07982068508863449,
      "learning_rate": 3.170031758423991e-05,
      "loss": 0.0473,
      "step": 6774
    },
    {
      "epoch": 1.6765652066320218,
      "grad_norm": 0.06786506623029709,
      "learning_rate": 3.165291401962886e-05,
      "loss": 0.0475,
      "step": 6775
    },
    {
      "epoch": 1.6768126701311556,
      "grad_norm": 0.04349638521671295,
      "learning_rate": 3.160554352887407e-05,
      "loss": 0.0425,
      "step": 6776
    },
    {
      "epoch": 1.6770601336302895,
      "grad_norm": 0.06659820675849915,
      "learning_rate": 3.155820611915075e-05,
      "loss": 0.0443,
      "step": 6777
    },
    {
      "epoch": 1.6773075971294236,
      "grad_norm": 0.05890257656574249,
      "learning_rate": 3.1510901797629445e-05,
      "loss": 0.0436,
      "step": 6778
    },
    {
      "epoch": 1.6775550606285572,
      "grad_norm": 0.0478900745511055,
      "learning_rate": 3.14636305714755e-05,
      "loss": 0.0261,
      "step": 6779
    },
    {
      "epoch": 1.6778025241276913,
      "grad_norm": 0.034546297043561935,
      "learning_rate": 3.1416392447849194e-05,
      "loss": 0.0337,
      "step": 6780
    },
    {
      "epoch": 1.678049987626825,
      "grad_norm": 0.04039507731795311,
      "learning_rate": 3.136918743390588e-05,
      "loss": 0.0379,
      "step": 6781
    },
    {
      "epoch": 1.678297451125959,
      "grad_norm": 0.040959201753139496,
      "learning_rate": 3.1322015536795865e-05,
      "loss": 0.0364,
      "step": 6782
    },
    {
      "epoch": 1.6785449146250928,
      "grad_norm": 0.07963770627975464,
      "learning_rate": 3.127487676366453e-05,
      "loss": 0.0507,
      "step": 6783
    },
    {
      "epoch": 1.6787923781242267,
      "grad_norm": 0.05095779523253441,
      "learning_rate": 3.1227771121652146e-05,
      "loss": 0.0472,
      "step": 6784
    },
    {
      "epoch": 1.6790398416233605,
      "grad_norm": 0.057547666132450104,
      "learning_rate": 3.118069861789386e-05,
      "loss": 0.0583,
      "step": 6785
    },
    {
      "epoch": 1.6792873051224944,
      "grad_norm": 0.04989757761359215,
      "learning_rate": 3.113365925952011e-05,
      "loss": 0.0489,
      "step": 6786
    },
    {
      "epoch": 1.6795347686216284,
      "grad_norm": 0.06952930241823196,
      "learning_rate": 3.108665305365596e-05,
      "loss": 0.0833,
      "step": 6787
    },
    {
      "epoch": 1.679782232120762,
      "grad_norm": 0.05798716843128204,
      "learning_rate": 3.1039680007421715e-05,
      "loss": 0.078,
      "step": 6788
    },
    {
      "epoch": 1.6800296956198961,
      "grad_norm": 0.04108448326587677,
      "learning_rate": 3.099274012793252e-05,
      "loss": 0.0361,
      "step": 6789
    },
    {
      "epoch": 1.68027715911903,
      "grad_norm": 0.051721297204494476,
      "learning_rate": 3.0945833422298536e-05,
      "loss": 0.0538,
      "step": 6790
    },
    {
      "epoch": 1.6805246226181638,
      "grad_norm": 0.05464329943060875,
      "learning_rate": 3.0898959897624976e-05,
      "loss": 0.0448,
      "step": 6791
    },
    {
      "epoch": 1.6807720861172977,
      "grad_norm": 0.04742085933685303,
      "learning_rate": 3.085211956101183e-05,
      "loss": 0.0493,
      "step": 6792
    },
    {
      "epoch": 1.6810195496164315,
      "grad_norm": 0.07897818088531494,
      "learning_rate": 3.080531241955417e-05,
      "loss": 0.1104,
      "step": 6793
    },
    {
      "epoch": 1.6812670131155656,
      "grad_norm": 0.03541730344295502,
      "learning_rate": 3.075853848034221e-05,
      "loss": 0.0419,
      "step": 6794
    },
    {
      "epoch": 1.6815144766146992,
      "grad_norm": 0.04427511245012283,
      "learning_rate": 3.071179775046085e-05,
      "loss": 0.0567,
      "step": 6795
    },
    {
      "epoch": 1.6817619401138333,
      "grad_norm": 0.03902669623494148,
      "learning_rate": 3.06650902369901e-05,
      "loss": 0.0439,
      "step": 6796
    },
    {
      "epoch": 1.6820094036129671,
      "grad_norm": 0.03877522796392441,
      "learning_rate": 3.061841594700493e-05,
      "loss": 0.0585,
      "step": 6797
    },
    {
      "epoch": 1.682256867112101,
      "grad_norm": 0.05041639879345894,
      "learning_rate": 3.057177488757526e-05,
      "loss": 0.0506,
      "step": 6798
    },
    {
      "epoch": 1.6825043306112348,
      "grad_norm": 0.062379930168390274,
      "learning_rate": 3.052516706576608e-05,
      "loss": 0.0642,
      "step": 6799
    },
    {
      "epoch": 1.6827517941103687,
      "grad_norm": 0.08976630121469498,
      "learning_rate": 3.047859248863713e-05,
      "loss": 0.03,
      "step": 6800
    },
    {
      "epoch": 1.6827517941103687,
      "eval_loss": 0.28914839029312134,
      "eval_runtime": 202.6037,
      "eval_samples_per_second": 4.936,
      "eval_steps_per_second": 0.311,
      "step": 6800
    },
    {
      "epoch": 1.6829992576095028,
      "grad_norm": 0.060731008648872375,
      "learning_rate": 3.0432051163243268e-05,
      "loss": 0.0857,
      "step": 6801
    },
    {
      "epoch": 1.6832467211086364,
      "grad_norm": 0.06404519081115723,
      "learning_rate": 3.0385543096634272e-05,
      "loss": 0.0666,
      "step": 6802
    },
    {
      "epoch": 1.6834941846077704,
      "grad_norm": 0.04294072836637497,
      "learning_rate": 3.033906829585495e-05,
      "loss": 0.0366,
      "step": 6803
    },
    {
      "epoch": 1.683741648106904,
      "grad_norm": 0.04312753677368164,
      "learning_rate": 3.0292626767945003e-05,
      "loss": 0.0558,
      "step": 6804
    },
    {
      "epoch": 1.6839891116060381,
      "grad_norm": 0.05565265193581581,
      "learning_rate": 3.0246218519939056e-05,
      "loss": 0.0564,
      "step": 6805
    },
    {
      "epoch": 1.684236575105172,
      "grad_norm": 0.044753313064575195,
      "learning_rate": 3.019984355886682e-05,
      "loss": 0.0476,
      "step": 6806
    },
    {
      "epoch": 1.6844840386043058,
      "grad_norm": 0.05425560101866722,
      "learning_rate": 3.0153501891752812e-05,
      "loss": 0.0286,
      "step": 6807
    },
    {
      "epoch": 1.6847315021034397,
      "grad_norm": 0.04516824334859848,
      "learning_rate": 3.0107193525616585e-05,
      "loss": 0.0472,
      "step": 6808
    },
    {
      "epoch": 1.6849789656025735,
      "grad_norm": 0.07108253985643387,
      "learning_rate": 3.0060918467472664e-05,
      "loss": 0.0544,
      "step": 6809
    },
    {
      "epoch": 1.6852264291017076,
      "grad_norm": 0.03223882243037224,
      "learning_rate": 3.0014676724330504e-05,
      "loss": 0.0576,
      "step": 6810
    },
    {
      "epoch": 1.6854738926008412,
      "grad_norm": 0.04113202169537544,
      "learning_rate": 2.9968468303194578e-05,
      "loss": 0.0488,
      "step": 6811
    },
    {
      "epoch": 1.6857213560999753,
      "grad_norm": 0.040464162826538086,
      "learning_rate": 2.9922293211064074e-05,
      "loss": 0.0341,
      "step": 6812
    },
    {
      "epoch": 1.6859688195991092,
      "grad_norm": 0.04866384342312813,
      "learning_rate": 2.987615145493347e-05,
      "loss": 0.0469,
      "step": 6813
    },
    {
      "epoch": 1.686216283098243,
      "grad_norm": 0.03863681107759476,
      "learning_rate": 2.9830043041792045e-05,
      "loss": 0.0427,
      "step": 6814
    },
    {
      "epoch": 1.6864637465973769,
      "grad_norm": 0.06691590696573257,
      "learning_rate": 2.9783967978623898e-05,
      "loss": 0.105,
      "step": 6815
    },
    {
      "epoch": 1.6867112100965107,
      "grad_norm": 0.04417692497372627,
      "learning_rate": 2.973792627240823e-05,
      "loss": 0.0497,
      "step": 6816
    },
    {
      "epoch": 1.6869586735956448,
      "grad_norm": 0.04072503745555878,
      "learning_rate": 2.9691917930119188e-05,
      "loss": 0.0398,
      "step": 6817
    },
    {
      "epoch": 1.6872061370947784,
      "grad_norm": 0.03793509677052498,
      "learning_rate": 2.9645942958725773e-05,
      "loss": 0.0417,
      "step": 6818
    },
    {
      "epoch": 1.6874536005939125,
      "grad_norm": 0.053952187299728394,
      "learning_rate": 2.9600001365192113e-05,
      "loss": 0.0183,
      "step": 6819
    },
    {
      "epoch": 1.6877010640930463,
      "grad_norm": 0.0478544682264328,
      "learning_rate": 2.9554093156476946e-05,
      "loss": 0.0547,
      "step": 6820
    },
    {
      "epoch": 1.6879485275921802,
      "grad_norm": 0.07687573879957199,
      "learning_rate": 2.9508218339534376e-05,
      "loss": 0.0487,
      "step": 6821
    },
    {
      "epoch": 1.688195991091314,
      "grad_norm": 0.048817262053489685,
      "learning_rate": 2.9462376921313094e-05,
      "loss": 0.0565,
      "step": 6822
    },
    {
      "epoch": 1.6884434545904479,
      "grad_norm": 0.054183389991521835,
      "learning_rate": 2.9416568908756934e-05,
      "loss": 0.043,
      "step": 6823
    },
    {
      "epoch": 1.688690918089582,
      "grad_norm": 0.045739080756902695,
      "learning_rate": 2.937079430880457e-05,
      "loss": 0.0683,
      "step": 6824
    },
    {
      "epoch": 1.6889383815887156,
      "grad_norm": 0.04998626932501793,
      "learning_rate": 2.932505312838968e-05,
      "loss": 0.0597,
      "step": 6825
    },
    {
      "epoch": 1.6891858450878496,
      "grad_norm": 0.039285123348236084,
      "learning_rate": 2.927934537444091e-05,
      "loss": 0.035,
      "step": 6826
    },
    {
      "epoch": 1.6894333085869833,
      "grad_norm": 0.03341127932071686,
      "learning_rate": 2.9233671053881665e-05,
      "loss": 0.0559,
      "step": 6827
    },
    {
      "epoch": 1.6896807720861173,
      "grad_norm": 0.042978256940841675,
      "learning_rate": 2.918803017363039e-05,
      "loss": 0.0351,
      "step": 6828
    },
    {
      "epoch": 1.6899282355852512,
      "grad_norm": 0.0506490133702755,
      "learning_rate": 2.9142422740600694e-05,
      "loss": 0.0452,
      "step": 6829
    },
    {
      "epoch": 1.690175699084385,
      "grad_norm": 0.09515441209077835,
      "learning_rate": 2.9096848761700683e-05,
      "loss": 0.1074,
      "step": 6830
    },
    {
      "epoch": 1.6904231625835189,
      "grad_norm": 0.07628577202558517,
      "learning_rate": 2.9051308243833785e-05,
      "loss": 0.0286,
      "step": 6831
    },
    {
      "epoch": 1.6906706260826527,
      "grad_norm": 0.06573633849620819,
      "learning_rate": 2.9005801193898007e-05,
      "loss": 0.0428,
      "step": 6832
    },
    {
      "epoch": 1.6909180895817868,
      "grad_norm": 0.06340483576059341,
      "learning_rate": 2.8960327618786613e-05,
      "loss": 0.0539,
      "step": 6833
    },
    {
      "epoch": 1.6911655530809204,
      "grad_norm": 0.12105559557676315,
      "learning_rate": 2.8914887525387677e-05,
      "loss": 0.1127,
      "step": 6834
    },
    {
      "epoch": 1.6914130165800545,
      "grad_norm": 0.06530765444040298,
      "learning_rate": 2.8869480920584085e-05,
      "loss": 0.0686,
      "step": 6835
    },
    {
      "epoch": 1.6916604800791883,
      "grad_norm": 0.05411897227168083,
      "learning_rate": 2.88241078112538e-05,
      "loss": 0.0248,
      "step": 6836
    },
    {
      "epoch": 1.6919079435783222,
      "grad_norm": 0.04389730095863342,
      "learning_rate": 2.877876820426964e-05,
      "loss": 0.0525,
      "step": 6837
    },
    {
      "epoch": 1.692155407077456,
      "grad_norm": 0.04575246572494507,
      "learning_rate": 2.873346210649938e-05,
      "loss": 0.0452,
      "step": 6838
    },
    {
      "epoch": 1.69240287057659,
      "grad_norm": 0.03212546184659004,
      "learning_rate": 2.868818952480573e-05,
      "loss": 0.0305,
      "step": 6839
    },
    {
      "epoch": 1.692650334075724,
      "grad_norm": 0.04422697424888611,
      "learning_rate": 2.8642950466046174e-05,
      "loss": 0.0723,
      "step": 6840
    },
    {
      "epoch": 1.6928977975748576,
      "grad_norm": 0.14611315727233887,
      "learning_rate": 2.8597744937073457e-05,
      "loss": 0.0474,
      "step": 6841
    },
    {
      "epoch": 1.6931452610739917,
      "grad_norm": 0.054606564342975616,
      "learning_rate": 2.8552572944734845e-05,
      "loss": 0.0469,
      "step": 6842
    },
    {
      "epoch": 1.6933927245731255,
      "grad_norm": 0.05160980671644211,
      "learning_rate": 2.8507434495872786e-05,
      "loss": 0.049,
      "step": 6843
    },
    {
      "epoch": 1.6936401880722594,
      "grad_norm": 0.04874647781252861,
      "learning_rate": 2.8462329597324534e-05,
      "loss": 0.0766,
      "step": 6844
    },
    {
      "epoch": 1.6938876515713932,
      "grad_norm": 0.059101998805999756,
      "learning_rate": 2.841725825592234e-05,
      "loss": 0.0632,
      "step": 6845
    },
    {
      "epoch": 1.694135115070527,
      "grad_norm": 0.024903908371925354,
      "learning_rate": 2.8372220478493388e-05,
      "loss": 0.0394,
      "step": 6846
    },
    {
      "epoch": 1.6943825785696611,
      "grad_norm": 0.05120491608977318,
      "learning_rate": 2.832721627185958e-05,
      "loss": 0.0682,
      "step": 6847
    },
    {
      "epoch": 1.6946300420687948,
      "grad_norm": 0.09685853123664856,
      "learning_rate": 2.828224564283788e-05,
      "loss": 0.1144,
      "step": 6848
    },
    {
      "epoch": 1.6948775055679288,
      "grad_norm": 0.08655078709125519,
      "learning_rate": 2.823730859824031e-05,
      "loss": 0.0833,
      "step": 6849
    },
    {
      "epoch": 1.6951249690670624,
      "grad_norm": 0.04256059601902962,
      "learning_rate": 2.8192405144873535e-05,
      "loss": 0.0683,
      "step": 6850
    },
    {
      "epoch": 1.6953724325661965,
      "grad_norm": 0.0498448945581913,
      "learning_rate": 2.8147535289539257e-05,
      "loss": 0.0182,
      "step": 6851
    },
    {
      "epoch": 1.6956198960653304,
      "grad_norm": 0.12383133918046951,
      "learning_rate": 2.810269903903412e-05,
      "loss": 0.1261,
      "step": 6852
    },
    {
      "epoch": 1.6958673595644642,
      "grad_norm": 0.06527550518512726,
      "learning_rate": 2.8057896400149606e-05,
      "loss": 0.0549,
      "step": 6853
    },
    {
      "epoch": 1.696114823063598,
      "grad_norm": 0.061915379017591476,
      "learning_rate": 2.801312737967221e-05,
      "loss": 0.0411,
      "step": 6854
    },
    {
      "epoch": 1.696362286562732,
      "grad_norm": 0.045055609196424484,
      "learning_rate": 2.796839198438317e-05,
      "loss": 0.0507,
      "step": 6855
    },
    {
      "epoch": 1.696609750061866,
      "grad_norm": 0.07446576654911041,
      "learning_rate": 2.7923690221058758e-05,
      "loss": 0.0754,
      "step": 6856
    },
    {
      "epoch": 1.6968572135609996,
      "grad_norm": 0.0376875214278698,
      "learning_rate": 2.787902209647014e-05,
      "loss": 0.0347,
      "step": 6857
    },
    {
      "epoch": 1.6971046770601337,
      "grad_norm": 0.04093741998076439,
      "learning_rate": 2.7834387617383328e-05,
      "loss": 0.0388,
      "step": 6858
    },
    {
      "epoch": 1.6973521405592675,
      "grad_norm": 0.03870832920074463,
      "learning_rate": 2.7789786790559297e-05,
      "loss": 0.0339,
      "step": 6859
    },
    {
      "epoch": 1.6975996040584014,
      "grad_norm": 0.08123140782117844,
      "learning_rate": 2.774521962275392e-05,
      "loss": 0.0694,
      "step": 6860
    },
    {
      "epoch": 1.6978470675575352,
      "grad_norm": 0.038830071687698364,
      "learning_rate": 2.7700686120717966e-05,
      "loss": 0.0481,
      "step": 6861
    },
    {
      "epoch": 1.698094531056669,
      "grad_norm": 0.04185376688838005,
      "learning_rate": 2.765618629119704e-05,
      "loss": 0.0422,
      "step": 6862
    },
    {
      "epoch": 1.6983419945558031,
      "grad_norm": 0.0513441301882267,
      "learning_rate": 2.761172014093169e-05,
      "loss": 0.0753,
      "step": 6863
    },
    {
      "epoch": 1.6985894580549368,
      "grad_norm": 0.03904782980680466,
      "learning_rate": 2.756728767665742e-05,
      "loss": 0.0263,
      "step": 6864
    },
    {
      "epoch": 1.6988369215540708,
      "grad_norm": 0.05471360310912132,
      "learning_rate": 2.752288890510457e-05,
      "loss": 0.052,
      "step": 6865
    },
    {
      "epoch": 1.6990843850532047,
      "grad_norm": 0.039367470890283585,
      "learning_rate": 2.7478523832998452e-05,
      "loss": 0.0534,
      "step": 6866
    },
    {
      "epoch": 1.6993318485523385,
      "grad_norm": 0.039129678159952164,
      "learning_rate": 2.7434192467059026e-05,
      "loss": 0.0287,
      "step": 6867
    },
    {
      "epoch": 1.6995793120514724,
      "grad_norm": 0.04581184312701225,
      "learning_rate": 2.7389894814001475e-05,
      "loss": 0.0319,
      "step": 6868
    },
    {
      "epoch": 1.6998267755506062,
      "grad_norm": 0.04201279580593109,
      "learning_rate": 2.7345630880535794e-05,
      "loss": 0.0425,
      "step": 6869
    },
    {
      "epoch": 1.7000742390497403,
      "grad_norm": 0.0529639795422554,
      "learning_rate": 2.7301400673366645e-05,
      "loss": 0.0516,
      "step": 6870
    },
    {
      "epoch": 1.700321702548874,
      "grad_norm": 0.045994892716407776,
      "learning_rate": 2.7257204199193836e-05,
      "loss": 0.0597,
      "step": 6871
    },
    {
      "epoch": 1.700569166048008,
      "grad_norm": 0.07859547436237335,
      "learning_rate": 2.721304146471196e-05,
      "loss": 0.0346,
      "step": 6872
    },
    {
      "epoch": 1.7008166295471416,
      "grad_norm": 0.04230911657214165,
      "learning_rate": 2.716891247661049e-05,
      "loss": 0.0352,
      "step": 6873
    },
    {
      "epoch": 1.7010640930462757,
      "grad_norm": 0.041784778237342834,
      "learning_rate": 2.7124817241573896e-05,
      "loss": 0.0549,
      "step": 6874
    },
    {
      "epoch": 1.7013115565454096,
      "grad_norm": 0.04412485286593437,
      "learning_rate": 2.708075576628127e-05,
      "loss": 0.0627,
      "step": 6875
    },
    {
      "epoch": 1.7015590200445434,
      "grad_norm": 0.047115523368120193,
      "learning_rate": 2.703672805740698e-05,
      "loss": 0.0436,
      "step": 6876
    },
    {
      "epoch": 1.7018064835436775,
      "grad_norm": 0.03755748271942139,
      "learning_rate": 2.6992734121619928e-05,
      "loss": 0.0314,
      "step": 6877
    },
    {
      "epoch": 1.702053947042811,
      "grad_norm": 0.03499159961938858,
      "learning_rate": 2.6948773965584093e-05,
      "loss": 0.0253,
      "step": 6878
    },
    {
      "epoch": 1.7023014105419452,
      "grad_norm": 0.06113222613930702,
      "learning_rate": 2.6904847595958266e-05,
      "loss": 0.0364,
      "step": 6879
    },
    {
      "epoch": 1.7025488740410788,
      "grad_norm": 0.09575212746858597,
      "learning_rate": 2.6860955019396173e-05,
      "loss": 0.018,
      "step": 6880
    },
    {
      "epoch": 1.7027963375402129,
      "grad_norm": 0.0617017038166523,
      "learning_rate": 2.6817096242546384e-05,
      "loss": 0.052,
      "step": 6881
    },
    {
      "epoch": 1.7030438010393467,
      "grad_norm": 0.04405336081981659,
      "learning_rate": 2.6773271272052313e-05,
      "loss": 0.054,
      "step": 6882
    },
    {
      "epoch": 1.7032912645384806,
      "grad_norm": 0.036818504333496094,
      "learning_rate": 2.6729480114552255e-05,
      "loss": 0.0311,
      "step": 6883
    },
    {
      "epoch": 1.7035387280376144,
      "grad_norm": 0.06413637101650238,
      "learning_rate": 2.6685722776679578e-05,
      "loss": 0.0594,
      "step": 6884
    },
    {
      "epoch": 1.7037861915367483,
      "grad_norm": 0.06285590678453445,
      "learning_rate": 2.664199926506222e-05,
      "loss": 0.0532,
      "step": 6885
    },
    {
      "epoch": 1.7040336550358823,
      "grad_norm": 0.23611804842948914,
      "learning_rate": 2.6598309586323227e-05,
      "loss": 0.0237,
      "step": 6886
    },
    {
      "epoch": 1.704281118535016,
      "grad_norm": 0.04959697648882866,
      "learning_rate": 2.6554653747080383e-05,
      "loss": 0.0595,
      "step": 6887
    },
    {
      "epoch": 1.70452858203415,
      "grad_norm": 0.0456111803650856,
      "learning_rate": 2.651103175394645e-05,
      "loss": 0.0426,
      "step": 6888
    },
    {
      "epoch": 1.7047760455332839,
      "grad_norm": 0.03831504285335541,
      "learning_rate": 2.646744361352907e-05,
      "loss": 0.0388,
      "step": 6889
    },
    {
      "epoch": 1.7050235090324177,
      "grad_norm": 0.0812234953045845,
      "learning_rate": 2.642388933243056e-05,
      "loss": 0.0518,
      "step": 6890
    },
    {
      "epoch": 1.7052709725315516,
      "grad_norm": 0.04491228610277176,
      "learning_rate": 2.6380368917248267e-05,
      "loss": 0.0463,
      "step": 6891
    },
    {
      "epoch": 1.7055184360306854,
      "grad_norm": 0.044234756380319595,
      "learning_rate": 2.6336882374574574e-05,
      "loss": 0.0487,
      "step": 6892
    },
    {
      "epoch": 1.7057658995298195,
      "grad_norm": 0.07323713600635529,
      "learning_rate": 2.6293429710996353e-05,
      "loss": 0.0538,
      "step": 6893
    },
    {
      "epoch": 1.7060133630289531,
      "grad_norm": 0.04957379773259163,
      "learning_rate": 2.6250010933095674e-05,
      "loss": 0.0579,
      "step": 6894
    },
    {
      "epoch": 1.7062608265280872,
      "grad_norm": 0.06698846817016602,
      "learning_rate": 2.6206626047449195e-05,
      "loss": 0.073,
      "step": 6895
    },
    {
      "epoch": 1.7065082900272208,
      "grad_norm": 0.05870078504085541,
      "learning_rate": 2.616327506062871e-05,
      "loss": 0.0556,
      "step": 6896
    },
    {
      "epoch": 1.706755753526355,
      "grad_norm": 0.031445231288671494,
      "learning_rate": 2.6119957979200776e-05,
      "loss": 0.0365,
      "step": 6897
    },
    {
      "epoch": 1.7070032170254887,
      "grad_norm": 0.049151208251714706,
      "learning_rate": 2.60766748097267e-05,
      "loss": 0.0404,
      "step": 6898
    },
    {
      "epoch": 1.7072506805246226,
      "grad_norm": 0.0634777620434761,
      "learning_rate": 2.6033425558762813e-05,
      "loss": 0.0501,
      "step": 6899
    },
    {
      "epoch": 1.7074981440237567,
      "grad_norm": 0.03016670234501362,
      "learning_rate": 2.599021023286019e-05,
      "loss": 0.0395,
      "step": 6900
    },
    {
      "epoch": 1.7077456075228903,
      "grad_norm": 0.07070278376340866,
      "learning_rate": 2.594702883856484e-05,
      "loss": 0.0747,
      "step": 6901
    },
    {
      "epoch": 1.7079930710220244,
      "grad_norm": 0.06985493749380112,
      "learning_rate": 2.59038813824177e-05,
      "loss": 0.0595,
      "step": 6902
    },
    {
      "epoch": 1.708240534521158,
      "grad_norm": 0.05518414452672005,
      "learning_rate": 2.5860767870954287e-05,
      "loss": 0.0471,
      "step": 6903
    },
    {
      "epoch": 1.708487998020292,
      "grad_norm": 0.051918044686317444,
      "learning_rate": 2.5817688310705382e-05,
      "loss": 0.0391,
      "step": 6904
    },
    {
      "epoch": 1.708735461519426,
      "grad_norm": 0.05479048937559128,
      "learning_rate": 2.5774642708196238e-05,
      "loss": 0.0535,
      "step": 6905
    },
    {
      "epoch": 1.7089829250185598,
      "grad_norm": 0.054915934801101685,
      "learning_rate": 2.573163106994722e-05,
      "loss": 0.0322,
      "step": 6906
    },
    {
      "epoch": 1.7092303885176936,
      "grad_norm": 0.06855322420597076,
      "learning_rate": 2.5688653402473422e-05,
      "loss": 0.0886,
      "step": 6907
    },
    {
      "epoch": 1.7094778520168274,
      "grad_norm": 0.06043997406959534,
      "learning_rate": 2.5645709712284888e-05,
      "loss": 0.049,
      "step": 6908
    },
    {
      "epoch": 1.7097253155159615,
      "grad_norm": 0.05642213299870491,
      "learning_rate": 2.560280000588644e-05,
      "loss": 0.0682,
      "step": 6909
    },
    {
      "epoch": 1.7099727790150951,
      "grad_norm": 0.04128569737076759,
      "learning_rate": 2.555992428977774e-05,
      "loss": 0.0417,
      "step": 6910
    },
    {
      "epoch": 1.7102202425142292,
      "grad_norm": 0.07743336260318756,
      "learning_rate": 2.5517082570453292e-05,
      "loss": 0.0619,
      "step": 6911
    },
    {
      "epoch": 1.710467706013363,
      "grad_norm": 0.05258965864777565,
      "learning_rate": 2.5474274854402652e-05,
      "loss": 0.0463,
      "step": 6912
    },
    {
      "epoch": 1.710715169512497,
      "grad_norm": 0.04139920696616173,
      "learning_rate": 2.5431501148109936e-05,
      "loss": 0.0299,
      "step": 6913
    },
    {
      "epoch": 1.7109626330116308,
      "grad_norm": 0.11808503419160843,
      "learning_rate": 2.5388761458054272e-05,
      "loss": 0.0823,
      "step": 6914
    },
    {
      "epoch": 1.7112100965107646,
      "grad_norm": 0.032855499535799026,
      "learning_rate": 2.5346055790709588e-05,
      "loss": 0.0277,
      "step": 6915
    },
    {
      "epoch": 1.7114575600098987,
      "grad_norm": 0.05011971667408943,
      "learning_rate": 2.5303384152544713e-05,
      "loss": 0.0378,
      "step": 6916
    },
    {
      "epoch": 1.7117050235090323,
      "grad_norm": 0.049358826130628586,
      "learning_rate": 2.5260746550023276e-05,
      "loss": 0.0749,
      "step": 6917
    },
    {
      "epoch": 1.7119524870081664,
      "grad_norm": 0.04462298005819321,
      "learning_rate": 2.5218142989603703e-05,
      "loss": 0.0451,
      "step": 6918
    },
    {
      "epoch": 1.7121999505073002,
      "grad_norm": 0.04628146439790726,
      "learning_rate": 2.5175573477739328e-05,
      "loss": 0.0517,
      "step": 6919
    },
    {
      "epoch": 1.712447414006434,
      "grad_norm": 0.047597795724868774,
      "learning_rate": 2.513303802087835e-05,
      "loss": 0.069,
      "step": 6920
    },
    {
      "epoch": 1.712694877505568,
      "grad_norm": 0.06982553005218506,
      "learning_rate": 2.509053662546376e-05,
      "loss": 0.0869,
      "step": 6921
    },
    {
      "epoch": 1.7129423410047018,
      "grad_norm": 0.07545945048332214,
      "learning_rate": 2.5048069297933407e-05,
      "loss": 0.0546,
      "step": 6922
    },
    {
      "epoch": 1.7131898045038358,
      "grad_norm": 0.04499422758817673,
      "learning_rate": 2.500563604471995e-05,
      "loss": 0.0329,
      "step": 6923
    },
    {
      "epoch": 1.7134372680029695,
      "grad_norm": 0.1023983284831047,
      "learning_rate": 2.4963236872251022e-05,
      "loss": 0.0765,
      "step": 6924
    },
    {
      "epoch": 1.7136847315021035,
      "grad_norm": 0.04733388125896454,
      "learning_rate": 2.4920871786948822e-05,
      "loss": 0.0309,
      "step": 6925
    },
    {
      "epoch": 1.7139321950012372,
      "grad_norm": 0.052998825907707214,
      "learning_rate": 2.4878540795230637e-05,
      "loss": 0.0437,
      "step": 6926
    },
    {
      "epoch": 1.7141796585003712,
      "grad_norm": 0.04409248009324074,
      "learning_rate": 2.48362439035085e-05,
      "loss": 0.0456,
      "step": 6927
    },
    {
      "epoch": 1.714427121999505,
      "grad_norm": 0.06270191073417664,
      "learning_rate": 2.4793981118189284e-05,
      "loss": 0.0804,
      "step": 6928
    },
    {
      "epoch": 1.714674585498639,
      "grad_norm": 0.05470460653305054,
      "learning_rate": 2.4751752445674735e-05,
      "loss": 0.0489,
      "step": 6929
    },
    {
      "epoch": 1.7149220489977728,
      "grad_norm": 0.10835923999547958,
      "learning_rate": 2.4709557892361208e-05,
      "loss": 0.097,
      "step": 6930
    },
    {
      "epoch": 1.7151695124969066,
      "grad_norm": 0.04912705346941948,
      "learning_rate": 2.4667397464640278e-05,
      "loss": 0.0462,
      "step": 6931
    },
    {
      "epoch": 1.7154169759960407,
      "grad_norm": 0.053598981350660324,
      "learning_rate": 2.4625271168898095e-05,
      "loss": 0.0605,
      "step": 6932
    },
    {
      "epoch": 1.7156644394951743,
      "grad_norm": 0.11371085792779922,
      "learning_rate": 2.4583179011515634e-05,
      "loss": 0.1302,
      "step": 6933
    },
    {
      "epoch": 1.7159119029943084,
      "grad_norm": 0.05924835056066513,
      "learning_rate": 2.4541120998868766e-05,
      "loss": 0.0478,
      "step": 6934
    },
    {
      "epoch": 1.7161593664934423,
      "grad_norm": 0.06522974371910095,
      "learning_rate": 2.4499097137328173e-05,
      "loss": 0.0348,
      "step": 6935
    },
    {
      "epoch": 1.716406829992576,
      "grad_norm": 0.06772296130657196,
      "learning_rate": 2.4457107433259408e-05,
      "loss": 0.0331,
      "step": 6936
    },
    {
      "epoch": 1.71665429349171,
      "grad_norm": 0.06293527781963348,
      "learning_rate": 2.441515189302282e-05,
      "loss": 0.1077,
      "step": 6937
    },
    {
      "epoch": 1.7169017569908438,
      "grad_norm": 0.03617359325289726,
      "learning_rate": 2.437323052297344e-05,
      "loss": 0.0251,
      "step": 6938
    },
    {
      "epoch": 1.7171492204899779,
      "grad_norm": 0.0613732673227787,
      "learning_rate": 2.4331343329461464e-05,
      "loss": 0.0923,
      "step": 6939
    },
    {
      "epoch": 1.7173966839891115,
      "grad_norm": 0.08114206790924072,
      "learning_rate": 2.4289490318831514e-05,
      "loss": 0.056,
      "step": 6940
    },
    {
      "epoch": 1.7176441474882456,
      "grad_norm": 0.058966923505067825,
      "learning_rate": 2.4247671497423323e-05,
      "loss": 0.0954,
      "step": 6941
    },
    {
      "epoch": 1.7178916109873794,
      "grad_norm": 0.03518635034561157,
      "learning_rate": 2.4205886871571326e-05,
      "loss": 0.0363,
      "step": 6942
    },
    {
      "epoch": 1.7181390744865133,
      "grad_norm": 0.03909081593155861,
      "learning_rate": 2.416413644760479e-05,
      "loss": 0.0355,
      "step": 6943
    },
    {
      "epoch": 1.7183865379856471,
      "grad_norm": 0.06699589639902115,
      "learning_rate": 2.412242023184788e-05,
      "loss": 0.0435,
      "step": 6944
    },
    {
      "epoch": 1.718634001484781,
      "grad_norm": 0.048963043838739395,
      "learning_rate": 2.4080738230619376e-05,
      "loss": 0.0334,
      "step": 6945
    },
    {
      "epoch": 1.718881464983915,
      "grad_norm": 0.05044930428266525,
      "learning_rate": 2.4039090450233032e-05,
      "loss": 0.0562,
      "step": 6946
    },
    {
      "epoch": 1.7191289284830487,
      "grad_norm": 0.052448295056819916,
      "learning_rate": 2.399747689699755e-05,
      "loss": 0.0587,
      "step": 6947
    },
    {
      "epoch": 1.7193763919821827,
      "grad_norm": 0.03390057757496834,
      "learning_rate": 2.395589757721611e-05,
      "loss": 0.028,
      "step": 6948
    },
    {
      "epoch": 1.7196238554813164,
      "grad_norm": 0.07626338303089142,
      "learning_rate": 2.391435249718696e-05,
      "loss": 0.0801,
      "step": 6949
    },
    {
      "epoch": 1.7198713189804504,
      "grad_norm": 0.06469892710447311,
      "learning_rate": 2.387284166320311e-05,
      "loss": 0.0596,
      "step": 6950
    },
    {
      "epoch": 1.7201187824795843,
      "grad_norm": 0.04438336566090584,
      "learning_rate": 2.3831365081552313e-05,
      "loss": 0.062,
      "step": 6951
    },
    {
      "epoch": 1.7203662459787181,
      "grad_norm": 0.035910312086343765,
      "learning_rate": 2.378992275851727e-05,
      "loss": 0.0182,
      "step": 6952
    },
    {
      "epoch": 1.720613709477852,
      "grad_norm": 0.04973408579826355,
      "learning_rate": 2.374851470037531e-05,
      "loss": 0.0366,
      "step": 6953
    },
    {
      "epoch": 1.7208611729769858,
      "grad_norm": 0.0375528484582901,
      "learning_rate": 2.3707140913398678e-05,
      "loss": 0.0402,
      "step": 6954
    },
    {
      "epoch": 1.72110863647612,
      "grad_norm": 0.04980000481009483,
      "learning_rate": 2.3665801403854463e-05,
      "loss": 0.0544,
      "step": 6955
    },
    {
      "epoch": 1.7213560999752535,
      "grad_norm": 0.09259600937366486,
      "learning_rate": 2.3624496178004495e-05,
      "loss": 0.1179,
      "step": 6956
    },
    {
      "epoch": 1.7216035634743876,
      "grad_norm": 0.04308824613690376,
      "learning_rate": 2.358322524210543e-05,
      "loss": 0.0449,
      "step": 6957
    },
    {
      "epoch": 1.7218510269735214,
      "grad_norm": 0.14980365335941315,
      "learning_rate": 2.3541988602408748e-05,
      "loss": 0.0751,
      "step": 6958
    },
    {
      "epoch": 1.7220984904726553,
      "grad_norm": 0.03850600868463516,
      "learning_rate": 2.350078626516075e-05,
      "loss": 0.0539,
      "step": 6959
    },
    {
      "epoch": 1.7223459539717891,
      "grad_norm": 0.03953014686703682,
      "learning_rate": 2.345961823660242e-05,
      "loss": 0.0431,
      "step": 6960
    },
    {
      "epoch": 1.722593417470923,
      "grad_norm": 0.04766635224223137,
      "learning_rate": 2.3418484522969657e-05,
      "loss": 0.0541,
      "step": 6961
    },
    {
      "epoch": 1.722840880970057,
      "grad_norm": 0.05901039391756058,
      "learning_rate": 2.33773851304932e-05,
      "loss": 0.0511,
      "step": 6962
    },
    {
      "epoch": 1.7230883444691907,
      "grad_norm": 0.057979993522167206,
      "learning_rate": 2.333632006539846e-05,
      "loss": 0.0406,
      "step": 6963
    },
    {
      "epoch": 1.7233358079683248,
      "grad_norm": 0.045207638293504715,
      "learning_rate": 2.3295289333905816e-05,
      "loss": 0.0354,
      "step": 6964
    },
    {
      "epoch": 1.7235832714674586,
      "grad_norm": 0.05968662351369858,
      "learning_rate": 2.325429294223022e-05,
      "loss": 0.0467,
      "step": 6965
    },
    {
      "epoch": 1.7238307349665924,
      "grad_norm": 0.06054200232028961,
      "learning_rate": 2.3213330896581565e-05,
      "loss": 0.0458,
      "step": 6966
    },
    {
      "epoch": 1.7240781984657263,
      "grad_norm": 0.05577465891838074,
      "learning_rate": 2.3172403203164688e-05,
      "loss": 0.0376,
      "step": 6967
    },
    {
      "epoch": 1.7243256619648601,
      "grad_norm": 0.036242417991161346,
      "learning_rate": 2.3131509868178886e-05,
      "loss": 0.0217,
      "step": 6968
    },
    {
      "epoch": 1.7245731254639942,
      "grad_norm": 0.05638362467288971,
      "learning_rate": 2.3090650897818477e-05,
      "loss": 0.0615,
      "step": 6969
    },
    {
      "epoch": 1.7248205889631278,
      "grad_norm": 0.03280603513121605,
      "learning_rate": 2.304982629827254e-05,
      "loss": 0.0345,
      "step": 6970
    },
    {
      "epoch": 1.725068052462262,
      "grad_norm": 0.09863848984241486,
      "learning_rate": 2.300903607572491e-05,
      "loss": 0.0832,
      "step": 6971
    },
    {
      "epoch": 1.7253155159613955,
      "grad_norm": 0.057484857738018036,
      "learning_rate": 2.2968280236354305e-05,
      "loss": 0.0722,
      "step": 6972
    },
    {
      "epoch": 1.7255629794605296,
      "grad_norm": 0.03806077316403389,
      "learning_rate": 2.2927558786334067e-05,
      "loss": 0.0441,
      "step": 6973
    },
    {
      "epoch": 1.7258104429596635,
      "grad_norm": 0.038021136075258255,
      "learning_rate": 2.2886871731832485e-05,
      "loss": 0.0349,
      "step": 6974
    },
    {
      "epoch": 1.7260579064587973,
      "grad_norm": 0.03856867551803589,
      "learning_rate": 2.2846219079012548e-05,
      "loss": 0.0538,
      "step": 6975
    },
    {
      "epoch": 1.7263053699579312,
      "grad_norm": 0.05749223381280899,
      "learning_rate": 2.2805600834032076e-05,
      "loss": 0.046,
      "step": 6976
    },
    {
      "epoch": 1.726552833457065,
      "grad_norm": 0.05238988250494003,
      "learning_rate": 2.2765017003043704e-05,
      "loss": 0.0491,
      "step": 6977
    },
    {
      "epoch": 1.726800296956199,
      "grad_norm": 0.033105626702308655,
      "learning_rate": 2.2724467592194763e-05,
      "loss": 0.0314,
      "step": 6978
    },
    {
      "epoch": 1.7270477604553327,
      "grad_norm": 0.046305615454912186,
      "learning_rate": 2.268395260762751e-05,
      "loss": 0.0679,
      "step": 6979
    },
    {
      "epoch": 1.7272952239544668,
      "grad_norm": 0.048698801547288895,
      "learning_rate": 2.2643472055478785e-05,
      "loss": 0.0535,
      "step": 6980
    },
    {
      "epoch": 1.7275426874536006,
      "grad_norm": 0.0569068118929863,
      "learning_rate": 2.2603025941880405e-05,
      "loss": 0.0424,
      "step": 6981
    },
    {
      "epoch": 1.7277901509527345,
      "grad_norm": 0.09093055129051208,
      "learning_rate": 2.2562614272958886e-05,
      "loss": 0.0702,
      "step": 6982
    },
    {
      "epoch": 1.7280376144518683,
      "grad_norm": 0.044152021408081055,
      "learning_rate": 2.2522237054835497e-05,
      "loss": 0.0386,
      "step": 6983
    },
    {
      "epoch": 1.7282850779510022,
      "grad_norm": 0.04747612774372101,
      "learning_rate": 2.248189429362643e-05,
      "loss": 0.0615,
      "step": 6984
    },
    {
      "epoch": 1.7285325414501362,
      "grad_norm": 0.05422661453485489,
      "learning_rate": 2.244158599544238e-05,
      "loss": 0.0387,
      "step": 6985
    },
    {
      "epoch": 1.7287800049492699,
      "grad_norm": 0.038882989436388016,
      "learning_rate": 2.2401312166389158e-05,
      "loss": 0.036,
      "step": 6986
    },
    {
      "epoch": 1.729027468448404,
      "grad_norm": 0.05089888721704483,
      "learning_rate": 2.236107281256719e-05,
      "loss": 0.0491,
      "step": 6987
    },
    {
      "epoch": 1.7292749319475378,
      "grad_norm": 0.03729793429374695,
      "learning_rate": 2.232086794007157e-05,
      "loss": 0.047,
      "step": 6988
    },
    {
      "epoch": 1.7295223954466716,
      "grad_norm": 0.08885291963815689,
      "learning_rate": 2.2280697554992345e-05,
      "loss": 0.0584,
      "step": 6989
    },
    {
      "epoch": 1.7297698589458055,
      "grad_norm": 0.05059400200843811,
      "learning_rate": 2.224056166341426e-05,
      "loss": 0.0533,
      "step": 6990
    },
    {
      "epoch": 1.7300173224449393,
      "grad_norm": 0.041028715670108795,
      "learning_rate": 2.2200460271416894e-05,
      "loss": 0.0291,
      "step": 6991
    },
    {
      "epoch": 1.7302647859440734,
      "grad_norm": 0.03171713650226593,
      "learning_rate": 2.216039338507453e-05,
      "loss": 0.0729,
      "step": 6992
    },
    {
      "epoch": 1.730512249443207,
      "grad_norm": 0.05586684122681618,
      "learning_rate": 2.212036101045617e-05,
      "loss": 0.0357,
      "step": 6993
    },
    {
      "epoch": 1.730759712942341,
      "grad_norm": 0.055722206830978394,
      "learning_rate": 2.2080363153625825e-05,
      "loss": 0.0739,
      "step": 6994
    },
    {
      "epoch": 1.7310071764414747,
      "grad_norm": 0.0635005533695221,
      "learning_rate": 2.2040399820641986e-05,
      "loss": 0.0498,
      "step": 6995
    },
    {
      "epoch": 1.7312546399406088,
      "grad_norm": 0.0414104238152504,
      "learning_rate": 2.2000471017558117e-05,
      "loss": 0.0488,
      "step": 6996
    },
    {
      "epoch": 1.7315021034397426,
      "grad_norm": 0.10098259896039963,
      "learning_rate": 2.1960576750422374e-05,
      "loss": 0.0775,
      "step": 6997
    },
    {
      "epoch": 1.7317495669388765,
      "grad_norm": 0.03257397562265396,
      "learning_rate": 2.1920717025277647e-05,
      "loss": 0.0439,
      "step": 6998
    },
    {
      "epoch": 1.7319970304380103,
      "grad_norm": 0.031108101829886436,
      "learning_rate": 2.188089184816175e-05,
      "loss": 0.0296,
      "step": 6999
    },
    {
      "epoch": 1.7322444939371442,
      "grad_norm": 0.06353121250867844,
      "learning_rate": 2.1841101225107045e-05,
      "loss": 0.0632,
      "step": 7000
    },
    {
      "epoch": 1.7322444939371442,
      "eval_loss": 0.2891307473182678,
      "eval_runtime": 202.5759,
      "eval_samples_per_second": 4.936,
      "eval_steps_per_second": 0.311,
      "step": 7000
    },
    {
      "epoch": 1.7324919574362783,
      "grad_norm": 0.049758587032556534,
      "learning_rate": 2.1801345162140745e-05,
      "loss": 0.038,
      "step": 7001
    },
    {
      "epoch": 1.732739420935412,
      "grad_norm": 0.05056402087211609,
      "learning_rate": 2.1761623665285e-05,
      "loss": 0.0603,
      "step": 7002
    },
    {
      "epoch": 1.732986884434546,
      "grad_norm": 0.038608796894550323,
      "learning_rate": 2.172193674055645e-05,
      "loss": 0.029,
      "step": 7003
    },
    {
      "epoch": 1.7332343479336798,
      "grad_norm": 0.0836944729089737,
      "learning_rate": 2.1682284393966635e-05,
      "loss": 0.087,
      "step": 7004
    },
    {
      "epoch": 1.7334818114328137,
      "grad_norm": 0.0855444148182869,
      "learning_rate": 2.1642666631521896e-05,
      "loss": 0.0483,
      "step": 7005
    },
    {
      "epoch": 1.7337292749319475,
      "grad_norm": 0.04306390881538391,
      "learning_rate": 2.160308345922324e-05,
      "loss": 0.0645,
      "step": 7006
    },
    {
      "epoch": 1.7339767384310814,
      "grad_norm": 0.04719384387135506,
      "learning_rate": 2.1563534883066533e-05,
      "loss": 0.0327,
      "step": 7007
    },
    {
      "epoch": 1.7342242019302154,
      "grad_norm": 0.06593813747167587,
      "learning_rate": 2.1524020909042292e-05,
      "loss": 0.0421,
      "step": 7008
    },
    {
      "epoch": 1.734471665429349,
      "grad_norm": 0.0555490106344223,
      "learning_rate": 2.1484541543135838e-05,
      "loss": 0.0434,
      "step": 7009
    },
    {
      "epoch": 1.7347191289284831,
      "grad_norm": 0.053533729165792465,
      "learning_rate": 2.1445096791327278e-05,
      "loss": 0.0395,
      "step": 7010
    },
    {
      "epoch": 1.734966592427617,
      "grad_norm": 0.05945294722914696,
      "learning_rate": 2.14056866595915e-05,
      "loss": 0.0577,
      "step": 7011
    },
    {
      "epoch": 1.7352140559267508,
      "grad_norm": 0.04864351823925972,
      "learning_rate": 2.136631115389806e-05,
      "loss": 0.0369,
      "step": 7012
    },
    {
      "epoch": 1.7354615194258847,
      "grad_norm": 0.09926070272922516,
      "learning_rate": 2.1326970280211298e-05,
      "loss": 0.1056,
      "step": 7013
    },
    {
      "epoch": 1.7357089829250185,
      "grad_norm": 0.038891736418008804,
      "learning_rate": 2.1287664044490396e-05,
      "loss": 0.0449,
      "step": 7014
    },
    {
      "epoch": 1.7359564464241526,
      "grad_norm": 0.08967790752649307,
      "learning_rate": 2.1248392452689196e-05,
      "loss": 0.1002,
      "step": 7015
    },
    {
      "epoch": 1.7362039099232862,
      "grad_norm": 0.03789130225777626,
      "learning_rate": 2.120915551075625e-05,
      "loss": 0.0406,
      "step": 7016
    },
    {
      "epoch": 1.7364513734224203,
      "grad_norm": 0.03946719318628311,
      "learning_rate": 2.116995322463494e-05,
      "loss": 0.0223,
      "step": 7017
    },
    {
      "epoch": 1.736698836921554,
      "grad_norm": 0.0657922774553299,
      "learning_rate": 2.1130785600263437e-05,
      "loss": 0.0552,
      "step": 7018
    },
    {
      "epoch": 1.736946300420688,
      "grad_norm": 0.10447104275226593,
      "learning_rate": 2.10916526435746e-05,
      "loss": 0.0304,
      "step": 7019
    },
    {
      "epoch": 1.7371937639198218,
      "grad_norm": 0.04042678326368332,
      "learning_rate": 2.1052554360495996e-05,
      "loss": 0.0417,
      "step": 7020
    },
    {
      "epoch": 1.7374412274189557,
      "grad_norm": 0.09755165874958038,
      "learning_rate": 2.101349075695003e-05,
      "loss": 0.102,
      "step": 7021
    },
    {
      "epoch": 1.7376886909180895,
      "grad_norm": 0.051939088851213455,
      "learning_rate": 2.097446183885385e-05,
      "loss": 0.083,
      "step": 7022
    },
    {
      "epoch": 1.7379361544172234,
      "grad_norm": 0.03787689656019211,
      "learning_rate": 2.093546761211923e-05,
      "loss": 0.0372,
      "step": 7023
    },
    {
      "epoch": 1.7381836179163574,
      "grad_norm": 0.03016466088593006,
      "learning_rate": 2.089650808265281e-05,
      "loss": 0.0233,
      "step": 7024
    },
    {
      "epoch": 1.738431081415491,
      "grad_norm": 0.04827504605054855,
      "learning_rate": 2.085758325635595e-05,
      "loss": 0.0443,
      "step": 7025
    },
    {
      "epoch": 1.7386785449146251,
      "grad_norm": 0.05847052484750748,
      "learning_rate": 2.0818693139124713e-05,
      "loss": 0.0544,
      "step": 7026
    },
    {
      "epoch": 1.738926008413759,
      "grad_norm": 0.0464165098965168,
      "learning_rate": 2.0779837736849995e-05,
      "loss": 0.0699,
      "step": 7027
    },
    {
      "epoch": 1.7391734719128928,
      "grad_norm": 0.044748786836862564,
      "learning_rate": 2.074101705541723e-05,
      "loss": 0.0309,
      "step": 7028
    },
    {
      "epoch": 1.7394209354120267,
      "grad_norm": 0.046582385897636414,
      "learning_rate": 2.070223110070685e-05,
      "loss": 0.0725,
      "step": 7029
    },
    {
      "epoch": 1.7396683989111605,
      "grad_norm": 0.03401094302535057,
      "learning_rate": 2.066347987859396e-05,
      "loss": 0.0191,
      "step": 7030
    },
    {
      "epoch": 1.7399158624102946,
      "grad_norm": 0.0561426617205143,
      "learning_rate": 2.0624763394948205e-05,
      "loss": 0.0854,
      "step": 7031
    },
    {
      "epoch": 1.7401633259094282,
      "grad_norm": 0.0491337776184082,
      "learning_rate": 2.058608165563422e-05,
      "loss": 0.0511,
      "step": 7032
    },
    {
      "epoch": 1.7404107894085623,
      "grad_norm": 0.07658986002206802,
      "learning_rate": 2.054743466651121e-05,
      "loss": 0.0598,
      "step": 7033
    },
    {
      "epoch": 1.7406582529076962,
      "grad_norm": 0.05642992630600929,
      "learning_rate": 2.0508822433433243e-05,
      "loss": 0.0593,
      "step": 7034
    },
    {
      "epoch": 1.74090571640683,
      "grad_norm": 0.034231334924697876,
      "learning_rate": 2.047024496224906e-05,
      "loss": 0.0198,
      "step": 7035
    },
    {
      "epoch": 1.7411531799059639,
      "grad_norm": 0.08471473306417465,
      "learning_rate": 2.043170225880203e-05,
      "loss": 0.0774,
      "step": 7036
    },
    {
      "epoch": 1.7414006434050977,
      "grad_norm": 0.08727198094129562,
      "learning_rate": 2.0393194328930497e-05,
      "loss": 0.0943,
      "step": 7037
    },
    {
      "epoch": 1.7416481069042318,
      "grad_norm": 0.041728563606739044,
      "learning_rate": 2.0354721178467346e-05,
      "loss": 0.0319,
      "step": 7038
    },
    {
      "epoch": 1.7418955704033654,
      "grad_norm": 0.0508025586605072,
      "learning_rate": 2.0316282813240217e-05,
      "loss": 0.0451,
      "step": 7039
    },
    {
      "epoch": 1.7421430339024995,
      "grad_norm": 0.02430731989443302,
      "learning_rate": 2.027787923907154e-05,
      "loss": 0.0182,
      "step": 7040
    },
    {
      "epoch": 1.742390497401633,
      "grad_norm": 0.049200303852558136,
      "learning_rate": 2.0239510461778465e-05,
      "loss": 0.0376,
      "step": 7041
    },
    {
      "epoch": 1.7426379609007672,
      "grad_norm": 0.051596906036138535,
      "learning_rate": 2.0201176487172873e-05,
      "loss": 0.0329,
      "step": 7042
    },
    {
      "epoch": 1.742885424399901,
      "grad_norm": 0.05884908512234688,
      "learning_rate": 2.0162877321061313e-05,
      "loss": 0.0664,
      "step": 7043
    },
    {
      "epoch": 1.7431328878990349,
      "grad_norm": 0.04241713508963585,
      "learning_rate": 2.0124612969245033e-05,
      "loss": 0.0441,
      "step": 7044
    },
    {
      "epoch": 1.7433803513981687,
      "grad_norm": 0.1195991262793541,
      "learning_rate": 2.0086383437520285e-05,
      "loss": 0.1087,
      "step": 7045
    },
    {
      "epoch": 1.7436278148973026,
      "grad_norm": 0.028623882681131363,
      "learning_rate": 2.0048188731677657e-05,
      "loss": 0.0323,
      "step": 7046
    },
    {
      "epoch": 1.7438752783964366,
      "grad_norm": 0.05997487157583237,
      "learning_rate": 2.001002885750275e-05,
      "loss": 0.0567,
      "step": 7047
    },
    {
      "epoch": 1.7441227418955703,
      "grad_norm": 0.03841334208846092,
      "learning_rate": 1.9971903820775657e-05,
      "loss": 0.0164,
      "step": 7048
    },
    {
      "epoch": 1.7443702053947043,
      "grad_norm": 0.028307532891631126,
      "learning_rate": 1.9933813627271453e-05,
      "loss": 0.0299,
      "step": 7049
    },
    {
      "epoch": 1.7446176688938382,
      "grad_norm": 0.037906989455223083,
      "learning_rate": 1.9895758282759803e-05,
      "loss": 0.0276,
      "step": 7050
    },
    {
      "epoch": 1.744865132392972,
      "grad_norm": 0.04913416504859924,
      "learning_rate": 1.985773779300498e-05,
      "loss": 0.0523,
      "step": 7051
    },
    {
      "epoch": 1.7451125958921059,
      "grad_norm": 0.03458227962255478,
      "learning_rate": 1.9819752163766165e-05,
      "loss": 0.0278,
      "step": 7052
    },
    {
      "epoch": 1.7453600593912397,
      "grad_norm": 0.04105844348669052,
      "learning_rate": 1.978180140079719e-05,
      "loss": 0.0342,
      "step": 7053
    },
    {
      "epoch": 1.7456075228903738,
      "grad_norm": 0.037604935467243195,
      "learning_rate": 1.974388550984657e-05,
      "loss": 0.0244,
      "step": 7054
    },
    {
      "epoch": 1.7458549863895074,
      "grad_norm": 0.035588908940553665,
      "learning_rate": 1.970600449665763e-05,
      "loss": 0.0294,
      "step": 7055
    },
    {
      "epoch": 1.7461024498886415,
      "grad_norm": 0.04268411174416542,
      "learning_rate": 1.9668158366968226e-05,
      "loss": 0.0394,
      "step": 7056
    },
    {
      "epoch": 1.7463499133877753,
      "grad_norm": 0.05498768016695976,
      "learning_rate": 1.963034712651121e-05,
      "loss": 0.0922,
      "step": 7057
    },
    {
      "epoch": 1.7465973768869092,
      "grad_norm": 0.0662940964102745,
      "learning_rate": 1.959257078101387e-05,
      "loss": 0.0811,
      "step": 7058
    },
    {
      "epoch": 1.746844840386043,
      "grad_norm": 0.10254789888858795,
      "learning_rate": 1.9554829336198372e-05,
      "loss": 0.0999,
      "step": 7059
    },
    {
      "epoch": 1.747092303885177,
      "grad_norm": 0.03799891844391823,
      "learning_rate": 1.9517122797781563e-05,
      "loss": 0.022,
      "step": 7060
    },
    {
      "epoch": 1.747339767384311,
      "grad_norm": 0.05834188684821129,
      "learning_rate": 1.947945117147498e-05,
      "loss": 0.0824,
      "step": 7061
    },
    {
      "epoch": 1.7475872308834446,
      "grad_norm": 0.04059125855565071,
      "learning_rate": 1.944181446298496e-05,
      "loss": 0.0284,
      "step": 7062
    },
    {
      "epoch": 1.7478346943825787,
      "grad_norm": 0.03540416434407234,
      "learning_rate": 1.9404212678012374e-05,
      "loss": 0.0323,
      "step": 7063
    },
    {
      "epoch": 1.7480821578817123,
      "grad_norm": 0.1888272911310196,
      "learning_rate": 1.936664582225289e-05,
      "loss": 0.0578,
      "step": 7064
    },
    {
      "epoch": 1.7483296213808464,
      "grad_norm": 0.08584069460630417,
      "learning_rate": 1.9329113901397023e-05,
      "loss": 0.0342,
      "step": 7065
    },
    {
      "epoch": 1.7485770848799802,
      "grad_norm": 0.0439738966524601,
      "learning_rate": 1.9291616921129794e-05,
      "loss": 0.0466,
      "step": 7066
    },
    {
      "epoch": 1.748824548379114,
      "grad_norm": 0.03792458027601242,
      "learning_rate": 1.925415488713103e-05,
      "loss": 0.0319,
      "step": 7067
    },
    {
      "epoch": 1.7490720118782481,
      "grad_norm": 0.1035153791308403,
      "learning_rate": 1.9216727805075217e-05,
      "loss": 0.1114,
      "step": 7068
    },
    {
      "epoch": 1.7493194753773817,
      "grad_norm": 0.05226771533489227,
      "learning_rate": 1.9179335680631628e-05,
      "loss": 0.0352,
      "step": 7069
    },
    {
      "epoch": 1.7495669388765158,
      "grad_norm": 0.07123241573572159,
      "learning_rate": 1.91419785194642e-05,
      "loss": 0.0763,
      "step": 7070
    },
    {
      "epoch": 1.7498144023756494,
      "grad_norm": 0.026597313582897186,
      "learning_rate": 1.9104656327231463e-05,
      "loss": 0.0225,
      "step": 7071
    },
    {
      "epoch": 1.7500618658747835,
      "grad_norm": 0.04829008877277374,
      "learning_rate": 1.906736910958684e-05,
      "loss": 0.036,
      "step": 7072
    },
    {
      "epoch": 1.7503093293739174,
      "grad_norm": 0.039218656718730927,
      "learning_rate": 1.9030116872178316e-05,
      "loss": 0.1169,
      "step": 7073
    },
    {
      "epoch": 1.7505567928730512,
      "grad_norm": 0.06604646146297455,
      "learning_rate": 1.899289962064868e-05,
      "loss": 0.0649,
      "step": 7074
    },
    {
      "epoch": 1.750804256372185,
      "grad_norm": 0.06408743560314178,
      "learning_rate": 1.895571736063531e-05,
      "loss": 0.0449,
      "step": 7075
    },
    {
      "epoch": 1.751051719871319,
      "grad_norm": 0.05052461475133896,
      "learning_rate": 1.8918570097770397e-05,
      "loss": 0.0485,
      "step": 7076
    },
    {
      "epoch": 1.751299183370453,
      "grad_norm": 0.0343976654112339,
      "learning_rate": 1.8881457837680776e-05,
      "loss": 0.0412,
      "step": 7077
    },
    {
      "epoch": 1.7515466468695866,
      "grad_norm": 0.12229090929031372,
      "learning_rate": 1.8844380585987948e-05,
      "loss": 0.0735,
      "step": 7078
    },
    {
      "epoch": 1.7517941103687207,
      "grad_norm": 0.05706678703427315,
      "learning_rate": 1.880733834830814e-05,
      "loss": 0.0374,
      "step": 7079
    },
    {
      "epoch": 1.7520415738678545,
      "grad_norm": 0.16158433258533478,
      "learning_rate": 1.8770331130252282e-05,
      "loss": 0.1058,
      "step": 7080
    },
    {
      "epoch": 1.7522890373669884,
      "grad_norm": 0.04142493009567261,
      "learning_rate": 1.873335893742603e-05,
      "loss": 0.049,
      "step": 7081
    },
    {
      "epoch": 1.7525365008661222,
      "grad_norm": 0.0629662349820137,
      "learning_rate": 1.8696421775429712e-05,
      "loss": 0.0507,
      "step": 7082
    },
    {
      "epoch": 1.752783964365256,
      "grad_norm": 0.05524946376681328,
      "learning_rate": 1.8659519649858207e-05,
      "loss": 0.041,
      "step": 7083
    },
    {
      "epoch": 1.7530314278643901,
      "grad_norm": 0.06275784224271774,
      "learning_rate": 1.8622652566301386e-05,
      "loss": 0.074,
      "step": 7084
    },
    {
      "epoch": 1.7532788913635238,
      "grad_norm": 0.05910562351346016,
      "learning_rate": 1.8585820530343607e-05,
      "loss": 0.0602,
      "step": 7085
    },
    {
      "epoch": 1.7535263548626578,
      "grad_norm": 0.05158086121082306,
      "learning_rate": 1.8549023547563908e-05,
      "loss": 0.0469,
      "step": 7086
    },
    {
      "epoch": 1.7537738183617915,
      "grad_norm": 0.039495356380939484,
      "learning_rate": 1.851226162353606e-05,
      "loss": 0.0523,
      "step": 7087
    },
    {
      "epoch": 1.7540212818609255,
      "grad_norm": 0.04420618712902069,
      "learning_rate": 1.847553476382857e-05,
      "loss": 0.0501,
      "step": 7088
    },
    {
      "epoch": 1.7542687453600594,
      "grad_norm": 0.06296679377555847,
      "learning_rate": 1.8438842974004584e-05,
      "loss": 0.0672,
      "step": 7089
    },
    {
      "epoch": 1.7545162088591932,
      "grad_norm": 0.0359046533703804,
      "learning_rate": 1.8402186259622005e-05,
      "loss": 0.0297,
      "step": 7090
    },
    {
      "epoch": 1.7547636723583273,
      "grad_norm": 0.05218236893415451,
      "learning_rate": 1.8365564626233177e-05,
      "loss": 0.0636,
      "step": 7091
    },
    {
      "epoch": 1.755011135857461,
      "grad_norm": 0.042505957186222076,
      "learning_rate": 1.832897807938555e-05,
      "loss": 0.0466,
      "step": 7092
    },
    {
      "epoch": 1.755258599356595,
      "grad_norm": 0.05728484317660332,
      "learning_rate": 1.8292426624620885e-05,
      "loss": 0.0468,
      "step": 7093
    },
    {
      "epoch": 1.7555060628557286,
      "grad_norm": 0.032881200313568115,
      "learning_rate": 1.8255910267475806e-05,
      "loss": 0.0577,
      "step": 7094
    },
    {
      "epoch": 1.7557535263548627,
      "grad_norm": 0.11376245319843292,
      "learning_rate": 1.821942901348156e-05,
      "loss": 0.0562,
      "step": 7095
    },
    {
      "epoch": 1.7560009898539966,
      "grad_norm": 0.06026579067111015,
      "learning_rate": 1.818298286816411e-05,
      "loss": 0.0295,
      "step": 7096
    },
    {
      "epoch": 1.7562484533531304,
      "grad_norm": 0.04987921565771103,
      "learning_rate": 1.8146571837044156e-05,
      "loss": 0.0351,
      "step": 7097
    },
    {
      "epoch": 1.7564959168522642,
      "grad_norm": 0.032705776393413544,
      "learning_rate": 1.8110195925636925e-05,
      "loss": 0.0357,
      "step": 7098
    },
    {
      "epoch": 1.756743380351398,
      "grad_norm": 0.03787309676408768,
      "learning_rate": 1.807385513945242e-05,
      "loss": 0.0473,
      "step": 7099
    },
    {
      "epoch": 1.7569908438505322,
      "grad_norm": 0.052706263959407806,
      "learning_rate": 1.8037549483995387e-05,
      "loss": 0.0729,
      "step": 7100
    },
    {
      "epoch": 1.7572383073496658,
      "grad_norm": 0.0513414703309536,
      "learning_rate": 1.800127896476514e-05,
      "loss": 0.0557,
      "step": 7101
    },
    {
      "epoch": 1.7574857708487999,
      "grad_norm": 0.08080891519784927,
      "learning_rate": 1.7965043587255704e-05,
      "loss": 0.029,
      "step": 7102
    },
    {
      "epoch": 1.7577332343479337,
      "grad_norm": 0.03859872370958328,
      "learning_rate": 1.792884335695574e-05,
      "loss": 0.0278,
      "step": 7103
    },
    {
      "epoch": 1.7579806978470676,
      "grad_norm": 0.04748351871967316,
      "learning_rate": 1.7892678279348734e-05,
      "loss": 0.0293,
      "step": 7104
    },
    {
      "epoch": 1.7582281613462014,
      "grad_norm": 0.0926046073436737,
      "learning_rate": 1.785654835991271e-05,
      "loss": 0.1041,
      "step": 7105
    },
    {
      "epoch": 1.7584756248453353,
      "grad_norm": 0.030512165278196335,
      "learning_rate": 1.782045360412038e-05,
      "loss": 0.0235,
      "step": 7106
    },
    {
      "epoch": 1.7587230883444693,
      "grad_norm": 0.058320801705121994,
      "learning_rate": 1.7784394017439144e-05,
      "loss": 0.0826,
      "step": 7107
    },
    {
      "epoch": 1.758970551843603,
      "grad_norm": 0.02617199532687664,
      "learning_rate": 1.7748369605331133e-05,
      "loss": 0.0322,
      "step": 7108
    },
    {
      "epoch": 1.759218015342737,
      "grad_norm": 0.06735197454690933,
      "learning_rate": 1.771238037325304e-05,
      "loss": 0.0323,
      "step": 7109
    },
    {
      "epoch": 1.7594654788418709,
      "grad_norm": 0.10064094513654709,
      "learning_rate": 1.7676426326656366e-05,
      "loss": 0.063,
      "step": 7110
    },
    {
      "epoch": 1.7597129423410047,
      "grad_norm": 0.03470746427774429,
      "learning_rate": 1.7640507470987084e-05,
      "loss": 0.0304,
      "step": 7111
    },
    {
      "epoch": 1.7599604058401386,
      "grad_norm": 0.045590296387672424,
      "learning_rate": 1.760462381168612e-05,
      "loss": 0.0682,
      "step": 7112
    },
    {
      "epoch": 1.7602078693392724,
      "grad_norm": 0.04963238909840584,
      "learning_rate": 1.7568775354188797e-05,
      "loss": 0.0772,
      "step": 7113
    },
    {
      "epoch": 1.7604553328384065,
      "grad_norm": 0.1272938996553421,
      "learning_rate": 1.7532962103925243e-05,
      "loss": 0.0867,
      "step": 7114
    },
    {
      "epoch": 1.7607027963375401,
      "grad_norm": 0.028159145265817642,
      "learning_rate": 1.74971840663202e-05,
      "loss": 0.0247,
      "step": 7115
    },
    {
      "epoch": 1.7609502598366742,
      "grad_norm": 0.08882509171962738,
      "learning_rate": 1.7461441246793136e-05,
      "loss": 0.0967,
      "step": 7116
    },
    {
      "epoch": 1.7611977233358078,
      "grad_norm": 0.06310241669416428,
      "learning_rate": 1.7425733650758197e-05,
      "loss": 0.0704,
      "step": 7117
    },
    {
      "epoch": 1.761445186834942,
      "grad_norm": 0.059788428246974945,
      "learning_rate": 1.7390061283624027e-05,
      "loss": 0.0518,
      "step": 7118
    },
    {
      "epoch": 1.7616926503340757,
      "grad_norm": 0.0858190655708313,
      "learning_rate": 1.7354424150794108e-05,
      "loss": 0.0971,
      "step": 7119
    },
    {
      "epoch": 1.7619401138332096,
      "grad_norm": 0.06441934406757355,
      "learning_rate": 1.7318822257666594e-05,
      "loss": 0.0651,
      "step": 7120
    },
    {
      "epoch": 1.7621875773323434,
      "grad_norm": 0.033328596502542496,
      "learning_rate": 1.7283255609634147e-05,
      "loss": 0.0289,
      "step": 7121
    },
    {
      "epoch": 1.7624350408314773,
      "grad_norm": 0.10391262173652649,
      "learning_rate": 1.7247724212084233e-05,
      "loss": 0.0445,
      "step": 7122
    },
    {
      "epoch": 1.7626825043306114,
      "grad_norm": 0.042293187230825424,
      "learning_rate": 1.7212228070398932e-05,
      "loss": 0.0385,
      "step": 7123
    },
    {
      "epoch": 1.762929967829745,
      "grad_norm": 0.028856096789240837,
      "learning_rate": 1.7176767189954922e-05,
      "loss": 0.0262,
      "step": 7124
    },
    {
      "epoch": 1.763177431328879,
      "grad_norm": 0.05784183368086815,
      "learning_rate": 1.714134157612371e-05,
      "loss": 0.0463,
      "step": 7125
    },
    {
      "epoch": 1.763424894828013,
      "grad_norm": 0.0339055061340332,
      "learning_rate": 1.7105951234271195e-05,
      "loss": 0.0314,
      "step": 7126
    },
    {
      "epoch": 1.7636723583271467,
      "grad_norm": 0.03544177487492561,
      "learning_rate": 1.70705961697582e-05,
      "loss": 0.0358,
      "step": 7127
    },
    {
      "epoch": 1.7639198218262806,
      "grad_norm": 0.028206760063767433,
      "learning_rate": 1.703527638794003e-05,
      "loss": 0.0548,
      "step": 7128
    },
    {
      "epoch": 1.7641672853254144,
      "grad_norm": 0.05391022562980652,
      "learning_rate": 1.6999991894166735e-05,
      "loss": 0.0954,
      "step": 7129
    },
    {
      "epoch": 1.7644147488245485,
      "grad_norm": 0.043097298592329025,
      "learning_rate": 1.696474269378298e-05,
      "loss": 0.0317,
      "step": 7130
    },
    {
      "epoch": 1.7646622123236821,
      "grad_norm": 0.02817295864224434,
      "learning_rate": 1.6929528792128086e-05,
      "loss": 0.0321,
      "step": 7131
    },
    {
      "epoch": 1.7649096758228162,
      "grad_norm": 0.07781358063220978,
      "learning_rate": 1.6894350194536084e-05,
      "loss": 0.1118,
      "step": 7132
    },
    {
      "epoch": 1.76515713932195,
      "grad_norm": 0.07437913864850998,
      "learning_rate": 1.685920690633555e-05,
      "loss": 0.0761,
      "step": 7133
    },
    {
      "epoch": 1.765404602821084,
      "grad_norm": 0.05805137753486633,
      "learning_rate": 1.682409893284978e-05,
      "loss": 0.037,
      "step": 7134
    },
    {
      "epoch": 1.7656520663202178,
      "grad_norm": 0.03824108466506004,
      "learning_rate": 1.678902627939674e-05,
      "loss": 0.029,
      "step": 7135
    },
    {
      "epoch": 1.7658995298193516,
      "grad_norm": 0.05063707381486893,
      "learning_rate": 1.675398895128899e-05,
      "loss": 0.0453,
      "step": 7136
    },
    {
      "epoch": 1.7661469933184857,
      "grad_norm": 0.07706335932016373,
      "learning_rate": 1.6718986953833782e-05,
      "loss": 0.0728,
      "step": 7137
    },
    {
      "epoch": 1.7663944568176193,
      "grad_norm": 0.036122072488069534,
      "learning_rate": 1.6684020292332984e-05,
      "loss": 0.0305,
      "step": 7138
    },
    {
      "epoch": 1.7666419203167534,
      "grad_norm": 0.04381478205323219,
      "learning_rate": 1.6649088972083142e-05,
      "loss": 0.0621,
      "step": 7139
    },
    {
      "epoch": 1.766889383815887,
      "grad_norm": 0.09497065842151642,
      "learning_rate": 1.6614192998375467e-05,
      "loss": 0.0685,
      "step": 7140
    },
    {
      "epoch": 1.767136847315021,
      "grad_norm": 0.03876248002052307,
      "learning_rate": 1.65793323764957e-05,
      "loss": 0.0297,
      "step": 7141
    },
    {
      "epoch": 1.767384310814155,
      "grad_norm": 0.07557710260152817,
      "learning_rate": 1.6544507111724367e-05,
      "loss": 0.0852,
      "step": 7142
    },
    {
      "epoch": 1.7676317743132888,
      "grad_norm": 0.03793637081980705,
      "learning_rate": 1.6509717209336582e-05,
      "loss": 0.0544,
      "step": 7143
    },
    {
      "epoch": 1.7678792378124226,
      "grad_norm": 0.05052763968706131,
      "learning_rate": 1.6474962674602078e-05,
      "loss": 0.0645,
      "step": 7144
    },
    {
      "epoch": 1.7681267013115565,
      "grad_norm": 0.08605114370584488,
      "learning_rate": 1.6440243512785337e-05,
      "loss": 0.0583,
      "step": 7145
    },
    {
      "epoch": 1.7683741648106905,
      "grad_norm": 0.09750155359506607,
      "learning_rate": 1.640555972914523e-05,
      "loss": 0.0908,
      "step": 7146
    },
    {
      "epoch": 1.7686216283098242,
      "grad_norm": 0.051384877413511276,
      "learning_rate": 1.6370911328935615e-05,
      "loss": 0.0561,
      "step": 7147
    },
    {
      "epoch": 1.7688690918089582,
      "grad_norm": 0.045361731201410294,
      "learning_rate": 1.633629831740477e-05,
      "loss": 0.0381,
      "step": 7148
    },
    {
      "epoch": 1.769116555308092,
      "grad_norm": 0.05558578297495842,
      "learning_rate": 1.6301720699795604e-05,
      "loss": 0.0965,
      "step": 7149
    },
    {
      "epoch": 1.769364018807226,
      "grad_norm": 0.03205528482794762,
      "learning_rate": 1.6267178481345736e-05,
      "loss": 0.0362,
      "step": 7150
    },
    {
      "epoch": 1.7696114823063598,
      "grad_norm": 0.06864010542631149,
      "learning_rate": 1.6232671667287425e-05,
      "loss": 0.075,
      "step": 7151
    },
    {
      "epoch": 1.7698589458054936,
      "grad_norm": 0.025443600490689278,
      "learning_rate": 1.6198200262847544e-05,
      "loss": 0.0219,
      "step": 7152
    },
    {
      "epoch": 1.7701064093046277,
      "grad_norm": 0.05613304674625397,
      "learning_rate": 1.6163764273247646e-05,
      "loss": 0.07,
      "step": 7153
    },
    {
      "epoch": 1.7703538728037613,
      "grad_norm": 0.05442706495523453,
      "learning_rate": 1.612936370370377e-05,
      "loss": 0.0578,
      "step": 7154
    },
    {
      "epoch": 1.7706013363028954,
      "grad_norm": 0.05487710237503052,
      "learning_rate": 1.609499855942684e-05,
      "loss": 0.0406,
      "step": 7155
    },
    {
      "epoch": 1.7708487998020292,
      "grad_norm": 0.0577130988240242,
      "learning_rate": 1.6060668845622183e-05,
      "loss": 0.0566,
      "step": 7156
    },
    {
      "epoch": 1.771096263301163,
      "grad_norm": 0.06071724370121956,
      "learning_rate": 1.602637456748987e-05,
      "loss": 0.1096,
      "step": 7157
    },
    {
      "epoch": 1.771343726800297,
      "grad_norm": 0.07154436409473419,
      "learning_rate": 1.5992115730224576e-05,
      "loss": 0.0828,
      "step": 7158
    },
    {
      "epoch": 1.7715911902994308,
      "grad_norm": 0.03541974350810051,
      "learning_rate": 1.5957892339015623e-05,
      "loss": 0.0398,
      "step": 7159
    },
    {
      "epoch": 1.7718386537985649,
      "grad_norm": 0.03078489750623703,
      "learning_rate": 1.5923704399047002e-05,
      "loss": 0.0307,
      "step": 7160
    },
    {
      "epoch": 1.7720861172976985,
      "grad_norm": 0.0350496880710125,
      "learning_rate": 1.5889551915497208e-05,
      "loss": 0.0395,
      "step": 7161
    },
    {
      "epoch": 1.7723335807968326,
      "grad_norm": 0.030312469229102135,
      "learning_rate": 1.5855434893539437e-05,
      "loss": 0.0324,
      "step": 7162
    },
    {
      "epoch": 1.7725810442959662,
      "grad_norm": 0.054278887808322906,
      "learning_rate": 1.582135333834167e-05,
      "loss": 0.04,
      "step": 7163
    },
    {
      "epoch": 1.7728285077951003,
      "grad_norm": 0.05926937237381935,
      "learning_rate": 1.5787307255066218e-05,
      "loss": 0.0696,
      "step": 7164
    },
    {
      "epoch": 1.773075971294234,
      "grad_norm": 0.04984498396515846,
      "learning_rate": 1.5753296648870236e-05,
      "loss": 0.0568,
      "step": 7165
    },
    {
      "epoch": 1.773323434793368,
      "grad_norm": 0.06627360731363297,
      "learning_rate": 1.5719321524905434e-05,
      "loss": 0.1439,
      "step": 7166
    },
    {
      "epoch": 1.7735708982925018,
      "grad_norm": 0.04801027849316597,
      "learning_rate": 1.568538188831811e-05,
      "loss": 0.0585,
      "step": 7167
    },
    {
      "epoch": 1.7738183617916357,
      "grad_norm": 0.02716570533812046,
      "learning_rate": 1.5651477744249326e-05,
      "loss": 0.0275,
      "step": 7168
    },
    {
      "epoch": 1.7740658252907697,
      "grad_norm": 0.07091992348432541,
      "learning_rate": 1.561760909783455e-05,
      "loss": 0.1121,
      "step": 7169
    },
    {
      "epoch": 1.7743132887899034,
      "grad_norm": 0.06240840628743172,
      "learning_rate": 1.5583775954204072e-05,
      "loss": 0.0465,
      "step": 7170
    },
    {
      "epoch": 1.7745607522890374,
      "grad_norm": 0.042986754328012466,
      "learning_rate": 1.5549978318482706e-05,
      "loss": 0.0579,
      "step": 7171
    },
    {
      "epoch": 1.7748082157881713,
      "grad_norm": 0.030419666320085526,
      "learning_rate": 1.5516216195789883e-05,
      "loss": 0.0244,
      "step": 7172
    },
    {
      "epoch": 1.7750556792873051,
      "grad_norm": 0.04000915586948395,
      "learning_rate": 1.5482489591239713e-05,
      "loss": 0.0405,
      "step": 7173
    },
    {
      "epoch": 1.775303142786439,
      "grad_norm": 0.06807880848646164,
      "learning_rate": 1.544879850994088e-05,
      "loss": 0.037,
      "step": 7174
    },
    {
      "epoch": 1.7755506062855728,
      "grad_norm": 0.07427459210157394,
      "learning_rate": 1.5415142956996725e-05,
      "loss": 0.0404,
      "step": 7175
    },
    {
      "epoch": 1.775798069784707,
      "grad_norm": 0.05106830969452858,
      "learning_rate": 1.5381522937505138e-05,
      "loss": 0.0624,
      "step": 7176
    },
    {
      "epoch": 1.7760455332838405,
      "grad_norm": 0.04709796607494354,
      "learning_rate": 1.5347938456558656e-05,
      "loss": 0.053,
      "step": 7177
    },
    {
      "epoch": 1.7762929967829746,
      "grad_norm": 0.03956569731235504,
      "learning_rate": 1.531438951924452e-05,
      "loss": 0.036,
      "step": 7178
    },
    {
      "epoch": 1.7765404602821084,
      "grad_norm": 0.04421130195260048,
      "learning_rate": 1.528087613064444e-05,
      "loss": 0.0478,
      "step": 7179
    },
    {
      "epoch": 1.7767879237812423,
      "grad_norm": 0.02718666009604931,
      "learning_rate": 1.5247398295834907e-05,
      "loss": 0.0428,
      "step": 7180
    },
    {
      "epoch": 1.7770353872803761,
      "grad_norm": 0.05823204293847084,
      "learning_rate": 1.5213956019886787e-05,
      "loss": 0.0458,
      "step": 7181
    },
    {
      "epoch": 1.77728285077951,
      "grad_norm": 0.03661543130874634,
      "learning_rate": 1.5180549307865831e-05,
      "loss": 0.0247,
      "step": 7182
    },
    {
      "epoch": 1.777530314278644,
      "grad_norm": 0.030463561415672302,
      "learning_rate": 1.5147178164832298e-05,
      "loss": 0.025,
      "step": 7183
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.053163208067417145,
      "learning_rate": 1.5113842595840921e-05,
      "loss": 0.0363,
      "step": 7184
    },
    {
      "epoch": 1.7780252412769117,
      "grad_norm": 0.02458188124001026,
      "learning_rate": 1.5080542605941272e-05,
      "loss": 0.0269,
      "step": 7185
    },
    {
      "epoch": 1.7782727047760454,
      "grad_norm": 0.06687130779027939,
      "learning_rate": 1.5047278200177344e-05,
      "loss": 0.0608,
      "step": 7186
    },
    {
      "epoch": 1.7785201682751794,
      "grad_norm": 0.1092299371957779,
      "learning_rate": 1.5014049383587886e-05,
      "loss": 0.05,
      "step": 7187
    },
    {
      "epoch": 1.7787676317743133,
      "grad_norm": 0.05567065253853798,
      "learning_rate": 1.4980856161206201e-05,
      "loss": 0.0531,
      "step": 7188
    },
    {
      "epoch": 1.7790150952734471,
      "grad_norm": 0.05271897092461586,
      "learning_rate": 1.4947698538060133e-05,
      "loss": 0.0451,
      "step": 7189
    },
    {
      "epoch": 1.779262558772581,
      "grad_norm": 0.05884329229593277,
      "learning_rate": 1.4914576519172218e-05,
      "loss": 0.1228,
      "step": 7190
    },
    {
      "epoch": 1.7795100222717148,
      "grad_norm": 0.07442692667245865,
      "learning_rate": 1.4881490109559554e-05,
      "loss": 0.0751,
      "step": 7191
    },
    {
      "epoch": 1.779757485770849,
      "grad_norm": 0.05063723772764206,
      "learning_rate": 1.4848439314233907e-05,
      "loss": 0.0469,
      "step": 7192
    },
    {
      "epoch": 1.7800049492699825,
      "grad_norm": 0.08731000870466232,
      "learning_rate": 1.4815424138201583e-05,
      "loss": 0.0809,
      "step": 7193
    },
    {
      "epoch": 1.7802524127691166,
      "grad_norm": 0.06472105532884598,
      "learning_rate": 1.4782444586463495e-05,
      "loss": 0.0394,
      "step": 7194
    },
    {
      "epoch": 1.7804998762682505,
      "grad_norm": 0.060733016580343246,
      "learning_rate": 1.4749500664015258e-05,
      "loss": 0.0402,
      "step": 7195
    },
    {
      "epoch": 1.7807473397673843,
      "grad_norm": 0.06811527162790298,
      "learning_rate": 1.471659237584691e-05,
      "loss": 0.0615,
      "step": 7196
    },
    {
      "epoch": 1.7809948032665182,
      "grad_norm": 0.03042709268629551,
      "learning_rate": 1.4683719726943211e-05,
      "loss": 0.0411,
      "step": 7197
    },
    {
      "epoch": 1.781242266765652,
      "grad_norm": 0.04959295317530632,
      "learning_rate": 1.4650882722283571e-05,
      "loss": 0.034,
      "step": 7198
    },
    {
      "epoch": 1.781489730264786,
      "grad_norm": 0.058540258556604385,
      "learning_rate": 1.4618081366841868e-05,
      "loss": 0.0754,
      "step": 7199
    },
    {
      "epoch": 1.7817371937639197,
      "grad_norm": 0.0418601930141449,
      "learning_rate": 1.4585315665586713e-05,
      "loss": 0.045,
      "step": 7200
    },
    {
      "epoch": 1.7817371937639197,
      "eval_loss": 0.28914937376976013,
      "eval_runtime": 202.7633,
      "eval_samples_per_second": 4.932,
      "eval_steps_per_second": 0.311,
      "step": 7200
    },
    {
      "epoch": 1.7819846572630538,
      "grad_norm": 0.0543949119746685,
      "learning_rate": 1.4552585623481163e-05,
      "loss": 0.0634,
      "step": 7201
    },
    {
      "epoch": 1.7822321207621876,
      "grad_norm": 0.04062182828783989,
      "learning_rate": 1.451989124548303e-05,
      "loss": 0.0326,
      "step": 7202
    },
    {
      "epoch": 1.7824795842613215,
      "grad_norm": 0.06418289989233017,
      "learning_rate": 1.4487232536544659e-05,
      "loss": 0.0515,
      "step": 7203
    },
    {
      "epoch": 1.7827270477604553,
      "grad_norm": 0.03209460526704788,
      "learning_rate": 1.4454609501612925e-05,
      "loss": 0.0377,
      "step": 7204
    },
    {
      "epoch": 1.7829745112595892,
      "grad_norm": 0.05903514474630356,
      "learning_rate": 1.4422022145629431e-05,
      "loss": 0.0575,
      "step": 7205
    },
    {
      "epoch": 1.7832219747587232,
      "grad_norm": 0.0520053505897522,
      "learning_rate": 1.4389470473530259e-05,
      "loss": 0.0728,
      "step": 7206
    },
    {
      "epoch": 1.7834694382578569,
      "grad_norm": 0.06130120903253555,
      "learning_rate": 1.4356954490246126e-05,
      "loss": 0.0925,
      "step": 7207
    },
    {
      "epoch": 1.783716901756991,
      "grad_norm": 0.045181021094322205,
      "learning_rate": 1.4324474200702431e-05,
      "loss": 0.0323,
      "step": 7208
    },
    {
      "epoch": 1.7839643652561246,
      "grad_norm": 0.036576103419065475,
      "learning_rate": 1.4292029609818957e-05,
      "loss": 0.0379,
      "step": 7209
    },
    {
      "epoch": 1.7842118287552586,
      "grad_norm": 0.04227784276008606,
      "learning_rate": 1.4259620722510359e-05,
      "loss": 0.0696,
      "step": 7210
    },
    {
      "epoch": 1.7844592922543925,
      "grad_norm": 0.03756749629974365,
      "learning_rate": 1.4227247543685622e-05,
      "loss": 0.0385,
      "step": 7211
    },
    {
      "epoch": 1.7847067557535263,
      "grad_norm": 0.05360837280750275,
      "learning_rate": 1.419491007824847e-05,
      "loss": 0.0678,
      "step": 7212
    },
    {
      "epoch": 1.7849542192526602,
      "grad_norm": 0.039000265300273895,
      "learning_rate": 1.416260833109717e-05,
      "loss": 0.0541,
      "step": 7213
    },
    {
      "epoch": 1.785201682751794,
      "grad_norm": 0.07243199646472931,
      "learning_rate": 1.4130342307124595e-05,
      "loss": 0.0377,
      "step": 7214
    },
    {
      "epoch": 1.785449146250928,
      "grad_norm": 0.03537961468100548,
      "learning_rate": 1.4098112011218217e-05,
      "loss": 0.0499,
      "step": 7215
    },
    {
      "epoch": 1.7856966097500617,
      "grad_norm": 0.0821392610669136,
      "learning_rate": 1.4065917448260051e-05,
      "loss": 0.0846,
      "step": 7216
    },
    {
      "epoch": 1.7859440732491958,
      "grad_norm": 0.027599792927503586,
      "learning_rate": 1.4033758623126724e-05,
      "loss": 0.028,
      "step": 7217
    },
    {
      "epoch": 1.7861915367483296,
      "grad_norm": 0.058620598167181015,
      "learning_rate": 1.4001635540689534e-05,
      "loss": 0.072,
      "step": 7218
    },
    {
      "epoch": 1.7864390002474635,
      "grad_norm": 0.049335796386003494,
      "learning_rate": 1.396954820581417e-05,
      "loss": 0.0314,
      "step": 7219
    },
    {
      "epoch": 1.7866864637465973,
      "grad_norm": 0.04379009082913399,
      "learning_rate": 1.3937496623361106e-05,
      "loss": 0.0507,
      "step": 7220
    },
    {
      "epoch": 1.7869339272457312,
      "grad_norm": 0.03376303240656853,
      "learning_rate": 1.3905480798185267e-05,
      "loss": 0.0233,
      "step": 7221
    },
    {
      "epoch": 1.7871813907448653,
      "grad_norm": 0.1123228669166565,
      "learning_rate": 1.3873500735136241e-05,
      "loss": 0.0648,
      "step": 7222
    },
    {
      "epoch": 1.7874288542439989,
      "grad_norm": 0.034348491579294205,
      "learning_rate": 1.3841556439058211e-05,
      "loss": 0.0249,
      "step": 7223
    },
    {
      "epoch": 1.787676317743133,
      "grad_norm": 0.06529444456100464,
      "learning_rate": 1.3809647914789807e-05,
      "loss": 0.0664,
      "step": 7224
    },
    {
      "epoch": 1.7879237812422668,
      "grad_norm": 0.027271902188658714,
      "learning_rate": 1.3777775167164358e-05,
      "loss": 0.0402,
      "step": 7225
    },
    {
      "epoch": 1.7881712447414007,
      "grad_norm": 0.04312681034207344,
      "learning_rate": 1.3745938201009777e-05,
      "loss": 0.0523,
      "step": 7226
    },
    {
      "epoch": 1.7884187082405345,
      "grad_norm": 0.07036875933408737,
      "learning_rate": 1.3714137021148515e-05,
      "loss": 0.052,
      "step": 7227
    },
    {
      "epoch": 1.7886661717396684,
      "grad_norm": 0.06696360558271408,
      "learning_rate": 1.3682371632397606e-05,
      "loss": 0.0534,
      "step": 7228
    },
    {
      "epoch": 1.7889136352388024,
      "grad_norm": 0.07684203237295151,
      "learning_rate": 1.36506420395687e-05,
      "loss": 0.0839,
      "step": 7229
    },
    {
      "epoch": 1.789161098737936,
      "grad_norm": 0.046168144792318344,
      "learning_rate": 1.3618948247468039e-05,
      "loss": 0.0486,
      "step": 7230
    },
    {
      "epoch": 1.7894085622370701,
      "grad_norm": 0.09639935940504074,
      "learning_rate": 1.3587290260896279e-05,
      "loss": 0.1032,
      "step": 7231
    },
    {
      "epoch": 1.7896560257362037,
      "grad_norm": 0.045685358345508575,
      "learning_rate": 1.3555668084648865e-05,
      "loss": 0.0343,
      "step": 7232
    },
    {
      "epoch": 1.7899034892353378,
      "grad_norm": 0.12086856365203857,
      "learning_rate": 1.3524081723515714e-05,
      "loss": 0.1153,
      "step": 7233
    },
    {
      "epoch": 1.7901509527344717,
      "grad_norm": 0.047706618905067444,
      "learning_rate": 1.3492531182281304e-05,
      "loss": 0.0493,
      "step": 7234
    },
    {
      "epoch": 1.7903984162336055,
      "grad_norm": 0.04103681445121765,
      "learning_rate": 1.3461016465724756e-05,
      "loss": 0.0611,
      "step": 7235
    },
    {
      "epoch": 1.7906458797327396,
      "grad_norm": 0.06088065728545189,
      "learning_rate": 1.3429537578619643e-05,
      "loss": 0.0836,
      "step": 7236
    },
    {
      "epoch": 1.7908933432318732,
      "grad_norm": 0.07222386449575424,
      "learning_rate": 1.3398094525734289e-05,
      "loss": 0.1102,
      "step": 7237
    },
    {
      "epoch": 1.7911408067310073,
      "grad_norm": 0.06268127262592316,
      "learning_rate": 1.3366687311831466e-05,
      "loss": 0.0819,
      "step": 7238
    },
    {
      "epoch": 1.791388270230141,
      "grad_norm": 0.03133876994252205,
      "learning_rate": 1.3335315941668508e-05,
      "loss": 0.0388,
      "step": 7239
    },
    {
      "epoch": 1.791635733729275,
      "grad_norm": 0.07474104315042496,
      "learning_rate": 1.3303980419997364e-05,
      "loss": 0.063,
      "step": 7240
    },
    {
      "epoch": 1.7918831972284088,
      "grad_norm": 0.07009705156087875,
      "learning_rate": 1.327268075156457e-05,
      "loss": 0.061,
      "step": 7241
    },
    {
      "epoch": 1.7921306607275427,
      "grad_norm": 0.06457546353340149,
      "learning_rate": 1.3241416941111167e-05,
      "loss": 0.0375,
      "step": 7242
    },
    {
      "epoch": 1.7923781242266765,
      "grad_norm": 0.07239004969596863,
      "learning_rate": 1.3210188993372868e-05,
      "loss": 0.0376,
      "step": 7243
    },
    {
      "epoch": 1.7926255877258104,
      "grad_norm": 0.0395626425743103,
      "learning_rate": 1.3178996913079777e-05,
      "loss": 0.0235,
      "step": 7244
    },
    {
      "epoch": 1.7928730512249444,
      "grad_norm": 0.029285086318850517,
      "learning_rate": 1.314784070495681e-05,
      "loss": 0.0262,
      "step": 7245
    },
    {
      "epoch": 1.793120514724078,
      "grad_norm": 0.0503891184926033,
      "learning_rate": 1.3116720373723217e-05,
      "loss": 0.061,
      "step": 7246
    },
    {
      "epoch": 1.7933679782232121,
      "grad_norm": 0.08551539480686188,
      "learning_rate": 1.3085635924092953e-05,
      "loss": 0.1081,
      "step": 7247
    },
    {
      "epoch": 1.793615441722346,
      "grad_norm": 0.0890323743224144,
      "learning_rate": 1.3054587360774444e-05,
      "loss": 0.0787,
      "step": 7248
    },
    {
      "epoch": 1.7938629052214798,
      "grad_norm": 0.049025021493434906,
      "learning_rate": 1.3023574688470818e-05,
      "loss": 0.0363,
      "step": 7249
    },
    {
      "epoch": 1.7941103687206137,
      "grad_norm": 0.02806801348924637,
      "learning_rate": 1.299259791187965e-05,
      "loss": 0.0285,
      "step": 7250
    },
    {
      "epoch": 1.7943578322197475,
      "grad_norm": 0.05358004570007324,
      "learning_rate": 1.2961657035693075e-05,
      "loss": 0.0375,
      "step": 7251
    },
    {
      "epoch": 1.7946052957188816,
      "grad_norm": 0.051789771765470505,
      "learning_rate": 1.2930752064597818e-05,
      "loss": 0.0614,
      "step": 7252
    },
    {
      "epoch": 1.7948527592180152,
      "grad_norm": 0.03359323740005493,
      "learning_rate": 1.2899883003275243e-05,
      "loss": 0.0248,
      "step": 7253
    },
    {
      "epoch": 1.7951002227171493,
      "grad_norm": 0.052762143313884735,
      "learning_rate": 1.2869049856401137e-05,
      "loss": 0.0336,
      "step": 7254
    },
    {
      "epoch": 1.795347686216283,
      "grad_norm": 0.036667246371507645,
      "learning_rate": 1.283825262864599e-05,
      "loss": 0.0384,
      "step": 7255
    },
    {
      "epoch": 1.795595149715417,
      "grad_norm": 0.04930118843913078,
      "learning_rate": 1.2807491324674625e-05,
      "loss": 0.0502,
      "step": 7256
    },
    {
      "epoch": 1.7958426132145509,
      "grad_norm": 0.038165315985679626,
      "learning_rate": 1.2776765949146728e-05,
      "loss": 0.0707,
      "step": 7257
    },
    {
      "epoch": 1.7960900767136847,
      "grad_norm": 0.05010688304901123,
      "learning_rate": 1.2746076506716331e-05,
      "loss": 0.0438,
      "step": 7258
    },
    {
      "epoch": 1.7963375402128188,
      "grad_norm": 0.04613601416349411,
      "learning_rate": 1.2715423002032073e-05,
      "loss": 0.0427,
      "step": 7259
    },
    {
      "epoch": 1.7965850037119524,
      "grad_norm": 0.03794601932168007,
      "learning_rate": 1.2684805439737129e-05,
      "loss": 0.0343,
      "step": 7260
    },
    {
      "epoch": 1.7968324672110865,
      "grad_norm": 0.03652581200003624,
      "learning_rate": 1.2654223824469319e-05,
      "loss": 0.0284,
      "step": 7261
    },
    {
      "epoch": 1.79707993071022,
      "grad_norm": 0.04245678707957268,
      "learning_rate": 1.2623678160860907e-05,
      "loss": 0.0322,
      "step": 7262
    },
    {
      "epoch": 1.7973273942093542,
      "grad_norm": 0.07350604981184006,
      "learning_rate": 1.2593168453538805e-05,
      "loss": 0.0655,
      "step": 7263
    },
    {
      "epoch": 1.797574857708488,
      "grad_norm": 0.02781619504094124,
      "learning_rate": 1.2562694707124344e-05,
      "loss": 0.0279,
      "step": 7264
    },
    {
      "epoch": 1.7978223212076219,
      "grad_norm": 0.03413086757063866,
      "learning_rate": 1.2532256926233582e-05,
      "loss": 0.0203,
      "step": 7265
    },
    {
      "epoch": 1.7980697847067557,
      "grad_norm": 0.04121488332748413,
      "learning_rate": 1.250185511547705e-05,
      "loss": 0.0463,
      "step": 7266
    },
    {
      "epoch": 1.7983172482058896,
      "grad_norm": 0.039324112236499786,
      "learning_rate": 1.247148927945979e-05,
      "loss": 0.0389,
      "step": 7267
    },
    {
      "epoch": 1.7985647117050236,
      "grad_norm": 0.04667793959379196,
      "learning_rate": 1.2441159422781401e-05,
      "loss": 0.0461,
      "step": 7268
    },
    {
      "epoch": 1.7988121752041573,
      "grad_norm": 0.049140579998493195,
      "learning_rate": 1.241086555003612e-05,
      "loss": 0.068,
      "step": 7269
    },
    {
      "epoch": 1.7990596387032913,
      "grad_norm": 0.034747254103422165,
      "learning_rate": 1.2380607665812615e-05,
      "loss": 0.0373,
      "step": 7270
    },
    {
      "epoch": 1.7993071022024252,
      "grad_norm": 0.04720430076122284,
      "learning_rate": 1.2350385774694272e-05,
      "loss": 0.044,
      "step": 7271
    },
    {
      "epoch": 1.799554565701559,
      "grad_norm": 0.0602954663336277,
      "learning_rate": 1.2320199881258736e-05,
      "loss": 0.0582,
      "step": 7272
    },
    {
      "epoch": 1.7998020292006929,
      "grad_norm": 0.0797375813126564,
      "learning_rate": 1.2290049990078572e-05,
      "loss": 0.0522,
      "step": 7273
    },
    {
      "epoch": 1.8000494926998267,
      "grad_norm": 0.0854535773396492,
      "learning_rate": 1.2259936105720543e-05,
      "loss": 0.1086,
      "step": 7274
    },
    {
      "epoch": 1.8002969561989608,
      "grad_norm": 0.0736711248755455,
      "learning_rate": 1.2229858232746193e-05,
      "loss": 0.1021,
      "step": 7275
    },
    {
      "epoch": 1.8005444196980944,
      "grad_norm": 0.04255633428692818,
      "learning_rate": 1.219981637571152e-05,
      "loss": 0.032,
      "step": 7276
    },
    {
      "epoch": 1.8007918831972285,
      "grad_norm": 0.05821652337908745,
      "learning_rate": 1.2169810539167075e-05,
      "loss": 0.0234,
      "step": 7277
    },
    {
      "epoch": 1.8010393466963621,
      "grad_norm": 0.03260563313961029,
      "learning_rate": 1.2139840727657975e-05,
      "loss": 0.029,
      "step": 7278
    },
    {
      "epoch": 1.8012868101954962,
      "grad_norm": 0.02693597972393036,
      "learning_rate": 1.210990694572378e-05,
      "loss": 0.0271,
      "step": 7279
    },
    {
      "epoch": 1.80153427369463,
      "grad_norm": 0.0430758111178875,
      "learning_rate": 1.2080009197898729e-05,
      "loss": 0.0377,
      "step": 7280
    },
    {
      "epoch": 1.8017817371937639,
      "grad_norm": 0.06566643714904785,
      "learning_rate": 1.2050147488711582e-05,
      "loss": 0.0524,
      "step": 7281
    },
    {
      "epoch": 1.802029200692898,
      "grad_norm": 0.03302346542477608,
      "learning_rate": 1.202032182268556e-05,
      "loss": 0.0265,
      "step": 7282
    },
    {
      "epoch": 1.8022766641920316,
      "grad_norm": 0.04066461697220802,
      "learning_rate": 1.199053220433849e-05,
      "loss": 0.0383,
      "step": 7283
    },
    {
      "epoch": 1.8025241276911657,
      "grad_norm": 0.0344277024269104,
      "learning_rate": 1.1960778638182678e-05,
      "loss": 0.024,
      "step": 7284
    },
    {
      "epoch": 1.8027715911902993,
      "grad_norm": 0.084090955555439,
      "learning_rate": 1.1931061128725052e-05,
      "loss": 0.0878,
      "step": 7285
    },
    {
      "epoch": 1.8030190546894334,
      "grad_norm": 0.06252309679985046,
      "learning_rate": 1.1901379680467034e-05,
      "loss": 0.0468,
      "step": 7286
    },
    {
      "epoch": 1.8032665181885672,
      "grad_norm": 0.0707879364490509,
      "learning_rate": 1.1871734297904557e-05,
      "loss": 0.0869,
      "step": 7287
    },
    {
      "epoch": 1.803513981687701,
      "grad_norm": 0.08810761570930481,
      "learning_rate": 1.1842124985528113e-05,
      "loss": 0.0684,
      "step": 7288
    },
    {
      "epoch": 1.803761445186835,
      "grad_norm": 0.04362459480762482,
      "learning_rate": 1.181255174782278e-05,
      "loss": 0.024,
      "step": 7289
    },
    {
      "epoch": 1.8040089086859687,
      "grad_norm": 0.026911085471510887,
      "learning_rate": 1.1783014589268088e-05,
      "loss": 0.0235,
      "step": 7290
    },
    {
      "epoch": 1.8042563721851028,
      "grad_norm": 0.07176385819911957,
      "learning_rate": 1.1753513514338149e-05,
      "loss": 0.0334,
      "step": 7291
    },
    {
      "epoch": 1.8045038356842364,
      "grad_norm": 0.08027107268571854,
      "learning_rate": 1.1724048527501612e-05,
      "loss": 0.0545,
      "step": 7292
    },
    {
      "epoch": 1.8047512991833705,
      "grad_norm": 0.051341842859983444,
      "learning_rate": 1.1694619633221687e-05,
      "loss": 0.0241,
      "step": 7293
    },
    {
      "epoch": 1.8049987626825044,
      "grad_norm": 0.02933012694120407,
      "learning_rate": 1.1665226835956027e-05,
      "loss": 0.0575,
      "step": 7294
    },
    {
      "epoch": 1.8052462261816382,
      "grad_norm": 0.08064165711402893,
      "learning_rate": 1.163587014015685e-05,
      "loss": 0.0728,
      "step": 7295
    },
    {
      "epoch": 1.805493689680772,
      "grad_norm": 0.030006829649209976,
      "learning_rate": 1.1606549550270989e-05,
      "loss": 0.0216,
      "step": 7296
    },
    {
      "epoch": 1.805741153179906,
      "grad_norm": 0.048427682369947433,
      "learning_rate": 1.1577265070739695e-05,
      "loss": 0.0544,
      "step": 7297
    },
    {
      "epoch": 1.80598861667904,
      "grad_norm": 0.04878772422671318,
      "learning_rate": 1.1548016705998865e-05,
      "loss": 0.0437,
      "step": 7298
    },
    {
      "epoch": 1.8062360801781736,
      "grad_norm": 0.039564743638038635,
      "learning_rate": 1.1518804460478733e-05,
      "loss": 0.0428,
      "step": 7299
    },
    {
      "epoch": 1.8064835436773077,
      "grad_norm": 0.08571646362543106,
      "learning_rate": 1.1489628338604286e-05,
      "loss": 0.0738,
      "step": 7300
    },
    {
      "epoch": 1.8067310071764415,
      "grad_norm": 0.04577377438545227,
      "learning_rate": 1.1460488344794961e-05,
      "loss": 0.0621,
      "step": 7301
    },
    {
      "epoch": 1.8069784706755754,
      "grad_norm": 0.0418483167886734,
      "learning_rate": 1.1431384483464646e-05,
      "loss": 0.0419,
      "step": 7302
    },
    {
      "epoch": 1.8072259341747092,
      "grad_norm": 0.03766469284892082,
      "learning_rate": 1.1402316759021841e-05,
      "loss": 0.0416,
      "step": 7303
    },
    {
      "epoch": 1.807473397673843,
      "grad_norm": 0.04802316799759865,
      "learning_rate": 1.1373285175869497e-05,
      "loss": 0.0446,
      "step": 7304
    },
    {
      "epoch": 1.8077208611729771,
      "grad_norm": 0.04675972834229469,
      "learning_rate": 1.1344289738405206e-05,
      "loss": 0.043,
      "step": 7305
    },
    {
      "epoch": 1.8079683246721108,
      "grad_norm": 0.05492321774363518,
      "learning_rate": 1.1315330451020984e-05,
      "loss": 0.0761,
      "step": 7306
    },
    {
      "epoch": 1.8082157881712448,
      "grad_norm": 0.04025057703256607,
      "learning_rate": 1.1286407318103347e-05,
      "loss": 0.0618,
      "step": 7307
    },
    {
      "epoch": 1.8084632516703785,
      "grad_norm": 0.05302635207772255,
      "learning_rate": 1.1257520344033517e-05,
      "loss": 0.0661,
      "step": 7308
    },
    {
      "epoch": 1.8087107151695125,
      "grad_norm": 0.034786954522132874,
      "learning_rate": 1.122866953318702e-05,
      "loss": 0.0285,
      "step": 7309
    },
    {
      "epoch": 1.8089581786686464,
      "grad_norm": 0.21094152331352234,
      "learning_rate": 1.1199854889933997e-05,
      "loss": 0.0637,
      "step": 7310
    },
    {
      "epoch": 1.8092056421677802,
      "grad_norm": 0.047708380967378616,
      "learning_rate": 1.1171076418639153e-05,
      "loss": 0.0406,
      "step": 7311
    },
    {
      "epoch": 1.809453105666914,
      "grad_norm": 0.04517403617501259,
      "learning_rate": 1.114233412366164e-05,
      "loss": 0.0339,
      "step": 7312
    },
    {
      "epoch": 1.809700569166048,
      "grad_norm": 0.03374407812952995,
      "learning_rate": 1.1113628009355226e-05,
      "loss": 0.022,
      "step": 7313
    },
    {
      "epoch": 1.809948032665182,
      "grad_norm": 0.04464704915881157,
      "learning_rate": 1.1084958080068014e-05,
      "loss": 0.0532,
      "step": 7314
    },
    {
      "epoch": 1.8101954961643156,
      "grad_norm": 0.056004516780376434,
      "learning_rate": 1.1056324340142809e-05,
      "loss": 0.0344,
      "step": 7315
    },
    {
      "epoch": 1.8104429596634497,
      "grad_norm": 0.16254769265651703,
      "learning_rate": 1.102772679391692e-05,
      "loss": 0.0875,
      "step": 7316
    },
    {
      "epoch": 1.8106904231625836,
      "grad_norm": 0.043882012367248535,
      "learning_rate": 1.0999165445722077e-05,
      "loss": 0.0605,
      "step": 7317
    },
    {
      "epoch": 1.8109378866617174,
      "grad_norm": 0.09325120598077774,
      "learning_rate": 1.0970640299884594e-05,
      "loss": 0.0626,
      "step": 7318
    },
    {
      "epoch": 1.8111853501608512,
      "grad_norm": 0.050636228173971176,
      "learning_rate": 1.094215136072521e-05,
      "loss": 0.0459,
      "step": 7319
    },
    {
      "epoch": 1.811432813659985,
      "grad_norm": 0.05775905400514603,
      "learning_rate": 1.0913698632559338e-05,
      "loss": 0.0415,
      "step": 7320
    },
    {
      "epoch": 1.8116802771591192,
      "grad_norm": 0.038300931453704834,
      "learning_rate": 1.0885282119696833e-05,
      "loss": 0.0492,
      "step": 7321
    },
    {
      "epoch": 1.8119277406582528,
      "grad_norm": 0.07222957164049149,
      "learning_rate": 1.0856901826441973e-05,
      "loss": 0.0823,
      "step": 7322
    },
    {
      "epoch": 1.8121752041573869,
      "grad_norm": 0.0390591099858284,
      "learning_rate": 1.0828557757093655e-05,
      "loss": 0.024,
      "step": 7323
    },
    {
      "epoch": 1.8124226676565207,
      "grad_norm": 0.06570005416870117,
      "learning_rate": 1.0800249915945275e-05,
      "loss": 0.0356,
      "step": 7324
    },
    {
      "epoch": 1.8126701311556546,
      "grad_norm": 0.04210788384079933,
      "learning_rate": 1.0771978307284742e-05,
      "loss": 0.033,
      "step": 7325
    },
    {
      "epoch": 1.8129175946547884,
      "grad_norm": 0.05554008111357689,
      "learning_rate": 1.0743742935394457e-05,
      "loss": 0.0479,
      "step": 7326
    },
    {
      "epoch": 1.8131650581539223,
      "grad_norm": 0.07076096534729004,
      "learning_rate": 1.071554380455128e-05,
      "loss": 0.091,
      "step": 7327
    },
    {
      "epoch": 1.8134125216530563,
      "grad_norm": 0.06330963969230652,
      "learning_rate": 1.068738091902674e-05,
      "loss": 0.0624,
      "step": 7328
    },
    {
      "epoch": 1.81365998515219,
      "grad_norm": 0.06465891748666763,
      "learning_rate": 1.06592542830867e-05,
      "loss": 0.0609,
      "step": 7329
    },
    {
      "epoch": 1.813907448651324,
      "grad_norm": 0.043255433440208435,
      "learning_rate": 1.063116390099164e-05,
      "loss": 0.068,
      "step": 7330
    },
    {
      "epoch": 1.8141549121504577,
      "grad_norm": 0.057839419692754745,
      "learning_rate": 1.0603109776996495e-05,
      "loss": 0.0666,
      "step": 7331
    },
    {
      "epoch": 1.8144023756495917,
      "grad_norm": 0.03222624957561493,
      "learning_rate": 1.057509191535072e-05,
      "loss": 0.0208,
      "step": 7332
    },
    {
      "epoch": 1.8146498391487256,
      "grad_norm": 0.05634443834424019,
      "learning_rate": 1.054711032029837e-05,
      "loss": 0.0464,
      "step": 7333
    },
    {
      "epoch": 1.8148973026478594,
      "grad_norm": 0.05739261955022812,
      "learning_rate": 1.051916499607783e-05,
      "loss": 0.0722,
      "step": 7334
    },
    {
      "epoch": 1.8151447661469933,
      "grad_norm": 0.07182186841964722,
      "learning_rate": 1.0491255946922046e-05,
      "loss": 0.072,
      "step": 7335
    },
    {
      "epoch": 1.8153922296461271,
      "grad_norm": 0.06582815945148468,
      "learning_rate": 1.0463383177058667e-05,
      "loss": 0.0734,
      "step": 7336
    },
    {
      "epoch": 1.8156396931452612,
      "grad_norm": 0.03734511509537697,
      "learning_rate": 1.0435546690709564e-05,
      "loss": 0.0418,
      "step": 7337
    },
    {
      "epoch": 1.8158871566443948,
      "grad_norm": 0.0380147248506546,
      "learning_rate": 1.0407746492091253e-05,
      "loss": 0.0477,
      "step": 7338
    },
    {
      "epoch": 1.8161346201435289,
      "grad_norm": 0.04748177155852318,
      "learning_rate": 1.0379982585414754e-05,
      "loss": 0.0531,
      "step": 7339
    },
    {
      "epoch": 1.8163820836426627,
      "grad_norm": 0.07845916599035263,
      "learning_rate": 1.0352254974885539e-05,
      "loss": 0.1009,
      "step": 7340
    },
    {
      "epoch": 1.8166295471417966,
      "grad_norm": 0.040614426136016846,
      "learning_rate": 1.032456366470369e-05,
      "loss": 0.0316,
      "step": 7341
    },
    {
      "epoch": 1.8168770106409304,
      "grad_norm": 0.04623398557305336,
      "learning_rate": 1.029690865906363e-05,
      "loss": 0.0433,
      "step": 7342
    },
    {
      "epoch": 1.8171244741400643,
      "grad_norm": 0.05996125936508179,
      "learning_rate": 1.02692899621544e-05,
      "loss": 0.0857,
      "step": 7343
    },
    {
      "epoch": 1.8173719376391984,
      "grad_norm": 0.04639219865202904,
      "learning_rate": 1.0241707578159509e-05,
      "loss": 0.0275,
      "step": 7344
    },
    {
      "epoch": 1.817619401138332,
      "grad_norm": 0.03241933137178421,
      "learning_rate": 1.0214161511256954e-05,
      "loss": 0.0413,
      "step": 7345
    },
    {
      "epoch": 1.817866864637466,
      "grad_norm": 0.04836398363113403,
      "learning_rate": 1.0186651765619254e-05,
      "loss": 0.0683,
      "step": 7346
    },
    {
      "epoch": 1.8181143281366,
      "grad_norm": 0.053761567920446396,
      "learning_rate": 1.0159178345413411e-05,
      "loss": 0.0572,
      "step": 7347
    },
    {
      "epoch": 1.8183617916357337,
      "grad_norm": 0.06656338274478912,
      "learning_rate": 1.013174125480093e-05,
      "loss": 0.0631,
      "step": 7348
    },
    {
      "epoch": 1.8186092551348676,
      "grad_norm": 0.03413688763976097,
      "learning_rate": 1.010434049793782e-05,
      "loss": 0.0358,
      "step": 7349
    },
    {
      "epoch": 1.8188567186340014,
      "grad_norm": 0.05656271055340767,
      "learning_rate": 1.0076976078974536e-05,
      "loss": 0.0466,
      "step": 7350
    },
    {
      "epoch": 1.8191041821331355,
      "grad_norm": 0.04265027493238449,
      "learning_rate": 1.00496480020561e-05,
      "loss": 0.0454,
      "step": 7351
    },
    {
      "epoch": 1.8193516456322691,
      "grad_norm": 0.046240489929914474,
      "learning_rate": 1.0022356271322002e-05,
      "loss": 0.0634,
      "step": 7352
    },
    {
      "epoch": 1.8195991091314032,
      "grad_norm": 0.059017304331064224,
      "learning_rate": 9.995100890906244e-06,
      "loss": 0.0387,
      "step": 7353
    },
    {
      "epoch": 1.8198465726305368,
      "grad_norm": 0.04553908482193947,
      "learning_rate": 9.967881864937218e-06,
      "loss": 0.0511,
      "step": 7354
    },
    {
      "epoch": 1.820094036129671,
      "grad_norm": 0.07669675350189209,
      "learning_rate": 9.940699197537984e-06,
      "loss": 0.052,
      "step": 7355
    },
    {
      "epoch": 1.8203414996288048,
      "grad_norm": 0.037319913506507874,
      "learning_rate": 9.913552892825972e-06,
      "loss": 0.0428,
      "step": 7356
    },
    {
      "epoch": 1.8205889631279386,
      "grad_norm": 0.04701799526810646,
      "learning_rate": 9.886442954913144e-06,
      "loss": 0.0617,
      "step": 7357
    },
    {
      "epoch": 1.8208364266270725,
      "grad_norm": 0.04399771988391876,
      "learning_rate": 9.859369387905909e-06,
      "loss": 0.0374,
      "step": 7358
    },
    {
      "epoch": 1.8210838901262063,
      "grad_norm": 0.08417467027902603,
      "learning_rate": 9.832332195905209e-06,
      "loss": 0.092,
      "step": 7359
    },
    {
      "epoch": 1.8213313536253404,
      "grad_norm": 0.053644511848688126,
      "learning_rate": 9.805331383006515e-06,
      "loss": 0.0233,
      "step": 7360
    },
    {
      "epoch": 1.821578817124474,
      "grad_norm": 0.08954916894435883,
      "learning_rate": 9.778366953299727e-06,
      "loss": 0.0616,
      "step": 7361
    },
    {
      "epoch": 1.821826280623608,
      "grad_norm": 0.04890797287225723,
      "learning_rate": 9.751438910869159e-06,
      "loss": 0.0609,
      "step": 7362
    },
    {
      "epoch": 1.822073744122742,
      "grad_norm": 0.07142210006713867,
      "learning_rate": 9.724547259793853e-06,
      "loss": 0.0512,
      "step": 7363
    },
    {
      "epoch": 1.8223212076218758,
      "grad_norm": 0.07024213671684265,
      "learning_rate": 9.697692004147107e-06,
      "loss": 0.0562,
      "step": 7364
    },
    {
      "epoch": 1.8225686711210096,
      "grad_norm": 0.03159913420677185,
      "learning_rate": 9.670873147996751e-06,
      "loss": 0.0388,
      "step": 7365
    },
    {
      "epoch": 1.8228161346201435,
      "grad_norm": 0.03248516097664833,
      "learning_rate": 9.644090695405228e-06,
      "loss": 0.0274,
      "step": 7366
    },
    {
      "epoch": 1.8230635981192775,
      "grad_norm": 0.0445324070751667,
      "learning_rate": 9.617344650429294e-06,
      "loss": 0.0583,
      "step": 7367
    },
    {
      "epoch": 1.8233110616184112,
      "grad_norm": 0.040361784398555756,
      "learning_rate": 9.590635017120347e-06,
      "loss": 0.0288,
      "step": 7368
    },
    {
      "epoch": 1.8235585251175452,
      "grad_norm": 0.050056375563144684,
      "learning_rate": 9.563961799524152e-06,
      "loss": 0.0755,
      "step": 7369
    },
    {
      "epoch": 1.823805988616679,
      "grad_norm": 0.023672394454479218,
      "learning_rate": 9.537325001680974e-06,
      "loss": 0.0164,
      "step": 7370
    },
    {
      "epoch": 1.824053452115813,
      "grad_norm": 0.04498209059238434,
      "learning_rate": 9.510724627625672e-06,
      "loss": 0.0327,
      "step": 7371
    },
    {
      "epoch": 1.8243009156149468,
      "grad_norm": 0.03417222574353218,
      "learning_rate": 9.48416068138741e-06,
      "loss": 0.0467,
      "step": 7372
    },
    {
      "epoch": 1.8245483791140806,
      "grad_norm": 0.05093546584248543,
      "learning_rate": 9.457633166989999e-06,
      "loss": 0.0498,
      "step": 7373
    },
    {
      "epoch": 1.8247958426132147,
      "grad_norm": 0.13377854228019714,
      "learning_rate": 9.43114208845164e-06,
      "loss": 0.0728,
      "step": 7374
    },
    {
      "epoch": 1.8250433061123483,
      "grad_norm": 0.0571138896048069,
      "learning_rate": 9.404687449785038e-06,
      "loss": 0.065,
      "step": 7375
    },
    {
      "epoch": 1.8252907696114824,
      "grad_norm": 0.10876456648111343,
      "learning_rate": 9.378269254997407e-06,
      "loss": 0.0694,
      "step": 7376
    },
    {
      "epoch": 1.825538233110616,
      "grad_norm": 0.04800673574209213,
      "learning_rate": 9.351887508090324e-06,
      "loss": 0.0648,
      "step": 7377
    },
    {
      "epoch": 1.82578569660975,
      "grad_norm": 0.06654471904039383,
      "learning_rate": 9.325542213060007e-06,
      "loss": 0.0418,
      "step": 7378
    },
    {
      "epoch": 1.826033160108884,
      "grad_norm": 0.07833864539861679,
      "learning_rate": 9.299233373897042e-06,
      "loss": 0.0685,
      "step": 7379
    },
    {
      "epoch": 1.8262806236080178,
      "grad_norm": 0.04282144829630852,
      "learning_rate": 9.272960994586549e-06,
      "loss": 0.0383,
      "step": 7380
    },
    {
      "epoch": 1.8265280871071516,
      "grad_norm": 0.04786913841962814,
      "learning_rate": 9.246725079108092e-06,
      "loss": 0.0267,
      "step": 7381
    },
    {
      "epoch": 1.8267755506062855,
      "grad_norm": 0.0869610384106636,
      "learning_rate": 9.220525631435745e-06,
      "loss": 0.0572,
      "step": 7382
    },
    {
      "epoch": 1.8270230141054196,
      "grad_norm": 0.04584145173430443,
      "learning_rate": 9.194362655538024e-06,
      "loss": 0.0248,
      "step": 7383
    },
    {
      "epoch": 1.8272704776045532,
      "grad_norm": 0.046364571899175644,
      "learning_rate": 9.16823615537793e-06,
      "loss": 0.0372,
      "step": 7384
    },
    {
      "epoch": 1.8275179411036873,
      "grad_norm": 0.03266732767224312,
      "learning_rate": 9.142146134912932e-06,
      "loss": 0.0391,
      "step": 7385
    },
    {
      "epoch": 1.827765404602821,
      "grad_norm": 0.07328920811414719,
      "learning_rate": 9.116092598094982e-06,
      "loss": 0.0378,
      "step": 7386
    },
    {
      "epoch": 1.828012868101955,
      "grad_norm": 0.03303048387169838,
      "learning_rate": 9.090075548870536e-06,
      "loss": 0.0433,
      "step": 7387
    },
    {
      "epoch": 1.8282603316010888,
      "grad_norm": 0.07496772706508636,
      "learning_rate": 9.064094991180495e-06,
      "loss": 0.0666,
      "step": 7388
    },
    {
      "epoch": 1.8285077951002227,
      "grad_norm": 0.06530489027500153,
      "learning_rate": 9.038150928960153e-06,
      "loss": 0.0754,
      "step": 7389
    },
    {
      "epoch": 1.8287552585993567,
      "grad_norm": 0.036560311913490295,
      "learning_rate": 9.012243366139482e-06,
      "loss": 0.0572,
      "step": 7390
    },
    {
      "epoch": 1.8290027220984904,
      "grad_norm": 0.042295120656490326,
      "learning_rate": 8.986372306642731e-06,
      "loss": 0.047,
      "step": 7391
    },
    {
      "epoch": 1.8292501855976244,
      "grad_norm": 0.056054431945085526,
      "learning_rate": 8.960537754388681e-06,
      "loss": 0.0445,
      "step": 7392
    },
    {
      "epoch": 1.8294976490967583,
      "grad_norm": 0.0691569447517395,
      "learning_rate": 8.934739713290563e-06,
      "loss": 0.0921,
      "step": 7393
    },
    {
      "epoch": 1.8297451125958921,
      "grad_norm": 0.0493277832865715,
      "learning_rate": 8.908978187256168e-06,
      "loss": 0.0558,
      "step": 7394
    },
    {
      "epoch": 1.829992576095026,
      "grad_norm": 0.023096702992916107,
      "learning_rate": 8.88325318018765e-06,
      "loss": 0.0184,
      "step": 7395
    },
    {
      "epoch": 1.8302400395941598,
      "grad_norm": 0.0778515413403511,
      "learning_rate": 8.857564695981729e-06,
      "loss": 0.0688,
      "step": 7396
    },
    {
      "epoch": 1.8304875030932939,
      "grad_norm": 0.10889316350221634,
      "learning_rate": 8.83191273852943e-06,
      "loss": 0.1063,
      "step": 7397
    },
    {
      "epoch": 1.8307349665924275,
      "grad_norm": 0.0297943577170372,
      "learning_rate": 8.806297311716421e-06,
      "loss": 0.0364,
      "step": 7398
    },
    {
      "epoch": 1.8309824300915616,
      "grad_norm": 0.0854811891913414,
      "learning_rate": 8.780718419422822e-06,
      "loss": 0.079,
      "step": 7399
    },
    {
      "epoch": 1.8312298935906952,
      "grad_norm": 0.02824685163795948,
      "learning_rate": 8.755176065523064e-06,
      "loss": 0.0183,
      "step": 7400
    },
    {
      "epoch": 1.8312298935906952,
      "eval_loss": 0.2891279458999634,
      "eval_runtime": 202.6534,
      "eval_samples_per_second": 4.935,
      "eval_steps_per_second": 0.311,
      "step": 7400
    },
    {
      "epoch": 1.8314773570898293,
      "grad_norm": 0.07009227573871613,
      "learning_rate": 8.729670253886157e-06,
      "loss": 0.0884,
      "step": 7401
    },
    {
      "epoch": 1.8317248205889631,
      "grad_norm": 0.06936975568532944,
      "learning_rate": 8.7042009883756e-06,
      "loss": 0.1528,
      "step": 7402
    },
    {
      "epoch": 1.831972284088097,
      "grad_norm": 0.08527031540870667,
      "learning_rate": 8.67876827284933e-06,
      "loss": 0.113,
      "step": 7403
    },
    {
      "epoch": 1.8322197475872308,
      "grad_norm": 0.08435007929801941,
      "learning_rate": 8.653372111159712e-06,
      "loss": 0.0615,
      "step": 7404
    },
    {
      "epoch": 1.8324672110863647,
      "grad_norm": 0.10063052922487259,
      "learning_rate": 8.62801250715356e-06,
      "loss": 0.0843,
      "step": 7405
    },
    {
      "epoch": 1.8327146745854987,
      "grad_norm": 0.03691976144909859,
      "learning_rate": 8.60268946467227e-06,
      "loss": 0.0368,
      "step": 7406
    },
    {
      "epoch": 1.8329621380846324,
      "grad_norm": 0.0532575398683548,
      "learning_rate": 8.577402987551553e-06,
      "loss": 0.0551,
      "step": 7407
    },
    {
      "epoch": 1.8332096015837664,
      "grad_norm": 0.0651688352227211,
      "learning_rate": 8.552153079621677e-06,
      "loss": 0.0641,
      "step": 7408
    },
    {
      "epoch": 1.8334570650829003,
      "grad_norm": 0.04364800453186035,
      "learning_rate": 8.526939744707335e-06,
      "loss": 0.0246,
      "step": 7409
    },
    {
      "epoch": 1.8337045285820341,
      "grad_norm": 0.034438058733940125,
      "learning_rate": 8.501762986627665e-06,
      "loss": 0.0342,
      "step": 7410
    },
    {
      "epoch": 1.833951992081168,
      "grad_norm": 0.06759582459926605,
      "learning_rate": 8.476622809196366e-06,
      "loss": 0.0633,
      "step": 7411
    },
    {
      "epoch": 1.8341994555803018,
      "grad_norm": 0.04164959862828255,
      "learning_rate": 8.451519216221393e-06,
      "loss": 0.0391,
      "step": 7412
    },
    {
      "epoch": 1.834446919079436,
      "grad_norm": 0.04286382347345352,
      "learning_rate": 8.426452211505342e-06,
      "loss": 0.0358,
      "step": 7413
    },
    {
      "epoch": 1.8346943825785695,
      "grad_norm": 0.048311442136764526,
      "learning_rate": 8.401421798845232e-06,
      "loss": 0.0516,
      "step": 7414
    },
    {
      "epoch": 1.8349418460777036,
      "grad_norm": 0.04660134017467499,
      "learning_rate": 8.376427982032503e-06,
      "loss": 0.0411,
      "step": 7415
    },
    {
      "epoch": 1.8351893095768375,
      "grad_norm": 0.13075970113277435,
      "learning_rate": 8.351470764853042e-06,
      "loss": 0.0543,
      "step": 7416
    },
    {
      "epoch": 1.8354367730759713,
      "grad_norm": 0.06854059547185898,
      "learning_rate": 8.326550151087187e-06,
      "loss": 0.0681,
      "step": 7417
    },
    {
      "epoch": 1.8356842365751052,
      "grad_norm": 0.122287817299366,
      "learning_rate": 8.301666144509807e-06,
      "loss": 0.0958,
      "step": 7418
    },
    {
      "epoch": 1.835931700074239,
      "grad_norm": 0.09307213872671127,
      "learning_rate": 8.276818748890192e-06,
      "loss": 0.103,
      "step": 7419
    },
    {
      "epoch": 1.836179163573373,
      "grad_norm": 0.03277166932821274,
      "learning_rate": 8.252007967992026e-06,
      "loss": 0.029,
      "step": 7420
    },
    {
      "epoch": 1.8364266270725067,
      "grad_norm": 0.04491426423192024,
      "learning_rate": 8.227233805573497e-06,
      "loss": 0.0237,
      "step": 7421
    },
    {
      "epoch": 1.8366740905716408,
      "grad_norm": 0.04662778228521347,
      "learning_rate": 8.202496265387244e-06,
      "loss": 0.0602,
      "step": 7422
    },
    {
      "epoch": 1.8369215540707744,
      "grad_norm": 0.13289044797420502,
      "learning_rate": 8.17779535118035e-06,
      "loss": 0.0693,
      "step": 7423
    },
    {
      "epoch": 1.8371690175699085,
      "grad_norm": 0.033655837178230286,
      "learning_rate": 8.153131066694408e-06,
      "loss": 0.03,
      "step": 7424
    },
    {
      "epoch": 1.8374164810690423,
      "grad_norm": 0.04185694456100464,
      "learning_rate": 8.12850341566529e-06,
      "loss": 0.0397,
      "step": 7425
    },
    {
      "epoch": 1.8376639445681762,
      "grad_norm": 0.047438573092222214,
      "learning_rate": 8.103912401823594e-06,
      "loss": 0.0433,
      "step": 7426
    },
    {
      "epoch": 1.8379114080673102,
      "grad_norm": 0.05374589562416077,
      "learning_rate": 8.079358028894118e-06,
      "loss": 0.0418,
      "step": 7427
    },
    {
      "epoch": 1.8381588715664439,
      "grad_norm": 0.06706894934177399,
      "learning_rate": 8.054840300596195e-06,
      "loss": 0.0536,
      "step": 7428
    },
    {
      "epoch": 1.838406335065578,
      "grad_norm": 0.030620507895946503,
      "learning_rate": 8.030359220643658e-06,
      "loss": 0.0361,
      "step": 7429
    },
    {
      "epoch": 1.8386537985647116,
      "grad_norm": 0.03384210541844368,
      "learning_rate": 8.005914792744734e-06,
      "loss": 0.0426,
      "step": 7430
    },
    {
      "epoch": 1.8389012620638456,
      "grad_norm": 0.059380125254392624,
      "learning_rate": 7.981507020602158e-06,
      "loss": 0.0573,
      "step": 7431
    },
    {
      "epoch": 1.8391487255629795,
      "grad_norm": 0.03690684586763382,
      "learning_rate": 7.95713590791297e-06,
      "loss": 0.0424,
      "step": 7432
    },
    {
      "epoch": 1.8393961890621133,
      "grad_norm": 0.08224724233150482,
      "learning_rate": 7.932801458368805e-06,
      "loss": 0.1002,
      "step": 7433
    },
    {
      "epoch": 1.8396436525612472,
      "grad_norm": 0.0397791787981987,
      "learning_rate": 7.908503675655738e-06,
      "loss": 0.0502,
      "step": 7434
    },
    {
      "epoch": 1.839891116060381,
      "grad_norm": 0.03352278098464012,
      "learning_rate": 7.884242563454185e-06,
      "loss": 0.0277,
      "step": 7435
    },
    {
      "epoch": 1.840138579559515,
      "grad_norm": 0.04824693873524666,
      "learning_rate": 7.8600181254391e-06,
      "loss": 0.0472,
      "step": 7436
    },
    {
      "epoch": 1.8403860430586487,
      "grad_norm": 0.04573248326778412,
      "learning_rate": 7.835830365279822e-06,
      "loss": 0.0303,
      "step": 7437
    },
    {
      "epoch": 1.8406335065577828,
      "grad_norm": 0.08505718410015106,
      "learning_rate": 7.811679286640199e-06,
      "loss": 0.1097,
      "step": 7438
    },
    {
      "epoch": 1.8408809700569166,
      "grad_norm": 0.06338343024253845,
      "learning_rate": 7.787564893178473e-06,
      "loss": 0.0902,
      "step": 7439
    },
    {
      "epoch": 1.8411284335560505,
      "grad_norm": 0.0815533697605133,
      "learning_rate": 7.763487188547302e-06,
      "loss": 0.0846,
      "step": 7440
    },
    {
      "epoch": 1.8413758970551843,
      "grad_norm": 0.031920529901981354,
      "learning_rate": 7.739446176393883e-06,
      "loss": 0.0295,
      "step": 7441
    },
    {
      "epoch": 1.8416233605543182,
      "grad_norm": 0.05515197291970253,
      "learning_rate": 7.715441860359745e-06,
      "loss": 0.0623,
      "step": 7442
    },
    {
      "epoch": 1.8418708240534523,
      "grad_norm": 0.03176377713680267,
      "learning_rate": 7.691474244080982e-06,
      "loss": 0.0219,
      "step": 7443
    },
    {
      "epoch": 1.8421182875525859,
      "grad_norm": 0.056295763701200485,
      "learning_rate": 7.667543331187993e-06,
      "loss": 0.0824,
      "step": 7444
    },
    {
      "epoch": 1.84236575105172,
      "grad_norm": 0.06628788262605667,
      "learning_rate": 7.643649125305712e-06,
      "loss": 0.0954,
      "step": 7445
    },
    {
      "epoch": 1.8426132145508536,
      "grad_norm": 0.05758637934923172,
      "learning_rate": 7.619791630053524e-06,
      "loss": 0.0738,
      "step": 7446
    },
    {
      "epoch": 1.8428606780499877,
      "grad_norm": 0.05867580324411392,
      "learning_rate": 7.59597084904512e-06,
      "loss": 0.0611,
      "step": 7447
    },
    {
      "epoch": 1.8431081415491215,
      "grad_norm": 0.072573222219944,
      "learning_rate": 7.57218678588878e-06,
      "loss": 0.0504,
      "step": 7448
    },
    {
      "epoch": 1.8433556050482554,
      "grad_norm": 0.04841586947441101,
      "learning_rate": 7.548439444187177e-06,
      "loss": 0.0587,
      "step": 7449
    },
    {
      "epoch": 1.8436030685473894,
      "grad_norm": 0.037106264382600784,
      "learning_rate": 7.524728827537408e-06,
      "loss": 0.0304,
      "step": 7450
    },
    {
      "epoch": 1.843850532046523,
      "grad_norm": 0.06420557200908661,
      "learning_rate": 7.501054939530988e-06,
      "loss": 0.068,
      "step": 7451
    },
    {
      "epoch": 1.8440979955456571,
      "grad_norm": 0.03757541626691818,
      "learning_rate": 7.477417783753854e-06,
      "loss": 0.0478,
      "step": 7452
    },
    {
      "epoch": 1.8443454590447907,
      "grad_norm": 0.0794423371553421,
      "learning_rate": 7.453817363786502e-06,
      "loss": 0.0726,
      "step": 7453
    },
    {
      "epoch": 1.8445929225439248,
      "grad_norm": 0.046958718448877335,
      "learning_rate": 7.4302536832037695e-06,
      "loss": 0.0567,
      "step": 7454
    },
    {
      "epoch": 1.8448403860430587,
      "grad_norm": 0.04753933101892471,
      "learning_rate": 7.406726745574855e-06,
      "loss": 0.0379,
      "step": 7455
    },
    {
      "epoch": 1.8450878495421925,
      "grad_norm": 0.10042478144168854,
      "learning_rate": 7.383236554463519e-06,
      "loss": 0.0693,
      "step": 7456
    },
    {
      "epoch": 1.8453353130413264,
      "grad_norm": 0.04778994619846344,
      "learning_rate": 7.359783113427887e-06,
      "loss": 0.0416,
      "step": 7457
    },
    {
      "epoch": 1.8455827765404602,
      "grad_norm": 0.06354261934757233,
      "learning_rate": 7.336366426020591e-06,
      "loss": 0.0721,
      "step": 7458
    },
    {
      "epoch": 1.8458302400395943,
      "grad_norm": 0.03735865280032158,
      "learning_rate": 7.312986495788654e-06,
      "loss": 0.0372,
      "step": 7459
    },
    {
      "epoch": 1.846077703538728,
      "grad_norm": 0.09641256183385849,
      "learning_rate": 7.28964332627341e-06,
      "loss": 0.0681,
      "step": 7460
    },
    {
      "epoch": 1.846325167037862,
      "grad_norm": 0.03803414851427078,
      "learning_rate": 7.266336921010863e-06,
      "loss": 0.0501,
      "step": 7461
    },
    {
      "epoch": 1.8465726305369958,
      "grad_norm": 0.0315077006816864,
      "learning_rate": 7.2430672835312465e-06,
      "loss": 0.0597,
      "step": 7462
    },
    {
      "epoch": 1.8468200940361297,
      "grad_norm": 0.023054374381899834,
      "learning_rate": 7.219834417359322e-06,
      "loss": 0.0268,
      "step": 7463
    },
    {
      "epoch": 1.8470675575352635,
      "grad_norm": 0.057030681520700455,
      "learning_rate": 7.196638326014249e-06,
      "loss": 0.0476,
      "step": 7464
    },
    {
      "epoch": 1.8473150210343974,
      "grad_norm": 0.04488731548190117,
      "learning_rate": 7.1734790130096596e-06,
      "loss": 0.0274,
      "step": 7465
    },
    {
      "epoch": 1.8475624845335314,
      "grad_norm": 0.049201760441064835,
      "learning_rate": 7.1503564818535815e-06,
      "loss": 0.025,
      "step": 7466
    },
    {
      "epoch": 1.847809948032665,
      "grad_norm": 0.04004664719104767,
      "learning_rate": 7.127270736048408e-06,
      "loss": 0.0391,
      "step": 7467
    },
    {
      "epoch": 1.8480574115317991,
      "grad_norm": 0.022537119686603546,
      "learning_rate": 7.104221779091064e-06,
      "loss": 0.0207,
      "step": 7468
    },
    {
      "epoch": 1.848304875030933,
      "grad_norm": 0.03505223989486694,
      "learning_rate": 7.081209614472895e-06,
      "loss": 0.0352,
      "step": 7469
    },
    {
      "epoch": 1.8485523385300668,
      "grad_norm": 0.05123233050107956,
      "learning_rate": 7.058234245679612e-06,
      "loss": 0.0502,
      "step": 7470
    },
    {
      "epoch": 1.8487998020292007,
      "grad_norm": 0.038529276847839355,
      "learning_rate": 7.035295676191378e-06,
      "loss": 0.0533,
      "step": 7471
    },
    {
      "epoch": 1.8490472655283345,
      "grad_norm": 0.04818253964185715,
      "learning_rate": 7.012393909482745e-06,
      "loss": 0.0413,
      "step": 7472
    },
    {
      "epoch": 1.8492947290274686,
      "grad_norm": 0.05452867969870567,
      "learning_rate": 6.9895289490228e-06,
      "loss": 0.049,
      "step": 7473
    },
    {
      "epoch": 1.8495421925266022,
      "grad_norm": 0.03859270364046097,
      "learning_rate": 6.966700798274994e-06,
      "loss": 0.0348,
      "step": 7474
    },
    {
      "epoch": 1.8497896560257363,
      "grad_norm": 0.07128206640481949,
      "learning_rate": 6.943909460697118e-06,
      "loss": 0.0864,
      "step": 7475
    },
    {
      "epoch": 1.85003711952487,
      "grad_norm": 0.048697639256715775,
      "learning_rate": 6.921154939741492e-06,
      "loss": 0.0393,
      "step": 7476
    },
    {
      "epoch": 1.850284583024004,
      "grad_norm": 0.05555305629968643,
      "learning_rate": 6.898437238854832e-06,
      "loss": 0.0817,
      "step": 7477
    },
    {
      "epoch": 1.8505320465231379,
      "grad_norm": 0.04907985031604767,
      "learning_rate": 6.8757563614782745e-06,
      "loss": 0.0476,
      "step": 7478
    },
    {
      "epoch": 1.8507795100222717,
      "grad_norm": 0.04228372871875763,
      "learning_rate": 6.853112311047432e-06,
      "loss": 0.0342,
      "step": 7479
    },
    {
      "epoch": 1.8510269735214055,
      "grad_norm": 0.03354721888899803,
      "learning_rate": 6.8305050909921455e-06,
      "loss": 0.0353,
      "step": 7480
    },
    {
      "epoch": 1.8512744370205394,
      "grad_norm": 0.03309810161590576,
      "learning_rate": 6.807934704736979e-06,
      "loss": 0.0408,
      "step": 7481
    },
    {
      "epoch": 1.8515219005196735,
      "grad_norm": 0.03158363327383995,
      "learning_rate": 6.785401155700643e-06,
      "loss": 0.0308,
      "step": 7482
    },
    {
      "epoch": 1.851769364018807,
      "grad_norm": 0.11475298553705215,
      "learning_rate": 6.762904447296409e-06,
      "loss": 0.0977,
      "step": 7483
    },
    {
      "epoch": 1.8520168275179412,
      "grad_norm": 0.03531269729137421,
      "learning_rate": 6.740444582931965e-06,
      "loss": 0.0277,
      "step": 7484
    },
    {
      "epoch": 1.852264291017075,
      "grad_norm": 0.059496719390153885,
      "learning_rate": 6.7180215660093415e-06,
      "loss": 0.0445,
      "step": 7485
    },
    {
      "epoch": 1.8525117545162089,
      "grad_norm": 0.09274805337190628,
      "learning_rate": 6.695635399925126e-06,
      "loss": 0.0787,
      "step": 7486
    },
    {
      "epoch": 1.8527592180153427,
      "grad_norm": 0.05103573575615883,
      "learning_rate": 6.673286088070108e-06,
      "loss": 0.0158,
      "step": 7487
    },
    {
      "epoch": 1.8530066815144766,
      "grad_norm": 0.04900147765874863,
      "learning_rate": 6.650973633829688e-06,
      "loss": 0.0333,
      "step": 7488
    },
    {
      "epoch": 1.8532541450136106,
      "grad_norm": 0.03918915614485741,
      "learning_rate": 6.6286980405836636e-06,
      "loss": 0.0516,
      "step": 7489
    },
    {
      "epoch": 1.8535016085127443,
      "grad_norm": 0.06266871839761734,
      "learning_rate": 6.606459311706142e-06,
      "loss": 0.0853,
      "step": 7490
    },
    {
      "epoch": 1.8537490720118783,
      "grad_norm": 0.0574411116540432,
      "learning_rate": 6.5842574505657064e-06,
      "loss": 0.0755,
      "step": 7491
    },
    {
      "epoch": 1.8539965355110122,
      "grad_norm": 0.03165441378951073,
      "learning_rate": 6.562092460525387e-06,
      "loss": 0.0424,
      "step": 7492
    },
    {
      "epoch": 1.854243999010146,
      "grad_norm": 0.03679540008306503,
      "learning_rate": 6.539964344942584e-06,
      "loss": 0.0407,
      "step": 7493
    },
    {
      "epoch": 1.8544914625092799,
      "grad_norm": 0.02749938890337944,
      "learning_rate": 6.517873107169142e-06,
      "loss": 0.0292,
      "step": 7494
    },
    {
      "epoch": 1.8547389260084137,
      "grad_norm": 0.06147104874253273,
      "learning_rate": 6.495818750551275e-06,
      "loss": 0.0597,
      "step": 7495
    },
    {
      "epoch": 1.8549863895075478,
      "grad_norm": 0.03912568464875221,
      "learning_rate": 6.473801278429642e-06,
      "loss": 0.0493,
      "step": 7496
    },
    {
      "epoch": 1.8552338530066814,
      "grad_norm": 0.09471269696950912,
      "learning_rate": 6.4518206941393264e-06,
      "loss": 0.065,
      "step": 7497
    },
    {
      "epoch": 1.8554813165058155,
      "grad_norm": 0.035039424896240234,
      "learning_rate": 6.429877001009804e-06,
      "loss": 0.0657,
      "step": 7498
    },
    {
      "epoch": 1.8557287800049491,
      "grad_norm": 0.05408673733472824,
      "learning_rate": 6.407970202364971e-06,
      "loss": 0.0633,
      "step": 7499
    },
    {
      "epoch": 1.8559762435040832,
      "grad_norm": 0.05391266942024231,
      "learning_rate": 6.386100301523145e-06,
      "loss": 0.0578,
      "step": 7500
    },
    {
      "epoch": 1.856223707003217,
      "grad_norm": 0.044913653284311295,
      "learning_rate": 6.364267301797039e-06,
      "loss": 0.0655,
      "step": 7501
    },
    {
      "epoch": 1.8564711705023509,
      "grad_norm": 0.0478881374001503,
      "learning_rate": 6.342471206493783e-06,
      "loss": 0.0333,
      "step": 7502
    },
    {
      "epoch": 1.8567186340014847,
      "grad_norm": 0.06427546590566635,
      "learning_rate": 6.320712018914876e-06,
      "loss": 0.0507,
      "step": 7503
    },
    {
      "epoch": 1.8569660975006186,
      "grad_norm": 0.06418422609567642,
      "learning_rate": 6.298989742356292e-06,
      "loss": 0.058,
      "step": 7504
    },
    {
      "epoch": 1.8572135609997527,
      "grad_norm": 0.042559828609228134,
      "learning_rate": 6.277304380108373e-06,
      "loss": 0.0472,
      "step": 7505
    },
    {
      "epoch": 1.8574610244988863,
      "grad_norm": 0.05286021903157234,
      "learning_rate": 6.255655935455934e-06,
      "loss": 0.0357,
      "step": 7506
    },
    {
      "epoch": 1.8577084879980204,
      "grad_norm": 0.045196279883384705,
      "learning_rate": 6.234044411678075e-06,
      "loss": 0.0312,
      "step": 7507
    },
    {
      "epoch": 1.8579559514971542,
      "grad_norm": 0.048250965774059296,
      "learning_rate": 6.212469812048399e-06,
      "loss": 0.0577,
      "step": 7508
    },
    {
      "epoch": 1.858203414996288,
      "grad_norm": 0.06451728194952011,
      "learning_rate": 6.1909321398349594e-06,
      "loss": 0.0579,
      "step": 7509
    },
    {
      "epoch": 1.858450878495422,
      "grad_norm": 0.05844089761376381,
      "learning_rate": 6.169431398300035e-06,
      "loss": 0.0621,
      "step": 7510
    },
    {
      "epoch": 1.8586983419945557,
      "grad_norm": 0.044476911425590515,
      "learning_rate": 6.147967590700493e-06,
      "loss": 0.0549,
      "step": 7511
    },
    {
      "epoch": 1.8589458054936898,
      "grad_norm": 0.034333955496549606,
      "learning_rate": 6.12654072028751e-06,
      "loss": 0.0528,
      "step": 7512
    },
    {
      "epoch": 1.8591932689928234,
      "grad_norm": 0.04598958417773247,
      "learning_rate": 6.10515079030674e-06,
      "loss": 0.0442,
      "step": 7513
    },
    {
      "epoch": 1.8594407324919575,
      "grad_norm": 0.08668813854455948,
      "learning_rate": 6.083797803998148e-06,
      "loss": 0.0608,
      "step": 7514
    },
    {
      "epoch": 1.8596881959910914,
      "grad_norm": 0.07891910523176193,
      "learning_rate": 6.062481764596145e-06,
      "loss": 0.0512,
      "step": 7515
    },
    {
      "epoch": 1.8599356594902252,
      "grad_norm": 0.04997531697154045,
      "learning_rate": 6.041202675329594e-06,
      "loss": 0.0489,
      "step": 7516
    },
    {
      "epoch": 1.860183122989359,
      "grad_norm": 0.044451240450143814,
      "learning_rate": 6.019960539421721e-06,
      "loss": 0.0375,
      "step": 7517
    },
    {
      "epoch": 1.860430586488493,
      "grad_norm": 0.0441395565867424,
      "learning_rate": 5.998755360090091e-06,
      "loss": 0.0456,
      "step": 7518
    },
    {
      "epoch": 1.860678049987627,
      "grad_norm": 0.05388534069061279,
      "learning_rate": 5.977587140546803e-06,
      "loss": 0.0545,
      "step": 7519
    },
    {
      "epoch": 1.8609255134867606,
      "grad_norm": 0.06078450754284859,
      "learning_rate": 5.956455883998235e-06,
      "loss": 0.0726,
      "step": 7520
    },
    {
      "epoch": 1.8611729769858947,
      "grad_norm": 0.07347451895475388,
      "learning_rate": 5.935361593645244e-06,
      "loss": 0.0564,
      "step": 7521
    },
    {
      "epoch": 1.8614204404850283,
      "grad_norm": 0.038599446415901184,
      "learning_rate": 5.91430427268308e-06,
      "loss": 0.0533,
      "step": 7522
    },
    {
      "epoch": 1.8616679039841624,
      "grad_norm": 0.03989174962043762,
      "learning_rate": 5.893283924301301e-06,
      "loss": 0.0531,
      "step": 7523
    },
    {
      "epoch": 1.8619153674832962,
      "grad_norm": 0.049860306084156036,
      "learning_rate": 5.872300551684029e-06,
      "loss": 0.0542,
      "step": 7524
    },
    {
      "epoch": 1.86216283098243,
      "grad_norm": 0.06583057343959808,
      "learning_rate": 5.851354158009636e-06,
      "loss": 0.0627,
      "step": 7525
    },
    {
      "epoch": 1.862410294481564,
      "grad_norm": 0.05192680284380913,
      "learning_rate": 5.830444746450974e-06,
      "loss": 0.0726,
      "step": 7526
    },
    {
      "epoch": 1.8626577579806978,
      "grad_norm": 0.07022417336702347,
      "learning_rate": 5.80957232017526e-06,
      "loss": 0.0494,
      "step": 7527
    },
    {
      "epoch": 1.8629052214798318,
      "grad_norm": 0.06895323842763901,
      "learning_rate": 5.788736882344131e-06,
      "loss": 0.0734,
      "step": 7528
    },
    {
      "epoch": 1.8631526849789655,
      "grad_norm": 0.06038200110197067,
      "learning_rate": 5.767938436113618e-06,
      "loss": 0.0571,
      "step": 7529
    },
    {
      "epoch": 1.8634001484780995,
      "grad_norm": 0.026596449315547943,
      "learning_rate": 5.747176984634117e-06,
      "loss": 0.033,
      "step": 7530
    },
    {
      "epoch": 1.8636476119772334,
      "grad_norm": 0.045686960220336914,
      "learning_rate": 5.726452531050419e-06,
      "loss": 0.0374,
      "step": 7531
    },
    {
      "epoch": 1.8638950754763672,
      "grad_norm": 0.030146818608045578,
      "learning_rate": 5.705765078501818e-06,
      "loss": 0.0387,
      "step": 7532
    },
    {
      "epoch": 1.864142538975501,
      "grad_norm": 0.031221216544508934,
      "learning_rate": 5.6851146301218625e-06,
      "loss": 0.0324,
      "step": 7533
    },
    {
      "epoch": 1.864390002474635,
      "grad_norm": 0.06500552594661713,
      "learning_rate": 5.6645011890385516e-06,
      "loss": 0.0545,
      "step": 7534
    },
    {
      "epoch": 1.864637465973769,
      "grad_norm": 0.05161606892943382,
      "learning_rate": 5.643924758374275e-06,
      "loss": 0.0433,
      "step": 7535
    },
    {
      "epoch": 1.8648849294729026,
      "grad_norm": 0.03743869438767433,
      "learning_rate": 5.623385341245846e-06,
      "loss": 0.0424,
      "step": 7536
    },
    {
      "epoch": 1.8651323929720367,
      "grad_norm": 0.047392338514328,
      "learning_rate": 5.602882940764442e-06,
      "loss": 0.0477,
      "step": 7537
    },
    {
      "epoch": 1.8653798564711705,
      "grad_norm": 0.05004068836569786,
      "learning_rate": 5.582417560035635e-06,
      "loss": 0.0264,
      "step": 7538
    },
    {
      "epoch": 1.8656273199703044,
      "grad_norm": 0.062354374676942825,
      "learning_rate": 5.561989202159417e-06,
      "loss": 0.0519,
      "step": 7539
    },
    {
      "epoch": 1.8658747834694382,
      "grad_norm": 0.03608753904700279,
      "learning_rate": 5.541597870230092e-06,
      "loss": 0.0368,
      "step": 7540
    },
    {
      "epoch": 1.866122246968572,
      "grad_norm": 0.04883953556418419,
      "learning_rate": 5.521243567336465e-06,
      "loss": 0.062,
      "step": 7541
    },
    {
      "epoch": 1.8663697104677062,
      "grad_norm": 0.05757565423846245,
      "learning_rate": 5.500926296561681e-06,
      "loss": 0.0553,
      "step": 7542
    },
    {
      "epoch": 1.8666171739668398,
      "grad_norm": 0.058589402586221695,
      "learning_rate": 5.480646060983196e-06,
      "loss": 0.0686,
      "step": 7543
    },
    {
      "epoch": 1.8668646374659739,
      "grad_norm": 0.05317412316799164,
      "learning_rate": 5.4604028636730526e-06,
      "loss": 0.0932,
      "step": 7544
    },
    {
      "epoch": 1.8671121009651075,
      "grad_norm": 0.03982970118522644,
      "learning_rate": 5.440196707697465e-06,
      "loss": 0.0436,
      "step": 7545
    },
    {
      "epoch": 1.8673595644642416,
      "grad_norm": 0.055643923580646515,
      "learning_rate": 5.420027596117177e-06,
      "loss": 0.0592,
      "step": 7546
    },
    {
      "epoch": 1.8676070279633754,
      "grad_norm": 0.0643874853849411,
      "learning_rate": 5.399895531987276e-06,
      "loss": 0.039,
      "step": 7547
    },
    {
      "epoch": 1.8678544914625093,
      "grad_norm": 0.05825376138091087,
      "learning_rate": 5.379800518357236e-06,
      "loss": 0.0518,
      "step": 7548
    },
    {
      "epoch": 1.868101954961643,
      "grad_norm": 0.06342805922031403,
      "learning_rate": 5.359742558270958e-06,
      "loss": 0.0593,
      "step": 7549
    },
    {
      "epoch": 1.868349418460777,
      "grad_norm": 0.10555082559585571,
      "learning_rate": 5.339721654766649e-06,
      "loss": 0.1218,
      "step": 7550
    },
    {
      "epoch": 1.868596881959911,
      "grad_norm": 0.046801112592220306,
      "learning_rate": 5.31973781087694e-06,
      "loss": 0.0418,
      "step": 7551
    },
    {
      "epoch": 1.8688443454590447,
      "grad_norm": 0.051150668412446976,
      "learning_rate": 5.299791029628936e-06,
      "loss": 0.0415,
      "step": 7552
    },
    {
      "epoch": 1.8690918089581787,
      "grad_norm": 0.13200634717941284,
      "learning_rate": 5.279881314044e-06,
      "loss": 0.0923,
      "step": 7553
    },
    {
      "epoch": 1.8693392724573126,
      "grad_norm": 0.06823354214429855,
      "learning_rate": 5.260008667137911e-06,
      "loss": 0.0728,
      "step": 7554
    },
    {
      "epoch": 1.8695867359564464,
      "grad_norm": 0.03974262252449989,
      "learning_rate": 5.240173091920902e-06,
      "loss": 0.0492,
      "step": 7555
    },
    {
      "epoch": 1.8698341994555803,
      "grad_norm": 0.0520242378115654,
      "learning_rate": 5.220374591397486e-06,
      "loss": 0.0435,
      "step": 7556
    },
    {
      "epoch": 1.8700816629547141,
      "grad_norm": 0.03871981054544449,
      "learning_rate": 5.200613168566709e-06,
      "loss": 0.0531,
      "step": 7557
    },
    {
      "epoch": 1.8703291264538482,
      "grad_norm": 0.07008141279220581,
      "learning_rate": 5.1808888264218145e-06,
      "loss": 0.0983,
      "step": 7558
    },
    {
      "epoch": 1.8705765899529818,
      "grad_norm": 0.05226900056004524,
      "learning_rate": 5.161201567950552e-06,
      "loss": 0.0769,
      "step": 7559
    },
    {
      "epoch": 1.8708240534521159,
      "grad_norm": 0.047158315777778625,
      "learning_rate": 5.141551396135036e-06,
      "loss": 0.0471,
      "step": 7560
    },
    {
      "epoch": 1.8710715169512497,
      "grad_norm": 0.07972398400306702,
      "learning_rate": 5.121938313951746e-06,
      "loss": 0.0873,
      "step": 7561
    },
    {
      "epoch": 1.8713189804503836,
      "grad_norm": 0.059752125293016434,
      "learning_rate": 5.102362324371557e-06,
      "loss": 0.0502,
      "step": 7562
    },
    {
      "epoch": 1.8715664439495174,
      "grad_norm": 0.030958445742726326,
      "learning_rate": 5.0828234303596785e-06,
      "loss": 0.0514,
      "step": 7563
    },
    {
      "epoch": 1.8718139074486513,
      "grad_norm": 0.0555870346724987,
      "learning_rate": 5.0633216348758285e-06,
      "loss": 0.0332,
      "step": 7564
    },
    {
      "epoch": 1.8720613709477854,
      "grad_norm": 0.05505039170384407,
      "learning_rate": 5.04385694087392e-06,
      "loss": 0.0277,
      "step": 7565
    },
    {
      "epoch": 1.872308834446919,
      "grad_norm": 0.06256188452243805,
      "learning_rate": 5.024429351302401e-06,
      "loss": 0.0548,
      "step": 7566
    },
    {
      "epoch": 1.872556297946053,
      "grad_norm": 0.04834381863474846,
      "learning_rate": 5.005038869104028e-06,
      "loss": 0.0635,
      "step": 7567
    },
    {
      "epoch": 1.8728037614451867,
      "grad_norm": 0.06838046759366989,
      "learning_rate": 4.9856854972159525e-06,
      "loss": 0.0416,
      "step": 7568
    },
    {
      "epoch": 1.8730512249443207,
      "grad_norm": 0.05372393876314163,
      "learning_rate": 4.96636923856969e-06,
      "loss": 0.0528,
      "step": 7569
    },
    {
      "epoch": 1.8732986884434546,
      "grad_norm": 0.04290023073554039,
      "learning_rate": 4.94709009609115e-06,
      "loss": 0.0544,
      "step": 7570
    },
    {
      "epoch": 1.8735461519425884,
      "grad_norm": 0.14646437764167786,
      "learning_rate": 4.9278480727006084e-06,
      "loss": 0.0412,
      "step": 7571
    },
    {
      "epoch": 1.8737936154417223,
      "grad_norm": 0.04801396280527115,
      "learning_rate": 4.908643171312788e-06,
      "loss": 0.0502,
      "step": 7572
    },
    {
      "epoch": 1.8740410789408561,
      "grad_norm": 0.032707445323467255,
      "learning_rate": 4.889475394836668e-06,
      "loss": 0.0201,
      "step": 7573
    },
    {
      "epoch": 1.8742885424399902,
      "grad_norm": 0.03885612264275551,
      "learning_rate": 4.870344746175648e-06,
      "loss": 0.0297,
      "step": 7574
    },
    {
      "epoch": 1.8745360059391238,
      "grad_norm": 0.042468149214982986,
      "learning_rate": 4.851251228227577e-06,
      "loss": 0.0321,
      "step": 7575
    },
    {
      "epoch": 1.874783469438258,
      "grad_norm": 0.06360800564289093,
      "learning_rate": 4.8321948438845575e-06,
      "loss": 0.0763,
      "step": 7576
    },
    {
      "epoch": 1.8750309329373918,
      "grad_norm": 0.03256841376423836,
      "learning_rate": 4.813175596033226e-06,
      "loss": 0.0342,
      "step": 7577
    },
    {
      "epoch": 1.8752783964365256,
      "grad_norm": 0.03331497684121132,
      "learning_rate": 4.794193487554388e-06,
      "loss": 0.0555,
      "step": 7578
    },
    {
      "epoch": 1.8755258599356595,
      "grad_norm": 0.04622378200292587,
      "learning_rate": 4.775248521323411e-06,
      "loss": 0.0575,
      "step": 7579
    },
    {
      "epoch": 1.8757733234347933,
      "grad_norm": 0.08717469125986099,
      "learning_rate": 4.756340700209943e-06,
      "loss": 0.0704,
      "step": 7580
    },
    {
      "epoch": 1.8760207869339274,
      "grad_norm": 0.024095013737678528,
      "learning_rate": 4.7374700270780005e-06,
      "loss": 0.023,
      "step": 7581
    },
    {
      "epoch": 1.876268250433061,
      "grad_norm": 0.06058496609330177,
      "learning_rate": 4.718636504786017e-06,
      "loss": 0.0538,
      "step": 7582
    },
    {
      "epoch": 1.876515713932195,
      "grad_norm": 0.04709480330348015,
      "learning_rate": 4.699840136186767e-06,
      "loss": 0.0613,
      "step": 7583
    },
    {
      "epoch": 1.876763177431329,
      "grad_norm": 0.03795480728149414,
      "learning_rate": 4.681080924127445e-06,
      "loss": 0.037,
      "step": 7584
    },
    {
      "epoch": 1.8770106409304628,
      "grad_norm": 0.04376330226659775,
      "learning_rate": 4.662358871449529e-06,
      "loss": 0.0339,
      "step": 7585
    },
    {
      "epoch": 1.8772581044295966,
      "grad_norm": 0.056464001536369324,
      "learning_rate": 4.643673980988916e-06,
      "loss": 0.0448,
      "step": 7586
    },
    {
      "epoch": 1.8775055679287305,
      "grad_norm": 0.056646574288606644,
      "learning_rate": 4.625026255575926e-06,
      "loss": 0.1069,
      "step": 7587
    },
    {
      "epoch": 1.8777530314278645,
      "grad_norm": 0.03877817839384079,
      "learning_rate": 4.60641569803516e-06,
      "loss": 0.0443,
      "step": 7588
    },
    {
      "epoch": 1.8780004949269982,
      "grad_norm": 0.028845908120274544,
      "learning_rate": 4.587842311185642e-06,
      "loss": 0.0219,
      "step": 7589
    },
    {
      "epoch": 1.8782479584261322,
      "grad_norm": 0.07992556691169739,
      "learning_rate": 4.569306097840759e-06,
      "loss": 0.0859,
      "step": 7590
    },
    {
      "epoch": 1.8784954219252659,
      "grad_norm": 0.054207943379879,
      "learning_rate": 4.5508070608082375e-06,
      "loss": 0.0556,
      "step": 7591
    },
    {
      "epoch": 1.8787428854244,
      "grad_norm": 0.045938555151224136,
      "learning_rate": 4.532345202890254e-06,
      "loss": 0.0482,
      "step": 7592
    },
    {
      "epoch": 1.8789903489235338,
      "grad_norm": 0.08824965357780457,
      "learning_rate": 4.513920526883236e-06,
      "loss": 0.0921,
      "step": 7593
    },
    {
      "epoch": 1.8792378124226676,
      "grad_norm": 0.067524753510952,
      "learning_rate": 4.4955330355780365e-06,
      "loss": 0.0445,
      "step": 7594
    },
    {
      "epoch": 1.8794852759218015,
      "grad_norm": 0.08204108476638794,
      "learning_rate": 4.477182731759927e-06,
      "loss": 0.08,
      "step": 7595
    },
    {
      "epoch": 1.8797327394209353,
      "grad_norm": 0.047089509665966034,
      "learning_rate": 4.458869618208461e-06,
      "loss": 0.0506,
      "step": 7596
    },
    {
      "epoch": 1.8799802029200694,
      "grad_norm": 0.07095105201005936,
      "learning_rate": 4.440593697697587e-06,
      "loss": 0.0519,
      "step": 7597
    },
    {
      "epoch": 1.880227666419203,
      "grad_norm": 0.06958702951669693,
      "learning_rate": 4.422354972995646e-06,
      "loss": 0.062,
      "step": 7598
    },
    {
      "epoch": 1.880475129918337,
      "grad_norm": 0.05062929540872574,
      "learning_rate": 4.404153446865372e-06,
      "loss": 0.0322,
      "step": 7599
    },
    {
      "epoch": 1.880722593417471,
      "grad_norm": 0.05665326490998268,
      "learning_rate": 4.3859891220637275e-06,
      "loss": 0.0517,
      "step": 7600
    },
    {
      "epoch": 1.880722593417471,
      "eval_loss": 0.289047509431839,
      "eval_runtime": 202.5635,
      "eval_samples_per_second": 4.937,
      "eval_steps_per_second": 0.311,
      "step": 7600
    },
    {
      "epoch": 1.8809700569166048,
      "grad_norm": 0.04412456229329109,
      "learning_rate": 4.367862001342177e-06,
      "loss": 0.056,
      "step": 7601
    },
    {
      "epoch": 1.8812175204157386,
      "grad_norm": 0.046805739402770996,
      "learning_rate": 4.349772087446524e-06,
      "loss": 0.0341,
      "step": 7602
    },
    {
      "epoch": 1.8814649839148725,
      "grad_norm": 0.05177785083651543,
      "learning_rate": 4.331719383116883e-06,
      "loss": 0.055,
      "step": 7603
    },
    {
      "epoch": 1.8817124474140066,
      "grad_norm": 0.08468236774206161,
      "learning_rate": 4.313703891087789e-06,
      "loss": 0.0409,
      "step": 7604
    },
    {
      "epoch": 1.8819599109131402,
      "grad_norm": 0.07404907792806625,
      "learning_rate": 4.2957256140880586e-06,
      "loss": 0.0685,
      "step": 7605
    },
    {
      "epoch": 1.8822073744122743,
      "grad_norm": 0.04474400356411934,
      "learning_rate": 4.2777845548410135e-06,
      "loss": 0.0367,
      "step": 7606
    },
    {
      "epoch": 1.882454837911408,
      "grad_norm": 0.0767245963215828,
      "learning_rate": 4.259880716064229e-06,
      "loss": 0.0579,
      "step": 7607
    },
    {
      "epoch": 1.882702301410542,
      "grad_norm": 0.034988004714250565,
      "learning_rate": 4.2420141004696214e-06,
      "loss": 0.0326,
      "step": 7608
    },
    {
      "epoch": 1.8829497649096758,
      "grad_norm": 0.09135949611663818,
      "learning_rate": 4.224184710763551e-06,
      "loss": 0.089,
      "step": 7609
    },
    {
      "epoch": 1.8831972284088097,
      "grad_norm": 0.07418923079967499,
      "learning_rate": 4.206392549646693e-06,
      "loss": 0.076,
      "step": 7610
    },
    {
      "epoch": 1.8834446919079437,
      "grad_norm": 0.045927196741104126,
      "learning_rate": 4.1886376198141115e-06,
      "loss": 0.034,
      "step": 7611
    },
    {
      "epoch": 1.8836921554070774,
      "grad_norm": 0.04230985417962074,
      "learning_rate": 4.170919923955185e-06,
      "loss": 0.0341,
      "step": 7612
    },
    {
      "epoch": 1.8839396189062114,
      "grad_norm": 0.039582256227731705,
      "learning_rate": 4.1532394647536835e-06,
      "loss": 0.0606,
      "step": 7613
    },
    {
      "epoch": 1.884187082405345,
      "grad_norm": 0.03500218316912651,
      "learning_rate": 4.135596244887796e-06,
      "loss": 0.0283,
      "step": 7614
    },
    {
      "epoch": 1.8844345459044791,
      "grad_norm": 0.055580154061317444,
      "learning_rate": 4.1179902670298865e-06,
      "loss": 0.0665,
      "step": 7615
    },
    {
      "epoch": 1.884682009403613,
      "grad_norm": 0.06575755029916763,
      "learning_rate": 4.100421533846904e-06,
      "loss": 0.0857,
      "step": 7616
    },
    {
      "epoch": 1.8849294729027468,
      "grad_norm": 0.061541877686977386,
      "learning_rate": 4.082890047999999e-06,
      "loss": 0.0694,
      "step": 7617
    },
    {
      "epoch": 1.8851769364018809,
      "grad_norm": 0.06089026480913162,
      "learning_rate": 4.065395812144768e-06,
      "loss": 0.0591,
      "step": 7618
    },
    {
      "epoch": 1.8854243999010145,
      "grad_norm": 0.12967300415039062,
      "learning_rate": 4.047938828931091e-06,
      "loss": 0.0767,
      "step": 7619
    },
    {
      "epoch": 1.8856718634001486,
      "grad_norm": 0.20439183712005615,
      "learning_rate": 4.030519101003272e-06,
      "loss": 0.0962,
      "step": 7620
    },
    {
      "epoch": 1.8859193268992822,
      "grad_norm": 0.030804961919784546,
      "learning_rate": 4.013136630999892e-06,
      "loss": 0.0458,
      "step": 7621
    },
    {
      "epoch": 1.8861667903984163,
      "grad_norm": 0.06190332770347595,
      "learning_rate": 3.995791421554013e-06,
      "loss": 0.0463,
      "step": 7622
    },
    {
      "epoch": 1.8864142538975501,
      "grad_norm": 0.0677449107170105,
      "learning_rate": 3.978483475292921e-06,
      "loss": 0.0979,
      "step": 7623
    },
    {
      "epoch": 1.886661717396684,
      "grad_norm": 0.053043339401483536,
      "learning_rate": 3.961212794838353e-06,
      "loss": 0.0421,
      "step": 7624
    },
    {
      "epoch": 1.8869091808958178,
      "grad_norm": 0.04474377632141113,
      "learning_rate": 3.943979382806329e-06,
      "loss": 0.0474,
      "step": 7625
    },
    {
      "epoch": 1.8871566443949517,
      "grad_norm": 0.05150425806641579,
      "learning_rate": 3.926783241807286e-06,
      "loss": 0.057,
      "step": 7626
    },
    {
      "epoch": 1.8874041078940857,
      "grad_norm": 0.041305311024188995,
      "learning_rate": 3.909624374446002e-06,
      "loss": 0.0576,
      "step": 7627
    },
    {
      "epoch": 1.8876515713932194,
      "grad_norm": 0.06356164067983627,
      "learning_rate": 3.892502783321567e-06,
      "loss": 0.0671,
      "step": 7628
    },
    {
      "epoch": 1.8878990348923534,
      "grad_norm": 0.03597978502511978,
      "learning_rate": 3.875418471027459e-06,
      "loss": 0.0371,
      "step": 7629
    },
    {
      "epoch": 1.8881464983914873,
      "grad_norm": 0.036142025142908096,
      "learning_rate": 3.858371440151471e-06,
      "loss": 0.0315,
      "step": 7630
    },
    {
      "epoch": 1.8883939618906211,
      "grad_norm": 0.03989221528172493,
      "learning_rate": 3.841361693275841e-06,
      "loss": 0.0443,
      "step": 7631
    },
    {
      "epoch": 1.888641425389755,
      "grad_norm": 0.027698084712028503,
      "learning_rate": 3.824389232977093e-06,
      "loss": 0.0314,
      "step": 7632
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.05600227043032646,
      "learning_rate": 3.80745406182606e-06,
      "loss": 0.0273,
      "step": 7633
    },
    {
      "epoch": 1.889136352388023,
      "grad_norm": 0.07500401139259338,
      "learning_rate": 3.790556182388022e-06,
      "loss": 0.1058,
      "step": 7634
    },
    {
      "epoch": 1.8893838158871565,
      "grad_norm": 0.030928565189242363,
      "learning_rate": 3.7736955972225426e-06,
      "loss": 0.0301,
      "step": 7635
    },
    {
      "epoch": 1.8896312793862906,
      "grad_norm": 0.03380154073238373,
      "learning_rate": 3.7568723088835798e-06,
      "loss": 0.0434,
      "step": 7636
    },
    {
      "epoch": 1.8898787428854242,
      "grad_norm": 0.03159559145569801,
      "learning_rate": 3.7400863199193725e-06,
      "loss": 0.0222,
      "step": 7637
    },
    {
      "epoch": 1.8901262063845583,
      "grad_norm": 0.040170181542634964,
      "learning_rate": 3.723337632872609e-06,
      "loss": 0.0508,
      "step": 7638
    },
    {
      "epoch": 1.8903736698836922,
      "grad_norm": 0.036247849464416504,
      "learning_rate": 3.7066262502802594e-06,
      "loss": 0.0371,
      "step": 7639
    },
    {
      "epoch": 1.890621133382826,
      "grad_norm": 0.04076717421412468,
      "learning_rate": 3.6899521746736607e-06,
      "loss": 0.0503,
      "step": 7640
    },
    {
      "epoch": 1.89086859688196,
      "grad_norm": 0.08622356504201889,
      "learning_rate": 3.673315408578459e-06,
      "loss": 0.0796,
      "step": 7641
    },
    {
      "epoch": 1.8911160603810937,
      "grad_norm": 0.13698038458824158,
      "learning_rate": 3.6567159545147776e-06,
      "loss": 0.1078,
      "step": 7642
    },
    {
      "epoch": 1.8913635238802278,
      "grad_norm": 0.03759698197245598,
      "learning_rate": 3.6401538149969106e-06,
      "loss": 0.0401,
      "step": 7643
    },
    {
      "epoch": 1.8916109873793614,
      "grad_norm": 0.08815031498670578,
      "learning_rate": 3.6236289925336287e-06,
      "loss": 0.0803,
      "step": 7644
    },
    {
      "epoch": 1.8918584508784955,
      "grad_norm": 0.04556608572602272,
      "learning_rate": 3.6071414896279854e-06,
      "loss": 0.034,
      "step": 7645
    },
    {
      "epoch": 1.8921059143776293,
      "grad_norm": 0.046310294419527054,
      "learning_rate": 3.590691308777455e-06,
      "loss": 0.0391,
      "step": 7646
    },
    {
      "epoch": 1.8923533778767632,
      "grad_norm": 0.07940035313367844,
      "learning_rate": 3.574278452473767e-06,
      "loss": 0.0656,
      "step": 7647
    },
    {
      "epoch": 1.892600841375897,
      "grad_norm": 0.026240143924951553,
      "learning_rate": 3.5579029232030435e-06,
      "loss": 0.0326,
      "step": 7648
    },
    {
      "epoch": 1.8928483048750309,
      "grad_norm": 0.03486321493983269,
      "learning_rate": 3.541564723445717e-06,
      "loss": 0.0382,
      "step": 7649
    },
    {
      "epoch": 1.893095768374165,
      "grad_norm": 0.04766300693154335,
      "learning_rate": 3.525263855676669e-06,
      "loss": 0.0592,
      "step": 7650
    },
    {
      "epoch": 1.8933432318732986,
      "grad_norm": 0.034320902079343796,
      "learning_rate": 3.509000322365008e-06,
      "loss": 0.0295,
      "step": 7651
    },
    {
      "epoch": 1.8935906953724326,
      "grad_norm": 0.045155446976423264,
      "learning_rate": 3.492774125974207e-06,
      "loss": 0.0472,
      "step": 7652
    },
    {
      "epoch": 1.8938381588715665,
      "grad_norm": 0.055615171790122986,
      "learning_rate": 3.4765852689621623e-06,
      "loss": 0.0496,
      "step": 7653
    },
    {
      "epoch": 1.8940856223707003,
      "grad_norm": 0.04455770552158356,
      "learning_rate": 3.4604337537810502e-06,
      "loss": 0.0218,
      "step": 7654
    },
    {
      "epoch": 1.8943330858698342,
      "grad_norm": 0.03598951920866966,
      "learning_rate": 3.444319582877414e-06,
      "loss": 0.0406,
      "step": 7655
    },
    {
      "epoch": 1.894580549368968,
      "grad_norm": 0.04058688133955002,
      "learning_rate": 3.4282427586920517e-06,
      "loss": 0.0282,
      "step": 7656
    },
    {
      "epoch": 1.894828012868102,
      "grad_norm": 0.03732966259121895,
      "learning_rate": 3.4122032836602646e-06,
      "loss": 0.0421,
      "step": 7657
    },
    {
      "epoch": 1.8950754763672357,
      "grad_norm": 0.04914819076657295,
      "learning_rate": 3.396201160211582e-06,
      "loss": 0.057,
      "step": 7658
    },
    {
      "epoch": 1.8953229398663698,
      "grad_norm": 0.05832716450095177,
      "learning_rate": 3.3802363907698974e-06,
      "loss": 0.0689,
      "step": 7659
    },
    {
      "epoch": 1.8955704033655036,
      "grad_norm": 0.06337417662143707,
      "learning_rate": 3.3643089777534443e-06,
      "loss": 0.0646,
      "step": 7660
    },
    {
      "epoch": 1.8958178668646375,
      "grad_norm": 0.08042684197425842,
      "learning_rate": 3.3484189235748475e-06,
      "loss": 0.0767,
      "step": 7661
    },
    {
      "epoch": 1.8960653303637713,
      "grad_norm": 0.03726722300052643,
      "learning_rate": 3.3325662306410152e-06,
      "loss": 0.0278,
      "step": 7662
    },
    {
      "epoch": 1.8963127938629052,
      "grad_norm": 0.0726151317358017,
      "learning_rate": 3.3167509013532215e-06,
      "loss": 0.0779,
      "step": 7663
    },
    {
      "epoch": 1.8965602573620393,
      "grad_norm": 0.09589245915412903,
      "learning_rate": 3.30097293810705e-06,
      "loss": 0.0617,
      "step": 7664
    },
    {
      "epoch": 1.8968077208611729,
      "grad_norm": 0.05584433674812317,
      "learning_rate": 3.285232343292449e-06,
      "loss": 0.0406,
      "step": 7665
    },
    {
      "epoch": 1.897055184360307,
      "grad_norm": 0.11313653737306595,
      "learning_rate": 3.269529119293735e-06,
      "loss": 0.0609,
      "step": 7666
    },
    {
      "epoch": 1.8973026478594406,
      "grad_norm": 0.05023351311683655,
      "learning_rate": 3.253863268489532e-06,
      "loss": 0.0552,
      "step": 7667
    },
    {
      "epoch": 1.8975501113585747,
      "grad_norm": 0.03290747478604317,
      "learning_rate": 3.238234793252748e-06,
      "loss": 0.0367,
      "step": 7668
    },
    {
      "epoch": 1.8977975748577085,
      "grad_norm": 0.05489838123321533,
      "learning_rate": 3.2226436959507666e-06,
      "loss": 0.0521,
      "step": 7669
    },
    {
      "epoch": 1.8980450383568424,
      "grad_norm": 0.05516549199819565,
      "learning_rate": 3.207089978945199e-06,
      "loss": 0.0498,
      "step": 7670
    },
    {
      "epoch": 1.8982925018559762,
      "grad_norm": 0.05312546715140343,
      "learning_rate": 3.191573644591994e-06,
      "loss": 0.0588,
      "step": 7671
    },
    {
      "epoch": 1.89853996535511,
      "grad_norm": 0.048085570335388184,
      "learning_rate": 3.1760946952415216e-06,
      "loss": 0.0518,
      "step": 7672
    },
    {
      "epoch": 1.8987874288542441,
      "grad_norm": 0.04289935529232025,
      "learning_rate": 3.160653133238378e-06,
      "loss": 0.0336,
      "step": 7673
    },
    {
      "epoch": 1.8990348923533777,
      "grad_norm": 0.029475770890712738,
      "learning_rate": 3.1452489609216097e-06,
      "loss": 0.0231,
      "step": 7674
    },
    {
      "epoch": 1.8992823558525118,
      "grad_norm": 0.056512314826250076,
      "learning_rate": 3.1298821806245437e-06,
      "loss": 0.064,
      "step": 7675
    },
    {
      "epoch": 1.8995298193516457,
      "grad_norm": 0.04697516933083534,
      "learning_rate": 3.1145527946747908e-06,
      "loss": 0.0342,
      "step": 7676
    },
    {
      "epoch": 1.8997772828507795,
      "grad_norm": 0.06918520480394363,
      "learning_rate": 3.0992608053944104e-06,
      "loss": 0.0744,
      "step": 7677
    },
    {
      "epoch": 1.9000247463499134,
      "grad_norm": 0.04471445456147194,
      "learning_rate": 3.0840062150996884e-06,
      "loss": 0.052,
      "step": 7678
    },
    {
      "epoch": 1.9002722098490472,
      "grad_norm": 0.034754861146211624,
      "learning_rate": 3.0687890261013318e-06,
      "loss": 0.0404,
      "step": 7679
    },
    {
      "epoch": 1.9005196733481813,
      "grad_norm": 0.07233890146017075,
      "learning_rate": 3.0536092407043027e-06,
      "loss": 0.0938,
      "step": 7680
    },
    {
      "epoch": 1.900767136847315,
      "grad_norm": 0.03165045008063316,
      "learning_rate": 3.038466861208011e-06,
      "loss": 0.0302,
      "step": 7681
    },
    {
      "epoch": 1.901014600346449,
      "grad_norm": 0.054365888237953186,
      "learning_rate": 3.0233618899060676e-06,
      "loss": 0.0444,
      "step": 7682
    },
    {
      "epoch": 1.9012620638455828,
      "grad_norm": 0.02630425989627838,
      "learning_rate": 3.008294329086503e-06,
      "loss": 0.0349,
      "step": 7683
    },
    {
      "epoch": 1.9015095273447167,
      "grad_norm": 0.06201861798763275,
      "learning_rate": 2.9932641810316297e-06,
      "loss": 0.0353,
      "step": 7684
    },
    {
      "epoch": 1.9017569908438505,
      "grad_norm": 0.03798094391822815,
      "learning_rate": 2.9782714480181825e-06,
      "loss": 0.0532,
      "step": 7685
    },
    {
      "epoch": 1.9020044543429844,
      "grad_norm": 0.037433817982673645,
      "learning_rate": 2.9633161323171234e-06,
      "loss": 0.0476,
      "step": 7686
    },
    {
      "epoch": 1.9022519178421184,
      "grad_norm": 0.06518179923295975,
      "learning_rate": 2.9483982361938053e-06,
      "loss": 0.0471,
      "step": 7687
    },
    {
      "epoch": 1.902499381341252,
      "grad_norm": 0.04587443545460701,
      "learning_rate": 2.9335177619078668e-06,
      "loss": 0.0481,
      "step": 7688
    },
    {
      "epoch": 1.9027468448403861,
      "grad_norm": 0.0493081733584404,
      "learning_rate": 2.9186747117133372e-06,
      "loss": 0.0453,
      "step": 7689
    },
    {
      "epoch": 1.9029943083395198,
      "grad_norm": 0.03409066051244736,
      "learning_rate": 2.9038690878585573e-06,
      "loss": 0.019,
      "step": 7690
    },
    {
      "epoch": 1.9032417718386538,
      "grad_norm": 0.08616511523723602,
      "learning_rate": 2.889100892586177e-06,
      "loss": 0.0969,
      "step": 7691
    },
    {
      "epoch": 1.9034892353377877,
      "grad_norm": 0.059266284108161926,
      "learning_rate": 2.8743701281331858e-06,
      "loss": 0.0657,
      "step": 7692
    },
    {
      "epoch": 1.9037366988369215,
      "grad_norm": 0.07782965898513794,
      "learning_rate": 2.859676796730909e-06,
      "loss": 0.0514,
      "step": 7693
    },
    {
      "epoch": 1.9039841623360554,
      "grad_norm": 0.03708804026246071,
      "learning_rate": 2.845020900605011e-06,
      "loss": 0.0393,
      "step": 7694
    },
    {
      "epoch": 1.9042316258351892,
      "grad_norm": 0.051346760243177414,
      "learning_rate": 2.830402441975466e-06,
      "loss": 0.0556,
      "step": 7695
    },
    {
      "epoch": 1.9044790893343233,
      "grad_norm": 0.06800399720668793,
      "learning_rate": 2.815821423056586e-06,
      "loss": 0.0585,
      "step": 7696
    },
    {
      "epoch": 1.904726552833457,
      "grad_norm": 0.06895418465137482,
      "learning_rate": 2.8012778460570486e-06,
      "loss": 0.0592,
      "step": 7697
    },
    {
      "epoch": 1.904974016332591,
      "grad_norm": 0.0989440381526947,
      "learning_rate": 2.786771713179759e-06,
      "loss": 0.125,
      "step": 7698
    },
    {
      "epoch": 1.9052214798317249,
      "grad_norm": 0.08014111965894699,
      "learning_rate": 2.7723030266220693e-06,
      "loss": 0.0682,
      "step": 7699
    },
    {
      "epoch": 1.9054689433308587,
      "grad_norm": 0.059382058680057526,
      "learning_rate": 2.757871788575589e-06,
      "loss": 0.0929,
      "step": 7700
    },
    {
      "epoch": 1.9057164068299925,
      "grad_norm": 0.040755875408649445,
      "learning_rate": 2.7434780012262626e-06,
      "loss": 0.0353,
      "step": 7701
    },
    {
      "epoch": 1.9059638703291264,
      "grad_norm": 0.04776523634791374,
      "learning_rate": 2.7291216667544026e-06,
      "loss": 0.0574,
      "step": 7702
    },
    {
      "epoch": 1.9062113338282605,
      "grad_norm": 0.06684792041778564,
      "learning_rate": 2.714802787334575e-06,
      "loss": 0.0896,
      "step": 7703
    },
    {
      "epoch": 1.906458797327394,
      "grad_norm": 0.03454175218939781,
      "learning_rate": 2.700521365135711e-06,
      "loss": 0.0581,
      "step": 7704
    },
    {
      "epoch": 1.9067062608265282,
      "grad_norm": 0.06453905999660492,
      "learning_rate": 2.6862774023211356e-06,
      "loss": 0.0821,
      "step": 7705
    },
    {
      "epoch": 1.906953724325662,
      "grad_norm": 0.044497910887002945,
      "learning_rate": 2.6720709010483736e-06,
      "loss": 0.036,
      "step": 7706
    },
    {
      "epoch": 1.9072011878247959,
      "grad_norm": 0.061646297574043274,
      "learning_rate": 2.657901863469342e-06,
      "loss": 0.054,
      "step": 7707
    },
    {
      "epoch": 1.9074486513239297,
      "grad_norm": 0.04151562228798866,
      "learning_rate": 2.643770291730324e-06,
      "loss": 0.0453,
      "step": 7708
    },
    {
      "epoch": 1.9076961148230636,
      "grad_norm": 0.03857731074094772,
      "learning_rate": 2.6296761879718302e-06,
      "loss": 0.0441,
      "step": 7709
    },
    {
      "epoch": 1.9079435783221976,
      "grad_norm": 0.02705422043800354,
      "learning_rate": 2.6156195543287907e-06,
      "loss": 0.031,
      "step": 7710
    },
    {
      "epoch": 1.9081910418213313,
      "grad_norm": 0.03769269585609436,
      "learning_rate": 2.6016003929303924e-06,
      "loss": 0.042,
      "step": 7711
    },
    {
      "epoch": 1.9084385053204653,
      "grad_norm": 0.0371529720723629,
      "learning_rate": 2.5876187059001576e-06,
      "loss": 0.0469,
      "step": 7712
    },
    {
      "epoch": 1.908685968819599,
      "grad_norm": 0.049312375485897064,
      "learning_rate": 2.5736744953559765e-06,
      "loss": 0.0555,
      "step": 7713
    },
    {
      "epoch": 1.908933432318733,
      "grad_norm": 0.06448078155517578,
      "learning_rate": 2.5597677634099924e-06,
      "loss": 0.0362,
      "step": 7714
    },
    {
      "epoch": 1.9091808958178669,
      "grad_norm": 0.06625719368457794,
      "learning_rate": 2.5458985121687427e-06,
      "loss": 0.0686,
      "step": 7715
    },
    {
      "epoch": 1.9094283593170007,
      "grad_norm": 0.05770625174045563,
      "learning_rate": 2.532066743733075e-06,
      "loss": 0.0696,
      "step": 7716
    },
    {
      "epoch": 1.9096758228161346,
      "grad_norm": 0.09069981426000595,
      "learning_rate": 2.51827246019809e-06,
      "loss": 0.0871,
      "step": 7717
    },
    {
      "epoch": 1.9099232863152684,
      "grad_norm": 0.03610488399863243,
      "learning_rate": 2.504515663653284e-06,
      "loss": 0.0247,
      "step": 7718
    },
    {
      "epoch": 1.9101707498144025,
      "grad_norm": 0.08473005145788193,
      "learning_rate": 2.4907963561824342e-06,
      "loss": 0.0484,
      "step": 7719
    },
    {
      "epoch": 1.9104182133135361,
      "grad_norm": 0.052182409912347794,
      "learning_rate": 2.4771145398636842e-06,
      "loss": 0.0578,
      "step": 7720
    },
    {
      "epoch": 1.9106656768126702,
      "grad_norm": 0.05069134011864662,
      "learning_rate": 2.463470216769459e-06,
      "loss": 0.0533,
      "step": 7721
    },
    {
      "epoch": 1.910913140311804,
      "grad_norm": 0.03910212963819504,
      "learning_rate": 2.4498633889664945e-06,
      "loss": 0.0535,
      "step": 7722
    },
    {
      "epoch": 1.9111606038109379,
      "grad_norm": 0.057192619889974594,
      "learning_rate": 2.4362940585158923e-06,
      "loss": 0.0542,
      "step": 7723
    },
    {
      "epoch": 1.9114080673100717,
      "grad_norm": 0.04289739578962326,
      "learning_rate": 2.4227622274730355e-06,
      "loss": 0.0575,
      "step": 7724
    },
    {
      "epoch": 1.9116555308092056,
      "grad_norm": 0.06071244925260544,
      "learning_rate": 2.4092678978876735e-06,
      "loss": 0.0428,
      "step": 7725
    },
    {
      "epoch": 1.9119029943083397,
      "grad_norm": 0.05336785316467285,
      "learning_rate": 2.3958110718038106e-06,
      "loss": 0.0325,
      "step": 7726
    },
    {
      "epoch": 1.9121504578074733,
      "grad_norm": 0.05919179692864418,
      "learning_rate": 2.382391751259788e-06,
      "loss": 0.0547,
      "step": 7727
    },
    {
      "epoch": 1.9123979213066074,
      "grad_norm": 0.051591914147138596,
      "learning_rate": 2.3690099382883414e-06,
      "loss": 0.0694,
      "step": 7728
    },
    {
      "epoch": 1.9126453848057412,
      "grad_norm": 0.0306028351187706,
      "learning_rate": 2.355665634916404e-06,
      "loss": 0.0235,
      "step": 7729
    },
    {
      "epoch": 1.912892848304875,
      "grad_norm": 0.06085364148020744,
      "learning_rate": 2.342358843165332e-06,
      "loss": 0.0555,
      "step": 7730
    },
    {
      "epoch": 1.913140311804009,
      "grad_norm": 0.06952819973230362,
      "learning_rate": 2.3290895650506793e-06,
      "loss": 0.0258,
      "step": 7731
    },
    {
      "epoch": 1.9133877753031427,
      "grad_norm": 0.039433665573596954,
      "learning_rate": 2.315857802582505e-06,
      "loss": 0.0242,
      "step": 7732
    },
    {
      "epoch": 1.9136352388022768,
      "grad_norm": 0.08185971528291702,
      "learning_rate": 2.302663557765011e-06,
      "loss": 0.1529,
      "step": 7733
    },
    {
      "epoch": 1.9138827023014104,
      "grad_norm": 0.061836495995521545,
      "learning_rate": 2.289506832596766e-06,
      "loss": 0.0296,
      "step": 7734
    },
    {
      "epoch": 1.9141301658005445,
      "grad_norm": 0.06180483475327492,
      "learning_rate": 2.276387629070703e-06,
      "loss": 0.0665,
      "step": 7735
    },
    {
      "epoch": 1.9143776292996781,
      "grad_norm": 0.06700584292411804,
      "learning_rate": 2.2633059491740105e-06,
      "loss": 0.0921,
      "step": 7736
    },
    {
      "epoch": 1.9146250927988122,
      "grad_norm": 0.046013154089450836,
      "learning_rate": 2.250261794888242e-06,
      "loss": 0.0393,
      "step": 7737
    },
    {
      "epoch": 1.914872556297946,
      "grad_norm": 0.059251293540000916,
      "learning_rate": 2.237255168189234e-06,
      "loss": 0.0791,
      "step": 7738
    },
    {
      "epoch": 1.91512001979708,
      "grad_norm": 0.09438372403383255,
      "learning_rate": 2.2242860710471323e-06,
      "loss": 0.0595,
      "step": 7739
    },
    {
      "epoch": 1.9153674832962138,
      "grad_norm": 0.030142299830913544,
      "learning_rate": 2.2113545054264495e-06,
      "loss": 0.0384,
      "step": 7740
    },
    {
      "epoch": 1.9156149467953476,
      "grad_norm": 0.06420377641916275,
      "learning_rate": 2.1984604732859794e-06,
      "loss": 0.0638,
      "step": 7741
    },
    {
      "epoch": 1.9158624102944817,
      "grad_norm": 0.054890409111976624,
      "learning_rate": 2.1856039765787983e-06,
      "loss": 0.052,
      "step": 7742
    },
    {
      "epoch": 1.9161098737936153,
      "grad_norm": 0.04203532636165619,
      "learning_rate": 2.1727850172523213e-06,
      "loss": 0.0489,
      "step": 7743
    },
    {
      "epoch": 1.9163573372927494,
      "grad_norm": 0.05643539875745773,
      "learning_rate": 2.160003597248328e-06,
      "loss": 0.064,
      "step": 7744
    },
    {
      "epoch": 1.9166048007918832,
      "grad_norm": 0.03568865358829498,
      "learning_rate": 2.1472597185028254e-06,
      "loss": 0.0458,
      "step": 7745
    },
    {
      "epoch": 1.916852264291017,
      "grad_norm": 0.033397041261196136,
      "learning_rate": 2.1345533829462137e-06,
      "loss": 0.0281,
      "step": 7746
    },
    {
      "epoch": 1.917099727790151,
      "grad_norm": 0.034221187233924866,
      "learning_rate": 2.121884592503148e-06,
      "loss": 0.0284,
      "step": 7747
    },
    {
      "epoch": 1.9173471912892848,
      "grad_norm": 0.07517111301422119,
      "learning_rate": 2.1092533490926213e-06,
      "loss": 0.0418,
      "step": 7748
    },
    {
      "epoch": 1.9175946547884188,
      "grad_norm": 0.04034661874175072,
      "learning_rate": 2.0966596546279357e-06,
      "loss": 0.0329,
      "step": 7749
    },
    {
      "epoch": 1.9178421182875525,
      "grad_norm": 0.06435929983854294,
      "learning_rate": 2.084103511016733e-06,
      "loss": 0.0722,
      "step": 7750
    },
    {
      "epoch": 1.9180895817866865,
      "grad_norm": 0.051458925008773804,
      "learning_rate": 2.0715849201608792e-06,
      "loss": 0.0474,
      "step": 7751
    },
    {
      "epoch": 1.9183370452858204,
      "grad_norm": 0.050927191972732544,
      "learning_rate": 2.0591038839566645e-06,
      "loss": 0.0703,
      "step": 7752
    },
    {
      "epoch": 1.9185845087849542,
      "grad_norm": 0.06767851859331131,
      "learning_rate": 2.0466604042946313e-06,
      "loss": 0.0706,
      "step": 7753
    },
    {
      "epoch": 1.918831972284088,
      "grad_norm": 0.06290004402399063,
      "learning_rate": 2.0342544830596056e-06,
      "loss": 0.0734,
      "step": 7754
    },
    {
      "epoch": 1.919079435783222,
      "grad_norm": 0.053084179759025574,
      "learning_rate": 2.0218861221307784e-06,
      "loss": 0.0378,
      "step": 7755
    },
    {
      "epoch": 1.919326899282356,
      "grad_norm": 0.050531432032585144,
      "learning_rate": 2.009555323381651e-06,
      "loss": 0.0571,
      "step": 7756
    },
    {
      "epoch": 1.9195743627814896,
      "grad_norm": 0.028890270739793777,
      "learning_rate": 1.9972620886800074e-06,
      "loss": 0.035,
      "step": 7757
    },
    {
      "epoch": 1.9198218262806237,
      "grad_norm": 0.03192412853240967,
      "learning_rate": 1.9850064198879414e-06,
      "loss": 0.0486,
      "step": 7758
    },
    {
      "epoch": 1.9200692897797573,
      "grad_norm": 0.03406744450330734,
      "learning_rate": 1.9727883188618566e-06,
      "loss": 0.0376,
      "step": 7759
    },
    {
      "epoch": 1.9203167532788914,
      "grad_norm": 0.04054272919893265,
      "learning_rate": 1.960607787452495e-06,
      "loss": 0.0335,
      "step": 7760
    },
    {
      "epoch": 1.9205642167780252,
      "grad_norm": 0.12830998003482819,
      "learning_rate": 1.9484648275048812e-06,
      "loss": 0.1078,
      "step": 7761
    },
    {
      "epoch": 1.920811680277159,
      "grad_norm": 0.03195573762059212,
      "learning_rate": 1.9363594408583485e-06,
      "loss": 0.0296,
      "step": 7762
    },
    {
      "epoch": 1.921059143776293,
      "grad_norm": 0.09080146998167038,
      "learning_rate": 1.924291629346542e-06,
      "loss": 0.0822,
      "step": 7763
    },
    {
      "epoch": 1.9213066072754268,
      "grad_norm": 0.07671919465065002,
      "learning_rate": 1.9122613947974432e-06,
      "loss": 0.0558,
      "step": 7764
    },
    {
      "epoch": 1.9215540707745609,
      "grad_norm": 0.05472273752093315,
      "learning_rate": 1.9002687390332895e-06,
      "loss": 0.0965,
      "step": 7765
    },
    {
      "epoch": 1.9218015342736945,
      "grad_norm": 0.03151189535856247,
      "learning_rate": 1.8883136638706833e-06,
      "loss": 0.047,
      "step": 7766
    },
    {
      "epoch": 1.9220489977728286,
      "grad_norm": 0.04046090319752693,
      "learning_rate": 1.8763961711204535e-06,
      "loss": 0.035,
      "step": 7767
    },
    {
      "epoch": 1.9222964612719624,
      "grad_norm": 0.036490317434072495,
      "learning_rate": 1.8645162625878508e-06,
      "loss": 0.0328,
      "step": 7768
    },
    {
      "epoch": 1.9225439247710963,
      "grad_norm": 0.036000825464725494,
      "learning_rate": 1.8526739400723247e-06,
      "loss": 0.0499,
      "step": 7769
    },
    {
      "epoch": 1.92279138827023,
      "grad_norm": 0.041758548468351364,
      "learning_rate": 1.84086920536769e-06,
      "loss": 0.0338,
      "step": 7770
    },
    {
      "epoch": 1.923038851769364,
      "grad_norm": 0.037308357656002045,
      "learning_rate": 1.8291020602620723e-06,
      "loss": 0.0356,
      "step": 7771
    },
    {
      "epoch": 1.923286315268498,
      "grad_norm": 0.03056395798921585,
      "learning_rate": 1.817372506537851e-06,
      "loss": 0.0368,
      "step": 7772
    },
    {
      "epoch": 1.9235337787676317,
      "grad_norm": 0.03367868438363075,
      "learning_rate": 1.8056805459717719e-06,
      "loss": 0.0203,
      "step": 7773
    },
    {
      "epoch": 1.9237812422667657,
      "grad_norm": 0.037433478981256485,
      "learning_rate": 1.7940261803348347e-06,
      "loss": 0.0358,
      "step": 7774
    },
    {
      "epoch": 1.9240287057658996,
      "grad_norm": 0.054585617035627365,
      "learning_rate": 1.782409411392405e-06,
      "loss": 0.0436,
      "step": 7775
    },
    {
      "epoch": 1.9242761692650334,
      "grad_norm": 0.05139783024787903,
      "learning_rate": 1.7708302409041033e-06,
      "loss": 0.0263,
      "step": 7776
    },
    {
      "epoch": 1.9245236327641673,
      "grad_norm": 0.03409520164132118,
      "learning_rate": 1.7592886706238597e-06,
      "loss": 0.0356,
      "step": 7777
    },
    {
      "epoch": 1.9247710962633011,
      "grad_norm": 0.035450827330350876,
      "learning_rate": 1.7477847022999426e-06,
      "loss": 0.0278,
      "step": 7778
    },
    {
      "epoch": 1.9250185597624352,
      "grad_norm": 0.05593771114945412,
      "learning_rate": 1.736318337674847e-06,
      "loss": 0.0525,
      "step": 7779
    },
    {
      "epoch": 1.9252660232615688,
      "grad_norm": 0.05473450571298599,
      "learning_rate": 1.7248895784855168e-06,
      "loss": 0.0696,
      "step": 7780
    },
    {
      "epoch": 1.9255134867607029,
      "grad_norm": 0.04669113829731941,
      "learning_rate": 1.7134984264630392e-06,
      "loss": 0.0402,
      "step": 7781
    },
    {
      "epoch": 1.9257609502598365,
      "grad_norm": 0.05563304200768471,
      "learning_rate": 1.702144883332868e-06,
      "loss": 0.0662,
      "step": 7782
    },
    {
      "epoch": 1.9260084137589706,
      "grad_norm": 0.04117807000875473,
      "learning_rate": 1.690828950814821e-06,
      "loss": 0.0438,
      "step": 7783
    },
    {
      "epoch": 1.9262558772581044,
      "grad_norm": 0.21663573384284973,
      "learning_rate": 1.679550630622917e-06,
      "loss": 0.0582,
      "step": 7784
    },
    {
      "epoch": 1.9265033407572383,
      "grad_norm": 0.03692955896258354,
      "learning_rate": 1.6683099244655663e-06,
      "loss": 0.0349,
      "step": 7785
    },
    {
      "epoch": 1.9267508042563724,
      "grad_norm": 0.06862773001194,
      "learning_rate": 1.6571068340454075e-06,
      "loss": 0.0725,
      "step": 7786
    },
    {
      "epoch": 1.926998267755506,
      "grad_norm": 0.06422361731529236,
      "learning_rate": 1.6459413610594443e-06,
      "loss": 0.0705,
      "step": 7787
    },
    {
      "epoch": 1.92724573125464,
      "grad_norm": 0.043291278183460236,
      "learning_rate": 1.6348135071989346e-06,
      "loss": 0.0279,
      "step": 7788
    },
    {
      "epoch": 1.9274931947537737,
      "grad_norm": 0.09411840885877609,
      "learning_rate": 1.6237232741494468e-06,
      "loss": 0.129,
      "step": 7789
    },
    {
      "epoch": 1.9277406582529077,
      "grad_norm": 0.04836086928844452,
      "learning_rate": 1.6126706635908595e-06,
      "loss": 0.0588,
      "step": 7790
    },
    {
      "epoch": 1.9279881217520416,
      "grad_norm": 0.0847831591963768,
      "learning_rate": 1.6016556771973889e-06,
      "loss": 0.0698,
      "step": 7791
    },
    {
      "epoch": 1.9282355852511754,
      "grad_norm": 0.04439929127693176,
      "learning_rate": 1.5906783166374782e-06,
      "loss": 0.0479,
      "step": 7792
    },
    {
      "epoch": 1.9284830487503093,
      "grad_norm": 0.10109297186136246,
      "learning_rate": 1.5797385835739364e-06,
      "loss": 0.1104,
      "step": 7793
    },
    {
      "epoch": 1.9287305122494431,
      "grad_norm": 0.0388607457280159,
      "learning_rate": 1.5688364796638265e-06,
      "loss": 0.0361,
      "step": 7794
    },
    {
      "epoch": 1.9289779757485772,
      "grad_norm": 0.0529959462583065,
      "learning_rate": 1.55797200655855e-06,
      "loss": 0.0826,
      "step": 7795
    },
    {
      "epoch": 1.9292254392477108,
      "grad_norm": 0.07299592345952988,
      "learning_rate": 1.5471451659037628e-06,
      "loss": 0.0824,
      "step": 7796
    },
    {
      "epoch": 1.929472902746845,
      "grad_norm": 0.04615744203329086,
      "learning_rate": 1.536355959339486e-06,
      "loss": 0.0322,
      "step": 7797
    },
    {
      "epoch": 1.9297203662459788,
      "grad_norm": 0.04134022444486618,
      "learning_rate": 1.5256043884999683e-06,
      "loss": 0.0246,
      "step": 7798
    },
    {
      "epoch": 1.9299678297451126,
      "grad_norm": 0.06572569906711578,
      "learning_rate": 1.5148904550137955e-06,
      "loss": 0.0387,
      "step": 7799
    },
    {
      "epoch": 1.9302152932442465,
      "grad_norm": 0.028915585950016975,
      "learning_rate": 1.504214160503864e-06,
      "loss": 0.0269,
      "step": 7800
    },
    {
      "epoch": 1.9302152932442465,
      "eval_loss": 0.2890268564224243,
      "eval_runtime": 202.8442,
      "eval_samples_per_second": 4.93,
      "eval_steps_per_second": 0.311,
      "step": 7800
    },
    {
      "epoch": 1.9304627567433803,
      "grad_norm": 0.06826407462358475,
      "learning_rate": 1.4935755065873247e-06,
      "loss": 0.0878,
      "step": 7801
    },
    {
      "epoch": 1.9307102202425144,
      "grad_norm": 0.05505334585905075,
      "learning_rate": 1.4829744948756663e-06,
      "loss": 0.0532,
      "step": 7802
    },
    {
      "epoch": 1.930957683741648,
      "grad_norm": 0.0530150905251503,
      "learning_rate": 1.4724111269746876e-06,
      "loss": 0.0426,
      "step": 7803
    },
    {
      "epoch": 1.931205147240782,
      "grad_norm": 0.04656084254384041,
      "learning_rate": 1.461885404484442e-06,
      "loss": 0.0352,
      "step": 7804
    },
    {
      "epoch": 1.9314526107399157,
      "grad_norm": 0.07029136270284653,
      "learning_rate": 1.4513973289992933e-06,
      "loss": 0.0498,
      "step": 7805
    },
    {
      "epoch": 1.9317000742390498,
      "grad_norm": 0.0502394400537014,
      "learning_rate": 1.4409469021079147e-06,
      "loss": 0.0326,
      "step": 7806
    },
    {
      "epoch": 1.9319475377381836,
      "grad_norm": 0.03643191605806351,
      "learning_rate": 1.4305341253932902e-06,
      "loss": 0.0185,
      "step": 7807
    },
    {
      "epoch": 1.9321950012373175,
      "grad_norm": 0.04986800253391266,
      "learning_rate": 1.4201590004326582e-06,
      "loss": 0.0632,
      "step": 7808
    },
    {
      "epoch": 1.9324424647364515,
      "grad_norm": 0.0716940239071846,
      "learning_rate": 1.4098215287975946e-06,
      "loss": 0.0931,
      "step": 7809
    },
    {
      "epoch": 1.9326899282355852,
      "grad_norm": 0.03356655314564705,
      "learning_rate": 1.3995217120539305e-06,
      "loss": 0.0358,
      "step": 7810
    },
    {
      "epoch": 1.9329373917347192,
      "grad_norm": 0.04542701318860054,
      "learning_rate": 1.3892595517618622e-06,
      "loss": 0.0469,
      "step": 7811
    },
    {
      "epoch": 1.9331848552338529,
      "grad_norm": 0.045606594532728195,
      "learning_rate": 1.3790350494757852e-06,
      "loss": 0.0541,
      "step": 7812
    },
    {
      "epoch": 1.933432318732987,
      "grad_norm": 0.07274100184440613,
      "learning_rate": 1.3688482067444607e-06,
      "loss": 0.0681,
      "step": 7813
    },
    {
      "epoch": 1.9336797822321208,
      "grad_norm": 0.04703708738088608,
      "learning_rate": 1.3586990251109598e-06,
      "loss": 0.0542,
      "step": 7814
    },
    {
      "epoch": 1.9339272457312546,
      "grad_norm": 0.03475932776927948,
      "learning_rate": 1.3485875061126085e-06,
      "loss": 0.023,
      "step": 7815
    },
    {
      "epoch": 1.9341747092303885,
      "grad_norm": 0.07107015699148178,
      "learning_rate": 1.338513651280987e-06,
      "loss": 0.0729,
      "step": 7816
    },
    {
      "epoch": 1.9344221727295223,
      "grad_norm": 0.0535365529358387,
      "learning_rate": 1.3284774621420693e-06,
      "loss": 0.0544,
      "step": 7817
    },
    {
      "epoch": 1.9346696362286564,
      "grad_norm": 0.0494462326169014,
      "learning_rate": 1.3184789402160557e-06,
      "loss": 0.0317,
      "step": 7818
    },
    {
      "epoch": 1.93491709972779,
      "grad_norm": 0.047661732882261276,
      "learning_rate": 1.308518087017485e-06,
      "loss": 0.0441,
      "step": 7819
    },
    {
      "epoch": 1.935164563226924,
      "grad_norm": 0.03742007538676262,
      "learning_rate": 1.2985949040551227e-06,
      "loss": 0.0313,
      "step": 7820
    },
    {
      "epoch": 1.935412026726058,
      "grad_norm": 0.035456910729408264,
      "learning_rate": 1.2887093928320991e-06,
      "loss": 0.0334,
      "step": 7821
    },
    {
      "epoch": 1.9356594902251918,
      "grad_norm": 0.046209532767534256,
      "learning_rate": 1.2788615548458005e-06,
      "loss": 0.0298,
      "step": 7822
    },
    {
      "epoch": 1.9359069537243256,
      "grad_norm": 0.06191977486014366,
      "learning_rate": 1.2690513915879497e-06,
      "loss": 0.0627,
      "step": 7823
    },
    {
      "epoch": 1.9361544172234595,
      "grad_norm": 0.03099200502038002,
      "learning_rate": 1.2592789045444697e-06,
      "loss": 0.0355,
      "step": 7824
    },
    {
      "epoch": 1.9364018807225936,
      "grad_norm": 0.03510073944926262,
      "learning_rate": 1.249544095195676e-06,
      "loss": 0.037,
      "step": 7825
    },
    {
      "epoch": 1.9366493442217272,
      "grad_norm": 0.028623508289456367,
      "learning_rate": 1.2398469650161114e-06,
      "loss": 0.0193,
      "step": 7826
    },
    {
      "epoch": 1.9368968077208613,
      "grad_norm": 0.05058390274643898,
      "learning_rate": 1.2301875154746567e-06,
      "loss": 0.0602,
      "step": 7827
    },
    {
      "epoch": 1.9371442712199949,
      "grad_norm": 0.048204176127910614,
      "learning_rate": 1.2205657480344745e-06,
      "loss": 0.0635,
      "step": 7828
    },
    {
      "epoch": 1.937391734719129,
      "grad_norm": 0.04821212217211723,
      "learning_rate": 1.2109816641529548e-06,
      "loss": 0.0365,
      "step": 7829
    },
    {
      "epoch": 1.9376391982182628,
      "grad_norm": 0.0356806181371212,
      "learning_rate": 1.201435265281936e-06,
      "loss": 0.0279,
      "step": 7830
    },
    {
      "epoch": 1.9378866617173967,
      "grad_norm": 0.04513675719499588,
      "learning_rate": 1.1919265528673452e-06,
      "loss": 0.0468,
      "step": 7831
    },
    {
      "epoch": 1.9381341252165307,
      "grad_norm": 0.032228872179985046,
      "learning_rate": 1.1824555283495575e-06,
      "loss": 0.0318,
      "step": 7832
    },
    {
      "epoch": 1.9383815887156643,
      "grad_norm": 0.09074681252241135,
      "learning_rate": 1.1730221931631758e-06,
      "loss": 0.0597,
      "step": 7833
    },
    {
      "epoch": 1.9386290522147984,
      "grad_norm": 0.03583669289946556,
      "learning_rate": 1.1636265487370846e-06,
      "loss": 0.0404,
      "step": 7834
    },
    {
      "epoch": 1.938876515713932,
      "grad_norm": 0.10389398038387299,
      "learning_rate": 1.1542685964945066e-06,
      "loss": 0.0507,
      "step": 7835
    },
    {
      "epoch": 1.9391239792130661,
      "grad_norm": 0.03807750716805458,
      "learning_rate": 1.1449483378528914e-06,
      "loss": 0.0496,
      "step": 7836
    },
    {
      "epoch": 1.9393714427122,
      "grad_norm": 0.06285832822322845,
      "learning_rate": 1.135665774224054e-06,
      "loss": 0.0531,
      "step": 7837
    },
    {
      "epoch": 1.9396189062113338,
      "grad_norm": 0.077076755464077,
      "learning_rate": 1.1264209070140362e-06,
      "loss": 0.1013,
      "step": 7838
    },
    {
      "epoch": 1.9398663697104677,
      "grad_norm": 0.03999076411128044,
      "learning_rate": 1.1172137376231906e-06,
      "loss": 0.0279,
      "step": 7839
    },
    {
      "epoch": 1.9401138332096015,
      "grad_norm": 0.059323400259017944,
      "learning_rate": 1.1080442674461788e-06,
      "loss": 0.0629,
      "step": 7840
    },
    {
      "epoch": 1.9403612967087356,
      "grad_norm": 0.04575757309794426,
      "learning_rate": 1.09891249787189e-06,
      "loss": 0.0529,
      "step": 7841
    },
    {
      "epoch": 1.9406087602078692,
      "grad_norm": 0.06628601253032684,
      "learning_rate": 1.0898184302836068e-06,
      "loss": 0.0433,
      "step": 7842
    },
    {
      "epoch": 1.9408562237070033,
      "grad_norm": 0.06849197298288345,
      "learning_rate": 1.0807620660588103e-06,
      "loss": 0.0638,
      "step": 7843
    },
    {
      "epoch": 1.9411036872061371,
      "grad_norm": 0.03241980820894241,
      "learning_rate": 1.0717434065693198e-06,
      "loss": 0.0145,
      "step": 7844
    },
    {
      "epoch": 1.941351150705271,
      "grad_norm": 0.04264305159449577,
      "learning_rate": 1.062762453181182e-06,
      "loss": 0.0347,
      "step": 7845
    },
    {
      "epoch": 1.9415986142044048,
      "grad_norm": 0.04720871150493622,
      "learning_rate": 1.0538192072548359e-06,
      "loss": 0.0445,
      "step": 7846
    },
    {
      "epoch": 1.9418460777035387,
      "grad_norm": 0.0656384825706482,
      "learning_rate": 1.0449136701449203e-06,
      "loss": 0.0587,
      "step": 7847
    },
    {
      "epoch": 1.9420935412026727,
      "grad_norm": 0.025151120498776436,
      "learning_rate": 1.036045843200384e-06,
      "loss": 0.0255,
      "step": 7848
    },
    {
      "epoch": 1.9423410047018064,
      "grad_norm": 0.04484790563583374,
      "learning_rate": 1.027215727764458e-06,
      "loss": 0.0554,
      "step": 7849
    },
    {
      "epoch": 1.9425884682009404,
      "grad_norm": 0.0700911208987236,
      "learning_rate": 1.018423325174711e-06,
      "loss": 0.0547,
      "step": 7850
    },
    {
      "epoch": 1.9428359317000743,
      "grad_norm": 0.04144373536109924,
      "learning_rate": 1.0096686367629393e-06,
      "loss": 0.0602,
      "step": 7851
    },
    {
      "epoch": 1.9430833951992081,
      "grad_norm": 0.05682259798049927,
      "learning_rate": 1.0009516638552484e-06,
      "loss": 0.0562,
      "step": 7852
    },
    {
      "epoch": 1.943330858698342,
      "grad_norm": 0.06452565640211105,
      "learning_rate": 9.922724077720269e-07,
      "loss": 0.036,
      "step": 7853
    },
    {
      "epoch": 1.9435783221974758,
      "grad_norm": 0.08526214957237244,
      "learning_rate": 9.836308698279729e-07,
      "loss": 0.0915,
      "step": 7854
    },
    {
      "epoch": 1.94382578569661,
      "grad_norm": 0.04843682423233986,
      "learning_rate": 9.75027051332067e-07,
      "loss": 0.0451,
      "step": 7855
    },
    {
      "epoch": 1.9440732491957435,
      "grad_norm": 0.03274863585829735,
      "learning_rate": 9.66460953587489e-07,
      "loss": 0.0237,
      "step": 7856
    },
    {
      "epoch": 1.9443207126948776,
      "grad_norm": 0.03890647366642952,
      "learning_rate": 9.579325778918402e-07,
      "loss": 0.0324,
      "step": 7857
    },
    {
      "epoch": 1.9445681761940112,
      "grad_norm": 0.07675067335367203,
      "learning_rate": 9.49441925536948e-07,
      "loss": 0.0518,
      "step": 7858
    },
    {
      "epoch": 1.9448156396931453,
      "grad_norm": 0.040426261723041534,
      "learning_rate": 9.409889978088948e-07,
      "loss": 0.0537,
      "step": 7859
    },
    {
      "epoch": 1.9450631031922792,
      "grad_norm": 0.049002159386873245,
      "learning_rate": 9.325737959880731e-07,
      "loss": 0.0549,
      "step": 7860
    },
    {
      "epoch": 1.945310566691413,
      "grad_norm": 0.043056342750787735,
      "learning_rate": 9.241963213492132e-07,
      "loss": 0.043,
      "step": 7861
    },
    {
      "epoch": 1.9455580301905468,
      "grad_norm": 0.0867096558213234,
      "learning_rate": 9.158565751612169e-07,
      "loss": 0.0798,
      "step": 7862
    },
    {
      "epoch": 1.9458054936896807,
      "grad_norm": 0.04251839593052864,
      "learning_rate": 9.075545586874067e-07,
      "loss": 0.0272,
      "step": 7863
    },
    {
      "epoch": 1.9460529571888148,
      "grad_norm": 0.06655153632164001,
      "learning_rate": 8.992902731852493e-07,
      "loss": 0.0402,
      "step": 7864
    },
    {
      "epoch": 1.9463004206879484,
      "grad_norm": 0.04078727588057518,
      "learning_rate": 8.910637199066318e-07,
      "loss": 0.0506,
      "step": 7865
    },
    {
      "epoch": 1.9465478841870825,
      "grad_norm": 0.08049829304218292,
      "learning_rate": 8.828749000976133e-07,
      "loss": 0.0748,
      "step": 7866
    },
    {
      "epoch": 1.9467953476862163,
      "grad_norm": 0.03775477409362793,
      "learning_rate": 8.747238149986181e-07,
      "loss": 0.0372,
      "step": 7867
    },
    {
      "epoch": 1.9470428111853502,
      "grad_norm": 0.03973822295665741,
      "learning_rate": 8.666104658442975e-07,
      "loss": 0.0457,
      "step": 7868
    },
    {
      "epoch": 1.947290274684484,
      "grad_norm": 0.06070427969098091,
      "learning_rate": 8.585348538636129e-07,
      "loss": 0.0661,
      "step": 7869
    },
    {
      "epoch": 1.9475377381836179,
      "grad_norm": 0.024563198909163475,
      "learning_rate": 8.504969802798357e-07,
      "loss": 0.0307,
      "step": 7870
    },
    {
      "epoch": 1.947785201682752,
      "grad_norm": 0.03612620010972023,
      "learning_rate": 8.424968463104643e-07,
      "loss": 0.0292,
      "step": 7871
    },
    {
      "epoch": 1.9480326651818856,
      "grad_norm": 0.05774848908185959,
      "learning_rate": 8.345344531673072e-07,
      "loss": 0.0331,
      "step": 7872
    },
    {
      "epoch": 1.9482801286810196,
      "grad_norm": 0.05018875375390053,
      "learning_rate": 8.266098020564827e-07,
      "loss": 0.0782,
      "step": 7873
    },
    {
      "epoch": 1.9485275921801535,
      "grad_norm": 0.04239758104085922,
      "learning_rate": 8.187228941783365e-07,
      "loss": 0.0414,
      "step": 7874
    },
    {
      "epoch": 1.9487750556792873,
      "grad_norm": 0.04891737177968025,
      "learning_rate": 8.108737307275516e-07,
      "loss": 0.039,
      "step": 7875
    },
    {
      "epoch": 1.9490225191784212,
      "grad_norm": 0.054009512066841125,
      "learning_rate": 8.030623128930659e-07,
      "loss": 0.0432,
      "step": 7876
    },
    {
      "epoch": 1.949269982677555,
      "grad_norm": 0.08318644762039185,
      "learning_rate": 7.952886418580996e-07,
      "loss": 0.1157,
      "step": 7877
    },
    {
      "epoch": 1.949517446176689,
      "grad_norm": 0.08332151919603348,
      "learning_rate": 7.87552718800183e-07,
      "loss": 0.0745,
      "step": 7878
    },
    {
      "epoch": 1.9497649096758227,
      "grad_norm": 0.03488878533244133,
      "learning_rate": 7.798545448910733e-07,
      "loss": 0.0468,
      "step": 7879
    },
    {
      "epoch": 1.9500123731749568,
      "grad_norm": 0.04065995663404465,
      "learning_rate": 7.721941212968653e-07,
      "loss": 0.0755,
      "step": 7880
    },
    {
      "epoch": 1.9502598366740904,
      "grad_norm": 0.03931828588247299,
      "learning_rate": 7.64571449177881e-07,
      "loss": 0.0503,
      "step": 7881
    },
    {
      "epoch": 1.9505073001732245,
      "grad_norm": 0.07322373241186142,
      "learning_rate": 7.569865296887802e-07,
      "loss": 0.0678,
      "step": 7882
    },
    {
      "epoch": 1.9507547636723583,
      "grad_norm": 0.04962366819381714,
      "learning_rate": 7.494393639784769e-07,
      "loss": 0.0366,
      "step": 7883
    },
    {
      "epoch": 1.9510022271714922,
      "grad_norm": 0.025401784107089043,
      "learning_rate": 7.41929953190168e-07,
      "loss": 0.0265,
      "step": 7884
    },
    {
      "epoch": 1.951249690670626,
      "grad_norm": 0.0661797970533371,
      "learning_rate": 7.344582984613324e-07,
      "loss": 0.0623,
      "step": 7885
    },
    {
      "epoch": 1.9514971541697599,
      "grad_norm": 0.03545595332980156,
      "learning_rate": 7.270244009237037e-07,
      "loss": 0.0265,
      "step": 7886
    },
    {
      "epoch": 1.951744617668894,
      "grad_norm": 0.031856562942266464,
      "learning_rate": 7.196282617033811e-07,
      "loss": 0.0303,
      "step": 7887
    },
    {
      "epoch": 1.9519920811680276,
      "grad_norm": 0.10453444719314575,
      "learning_rate": 7.122698819206353e-07,
      "loss": 0.0949,
      "step": 7888
    },
    {
      "epoch": 1.9522395446671617,
      "grad_norm": 0.03945225477218628,
      "learning_rate": 7.049492626900745e-07,
      "loss": 0.0308,
      "step": 7889
    },
    {
      "epoch": 1.9524870081662955,
      "grad_norm": 0.052692241966724396,
      "learning_rate": 6.976664051206172e-07,
      "loss": 0.0584,
      "step": 7890
    },
    {
      "epoch": 1.9527344716654293,
      "grad_norm": 0.059336427599191666,
      "learning_rate": 6.904213103153812e-07,
      "loss": 0.0596,
      "step": 7891
    },
    {
      "epoch": 1.9529819351645632,
      "grad_norm": 0.05907667800784111,
      "learning_rate": 6.832139793718217e-07,
      "loss": 0.0626,
      "step": 7892
    },
    {
      "epoch": 1.953229398663697,
      "grad_norm": 0.04706154391169548,
      "learning_rate": 6.76044413381649e-07,
      "loss": 0.0638,
      "step": 7893
    },
    {
      "epoch": 1.9534768621628311,
      "grad_norm": 0.029716653749346733,
      "learning_rate": 6.689126134309109e-07,
      "loss": 0.0375,
      "step": 7894
    },
    {
      "epoch": 1.9537243256619647,
      "grad_norm": 0.03924475610256195,
      "learning_rate": 6.618185805998267e-07,
      "loss": 0.0509,
      "step": 7895
    },
    {
      "epoch": 1.9539717891610988,
      "grad_norm": 0.0733780488371849,
      "learning_rate": 6.54762315963009e-07,
      "loss": 0.0766,
      "step": 7896
    },
    {
      "epoch": 1.9542192526602327,
      "grad_norm": 0.0783686563372612,
      "learning_rate": 6.477438205892694e-07,
      "loss": 0.0533,
      "step": 7897
    },
    {
      "epoch": 1.9544667161593665,
      "grad_norm": 0.040579892694950104,
      "learning_rate": 6.407630955417298e-07,
      "loss": 0.0456,
      "step": 7898
    },
    {
      "epoch": 1.9547141796585004,
      "grad_norm": 0.05779944732785225,
      "learning_rate": 6.338201418777667e-07,
      "loss": 0.0275,
      "step": 7899
    },
    {
      "epoch": 1.9549616431576342,
      "grad_norm": 0.04640867933630943,
      "learning_rate": 6.269149606490943e-07,
      "loss": 0.0285,
      "step": 7900
    },
    {
      "epoch": 1.9552091066567683,
      "grad_norm": 0.044648427516222,
      "learning_rate": 6.200475529016536e-07,
      "loss": 0.0609,
      "step": 7901
    },
    {
      "epoch": 1.955456570155902,
      "grad_norm": 0.05007760599255562,
      "learning_rate": 6.132179196756405e-07,
      "loss": 0.0685,
      "step": 7902
    },
    {
      "epoch": 1.955704033655036,
      "grad_norm": 0.047286730259656906,
      "learning_rate": 6.064260620056162e-07,
      "loss": 0.0568,
      "step": 7903
    },
    {
      "epoch": 1.9559514971541696,
      "grad_norm": 0.03384914621710777,
      "learning_rate": 5.996719809203411e-07,
      "loss": 0.037,
      "step": 7904
    },
    {
      "epoch": 1.9561989606533037,
      "grad_norm": 0.039809420704841614,
      "learning_rate": 5.929556774429134e-07,
      "loss": 0.0492,
      "step": 7905
    },
    {
      "epoch": 1.9564464241524375,
      "grad_norm": 0.046340461820364,
      "learning_rate": 5.862771525906307e-07,
      "loss": 0.0489,
      "step": 7906
    },
    {
      "epoch": 1.9566938876515714,
      "grad_norm": 0.04200863093137741,
      "learning_rate": 5.796364073751281e-07,
      "loss": 0.0587,
      "step": 7907
    },
    {
      "epoch": 1.9569413511507052,
      "grad_norm": 0.06928399205207825,
      "learning_rate": 5.730334428023232e-07,
      "loss": 0.0559,
      "step": 7908
    },
    {
      "epoch": 1.957188814649839,
      "grad_norm": 0.08296280354261398,
      "learning_rate": 5.664682598723881e-07,
      "loss": 0.0537,
      "step": 7909
    },
    {
      "epoch": 1.9574362781489731,
      "grad_norm": 0.03231476992368698,
      "learning_rate": 5.599408595797495e-07,
      "loss": 0.0441,
      "step": 7910
    },
    {
      "epoch": 1.9576837416481068,
      "grad_norm": 0.03849980980157852,
      "learning_rate": 5.534512429131722e-07,
      "loss": 0.0371,
      "step": 7911
    },
    {
      "epoch": 1.9579312051472408,
      "grad_norm": 0.060284510254859924,
      "learning_rate": 5.469994108556475e-07,
      "loss": 0.0826,
      "step": 7912
    },
    {
      "epoch": 1.9581786686463747,
      "grad_norm": 0.04398350045084953,
      "learning_rate": 5.405853643844494e-07,
      "loss": 0.0392,
      "step": 7913
    },
    {
      "epoch": 1.9584261321455085,
      "grad_norm": 0.08683060109615326,
      "learning_rate": 5.342091044711616e-07,
      "loss": 0.1659,
      "step": 7914
    },
    {
      "epoch": 1.9586735956446424,
      "grad_norm": 0.054808687418699265,
      "learning_rate": 5.278706320815952e-07,
      "loss": 0.0738,
      "step": 7915
    },
    {
      "epoch": 1.9589210591437762,
      "grad_norm": 0.046785224229097366,
      "learning_rate": 5.215699481758707e-07,
      "loss": 0.0291,
      "step": 7916
    },
    {
      "epoch": 1.9591685226429103,
      "grad_norm": 0.025989200919866562,
      "learning_rate": 5.153070537083637e-07,
      "loss": 0.0281,
      "step": 7917
    },
    {
      "epoch": 1.959415986142044,
      "grad_norm": 0.050922688096761703,
      "learning_rate": 5.090819496277877e-07,
      "loss": 0.0552,
      "step": 7918
    },
    {
      "epoch": 1.959663449641178,
      "grad_norm": 0.05002044886350632,
      "learning_rate": 5.02894636877027e-07,
      "loss": 0.0719,
      "step": 7919
    },
    {
      "epoch": 1.9599109131403119,
      "grad_norm": 0.030166374519467354,
      "learning_rate": 4.967451163933045e-07,
      "loss": 0.0312,
      "step": 7920
    },
    {
      "epoch": 1.9601583766394457,
      "grad_norm": 0.08699901401996613,
      "learning_rate": 4.906333891081527e-07,
      "loss": 0.1023,
      "step": 7921
    },
    {
      "epoch": 1.9604058401385795,
      "grad_norm": 0.11203594505786896,
      "learning_rate": 4.845594559472755e-07,
      "loss": 0.044,
      "step": 7922
    },
    {
      "epoch": 1.9606533036377134,
      "grad_norm": 0.04712080955505371,
      "learning_rate": 4.785233178307702e-07,
      "loss": 0.0628,
      "step": 7923
    },
    {
      "epoch": 1.9609007671368475,
      "grad_norm": 0.0558040477335453,
      "learning_rate": 4.725249756729333e-07,
      "loss": 0.0434,
      "step": 7924
    },
    {
      "epoch": 1.961148230635981,
      "grad_norm": 0.03558734059333801,
      "learning_rate": 4.665644303823435e-07,
      "loss": 0.0468,
      "step": 7925
    },
    {
      "epoch": 1.9613956941351152,
      "grad_norm": 0.04148154333233833,
      "learning_rate": 4.6064168286191753e-07,
      "loss": 0.0371,
      "step": 7926
    },
    {
      "epoch": 1.9616431576342488,
      "grad_norm": 0.03552575781941414,
      "learning_rate": 4.5475673400871553e-07,
      "loss": 0.0274,
      "step": 7927
    },
    {
      "epoch": 1.9618906211333829,
      "grad_norm": 0.046153679490089417,
      "learning_rate": 4.4890958471421886e-07,
      "loss": 0.0439,
      "step": 7928
    },
    {
      "epoch": 1.9621380846325167,
      "grad_norm": 0.05722897872328758,
      "learning_rate": 4.431002358640801e-07,
      "loss": 0.0772,
      "step": 7929
    },
    {
      "epoch": 1.9623855481316506,
      "grad_norm": 0.05563027784228325,
      "learning_rate": 4.373286883382621e-07,
      "loss": 0.0315,
      "step": 7930
    },
    {
      "epoch": 1.9626330116307844,
      "grad_norm": 0.01920567825436592,
      "learning_rate": 4.3159494301103774e-07,
      "loss": 0.0183,
      "step": 7931
    },
    {
      "epoch": 1.9628804751299183,
      "grad_norm": 0.05541412904858589,
      "learning_rate": 4.258990007509067e-07,
      "loss": 0.1075,
      "step": 7932
    },
    {
      "epoch": 1.9631279386290523,
      "grad_norm": 0.02864822931587696,
      "learning_rate": 4.2024086242062334e-07,
      "loss": 0.0244,
      "step": 7933
    },
    {
      "epoch": 1.963375402128186,
      "grad_norm": 0.04305526986718178,
      "learning_rate": 4.1462052887727975e-07,
      "loss": 0.0354,
      "step": 7934
    },
    {
      "epoch": 1.96362286562732,
      "grad_norm": 0.047767288982868195,
      "learning_rate": 4.090380009722228e-07,
      "loss": 0.0511,
      "step": 7935
    },
    {
      "epoch": 1.9638703291264539,
      "grad_norm": 0.055675528943538666,
      "learning_rate": 4.034932795509982e-07,
      "loss": 0.0424,
      "step": 7936
    },
    {
      "epoch": 1.9641177926255877,
      "grad_norm": 0.07504352927207947,
      "learning_rate": 3.979863654535454e-07,
      "loss": 0.0707,
      "step": 7937
    },
    {
      "epoch": 1.9643652561247216,
      "grad_norm": 0.03161681815981865,
      "learning_rate": 3.9251725951400255e-07,
      "loss": 0.0438,
      "step": 7938
    },
    {
      "epoch": 1.9646127196238554,
      "grad_norm": 0.09318316727876663,
      "learning_rate": 3.8708596256076257e-07,
      "loss": 0.0857,
      "step": 7939
    },
    {
      "epoch": 1.9648601831229895,
      "grad_norm": 0.023728903383016586,
      "learning_rate": 3.8169247541655625e-07,
      "loss": 0.0195,
      "step": 7940
    },
    {
      "epoch": 1.9651076466221231,
      "grad_norm": 0.05170062929391861,
      "learning_rate": 3.7633679889836905e-07,
      "loss": 0.1509,
      "step": 7941
    },
    {
      "epoch": 1.9653551101212572,
      "grad_norm": 0.046480774879455566,
      "learning_rate": 3.7101893381741304e-07,
      "loss": 0.0236,
      "step": 7942
    },
    {
      "epoch": 1.965602573620391,
      "grad_norm": 0.031878843903541565,
      "learning_rate": 3.6573888097923833e-07,
      "loss": 0.0361,
      "step": 7943
    },
    {
      "epoch": 1.9658500371195249,
      "grad_norm": 0.028895672410726547,
      "learning_rate": 3.6049664118359415e-07,
      "loss": 0.019,
      "step": 7944
    },
    {
      "epoch": 1.9660975006186587,
      "grad_norm": 0.04562436789274216,
      "learning_rate": 3.5529221522456743e-07,
      "loss": 0.0313,
      "step": 7945
    },
    {
      "epoch": 1.9663449641177926,
      "grad_norm": 0.025731904432177544,
      "learning_rate": 3.5012560389049983e-07,
      "loss": 0.0238,
      "step": 7946
    },
    {
      "epoch": 1.9665924276169267,
      "grad_norm": 0.042703427374362946,
      "learning_rate": 3.449968079639598e-07,
      "loss": 0.0384,
      "step": 7947
    },
    {
      "epoch": 1.9668398911160603,
      "grad_norm": 0.06567924469709396,
      "learning_rate": 3.399058282218537e-07,
      "loss": 0.0815,
      "step": 7948
    },
    {
      "epoch": 1.9670873546151944,
      "grad_norm": 0.04709701985120773,
      "learning_rate": 3.348526654353423e-07,
      "loss": 0.0337,
      "step": 7949
    },
    {
      "epoch": 1.967334818114328,
      "grad_norm": 0.10768592357635498,
      "learning_rate": 3.298373203698413e-07,
      "loss": 0.0574,
      "step": 7950
    },
    {
      "epoch": 1.967582281613462,
      "grad_norm": 0.039953045547008514,
      "learning_rate": 3.248597937850206e-07,
      "loss": 0.0292,
      "step": 7951
    },
    {
      "epoch": 1.967829745112596,
      "grad_norm": 0.027194330468773842,
      "learning_rate": 3.199200864348606e-07,
      "loss": 0.0322,
      "step": 7952
    },
    {
      "epoch": 1.9680772086117297,
      "grad_norm": 0.04029350355267525,
      "learning_rate": 3.15018199067596e-07,
      "loss": 0.04,
      "step": 7953
    },
    {
      "epoch": 1.9683246721108636,
      "grad_norm": 0.03872805833816528,
      "learning_rate": 3.1015413242574397e-07,
      "loss": 0.0708,
      "step": 7954
    },
    {
      "epoch": 1.9685721356099974,
      "grad_norm": 0.07057095319032669,
      "learning_rate": 3.053278872460763e-07,
      "loss": 0.0541,
      "step": 7955
    },
    {
      "epoch": 1.9688195991091315,
      "grad_norm": 0.030344467610120773,
      "learning_rate": 3.005394642596471e-07,
      "loss": 0.0325,
      "step": 7956
    },
    {
      "epoch": 1.9690670626082651,
      "grad_norm": 0.05399814993143082,
      "learning_rate": 2.957888641917372e-07,
      "loss": 0.0456,
      "step": 7957
    },
    {
      "epoch": 1.9693145261073992,
      "grad_norm": 0.06874334812164307,
      "learning_rate": 2.9107608776199336e-07,
      "loss": 0.0636,
      "step": 7958
    },
    {
      "epoch": 1.969561989606533,
      "grad_norm": 0.044518690556287766,
      "learning_rate": 2.8640113568426107e-07,
      "loss": 0.0655,
      "step": 7959
    },
    {
      "epoch": 1.969809453105667,
      "grad_norm": 0.02679257281124592,
      "learning_rate": 2.817640086666684e-07,
      "loss": 0.0353,
      "step": 7960
    },
    {
      "epoch": 1.9700569166048008,
      "grad_norm": 0.059769414365291595,
      "learning_rate": 2.7716470741159793e-07,
      "loss": 0.0554,
      "step": 7961
    },
    {
      "epoch": 1.9703043801039346,
      "grad_norm": 0.08711078763008118,
      "learning_rate": 2.7260323261577015e-07,
      "loss": 0.1204,
      "step": 7962
    },
    {
      "epoch": 1.9705518436030687,
      "grad_norm": 0.04906633123755455,
      "learning_rate": 2.680795849700768e-07,
      "loss": 0.0565,
      "step": 7963
    },
    {
      "epoch": 1.9707993071022023,
      "grad_norm": 0.045775268226861954,
      "learning_rate": 2.635937651597753e-07,
      "loss": 0.0356,
      "step": 7964
    },
    {
      "epoch": 1.9710467706013364,
      "grad_norm": 0.02491629868745804,
      "learning_rate": 2.5914577386432217e-07,
      "loss": 0.0232,
      "step": 7965
    },
    {
      "epoch": 1.9712942341004702,
      "grad_norm": 0.04591597244143486,
      "learning_rate": 2.5473561175748397e-07,
      "loss": 0.0418,
      "step": 7966
    },
    {
      "epoch": 1.971541697599604,
      "grad_norm": 0.04395455867052078,
      "learning_rate": 2.503632795072541e-07,
      "loss": 0.0591,
      "step": 7967
    },
    {
      "epoch": 1.971789161098738,
      "grad_norm": 0.035051409155130386,
      "learning_rate": 2.4602877777599176e-07,
      "loss": 0.0185,
      "step": 7968
    },
    {
      "epoch": 1.9720366245978718,
      "grad_norm": 0.06404201686382294,
      "learning_rate": 2.4173210722019945e-07,
      "loss": 0.0406,
      "step": 7969
    },
    {
      "epoch": 1.9722840880970058,
      "grad_norm": 0.024932384490966797,
      "learning_rate": 2.3747326849074547e-07,
      "loss": 0.0324,
      "step": 7970
    },
    {
      "epoch": 1.9725315515961395,
      "grad_norm": 0.054366640746593475,
      "learning_rate": 2.3325226223269713e-07,
      "loss": 0.0717,
      "step": 7971
    },
    {
      "epoch": 1.9727790150952735,
      "grad_norm": 0.04316626116633415,
      "learning_rate": 2.2906908908545965e-07,
      "loss": 0.0553,
      "step": 7972
    },
    {
      "epoch": 1.9730264785944072,
      "grad_norm": 0.036352988332509995,
      "learning_rate": 2.2492374968266504e-07,
      "loss": 0.0376,
      "step": 7973
    },
    {
      "epoch": 1.9732739420935412,
      "grad_norm": 0.07535801827907562,
      "learning_rate": 2.2081624465219996e-07,
      "loss": 0.1195,
      "step": 7974
    },
    {
      "epoch": 1.973521405592675,
      "grad_norm": 0.05251055210828781,
      "learning_rate": 2.1674657461628888e-07,
      "loss": 0.0561,
      "step": 7975
    },
    {
      "epoch": 1.973768869091809,
      "grad_norm": 0.06375385820865631,
      "learning_rate": 2.127147401913554e-07,
      "loss": 0.0627,
      "step": 7976
    },
    {
      "epoch": 1.974016332590943,
      "grad_norm": 0.047669027000665665,
      "learning_rate": 2.0872074198810543e-07,
      "loss": 0.0369,
      "step": 7977
    },
    {
      "epoch": 1.9742637960900766,
      "grad_norm": 0.04701390117406845,
      "learning_rate": 2.0476458061152725e-07,
      "loss": 0.0444,
      "step": 7978
    },
    {
      "epoch": 1.9745112595892107,
      "grad_norm": 0.10182656347751617,
      "learning_rate": 2.008462566608915e-07,
      "loss": 0.0609,
      "step": 7979
    },
    {
      "epoch": 1.9747587230883443,
      "grad_norm": 0.04161497950553894,
      "learning_rate": 1.9696577072972344e-07,
      "loss": 0.0619,
      "step": 7980
    },
    {
      "epoch": 1.9750061865874784,
      "grad_norm": 0.03981400653719902,
      "learning_rate": 1.9312312340580284e-07,
      "loss": 0.03,
      "step": 7981
    },
    {
      "epoch": 1.9752536500866122,
      "grad_norm": 0.0735788494348526,
      "learning_rate": 1.8931831527119192e-07,
      "loss": 0.0538,
      "step": 7982
    },
    {
      "epoch": 1.975501113585746,
      "grad_norm": 0.034934673458337784,
      "learning_rate": 1.8555134690220743e-07,
      "loss": 0.0405,
      "step": 7983
    },
    {
      "epoch": 1.97574857708488,
      "grad_norm": 0.042305201292037964,
      "learning_rate": 1.8182221886947625e-07,
      "loss": 0.0386,
      "step": 7984
    },
    {
      "epoch": 1.9759960405840138,
      "grad_norm": 0.09245026856660843,
      "learning_rate": 1.781309317378521e-07,
      "loss": 0.0641,
      "step": 7985
    },
    {
      "epoch": 1.9762435040831479,
      "grad_norm": 0.04734398424625397,
      "learning_rate": 1.7447748606644333e-07,
      "loss": 0.047,
      "step": 7986
    },
    {
      "epoch": 1.9764909675822815,
      "grad_norm": 0.0633014440536499,
      "learning_rate": 1.7086188240866828e-07,
      "loss": 0.07,
      "step": 7987
    },
    {
      "epoch": 1.9767384310814156,
      "grad_norm": 0.036428969353437424,
      "learning_rate": 1.672841213122278e-07,
      "loss": 0.0283,
      "step": 7988
    },
    {
      "epoch": 1.9769858945805494,
      "grad_norm": 0.04838090017437935,
      "learning_rate": 1.6374420331899397e-07,
      "loss": 0.0365,
      "step": 7989
    },
    {
      "epoch": 1.9772333580796833,
      "grad_norm": 0.07531695812940598,
      "learning_rate": 1.6024212896523227e-07,
      "loss": 0.0551,
      "step": 7990
    },
    {
      "epoch": 1.977480821578817,
      "grad_norm": 0.07819952070713043,
      "learning_rate": 1.5677789878137948e-07,
      "loss": 0.1006,
      "step": 7991
    },
    {
      "epoch": 1.977728285077951,
      "grad_norm": 0.07329270988702774,
      "learning_rate": 1.5335151329218255e-07,
      "loss": 0.0456,
      "step": 7992
    },
    {
      "epoch": 1.977975748577085,
      "grad_norm": 0.04431527480483055,
      "learning_rate": 1.4996297301664297e-07,
      "loss": 0.0531,
      "step": 7993
    },
    {
      "epoch": 1.9782232120762187,
      "grad_norm": 0.061858125030994415,
      "learning_rate": 1.466122784680446e-07,
      "loss": 0.1752,
      "step": 7994
    },
    {
      "epoch": 1.9784706755753527,
      "grad_norm": 0.041021376848220825,
      "learning_rate": 1.4329943015395363e-07,
      "loss": 0.0382,
      "step": 7995
    },
    {
      "epoch": 1.9787181390744863,
      "grad_norm": 0.03470322862267494,
      "learning_rate": 1.400244285761354e-07,
      "loss": 0.0284,
      "step": 7996
    },
    {
      "epoch": 1.9789656025736204,
      "grad_norm": 0.04602906107902527,
      "learning_rate": 1.3678727423069303e-07,
      "loss": 0.0536,
      "step": 7997
    },
    {
      "epoch": 1.9792130660727543,
      "grad_norm": 0.05279349908232689,
      "learning_rate": 1.3358796760795655e-07,
      "loss": 0.0441,
      "step": 7998
    },
    {
      "epoch": 1.9794605295718881,
      "grad_norm": 0.05909743905067444,
      "learning_rate": 1.3042650919256606e-07,
      "loss": 0.0814,
      "step": 7999
    },
    {
      "epoch": 1.9797079930710222,
      "grad_norm": 0.06284372508525848,
      "learning_rate": 1.273028994633607e-07,
      "loss": 0.0661,
      "step": 8000
    },
    {
      "epoch": 1.9797079930710222,
      "eval_loss": 0.2890240252017975,
      "eval_runtime": 202.5836,
      "eval_samples_per_second": 4.936,
      "eval_steps_per_second": 0.311,
      "step": 8000
    },
    {
      "epoch": 1.9799554565701558,
      "grad_norm": 0.050553809851408005,
      "learning_rate": 1.242171388935176e-07,
      "loss": 0.0435,
      "step": 8001
    },
    {
      "epoch": 1.9802029200692899,
      "grad_norm": 0.03790350630879402,
      "learning_rate": 1.2116922795041286e-07,
      "loss": 0.0293,
      "step": 8002
    },
    {
      "epoch": 1.9804503835684235,
      "grad_norm": 0.030464475974440575,
      "learning_rate": 1.1815916709576047e-07,
      "loss": 0.0297,
      "step": 8003
    },
    {
      "epoch": 1.9806978470675576,
      "grad_norm": 0.07296853512525558,
      "learning_rate": 1.1518695678550128e-07,
      "loss": 0.04,
      "step": 8004
    },
    {
      "epoch": 1.9809453105666914,
      "grad_norm": 0.11411870270967484,
      "learning_rate": 1.1225259746983074e-07,
      "loss": 0.1329,
      "step": 8005
    },
    {
      "epoch": 1.9811927740658253,
      "grad_norm": 0.06796228140592575,
      "learning_rate": 1.0935608959322663e-07,
      "loss": 0.0903,
      "step": 8006
    },
    {
      "epoch": 1.9814402375649591,
      "grad_norm": 0.06964318454265594,
      "learning_rate": 1.0649743359444907e-07,
      "loss": 0.0544,
      "step": 8007
    },
    {
      "epoch": 1.981687701064093,
      "grad_norm": 0.04988424479961395,
      "learning_rate": 1.0367662990651283e-07,
      "loss": 0.0537,
      "step": 8008
    },
    {
      "epoch": 1.981935164563227,
      "grad_norm": 0.035422079265117645,
      "learning_rate": 1.0089367895665946e-07,
      "loss": 0.042,
      "step": 8009
    },
    {
      "epoch": 1.9821826280623607,
      "grad_norm": 0.04454488679766655,
      "learning_rate": 9.814858116646841e-08,
      "loss": 0.043,
      "step": 8010
    },
    {
      "epoch": 1.9824300915614947,
      "grad_norm": 0.05566742271184921,
      "learning_rate": 9.544133695174595e-08,
      "loss": 0.0833,
      "step": 8011
    },
    {
      "epoch": 1.9826775550606286,
      "grad_norm": 0.0798061266541481,
      "learning_rate": 9.277194672255295e-08,
      "loss": 0.0824,
      "step": 8012
    },
    {
      "epoch": 1.9829250185597624,
      "grad_norm": 0.03982746601104736,
      "learning_rate": 9.014041088323266e-08,
      "loss": 0.0503,
      "step": 8013
    },
    {
      "epoch": 1.9831724820588963,
      "grad_norm": 0.0433633029460907,
      "learning_rate": 8.754672983243839e-08,
      "loss": 0.058,
      "step": 8014
    },
    {
      "epoch": 1.9834199455580301,
      "grad_norm": 0.03396439552307129,
      "learning_rate": 8.499090396296705e-08,
      "loss": 0.0446,
      "step": 8015
    },
    {
      "epoch": 1.9836674090571642,
      "grad_norm": 0.038365527987480164,
      "learning_rate": 8.247293366203667e-08,
      "loss": 0.0561,
      "step": 8016
    },
    {
      "epoch": 1.9839148725562978,
      "grad_norm": 0.05051109939813614,
      "learning_rate": 7.999281931100888e-08,
      "loss": 0.1078,
      "step": 8017
    },
    {
      "epoch": 1.984162336055432,
      "grad_norm": 0.038308899849653244,
      "learning_rate": 7.755056128555537e-08,
      "loss": 0.0459,
      "step": 8018
    },
    {
      "epoch": 1.9844097995545658,
      "grad_norm": 0.0421270914375782,
      "learning_rate": 7.514615995563023e-08,
      "loss": 0.049,
      "step": 8019
    },
    {
      "epoch": 1.9846572630536996,
      "grad_norm": 0.04550613462924957,
      "learning_rate": 7.277961568541436e-08,
      "loss": 0.0458,
      "step": 8020
    },
    {
      "epoch": 1.9849047265528335,
      "grad_norm": 0.04644522815942764,
      "learning_rate": 7.045092883342652e-08,
      "loss": 0.0425,
      "step": 8021
    },
    {
      "epoch": 1.9851521900519673,
      "grad_norm": 0.05380076915025711,
      "learning_rate": 6.816009975232906e-08,
      "loss": 0.0413,
      "step": 8022
    },
    {
      "epoch": 1.9853996535511014,
      "grad_norm": 0.056692469865083694,
      "learning_rate": 6.590712878920546e-08,
      "loss": 0.0624,
      "step": 8023
    },
    {
      "epoch": 1.985647117050235,
      "grad_norm": 0.056120943278074265,
      "learning_rate": 6.369201628525501e-08,
      "loss": 0.0434,
      "step": 8024
    },
    {
      "epoch": 1.985894580549369,
      "grad_norm": 0.03901435434818268,
      "learning_rate": 6.151476257601484e-08,
      "loss": 0.0308,
      "step": 8025
    },
    {
      "epoch": 1.9861420440485027,
      "grad_norm": 0.03304244577884674,
      "learning_rate": 5.937536799133225e-08,
      "loss": 0.0322,
      "step": 8026
    },
    {
      "epoch": 1.9863895075476368,
      "grad_norm": 0.03840230405330658,
      "learning_rate": 5.727383285522581e-08,
      "loss": 0.0346,
      "step": 8027
    },
    {
      "epoch": 1.9866369710467706,
      "grad_norm": 0.04965968057513237,
      "learning_rate": 5.5210157486024206e-08,
      "loss": 0.0502,
      "step": 8028
    },
    {
      "epoch": 1.9868844345459045,
      "grad_norm": 0.05234144628047943,
      "learning_rate": 5.3184342196338496e-08,
      "loss": 0.0604,
      "step": 8029
    },
    {
      "epoch": 1.9871318980450383,
      "grad_norm": 0.03402520343661308,
      "learning_rate": 5.119638729303433e-08,
      "loss": 0.0388,
      "step": 8030
    },
    {
      "epoch": 1.9873793615441722,
      "grad_norm": 0.05539831891655922,
      "learning_rate": 4.924629307720419e-08,
      "loss": 0.0502,
      "step": 8031
    },
    {
      "epoch": 1.9876268250433062,
      "grad_norm": 0.054076485335826874,
      "learning_rate": 4.733405984425066e-08,
      "loss": 0.0479,
      "step": 8032
    },
    {
      "epoch": 1.9878742885424399,
      "grad_norm": 0.03404160961508751,
      "learning_rate": 4.545968788380317e-08,
      "loss": 0.0469,
      "step": 8033
    },
    {
      "epoch": 1.988121752041574,
      "grad_norm": 0.12315691262483597,
      "learning_rate": 4.3623177479829024e-08,
      "loss": 0.1296,
      "step": 8034
    },
    {
      "epoch": 1.9883692155407078,
      "grad_norm": 0.036772869527339935,
      "learning_rate": 4.1824528910494595e-08,
      "loss": 0.0465,
      "step": 8035
    },
    {
      "epoch": 1.9886166790398416,
      "grad_norm": 0.05970873683691025,
      "learning_rate": 4.006374244822086e-08,
      "loss": 0.0588,
      "step": 8036
    },
    {
      "epoch": 1.9888641425389755,
      "grad_norm": 0.13009116053581238,
      "learning_rate": 3.8340818359738903e-08,
      "loss": 0.1313,
      "step": 8037
    },
    {
      "epoch": 1.9891116060381093,
      "grad_norm": 0.04240589216351509,
      "learning_rate": 3.66557569060344e-08,
      "loss": 0.0441,
      "step": 8038
    },
    {
      "epoch": 1.9893590695372434,
      "grad_norm": 0.059859734028577805,
      "learning_rate": 3.500855834234762e-08,
      "loss": 0.0402,
      "step": 8039
    },
    {
      "epoch": 1.989606533036377,
      "grad_norm": 0.03765971586108208,
      "learning_rate": 3.3399222918173436e-08,
      "loss": 0.0371,
      "step": 8040
    },
    {
      "epoch": 1.989853996535511,
      "grad_norm": 0.04459073022007942,
      "learning_rate": 3.182775087728906e-08,
      "loss": 0.0549,
      "step": 8041
    },
    {
      "epoch": 1.990101460034645,
      "grad_norm": 0.04391088709235191,
      "learning_rate": 3.029414245775408e-08,
      "loss": 0.0291,
      "step": 8042
    },
    {
      "epoch": 1.9903489235337788,
      "grad_norm": 0.09052283316850662,
      "learning_rate": 2.8798397891827145e-08,
      "loss": 0.1487,
      "step": 8043
    },
    {
      "epoch": 1.9905963870329126,
      "grad_norm": 0.03901444375514984,
      "learning_rate": 2.734051740610477e-08,
      "loss": 0.0396,
      "step": 8044
    },
    {
      "epoch": 1.9908438505320465,
      "grad_norm": 0.08322849869728088,
      "learning_rate": 2.5920501221382563e-08,
      "loss": 0.1245,
      "step": 8045
    },
    {
      "epoch": 1.9910913140311806,
      "grad_norm": 0.08407215029001236,
      "learning_rate": 2.4538349552821747e-08,
      "loss": 0.0924,
      "step": 8046
    },
    {
      "epoch": 1.9913387775303142,
      "grad_norm": 0.051938265562057495,
      "learning_rate": 2.3194062609727117e-08,
      "loss": 0.0446,
      "step": 8047
    },
    {
      "epoch": 1.9915862410294483,
      "grad_norm": 0.048882681876420975,
      "learning_rate": 2.1887640595741332e-08,
      "loss": 0.0522,
      "step": 8048
    },
    {
      "epoch": 1.9918337045285819,
      "grad_norm": 0.08956338465213776,
      "learning_rate": 2.061908370873389e-08,
      "loss": 0.053,
      "step": 8049
    },
    {
      "epoch": 1.992081168027716,
      "grad_norm": 0.059817634522914886,
      "learning_rate": 1.9388392140884393e-08,
      "loss": 0.0733,
      "step": 8050
    },
    {
      "epoch": 1.9923286315268498,
      "grad_norm": 0.04568201303482056,
      "learning_rate": 1.819556607859929e-08,
      "loss": 0.0555,
      "step": 8051
    },
    {
      "epoch": 1.9925760950259837,
      "grad_norm": 0.08129750937223434,
      "learning_rate": 1.704060570256738e-08,
      "loss": 0.1409,
      "step": 8052
    },
    {
      "epoch": 1.9928235585251175,
      "grad_norm": 0.03857370465993881,
      "learning_rate": 1.5923511187732055e-08,
      "loss": 0.0414,
      "step": 8053
    },
    {
      "epoch": 1.9930710220242513,
      "grad_norm": 0.03979240730404854,
      "learning_rate": 1.4844282703291302e-08,
      "loss": 0.0462,
      "step": 8054
    },
    {
      "epoch": 1.9933184855233854,
      "grad_norm": 0.033891040831804276,
      "learning_rate": 1.3802920412725461e-08,
      "loss": 0.0393,
      "step": 8055
    },
    {
      "epoch": 1.993565949022519,
      "grad_norm": 0.05203624442219734,
      "learning_rate": 1.2799424473797228e-08,
      "loss": 0.0725,
      "step": 8056
    },
    {
      "epoch": 1.9938134125216531,
      "grad_norm": 0.05811012163758278,
      "learning_rate": 1.1833795038468376e-08,
      "loss": 0.0449,
      "step": 8057
    },
    {
      "epoch": 1.994060876020787,
      "grad_norm": 0.055405858904123306,
      "learning_rate": 1.0906032253038545e-08,
      "loss": 0.0473,
      "step": 8058
    },
    {
      "epoch": 1.9943083395199208,
      "grad_norm": 0.06449615955352783,
      "learning_rate": 1.0016136258034215e-08,
      "loss": 0.0463,
      "step": 8059
    },
    {
      "epoch": 1.9945558030190547,
      "grad_norm": 0.0468459315598011,
      "learning_rate": 9.164107188236459e-09,
      "loss": 0.059,
      "step": 8060
    },
    {
      "epoch": 1.9948032665181885,
      "grad_norm": 0.04632985591888428,
      "learning_rate": 8.349945172708705e-09,
      "loss": 0.0485,
      "step": 8061
    },
    {
      "epoch": 1.9950507300173226,
      "grad_norm": 0.0567987821996212,
      "learning_rate": 7.573650334796734e-09,
      "loss": 0.052,
      "step": 8062
    },
    {
      "epoch": 1.9952981935164562,
      "grad_norm": 0.04014862701296806,
      "learning_rate": 6.835222792073159e-09,
      "loss": 0.0517,
      "step": 8063
    },
    {
      "epoch": 1.9955456570155903,
      "grad_norm": 0.04955228418111801,
      "learning_rate": 6.134662656365198e-09,
      "loss": 0.0796,
      "step": 8064
    },
    {
      "epoch": 1.9957931205147241,
      "grad_norm": 0.08245652168989182,
      "learning_rate": 5.471970033837925e-09,
      "loss": 0.0791,
      "step": 8065
    },
    {
      "epoch": 1.996040584013858,
      "grad_norm": 0.056660573929548264,
      "learning_rate": 4.847145024855504e-09,
      "loss": 0.0379,
      "step": 8066
    },
    {
      "epoch": 1.9962880475129918,
      "grad_norm": 0.061200350522994995,
      "learning_rate": 4.260187724036691e-09,
      "loss": 0.0682,
      "step": 8067
    },
    {
      "epoch": 1.9965355110121257,
      "grad_norm": 0.07656279951334,
      "learning_rate": 3.7110982203103495e-09,
      "loss": 0.1136,
      "step": 8068
    },
    {
      "epoch": 1.9967829745112597,
      "grad_norm": 0.042031265795230865,
      "learning_rate": 3.199876596859941e-09,
      "loss": 0.0321,
      "step": 8069
    },
    {
      "epoch": 1.9970304380103934,
      "grad_norm": 0.03466544672846794,
      "learning_rate": 2.7265229311235208e-09,
      "loss": 0.0328,
      "step": 8070
    },
    {
      "epoch": 1.9972779015095274,
      "grad_norm": 0.040242888033390045,
      "learning_rate": 2.2910372947659853e-09,
      "loss": 0.031,
      "step": 8071
    },
    {
      "epoch": 1.997525365008661,
      "grad_norm": 0.0457879863679409,
      "learning_rate": 1.8934197537623377e-09,
      "loss": 0.0303,
      "step": 8072
    },
    {
      "epoch": 1.9977728285077951,
      "grad_norm": 0.04931371659040451,
      "learning_rate": 1.5336703683699326e-09,
      "loss": 0.0383,
      "step": 8073
    },
    {
      "epoch": 1.998020292006929,
      "grad_norm": 0.03448353335261345,
      "learning_rate": 1.2117891930729651e-09,
      "loss": 0.0395,
      "step": 8074
    },
    {
      "epoch": 1.9982677555060628,
      "grad_norm": 0.04662949591875076,
      "learning_rate": 9.27776276610226e-10,
      "loss": 0.0558,
      "step": 8075
    },
    {
      "epoch": 1.9985152190051967,
      "grad_norm": 0.051350872963666916,
      "learning_rate": 6.816316620306129e-10,
      "loss": 0.0597,
      "step": 8076
    },
    {
      "epoch": 1.9987626825043305,
      "grad_norm": 0.07348883897066116,
      "learning_rate": 4.733553865821083e-10,
      "loss": 0.0447,
      "step": 8077
    },
    {
      "epoch": 1.9990101460034646,
      "grad_norm": 0.07636662572622299,
      "learning_rate": 3.0294748182280174e-10,
      "loss": 0.0318,
      "step": 8078
    },
    {
      "epoch": 1.9992576095025982,
      "grad_norm": 0.03441130742430687,
      "learning_rate": 1.704079735931341e-10,
      "loss": 0.0566,
      "step": 8079
    },
    {
      "epoch": 1.9995050730017323,
      "grad_norm": 0.0388561449944973,
      "learning_rate": 7.573688193263096e-11,
      "loss": 0.0515,
      "step": 8080
    },
    {
      "epoch": 1.9997525365008662,
      "grad_norm": 0.03091464377939701,
      "learning_rate": 1.8934221190924917e-11,
      "loss": 0.0342,
      "step": 8081
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.28434672951698303,
      "learning_rate": 0.0,
      "loss": 0.1069,
      "step": 8082
    },
    {
      "epoch": 2.0,
      "step": 8082,
      "total_flos": 5.250854971857961e+18,
      "train_loss": 0.05532350330317402,
      "train_runtime": 62151.6869,
      "train_samples_per_second": 2.08,
      "train_steps_per_second": 0.13
    }
  ],
  "logging_steps": 1,
  "max_steps": 8082,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 5.250854971857961e+18,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
