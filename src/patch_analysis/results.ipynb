{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis\n",
    "\n",
    "In this notebook we plot the patch assessement metrics, and other useful ones from the patch evaluation journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start by loading the results.\n",
    "\n",
    "Note that we remove three bugs (Math-28, Math-44, JacksonDatabind-82) from the results since the function they change is included in Megadiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        bugs = [json.loads(line) for line in f]\n",
    "\n",
    "    # Keep only bugs in the single-function benchmarks\n",
    "    defects4j_sf_bugs = \"../../results/benchmarks/defects4j_sf.txt\"\n",
    "    with open(defects4j_sf_bugs, \"r\") as f:\n",
    "        sf_bugs = set([line.strip() for line in f.readlines()])\n",
    "    humanevaljava_sf_bugs = \"../../results/benchmarks/humanevaljava_sf.txt\"\n",
    "    with open(humanevaljava_sf_bugs, \"r\") as f:\n",
    "        sf_bugs.update([line.strip() for line in f.readlines()])\n",
    "    gitbugjava_sf_bugs = \"../../results/benchmarks/gitbugjava_sf.txt\"\n",
    "    with open(gitbugjava_sf_bugs, \"r\") as f:\n",
    "        sf_bugs.update([line.strip() for line in f.readlines()])\n",
    "\n",
    "    bugs = [bug for bug in bugs if bug[\"identifier\"] in sf_bugs]\n",
    "\n",
    "    # Remove bugs that might be leaked by Megadiff\n",
    "    to_remove = [\"Math-28\", \"Math-44\", \"JacksonDatabind-82\"]\n",
    "    bugs = [bug for bug in bugs if bug[\"identifier\"] not in to_remove]\n",
    "\n",
    "    return bugs\n",
    "\n",
    "def read_multi_loc_bugs(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        bugs = set([line.strip() for line in f.readlines()])\n",
    "    return bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the results we present in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model    Repr      Benchmark  Total  Plausible  Semantical  AST  Exact  Correct Multi-loc\n",
      "      codellama ir3_or2      Defects4J    478        131          83   70     52                 14\n",
      "  codellama-ir4 ir4_or2      Defects4J    476        107          69   60     50                  7\n",
      "          gpt35     gpt      Defects4J    483         71          45   33     23                 11\n",
      "           gpt4     gpt      Defects4J    483        119          72   60     47                 20\n",
      "    repairllama ir1_or1      Defects4J    476         79          45   31     29                  7\n",
      "    repairllama ir1_or3      Defects4J    476         41          24   17     15                  6\n",
      "    repairllama ir1_or4      Defects4J    477         12           3    2      2                  0\n",
      "    repairllama ir2_or2      Defects4J    477        198         139  122    121                 32\n",
      "    repairllama ir3_or2      Defects4J    480        153         102   86     83                 13\n",
      "    repairllama ir4_or2      Defects4J    477        195         144  125    124                 35\n",
      "repairllama-fft ir4_or2      Defects4J    476        146          98   84     66                 11\n",
      "      codellama ir3_or2    GitBug-Java     84         17          12    9      8                  0\n",
      "  codellama-ir4 ir4_or2    GitBug-Java     83         19          13   12     11                  0\n",
      "          gpt35     gpt    GitBug-Java     90          9           8    7      7                  0\n",
      "           gpt4     gpt    GitBug-Java     90         14          10    7      6                  0\n",
      "    repairllama ir1_or1    GitBug-Java     85         10           4    4      4                  0\n",
      "    repairllama ir1_or3    GitBug-Java     85          6           1    1      1                  0\n",
      "    repairllama ir1_or4    GitBug-Java     85          1           1    1      1                  0\n",
      "    repairllama ir2_or2    GitBug-Java     84         23          19   17     16                  2\n",
      "    repairllama ir3_or2    GitBug-Java     84         21          13   12     12                  1\n",
      "    repairllama ir4_or2    GitBug-Java     84         25          20   16     15                  3\n",
      "repairllama-fft ir4_or2    GitBug-Java     83         21          13   11     11                  0\n",
      "      codellama ir3_or2 HumanEval-Java    162        107         103   81     71                 11\n",
      "  codellama-ir4 ir4_or2 HumanEval-Java    162         95          91   72     65                  5\n",
      "          gpt35     gpt HumanEval-Java    162        107          97   63     50                 12\n",
      "           gpt4     gpt HumanEval-Java    162        124         116   74     64                 15\n",
      "    repairllama ir1_or1 HumanEval-Java    162         78          72   54     52                  8\n",
      "    repairllama ir1_or3 HumanEval-Java    162         39          37   21     21                  2\n",
      "    repairllama ir1_or4 HumanEval-Java    162          5           4    2      2                  0\n",
      "    repairllama ir2_or2 HumanEval-Java    162        118         108   77     69                 12\n",
      "    repairllama ir3_or2 HumanEval-Java    162        103          99   68     63                 11\n",
      "    repairllama ir4_or2 HumanEval-Java    162        118         109   82     75                 13\n",
      "repairllama-fft ir4_or2 HumanEval-Java    162        109         100   83     74                 15\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def plot_table(experiments: List[Tuple[str, List[dict]]]):\n",
    "    # Plot a table with the results of each experiment\n",
    "    # The table has the following columns:\n",
    "    # - Model name\n",
    "    # - Representation\n",
    "    # - Benchmark\n",
    "    # - Number of bugs with an exact match fix\n",
    "    # - Number of bugs with a AST match fix\n",
    "    # - Number of bugs with a semantical match fix\n",
    "    # - Number of bugs with a plausible fix\n",
    "    # - Total number of bugs with patches\n",
    "\n",
    "    # Define the table data\n",
    "    data = []\n",
    "    multi_loc_bugs = read_multi_loc_bugs(\"multi-loc-bugs.txt\")\n",
    "    for file_path, bugs in experiments:\n",
    "        # Compute metrics\n",
    "        exact_match = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] is not None and (\n",
    "            any(patch[\"exact_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        ))\n",
    "        ast_match = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] is not None and (\n",
    "            any(patch[\"ast_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        ))\n",
    "        semantical_match = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] is not None and (\n",
    "           any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(patch[\"ast_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        ))\n",
    "        plausible = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] is not None and (\n",
    "            any(patch[\"test\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(patch[\"ast_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        ))\n",
    "        total = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] is not None)\n",
    "        correct_multi_loc = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] is not None and bug[\"identifier\"] in multi_loc_bugs and (\n",
    "           any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        or any(patch[\"ast_match\"] for patch in bug[\"evaluation\"] if patch is not None)\n",
    "        ))\n",
    "\n",
    "        # Extract meta-data from file_path\n",
    "        benchmark = \"Defects4J\" if \"defects4j\" in file_path else \"HumanEval-Java\" if \"humanevaljava\" in file_path else \"GitBug-Java\"\n",
    "        model = \"repairllama-fft\" if \"repairllama-fft\" in file_path else \"repairllama\" if \"repairllama\" in file_path else \"gpt4\" if \"gpt4\" in file_path else \"gpt35\" if \"gpt35\" in file_path else \"codellama-ir4\" if \"codellama-ir4\" in file_path else \"codellama\"\n",
    "        pattern = r\"ir\\d+_or\\d+\"\n",
    "        ir_or = re.search(pattern, file_path)\n",
    "        if ir_or:\n",
    "            ir_or = ir_or.group()\n",
    "        representation = {\"gpt4\": \"gpt\", \"gpt35\": \"gpt\", \"repairllama\": ir_or, \"repairllama-fft\": \"ir4_or2\", \"codellama\": \"ir3_or2\", \"codellama-ir4\": \"ir4_or2\"}[model]\n",
    "\n",
    "        data.append([model, representation, benchmark, total, plausible, semantical_match, ast_match, exact_match, correct_multi_loc])\n",
    "\n",
    "    # Sort the data according to representation\n",
    "    data = sorted(data, key=lambda x: x[1])\n",
    "    # Sort the data according to model\n",
    "    data = sorted(data, key=lambda x: x[0])\n",
    "    # Sort the data according to benchmark\n",
    "    data = sorted(data, key=lambda x: x[2])\n",
    "\n",
    "    # Show the table with pandas, do not split the table\n",
    "    df = pd.DataFrame(data, columns=[\"Model\", \"Repr\", \"Benchmark\", \"Total\", \"Plausible\", \"Semantical\", \"AST\", \"Exact\", \"Correct Multi-loc\"])\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "def plot_experiments(experiments_path: str):\n",
    "    experiments = []\n",
    "    for file_path in Path(experiments_path).glob(\"*.jsonl\"):\n",
    "        experiments.append((file_path.stem, read_jsonl_file(file_path)))\n",
    "\n",
    "    plot_table(experiments)\n",
    "\n",
    "plot_experiments(\"../../results/3_martin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significant and effect size analysis\n",
    "\n",
    "We measure the statistical significance and effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "defects4j_set = set()\n",
    "humanevaljava_set = set()\n",
    "gitbugjava_set = set()\n",
    "\n",
    "with open(\"../../results/benchmarks/defects4j_sf.txt\", \"r\") as f:\n",
    "    defects4j_set.update([line.strip() for line in f.readlines()])\n",
    "\n",
    "with open(\"../../results/benchmarks/humanevaljava_sf.txt\", \"r\") as f:\n",
    "    humanevaljava_set.update([line.strip() for line in f.readlines()])\n",
    "\n",
    "with open(\"../../results/benchmarks/gitbugjava_sf.txt\", \"r\") as f:\n",
    "    gitbugjava_set.update([line.strip() for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_df(experiments: List[Tuple[str, List[dict]]]):\n",
    "    # Generate a granular dataframe with the results of each experiment\n",
    "    # The table has the following columns:\n",
    "    # - Model name\n",
    "    # - Representation\n",
    "    # - Benchmark\n",
    "    # - Bitvector of bugs with an exact match fix\n",
    "    # - Bitvector of bugs with a AST match fix\n",
    "    # - Bitvector of bugs with a semantical match fix\n",
    "    # - Bitvector of bugs with a plausible fix\n",
    "\n",
    "    # Define the table data\n",
    "    data = []\n",
    "    multi_loc_bugs = read_multi_loc_bugs(\"multi-loc-bugs.txt\")\n",
    "    for file_path, bugs in experiments:\n",
    "        # Extract meta-data from file_path\n",
    "        benchmark = \"Defects4J\" if \"defects4j\" in file_path else \"HumanEval-Java\"\n",
    "        model = \"repairllama-fft\" if \"repairllama-fft\" in file_path else \"repairllama\" if \"repairllama\" in file_path else \"gpt4\" if \"gpt4\" in file_path else \"gpt35\" if \"gpt35\" in file_path else \"codellama-ir4\" if \"codellama-ir4\" in file_path else \"codellama\"\n",
    "        pattern = r\"ir\\d+_or\\d+\"\n",
    "        ir_or = re.search(pattern, file_path)\n",
    "        if ir_or:\n",
    "            ir_or = ir_or.group()\n",
    "        representation = {\"gpt4\": \"gpt\", \"gpt35\": \"gpt\", \"repairllama\": ir_or, \"repairllama-fft\": \"ir4_or2\", \"codellama\": \"ir3_or2\", \"codellama-ir4\": \"ir4_or2\"}[model]\n",
    "\n",
    "        # Bitvector index\n",
    "        index = defects4j_set if benchmark == \"Defects4J\" else humanevaljava_set\n",
    "\n",
    "        # Compute bitvectors\n",
    "        exp_df = pd.DataFrame(bugs)\n",
    "        exact_match = []\n",
    "        ast_match = []\n",
    "        semantical_match = []\n",
    "        plausible = []\n",
    "\n",
    "        for bug in index:\n",
    "            bug_df = exp_df[exp_df[\"identifier\"] == bug]\n",
    "            if bug_df.empty or \"evaluation\" not in bug_df or bug_df[\"evaluation\"].values[0] is None:\n",
    "                exact_match.append(False)\n",
    "                ast_match.append(False)\n",
    "                semantical_match.append(False)\n",
    "                plausible.append(False)\n",
    "            else:\n",
    "                evaluation = bug_df[\"evaluation\"].values[0]\n",
    "\n",
    "                exact_match.append(any(patch[\"exact_match\"] for patch in evaluation))\n",
    "                ast_match.append(any(patch[\"ast_match\"] for patch in evaluation) \n",
    "                                 or any(patch[\"exact_match\"] for patch in evaluation))\n",
    "                semantical_match.append(any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in evaluation) \n",
    "                                        or any(patch[\"exact_match\"] for patch in evaluation) \n",
    "                                        or any(patch[\"ast_match\"] for patch in evaluation))\n",
    "                plausible.append(any(patch[\"test\"] for patch in evaluation) \n",
    "                                 or any(patch[\"exact_match\"] for patch in evaluation) \n",
    "                                 or any(patch[\"ast_match\"] for patch in evaluation) \n",
    "                                 or any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in evaluation))\n",
    "\n",
    "        data.append([model, representation, benchmark, plausible, semantical_match, ast_match, exact_match])\n",
    "\n",
    "    # Sort the data according to representation\n",
    "    data = sorted(data, key=lambda x: x[1])\n",
    "    # Sort the data according to model\n",
    "    data = sorted(data, key=lambda x: x[0])\n",
    "    # Sort the data according to benchmark\n",
    "    data = sorted(data, key=lambda x: x[2])\n",
    "\n",
    "    # Show the table with pandas, do not split the table\n",
    "    df = pd.DataFrame(data, columns=[\"Model\", \"Repr\", \"Benchmark\", \"Plausible\", \"Semantical\", \"AST\", \"Exact\"])\n",
    "    return df\n",
    "\n",
    "def generate_granular_df(experiments_path: str):\n",
    "    experiments = []\n",
    "    for file_path in Path(experiments_path).glob(\"*.jsonl\"):\n",
    "        experiments.append((file_path.stem, read_jsonl_file(file_path)))\n",
    "\n",
    "    return generate_df(experiments)\n",
    "\n",
    "experiments_df = generate_granular_df(\"../../results/3_martin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Semantical for merged benchmarks\n",
      "Cochran's Q: 587.61, p-value: 0.0000\n",
      "\n",
      "LaTeX Table:\n",
      "\n",
      "\\begin{table*}[h]\n",
      "\\centering\n",
      "\\caption{P-values from McNemar's test comparing different models.}\n",
      "\\label{tab:mcnemar_results}\n",
      "\\resizebox{\\linewidth}{!}{\n",
      "\\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}\n",
      "\\hline\n",
      "\\textbf{Model} & IR3 x OR2 (no fine-tuning)\n",
      " & IR4 x OR2 (no fine-tuning)\n",
      " & GPT-3.5\n",
      " & GPT-4\n",
      " & IR1 x OR1\n",
      " & IR1 x OR3\n",
      " & IR1 x OR4\n",
      " & IR2 x OR2\n",
      " & IR3 x OR2\n",
      " & IR4 x OR2 (RepairLLaMA)\n",
      " & IR4 x OR2 (full fine-tuning)\n",
      " \\\\\n",
      "\\hline\n",
      "IR3 x OR2 (no fine-tuning) & - & 8.78e-03 & 9.52e-03 & 9.55e-01 & 2.49e-05 & 3.89e-17 & 4.10e-47 & 5.94e-04 & 3.65e-01 & 1.27e-04 & 2.76e-01 \\\\\n",
      "\\hline\n",
      "IR4 x OR2 (no fine-tuning) & 8.78e-03 & - & 2.80e-01 & 1.09e-01 & 4.43e-03 & 9.49e-13 & 1.14e-39 & 4.60e-07 & 1.21e-02 & 6.67e-08 & 4.08e-05 \\\\\n",
      "\\hline\n",
      "GPT-3.5 & 9.52e-03 & 2.80e-01 & - & 6.22e-08 & 1.65e-02 & 4.55e-13 & 3.35e-37 & 1.91e-19 & 2.04e-07 & 7.05e-22 & 1.02e-03 \\\\\n",
      "\\hline\n",
      "GPT-4 & 9.55e-01 & 1.09e-01 & 6.22e-08 & - & 2.47e-10 & 4.42e-25 & 1.11e-50 & 8.74e-07 & 3.02e-01 & 4.10e-08 & 6.08e-01 \\\\\n",
      "\\hline\n",
      "IR1 x OR1 & 2.49e-05 & 4.43e-03 & 1.65e-02 & 2.47e-10 & - & 3.10e-09 & 6.31e-31 & 1.92e-27 & 3.21e-12 & 1.94e-30 & 3.54e-07 \\\\\n",
      "\\hline\n",
      "IR1 x OR3 & 3.89e-17 & 9.49e-13 & 4.55e-13 & 4.42e-25 & 3.10e-09 & - & 1.58e-15 & 1.94e-47 & 5.08e-30 & 3.84e-49 & 1.56e-20 \\\\\n",
      "\\hline\n",
      "IR1 x OR4 & 4.10e-47 & 1.14e-39 & 3.35e-37 & 1.11e-50 & 6.31e-31 & 1.58e-15 & - & 1.13e-72 & 1.66e-54 & 1.77e-74 & 1.61e-52 \\\\\n",
      "\\hline\n",
      "IR2 x OR2 & 5.94e-04 & 4.60e-07 & 1.91e-19 & 8.74e-07 & 1.92e-27 & 1.94e-47 & 1.13e-72 & - & 2.76e-05 & 5.45e-01 & 3.85e-03 \\\\\n",
      "\\hline\n",
      "IR3 x OR2 & 3.65e-01 & 1.21e-02 & 2.04e-07 & 3.02e-01 & 3.21e-12 & 5.08e-30 & 1.66e-54 & 2.76e-05 & - & 7.29e-07 & 9.03e-01 \\\\\n",
      "\\hline\n",
      "IR4 x OR2 (RepairLLaMA) & 1.27e-04 & 6.67e-08 & 7.05e-22 & 4.10e-08 & 1.94e-30 & 3.84e-49 & 1.77e-74 & 5.45e-01 & 7.29e-07 & - & 1.29e-03 \\\\\n",
      "\\hline\n",
      "IR4 x OR2 (full fine-tuning) & 2.76e-01 & 4.08e-05 & 1.02e-03 & 6.08e-01 & 3.54e-07 & 1.56e-20 & 1.61e-52 & 3.85e-03 & 9.03e-01 & 1.29e-03 & - \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "}\n",
      "\\end{table*}\n",
      "\n",
      "Markdown Table:\n",
      "\n",
      "|                              | IR3 x OR2 (no fine-tuning)   | IR4 x OR2 (no fine-tuning)   | GPT-3.5   | GPT-4    | IR1 x OR1   | IR1 x OR3   | IR1 x OR4   | IR2 x OR2   | IR3 x OR2   | IR4 x OR2 (RepairLLaMA)   | IR4 x OR2 (full fine-tuning)   |\n",
      "|:-----------------------------|:-----------------------------|:-----------------------------|:----------|:---------|:------------|:------------|:------------|:------------|:------------|:--------------------------|:-------------------------------|\n",
      "| IR3 x OR2 (no fine-tuning)   | N/A                          | 8.78e-03                     | 9.52e-03  | 9.55e-01 | 2.49e-05    | 3.89e-17    | 4.10e-47    | 5.94e-04    | 3.65e-01    | 1.27e-04                  | 2.76e-01                       |\n",
      "| IR4 x OR2 (no fine-tuning)   | 8.78e-03                     | N/A                          | 2.80e-01  | 1.09e-01 | 4.43e-03    | 9.49e-13    | 1.14e-39    | 4.60e-07    | 1.21e-02    | 6.67e-08                  | 4.08e-05                       |\n",
      "| GPT-3.5                      | 9.52e-03                     | 2.80e-01                     | N/A       | 6.22e-08 | 1.65e-02    | 4.55e-13    | 3.35e-37    | 1.91e-19    | 2.04e-07    | 7.05e-22                  | 1.02e-03                       |\n",
      "| GPT-4                        | 9.55e-01                     | 1.09e-01                     | 6.22e-08  | N/A      | 2.47e-10    | 4.42e-25    | 1.11e-50    | 8.74e-07    | 3.02e-01    | 4.10e-08                  | 6.08e-01                       |\n",
      "| IR1 x OR1                    | 2.49e-05                     | 4.43e-03                     | 1.65e-02  | 2.47e-10 | N/A         | 3.10e-09    | 6.31e-31    | 1.92e-27    | 3.21e-12    | 1.94e-30                  | 3.54e-07                       |\n",
      "| IR1 x OR3                    | 3.89e-17                     | 9.49e-13                     | 4.55e-13  | 4.42e-25 | 3.10e-09    | N/A         | 1.58e-15    | 1.94e-47    | 5.08e-30    | 3.84e-49                  | 1.56e-20                       |\n",
      "| IR1 x OR4                    | 4.10e-47                     | 1.14e-39                     | 3.35e-37  | 1.11e-50 | 6.31e-31    | 1.58e-15    | N/A         | 1.13e-72    | 1.66e-54    | 1.77e-74                  | 1.61e-52                       |\n",
      "| IR2 x OR2                    | 5.94e-04                     | 4.60e-07                     | 1.91e-19  | 8.74e-07 | 1.92e-27    | 1.94e-47    | 1.13e-72    | N/A         | 2.76e-05    | 5.45e-01                  | 3.85e-03                       |\n",
      "| IR3 x OR2                    | 3.65e-01                     | 1.21e-02                     | 2.04e-07  | 3.02e-01 | 3.21e-12    | 5.08e-30    | 1.66e-54    | 2.76e-05    | N/A         | 7.29e-07                  | 9.03e-01                       |\n",
      "| IR4 x OR2 (RepairLLaMA)      | 1.27e-04                     | 6.67e-08                     | 7.05e-22  | 4.10e-08 | 1.94e-30    | 3.84e-49    | 1.77e-74    | 5.45e-01    | 7.29e-07    | N/A                       | 1.29e-03                       |\n",
      "| IR4 x OR2 (full fine-tuning) | 2.76e-01                     | 4.08e-05                     | 1.02e-03  | 6.08e-01 | 3.54e-07    | 1.56e-20    | 1.61e-52    | 3.85e-03    | 9.03e-01    | 1.29e-03                  | N/A                            |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30623/1200271069.py:88: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  formatted_df = p_values_df.applymap(lambda x: \"N/A\" if x == 0 else f\"{x:.2e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.contingency_tables import cochrans_q, mcnemar\n",
    "from itertools import combinations\n",
    "\n",
    "def analyze_model_results(df, results_column='Semantical', alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform statistical analysis on binary results from multiple models and display results in tables.\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing {results_column} for merged benchmarks\")\n",
    "\n",
    "    # Group by Model and Repr, concatenating the result vectors\n",
    "    merged_df = df.groupby(['Model', 'Repr']).agg({results_column: lambda x: np.concatenate(x.values)}).reset_index()\n",
    "\n",
    "    # Extract binary vectors and build dataframe\n",
    "    ext_df = pd.DataFrame(np.vstack(merged_df[results_column].values), index=merged_df.index)\n",
    "\n",
    "    # Compute cochran's q\n",
    "    q_stat, q_p_value, _ = cochrans_q(ext_df.T, return_object=False)\n",
    "    print(f\"Cochran's Q: {q_stat:.2f}, p-value: {q_p_value:.4f}\")\n",
    "\n",
    "    # Create empty dataframe for p-values\n",
    "    p_values = np.zeros((len(merged_df.index), len(merged_df.index)))\n",
    "    \n",
    "    # Compute mcnemar's tests\n",
    "    for i, j in combinations(merged_df.index, 2):\n",
    "        cur_df = ext_df.T[[i, j]]\n",
    "        a = sum(cur_df[i] & cur_df[j])\n",
    "        b = sum(~cur_df[i] & cur_df[j])\n",
    "        c = sum(cur_df[i] & ~cur_df[j])\n",
    "        d = sum(~cur_df[i] & ~cur_df[j])\n",
    "        bunch = mcnemar(np.array([[a, b], [c, d]]))\n",
    "        p_values[i, j] = bunch.pvalue\n",
    "        p_values[j, i] = bunch.pvalue\n",
    "\n",
    "    model_names = [f\"{row['Model']} ({row['Repr'].replace('_', '\\\\_')})\" for _, row in merged_df.iterrows()]\n",
    "    model_names_map = {\n",
    "        \"codellama (ir3\\\\_or2)\": \"IR3 x OR2 (no fine-tuning)\",\n",
    "        \"codellama-ir4 (ir4\\\\_or2)\": \"IR4 x OR2 (no fine-tuning)\",\n",
    "        \"gpt35 (gpt)\": \"GPT-3.5\",\n",
    "        \"gpt4 (gpt)\": \"GPT-4\",\n",
    "        \"repairllama (ir1\\\\_or1)\": \"IR1 x OR1\",\n",
    "        \"repairllama (ir1\\\\_or3)\": \"IR1 x OR3\",\n",
    "        \"repairllama (ir1\\\\_or4)\": \"IR1 x OR4\",\n",
    "        \"repairllama (ir2\\\\_or2)\": \"IR2 x OR2\",\n",
    "        \"repairllama (ir3\\\\_or2)\": \"IR3 x OR2\",\n",
    "        \"repairllama (ir4\\\\_or2)\": \"IR4 x OR2 (RepairLLaMA)\",\n",
    "        \"repairllama-fft (ir4\\\\_or2)\": \"IR4 x OR2 (full fine-tuning)\",\n",
    "    }\n",
    "    model_names = [model_names_map[name] for name in model_names]\n",
    "\n",
    "    p_values_df = pd.DataFrame(p_values, index=merged_df.index, columns=merged_df.index)\n",
    "\n",
    "    # Generate LaTeX table\n",
    "    latex_table = \"\\\\begin{table*}[h]\\n\\\\centering\\n\"\n",
    "    latex_table += \"\\\\caption{P-values from McNemar's test comparing different models.}\\n\"\n",
    "    latex_table += \"\\\\label{tab:mcnemar_results}\\n\"\n",
    "    latex_table += \"\\\\resizebox{\\\\linewidth}{!}{\\n\"\n",
    "    latex_table += \"\\\\begin{tabular}{|l|\" + \"c|\" * len(model_names) + \"}\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    latex_table += \"\\\\textbf{Model} & \" + \" & \".join([f\"{name}\\n\" for name in model_names]) + \" \\\\\\\\\\n\"\n",
    "    latex_table += \"\\\\hline\\n\"\n",
    "    \n",
    "    for i, row_name in enumerate(model_names):\n",
    "        row_values = []\n",
    "        for j in range(len(model_names)):\n",
    "            if i == j:\n",
    "                row_values.append(\"-\")\n",
    "            else:\n",
    "                p_val = p_values[i, j]\n",
    "                # pvalue printed in exponent form with max 2 decimals\n",
    "                row_values.append(f\"{p_val:.2e}\")\n",
    "        latex_table += f\"{row_name} & \" + \" & \".join(row_values) + \" \\\\\\\\\\n\"\n",
    "        latex_table += \"\\\\hline\\n\"\n",
    "    \n",
    "    latex_table += \"\\\\end{tabular}\\n}\\n\\\\end{table*}\"\n",
    "    \n",
    "    print(\"\\nLaTeX Table:\\n\")\n",
    "    print(latex_table)\n",
    "\n",
    "    # print table in Markdown\n",
    "    print(\"\\nMarkdown Table:\\n\")\n",
    "    # Rename the index and columns to match the LaTeX table names\n",
    "    p_values_df.index = model_names\n",
    "    p_values_df.columns = model_names\n",
    "    \n",
    "    # Format the values: replace 0s with \"N/A\" and use 2 decimal precision for other values\n",
    "    formatted_df = p_values_df.applymap(lambda x: \"N/A\" if x == 0 else f\"{x:.2e}\")\n",
    "    \n",
    "    print(formatted_df.to_markdown())\n",
    "\n",
    "analyze_model_results(experiments_df, \"Semantical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic equivalence assessement\n",
    "\n",
    "Another interesting thing to look at is the agreement between raters (André, Sen, Martin).\n",
    "\n",
    "The following process was adopted during manual patch assessement:\n",
    "1. André and Sen both analyse all plausible patches independently\n",
    "2. André and Sen's results are merged, with the patches whose assessement is not agreed upon being flagged\n",
    "3. Martin looks at the flagged patches and breaks the tie (note that if Martin selects one patch as equivalent, the possible remaining flagged patches in the same bug are skipped since they won't change the result)\n",
    "\n",
    "We now want to look at the agreement between:\n",
    "1. André and Sen (across all patches)\n",
    "2. André and Martin (across the patches Martin looked at)\n",
    "3. Sen and Martin (across the patches Martin looked at)\n",
    "\n",
    "For this we compute Cohen's kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "andre_experiments = {}\n",
    "for file_path in Path(\"../../results/1_andre\").glob(\"*.jsonl\"):\n",
    "    experiment_name = file_path.stem.replace(\"_andre\", \"\")\n",
    "    andre_experiments[experiment_name] = read_jsonl_file(file_path)\n",
    "\n",
    "sen_experiments = {}\n",
    "for file_path in Path(\"../../results/1_sen\").glob(\"*.jsonl\"):\n",
    "    experiment_name = file_path.stem.replace(\"_sen\", \"\")\n",
    "    sen_experiments[experiment_name] = read_jsonl_file(file_path)\n",
    "\n",
    "martin_experiments = {}\n",
    "for file_path in Path(\"../../results/3_martin\").glob(\"*.jsonl\"):\n",
    "    experiment_name = file_path.stem.replace(\"_martin\", \"\")\n",
    "    martin_experiments[experiment_name] = read_jsonl_file(file_path)\n",
    "\n",
    "# Ensure that the experiments are the same\n",
    "assert set(andre_experiments.keys()) == set(sen_experiments.keys()) == set(martin_experiments.keys())\n",
    "\n",
    "# We now need to match the experiments between raters. We want to get a single list with tuples of the form (experiment_andre, experiment_sen, experiment_martin, experiment_merged)\n",
    "experiments = []\n",
    "for experiment in andre_experiments.keys():\n",
    "    experiments.append((experiment, andre_experiments[experiment], sen_experiments[experiment], martin_experiments[experiment]))\n",
    "\n",
    "assert len(experiments) == 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches evaluated by both André and Sen: 3105\n",
      "Kappa between André and Sen: 0.7348921816694537\n",
      "Raw agreement between André and Sen: 0.8702093397745572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def get_bug_labels_first_pass(bug):\n",
    "    labels = []\n",
    "    if \"evaluation\" in bug and bug[\"evaluation\"] is not None:\n",
    "        # Skip bugs that were not evaluated\n",
    "        if any(x[\"exact_match\"] or x[\"ast_match\"] for x in bug[\"evaluation\"] if x is not None) or not any(x[\"test\"] for x in bug[\"evaluation\"] if x is not None):\n",
    "            return labels\n",
    "\n",
    "        for i, evaluation in enumerate(bug[\"evaluation\"]):\n",
    "            # Skip if the patch is not plausible\n",
    "            if not evaluation[\"test\"]:\n",
    "                continue\n",
    "\n",
    "            assert \"semantical_match\" in evaluation, f\"Missing semantical_match for {bug['identifier']}\"\n",
    "            labels.append((i, evaluation[\"semantical_match\"]))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def get_labels_first_pass(exp_a, exp_b):\n",
    "    # Sort bugs by bug-id so we get the same order of labels for everyone\n",
    "    exp_a = sorted(exp_a, key=lambda x: x[\"identifier\"])\n",
    "    exp_b = sorted(exp_b, key=lambda x: x[\"identifier\"])\n",
    "\n",
    "    # Ensure they have the same bugs\n",
    "    assert len(exp_a) == len(exp_b), f\"Number of bugs do not match ({len(exp_a)} vs {len(exp_b)})\"\n",
    "    assert all(a[\"identifier\"] == b[\"identifier\"] for a, b in zip(exp_a, exp_b))\n",
    "\n",
    "    labels_a = []\n",
    "    labels_b = []\n",
    "    for bug_a, bug_b in zip(exp_a, exp_b):\n",
    "        labels_bug_a = get_bug_labels_first_pass(bug_a)\n",
    "        labels_bug_b = get_bug_labels_first_pass(bug_b)\n",
    "        if len(labels_bug_a) == len(labels_bug_b) and all([a[0] == b[0] for a, b in zip(labels_bug_a, labels_bug_b)]):\n",
    "            labels_a.extend([a[1] for a in labels_bug_a])\n",
    "            labels_b.extend([b[1] for b in labels_bug_b])\n",
    "        else:\n",
    "            # HACK: In some of Sen's files, the original evaluation did not include any exact_match or ast_match\n",
    "            # This means that the get_labels function_first_pass will return more patches than it should.\n",
    "            # To overcome this we only keep the patches that André looked at by removing the patches only Sen (potentially) looked at\n",
    "            labels_a.extend(labels_bug_a)\n",
    "            labels_b.extend([b[1] for b in labels_bug_b if b[0] in [a[0] for a in labels_bug_a]])\n",
    "\n",
    "    return labels_a, labels_b\n",
    "\n",
    "def compute_kappa(experiments):\n",
    "    # First look at the agreement between André and Sen\n",
    "    andre_labels = []\n",
    "    sen_labels = []\n",
    "\n",
    "    # We will only consider bugs that have been evaluated by both André and Sen\n",
    "    for experiment, andre_experiment, sen_experiment, _ in experiments:\n",
    "        # print(f\"Computing kappa for {experiment}\")\n",
    "        labels_a, labels_b = get_labels_first_pass(andre_experiment, sen_experiment)\n",
    "        andre_labels.extend(labels_a)\n",
    "        sen_labels.extend(labels_b)\n",
    "\n",
    "    assert len(andre_labels) == len(sen_labels), f\"Number of bugs evaluated by André and Sen do not match ({len(andre_labels)} vs {len(sen_labels)})\"\n",
    "\n",
    "    kappa = cohen_kappa_score(andre_labels, sen_labels)\n",
    "    print(f\"Number of patches evaluated by both André and Sen: {len(andre_labels)}\")\n",
    "    print(f\"Kappa between André and Sen: {kappa}\")\n",
    "    print(f\"Raw agreement between André and Sen: {sum(1 for a, b in zip(andre_labels, sen_labels) if a == b) / len(andre_labels)}\")\n",
    "\n",
    "compute_kappa(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation_defects4j_repairllama_ir1_or1\n",
      "evaluation_defects4j_repairllama_ir1_or4\n",
      "evaluation_defects4j_repairllama_ir4_or2\n",
      "evaluation_defects4j_repairllama_ir3_or2\n",
      "evaluation_defects4j_repairllama_ir1_or3\n",
      "evaluation_defects4j_repairllama_ir2_or2\n",
      "evaluation_humanevaljava_repairllama_ir2_or2\n",
      "evaluation_humanevaljava_gpt35_gpt-zero-shot\n",
      "evaluation_humanevaljava_repairllama_ir1_or1\n",
      "evaluation_humanevaljava_repairllama_ir1_or3\n",
      "evaluation_humanevaljava_repairllama_ir4_or2\n",
      "evaluation_humanevaljava_repairllama_ir1_or4\n",
      "evaluation_humanevaljava_gpt4_gpt-zero-shot\n",
      "evaluation_humanevaljava_repairllama_ir3_or2\n",
      "evaluation_defects4j_gpt35_gpt-zero-shot\n",
      "evaluation_defects4j_gpt4_gpt-zero-shot\n",
      "evaluation_gitbugjava_repairllama_ir1_or1\n",
      "evaluation_gitbugjava_repairllama_ir1_or3\n",
      "evaluation_gitbugjava_repairllama_ir2_or2\n",
      "evaluation_gitbugjava_repairllama_ir3_or2\n",
      "evaluation_gitbugjava_repairllama_ir4_or2\n",
      "evaluation_gitbugjava_repairllama_ir1_or4\n",
      "evaluation_gitbugjava_gpt35_gpt-zero-shot\n",
      "evaluation_gitbugjava_gpt4_gpt-zero-shot\n",
      "evaluation_gitbugjava_zero-shot-cloze_codellama\n",
      "evaluation_gitbugjava_zero-shot-cloze_codellama-ir4\n",
      "evaluation_gitbugjava_zero-shot-cloze_repairllama-fft\n",
      "evaluation_defects4j_zero-shot-cloze_repairllama-fft\n",
      "evaluation_defects4j_zero-shot-cloze_codellama\n",
      "evaluation_humanevaljava_zero-shot-cloze_codellama\n",
      "evaluation_humanevaljava_zero-shot-cloze_repairllama-fft\n",
      "evaluation_humanevaljava_zero-shot-cloze_codellama-ir4\n",
      "evaluation_defects4j_zero-shot-cloze_codellama-ir4\n",
      "Number of disagreeing patches evaluated by Martin: 258\n",
      "Kappa between André and Sen: -0.9160621761658032\n",
      "Kappa between André and Martin: 0.036341611144760644\n",
      "Kappa between Sen and Martin: -0.029311187103077563\n",
      "Raw agreement between André and Sen: 0.0\n",
      "Raw agreement between André and Martin: 0.5697674418604651\n",
      "Raw agreement between Sen and Martin: 0.43023255813953487\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def get_bug_labels_martin_pass(bug):\n",
    "    labels = []\n",
    "    if \"evaluation\" in bug and bug[\"evaluation\"] is not None:\n",
    "        # Skip bugs that were not evaluated\n",
    "        if any(x[\"exact_match\"] or x[\"ast_match\"] for x in bug[\"evaluation\"] if x is not None) or not any(x[\"test\"] for x in bug[\"evaluation\"] if x is not None):\n",
    "            return labels\n",
    "\n",
    "        for i, evaluation in enumerate(bug[\"evaluation\"]):\n",
    "            # Skip if the patch is not plausible\n",
    "            if not evaluation[\"test\"]:\n",
    "                continue\n",
    "\n",
    "            # Skip those that are still disagree, which means Martin did not evaluate\n",
    "            if evaluation[\"semantical_match\"] == \"Disagree\":\n",
    "                continue\n",
    "\n",
    "            if evaluation[\"semantical_match\"] == True:\n",
    "                labels.append((i, True))\n",
    "                break\n",
    "\n",
    "            labels.append((i, evaluation[\"semantical_match\"]))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def get_labels_second_pass(exp_a, exp_b, exp_c):\n",
    "    # Sort bugs by bug-id so we get the same order of labels for everyone\n",
    "    exp_a = sorted(exp_a, key=lambda x: x[\"identifier\"])\n",
    "    exp_b = sorted(exp_b, key=lambda x: x[\"identifier\"])\n",
    "    exp_c = sorted(exp_c, key=lambda x: x[\"identifier\"])\n",
    "\n",
    "    # Ensure they have the same bugs\n",
    "    assert len(exp_a) == len(exp_b) == len(exp_c), f\"Number of bugs do not match ({len(exp_a)} vs {len(exp_b)} vs {len(exp_c)})\"\n",
    "    assert all(a[\"identifier\"] == b[\"identifier\"] == c[\"identifier\"] for a, b, c in zip(exp_a, exp_b, exp_c))\n",
    "\n",
    "    labels_a = []\n",
    "    labels_b = []\n",
    "    labels_c = []\n",
    "    for bug_a, bug_b, bug_c in zip(exp_a, exp_b, exp_c):\n",
    "        labels_bug_a = get_bug_labels_first_pass(bug_a)\n",
    "        labels_bug_b = get_bug_labels_first_pass(bug_b)\n",
    "        labels_bug_c = get_bug_labels_martin_pass(bug_c)\n",
    "        # HACK: same as above, plus we only keep those which have different labels in André and Sen's eval round\n",
    "        labels_bug_b = [b for b in labels_bug_b if b[0] in [a[0] for a in labels_bug_a] and b[0] in [c[0] for c in labels_bug_c] and b[1] != {a[0]: a[1] for a in labels_bug_a}[b[0]]]\n",
    "        labels_bug_a = [a for a in labels_bug_a if a[0] in [b[0] for b in labels_bug_b]]\n",
    "        labels_bug_c = [c for c in labels_bug_c if c[0] in [a[0] for a in labels_bug_a]]\n",
    "\n",
    "        # Extend\n",
    "        labels_a.extend([a[1] for a in labels_bug_a])\n",
    "        labels_b.extend([b[1] for b in labels_bug_b])\n",
    "        labels_c.extend([c[1] for c in labels_bug_c])\n",
    "\n",
    "    return labels_a, labels_b, labels_c\n",
    "\n",
    "def compute_kappa(experiments):\n",
    "    andre_labels = []\n",
    "    sen_labels = []\n",
    "    martin_labels = []\n",
    "\n",
    "    for experiment, andre_experiment, sen_experiment, martin_experiment in experiments:\n",
    "        # print(f\"Computing kappa for {experiment}\")\n",
    "        print(experiment)\n",
    "        labels_a, labels_b, labels_c = get_labels_second_pass(andre_experiment, sen_experiment, martin_experiment)\n",
    "        andre_labels.extend(labels_a)\n",
    "        sen_labels.extend(labels_b)\n",
    "        martin_labels.extend(labels_c)\n",
    "\n",
    "    assert len(andre_labels) == len(sen_labels), f\"Number of bugs evaluated by André and Sen do not match ({len(andre_labels)} vs {len(sen_labels)})\"\n",
    "    assert len(andre_labels) == len(martin_labels), f\"Number of bugs evaluated by André and Martin do not match ({len(andre_labels)} vs {len(martin_labels)})\"\n",
    "    assert len(sen_labels) == len(martin_labels), f\"Number of bugs evaluated by Sen and Martin do not match ({len(sen_labels)} vs {len(martin_labels)})\"\n",
    "\n",
    "    kappa_andre_sen = cohen_kappa_score(andre_labels, sen_labels, labels=[True, False])\n",
    "    kappa_andre_martin = cohen_kappa_score(andre_labels, martin_labels, labels=[True, False])\n",
    "    kappa_sen_martin = cohen_kappa_score(sen_labels, martin_labels, labels=[True, False])\n",
    "    print(f\"Number of disagreeing patches evaluated by Martin: {len(andre_labels)}\")\n",
    "    print(f\"Kappa between André and Sen: {kappa_andre_sen}\")\n",
    "    print(f\"Kappa between André and Martin: {kappa_andre_martin}\")\n",
    "    print(f\"Kappa between Sen and Martin: {kappa_sen_martin}\")\n",
    "\n",
    "    print(f\"Raw agreement between André and Sen: {sum(a == b for a, b in zip(andre_labels, sen_labels)) / len(andre_labels)}\")\n",
    "    print(f\"Raw agreement between André and Martin: {sum(a == b for a, b in zip(andre_labels, martin_labels)) / len(andre_labels)}\")\n",
    "    print(f\"Raw agreement between Sen and Martin: {sum(a == b for a, b in zip(sen_labels, martin_labels)) / len(andre_labels)}\")\n",
    "\n",
    "compute_kappa(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: evaluation_defects4j_repairllama_ir4_or2_martin\n",
      "Number of correct multi-loc bugs: 175\n",
      "Chart-26\n",
      "Chart-26\n",
      "Chart-26\n",
      "Chart-26\n",
      "Chart-26\n",
      "Chart-4\n",
      "Chart-4\n",
      "Chart-4\n",
      "Chart-4\n",
      "Chart-4\n",
      "Chart-7\n",
      "Chart-7\n",
      "Chart-7\n",
      "Chart-7\n",
      "Chart-7\n",
      "Closure-101\n",
      "Closure-101\n",
      "Closure-101\n",
      "Closure-101\n",
      "Closure-101\n",
      "Closure-102\n",
      "Closure-102\n",
      "Closure-102\n",
      "Closure-102\n",
      "Closure-102\n",
      "Closure-115\n",
      "Closure-115\n",
      "Closure-115\n",
      "Closure-115\n",
      "Closure-115\n",
      "Closure-124\n",
      "Closure-124\n",
      "Closure-124\n",
      "Closure-124\n",
      "Closure-124\n",
      "Closure-128\n",
      "Closure-128\n",
      "Closure-128\n",
      "Closure-128\n",
      "Closure-128\n",
      "Closure-13\n",
      "Closure-13\n",
      "Closure-13\n",
      "Closure-13\n",
      "Closure-13\n",
      "Compress-32\n",
      "Compress-32\n",
      "Compress-32\n",
      "Compress-32\n",
      "Compress-32\n",
      "Compress-44\n",
      "Compress-44\n",
      "Compress-44\n",
      "Compress-44\n",
      "Compress-44\n",
      "Compress-45\n",
      "Compress-45\n",
      "Compress-45\n",
      "Compress-45\n",
      "Compress-45\n",
      "Compress-7\n",
      "Compress-7\n",
      "Compress-7\n",
      "Compress-7\n",
      "Compress-7\n",
      "Csv-5\n",
      "Csv-5\n",
      "Csv-5\n",
      "Csv-5\n",
      "Csv-5\n",
      "Csv-6\n",
      "Csv-6\n",
      "Csv-6\n",
      "Csv-6\n",
      "Csv-6\n",
      "Gson-16\n",
      "Gson-16\n",
      "Gson-16\n",
      "Gson-16\n",
      "Gson-16\n",
      "Gson-6\n",
      "Gson-6\n",
      "Gson-6\n",
      "Gson-6\n",
      "Gson-6\n",
      "JacksonDatabind-24\n",
      "JacksonDatabind-24\n",
      "JacksonDatabind-24\n",
      "JacksonDatabind-24\n",
      "JacksonDatabind-24\n",
      "JacksonDatabind-47\n",
      "JacksonDatabind-47\n",
      "JacksonDatabind-47\n",
      "JacksonDatabind-47\n",
      "JacksonDatabind-47\n",
      "JacksonDatabind-49\n",
      "JacksonDatabind-49\n",
      "JacksonDatabind-49\n",
      "JacksonDatabind-49\n",
      "JacksonDatabind-49\n",
      "JacksonDatabind-54\n",
      "JacksonDatabind-54\n",
      "JacksonDatabind-54\n",
      "JacksonDatabind-54\n",
      "JacksonDatabind-54\n",
      "JacksonDatabind-67\n",
      "JacksonDatabind-67\n",
      "JacksonDatabind-67\n",
      "JacksonDatabind-67\n",
      "JacksonDatabind-67\n",
      "JacksonDatabind-83\n",
      "JacksonDatabind-83\n",
      "JacksonDatabind-83\n",
      "JacksonDatabind-83\n",
      "JacksonDatabind-83\n",
      "Jsoup-49\n",
      "Jsoup-49\n",
      "Jsoup-49\n",
      "Jsoup-49\n",
      "Jsoup-49\n",
      "Jsoup-6\n",
      "Jsoup-6\n",
      "Jsoup-6\n",
      "Jsoup-6\n",
      "Jsoup-6\n",
      "Jsoup-64\n",
      "Jsoup-64\n",
      "Jsoup-64\n",
      "Jsoup-64\n",
      "Jsoup-64\n",
      "Jsoup-80\n",
      "Jsoup-80\n",
      "Jsoup-80\n",
      "Jsoup-80\n",
      "Jsoup-80\n",
      "Jsoup-85\n",
      "Jsoup-85\n",
      "Jsoup-85\n",
      "Jsoup-85\n",
      "Jsoup-85\n",
      "Lang-10\n",
      "Lang-10\n",
      "Lang-10\n",
      "Lang-10\n",
      "Lang-10\n",
      "Math-3\n",
      "Math-3\n",
      "Math-3\n",
      "Math-3\n",
      "Math-3\n",
      "Math-72\n",
      "Math-72\n",
      "Math-72\n",
      "Math-72\n",
      "Math-72\n",
      "Math-79\n",
      "Math-79\n",
      "Math-79\n",
      "Math-79\n",
      "Math-79\n",
      "Math-8\n",
      "Math-8\n",
      "Math-8\n",
      "Math-8\n",
      "Math-8\n",
      "Math-86\n",
      "Math-86\n",
      "Math-86\n",
      "Math-86\n",
      "Math-86\n",
      "Math-95\n",
      "Math-95\n",
      "Math-95\n",
      "Math-95\n",
      "Math-95\n",
      "Experiment: evaluation_humanevaljava_repairllama_ir4_or2_martin\n",
      "Number of correct multi-loc bugs: 65\n",
      "ANTI_SHUFFLE\n",
      "ANTI_SHUFFLE\n",
      "ANTI_SHUFFLE\n",
      "ANTI_SHUFFLE\n",
      "ANTI_SHUFFLE\n",
      "CORRECT_BRACKETING\n",
      "CORRECT_BRACKETING\n",
      "CORRECT_BRACKETING\n",
      "CORRECT_BRACKETING\n",
      "CORRECT_BRACKETING\n",
      "COUNT_UP_TO\n",
      "COUNT_UP_TO\n",
      "COUNT_UP_TO\n",
      "COUNT_UP_TO\n",
      "COUNT_UP_TO\n",
      "FLIP_CASE\n",
      "FLIP_CASE\n",
      "FLIP_CASE\n",
      "FLIP_CASE\n",
      "FLIP_CASE\n",
      "GREATEST_COMMON_DIVISOR\n",
      "GREATEST_COMMON_DIVISOR\n",
      "GREATEST_COMMON_DIVISOR\n",
      "GREATEST_COMMON_DIVISOR\n",
      "GREATEST_COMMON_DIVISOR\n",
      "IS_PALINDROME\n",
      "IS_PALINDROME\n",
      "IS_PALINDROME\n",
      "IS_PALINDROME\n",
      "IS_PALINDROME\n",
      "LARGEST_SMALLEST_INTEGERS\n",
      "LARGEST_SMALLEST_INTEGERS\n",
      "LARGEST_SMALLEST_INTEGERS\n",
      "LARGEST_SMALLEST_INTEGERS\n",
      "LARGEST_SMALLEST_INTEGERS\n",
      "MODP\n",
      "MODP\n",
      "MODP\n",
      "MODP\n",
      "MODP\n",
      "NUMERICAL_LETTER_GRADE\n",
      "NUMERICAL_LETTER_GRADE\n",
      "NUMERICAL_LETTER_GRADE\n",
      "NUMERICAL_LETTER_GRADE\n",
      "NUMERICAL_LETTER_GRADE\n",
      "RESCALE_TO_UNIT\n",
      "RESCALE_TO_UNIT\n",
      "RESCALE_TO_UNIT\n",
      "RESCALE_TO_UNIT\n",
      "RESCALE_TO_UNIT\n",
      "SORT_ARRAY\n",
      "SORT_ARRAY\n",
      "SORT_ARRAY\n",
      "SORT_ARRAY\n",
      "SORT_ARRAY\n",
      "STRONGEST_EXTENSION\n",
      "STRONGEST_EXTENSION\n",
      "STRONGEST_EXTENSION\n",
      "STRONGEST_EXTENSION\n",
      "STRONGEST_EXTENSION\n",
      "X_OR_Y\n",
      "X_OR_Y\n",
      "X_OR_Y\n",
      "X_OR_Y\n",
      "X_OR_Y\n",
      "Experiment: evaluation_gitbugjava_repairllama_ir4_or2_martin\n",
      "Number of correct multi-loc bugs: 3\n",
      "aws-aws-secretsmanager-jdbc-d25e52d637cf\n",
      "traccar-traccar-1b8993293646\n",
      "traccar-traccar-8de9a36abef8\n"
     ]
    }
   ],
   "source": [
    "def print_multi_loc_bugs():\n",
    "    multi_loc_bugs = read_multi_loc_bugs(\"multi-loc-bugs.txt\")\n",
    "    experiments = []\n",
    "    for file_path in Path(\"../../results/3_martin\").glob(\"*.jsonl\"):\n",
    "        if \"ir4_or2\" in file_path.stem:\n",
    "            experiments.append((file_path.stem, read_jsonl_file(file_path)))\n",
    "\n",
    "    for file_path, bugs in experiments:\n",
    "        print(f\"Experiment: {file_path}\")\n",
    "        correct_multi_loc = list(bug for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] is not None and bug[\"identifier\"] in multi_loc_bugs and (\n",
    "           any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in bug[\"evaluation\"])\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"])\n",
    "        or any(patch[\"ast_match\"] for patch in bug[\"evaluation\"])\n",
    "        ))\n",
    "        print(f\"Number of correct multi-loc bugs: {len(correct_multi_loc)}\")\n",
    "        for bug in sorted(correct_multi_loc, key=lambda x: x[\"identifier\"]):\n",
    "            print(f\"{bug['identifier']}\")\n",
    "\n",
    "print_multi_loc_bugs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
