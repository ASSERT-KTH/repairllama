{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Analysis\n",
    "\n",
    "In this notebook we plot the patch assessement metrics, and other useful ones from the patch evaluation journey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start by loading the results.\n",
    "\n",
    "Note that we remove three bugs (Math-28, Math-44, JacksonDatabind-82) from the results since the function they change is included in Megadiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        bugs = [json.loads(line) for line in f]\n",
    "\n",
    "    # Remove bugs that might be leaked by Megadiff\n",
    "    to_remove = [\"Math-28\", \"Math-44\", \"JacksonDatabind-82\"]\n",
    "    bugs = [bug for bug in bugs if bug[\"identifier\"] not in to_remove]\n",
    "\n",
    "    return bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the results we present in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Model    Repr      Benchmark  Total  Plausible  Semantical  AST  Exact\n",
      "      codellama ir3_or2      Defects4J    478        131          83   70     52\n",
      "          gpt35     gpt      Defects4J    458         68          42   31     23\n",
      "           gpt4     gpt      Defects4J    461        114          68   57     47\n",
      "    repairllama ir1_or1      Defects4J    476         79          45   31     29\n",
      "    repairllama ir1_or3      Defects4J    476         41          24   17     15\n",
      "    repairllama ir1_or4      Defects4J    477         12           3    2      2\n",
      "    repairllama ir2_or2      Defects4J    477        198         140  122    121\n",
      "    repairllama ir3_or2      Defects4J    480        153         102   86     83\n",
      "    repairllama ir4_or2      Defects4J    477        195         144  125    124\n",
      "repairllama-fft ir4_or2      Defects4J    476        146          98   84     66\n",
      "      codellama ir3_or2 HumanEval-Java    162        107         103   81     71\n",
      "          gpt35     gpt HumanEval-Java    162        107          97   63     50\n",
      "           gpt4     gpt HumanEval-Java    162        124         116   74     64\n",
      "    repairllama ir1_or1 HumanEval-Java    162         78          72   54     52\n",
      "    repairllama ir1_or3 HumanEval-Java    162         39          37   21     21\n",
      "    repairllama ir1_or4 HumanEval-Java    162          5           4    2      2\n",
      "    repairllama ir2_or2 HumanEval-Java    162        118         108   77     69\n",
      "    repairllama ir3_or2 HumanEval-Java    162        103          99   68     63\n",
      "    repairllama ir4_or2 HumanEval-Java    162        118         109   82     75\n",
      "repairllama-fft ir4_or2 HumanEval-Java    162        109         100   83     74\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def plot_table(experiments: List[Tuple[str, List[dict]]]):\n",
    "    # Plot a table with the results of each experiment\n",
    "    # The table has the following columns:\n",
    "    # - Model name\n",
    "    # - Representation\n",
    "    # - Benchmark\n",
    "    # - Number of bugs with an exact match fix\n",
    "    # - Number of bugs with a AST match fix\n",
    "    # - Number of bugs with a semantical match fix\n",
    "    # - Number of bugs with a plausible fix\n",
    "    # - Total number of bugs with patches\n",
    "\n",
    "    # Define the table data\n",
    "    data = []\n",
    "    for file_path, bugs in experiments:\n",
    "        # Compute metrics\n",
    "        exact_match = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] != None and (any(patch[\"exact_match\"] for patch in bug[\"evaluation\"])))\n",
    "        ast_match = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] != None and (\n",
    "            any(patch[\"ast_match\"] for patch in bug[\"evaluation\"])\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"])\n",
    "        ))\n",
    "        semantical_match = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] != None and (\n",
    "           any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in bug[\"evaluation\"])\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"])\n",
    "        or any(patch[\"ast_match\"] for patch in bug[\"evaluation\"])\n",
    "        ))\n",
    "        plausible = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] != None and (\n",
    "            any(patch[\"test\"] for patch in bug[\"evaluation\"])\n",
    "        or any(patch[\"exact_match\"] for patch in bug[\"evaluation\"])\n",
    "        or any(patch[\"ast_match\"] for patch in bug[\"evaluation\"])\n",
    "        or any(\"semantical_match\" in patch and patch[\"semantical_match\"] == True for patch in bug[\"evaluation\"])\n",
    "        ))\n",
    "        total = sum(1 for bug in bugs if \"evaluation\" in bug and bug[\"evaluation\"] != None)\n",
    "\n",
    "        # Extract meta-data from file_path\n",
    "        benchmark = \"Defects4J\" if \"defects4j\" in file_path else \"HumanEval-Java\"\n",
    "        model = \"repairllama-fft\" if \"repairllama-fft\" in file_path else \"repairllama\" if \"repairllama\" in file_path else \"gpt4\" if \"gpt4\" in file_path else \"gpt35\" if \"gpt35\" in file_path else \"codellama\"\n",
    "        pattern = r\"ir\\d+_or\\d+\"\n",
    "        ir_or = re.search(pattern, file_path)\n",
    "        if ir_or:\n",
    "            ir_or = ir_or.group()\n",
    "        representation = {\"gpt4\": \"gpt\", \"gpt35\": \"gpt\", \"repairllama\": ir_or, \"repairllama-fft\": \"ir4_or2\", \"codellama\": \"ir3_or2\"}[model]\n",
    "\n",
    "        data.append([model, representation, benchmark, total, plausible, semantical_match, ast_match, exact_match])\n",
    "\n",
    "    # Sort the data according to representation\n",
    "    data = sorted(data, key=lambda x: x[1])\n",
    "    # Sort the data according to model\n",
    "    data = sorted(data, key=lambda x: x[0])\n",
    "    # Sort the data according to benchmark\n",
    "    data = sorted(data, key=lambda x: x[2])\n",
    "\n",
    "    # Show the table with pandas, do not split the table\n",
    "    df = pd.DataFrame(data, columns=[\"Model\", \"Repr\", \"Benchmark\", \"Total\", \"Plausible\", \"Semantical\", \"AST\", \"Exact\"])\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "def plot_experiments(experiments_path: str):\n",
    "    experiments = []\n",
    "    for file_path in Path(experiments_path).glob(\"*.jsonl\"):\n",
    "        experiments.append((file_path.stem, read_jsonl_file(file_path)))\n",
    "\n",
    "    plot_table(experiments)\n",
    "\n",
    "plot_experiments(\"../../results/3_martin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic equivalence assessement\n",
    "\n",
    "Another interesting thing to look at is the agreement between raters (André, Sen, Martin).\n",
    "\n",
    "The following process was adopted during manual patch assessement:\n",
    "1. André and Sen both analyse all plausible patches independently\n",
    "2. André and Sen's results are merged, with the patches whose assessement is not agreed upon being flagged\n",
    "3. Martin looks at the flagged patches and breaks the tie (note that if Martin selects one patch as equivalent, the possible remaining flagged patches in the same bug are skipped since they won't change the result)\n",
    "\n",
    "We now want to look at the agreement between:\n",
    "1. André and Sen (across all patches)\n",
    "2. André and Martin (across the patches Martin looked at)\n",
    "3. Sen and Martin (across the patches Martin looked at)\n",
    "\n",
    "For this we compute Cohen's kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "andre_experiments = {}\n",
    "for file_path in Path(\"../../results/1_andre\").glob(\"*.jsonl\"):\n",
    "    experiment_name = file_path.stem.replace(\"_andre\", \"\")\n",
    "    andre_experiments[experiment_name] = read_jsonl_file(file_path)\n",
    "\n",
    "sen_experiments = {}\n",
    "for file_path in Path(\"../../results/1_sen\").glob(\"*.jsonl\"):\n",
    "    experiment_name = file_path.stem.replace(\"_sen\", \"\")\n",
    "    sen_experiments[experiment_name] = read_jsonl_file(file_path)\n",
    "\n",
    "martin_experiments = {}\n",
    "for file_path in Path(\"../../results/3_martin\").glob(\"*.jsonl\"):\n",
    "    experiment_name = file_path.stem.replace(\"_martin\", \"\")\n",
    "    martin_experiments[experiment_name] = read_jsonl_file(file_path)\n",
    "\n",
    "# Ensure that the experiments are the same\n",
    "assert set(andre_experiments.keys()) == set(sen_experiments.keys()) == set(martin_experiments.keys())\n",
    "\n",
    "# We now need to match the experiments between raters. We want to get a single list with tuples of the form (experiment_andre, experiment_sen, experiment_martin, experiment_merged)\n",
    "experiments = []\n",
    "for experiment in andre_experiments.keys():\n",
    "    experiments.append((experiment, andre_experiments[experiment], sen_experiments[experiment], martin_experiments[experiment]))\n",
    "\n",
    "assert len(experiments) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches evaluated by both André and Sen: 2342\n",
      "Kappa between André and Sen: 0.6886071670293628\n",
      "Raw agreement between André and Sen: 0.8441502988898377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def get_bug_labels_first_pass(bug):\n",
    "    labels = []\n",
    "    if \"evaluation\" in bug and bug[\"evaluation\"] != None:\n",
    "        # Skip bugs that were not evaluated\n",
    "        if any(x[\"exact_match\"] or x[\"ast_match\"] for x in bug[\"evaluation\"]) or not any(x[\"test\"] for x in bug[\"evaluation\"]):\n",
    "            return labels\n",
    "\n",
    "        for i, evaluation in enumerate(bug[\"evaluation\"]):\n",
    "            # Skip if the patch is not plausible\n",
    "            if not evaluation[\"test\"]:\n",
    "                continue\n",
    "\n",
    "            assert \"semantical_match\" in evaluation, f\"Missing semantical_match for {bug['identifier']}\"\n",
    "            labels.append((i, evaluation[\"semantical_match\"]))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def get_labels_first_pass(exp_a, exp_b):\n",
    "    # Sort bugs by bug-id so we get the same order of labels for everyone\n",
    "    exp_a = sorted(exp_a, key=lambda x: x[\"identifier\"])\n",
    "    exp_b = sorted(exp_b, key=lambda x: x[\"identifier\"])\n",
    "\n",
    "    # Ensure they have the same bugs\n",
    "    assert len(exp_a) == len(exp_b), f\"Number of bugs do not match ({len(exp_a)} vs {len(exp_b)})\"\n",
    "    assert all(a[\"identifier\"] == b[\"identifier\"] for a, b in zip(exp_a, exp_b))\n",
    "\n",
    "    labels_a = []\n",
    "    labels_b = []\n",
    "    for bug_a, bug_b in zip(exp_a, exp_b):\n",
    "        labels_bug_a = get_bug_labels_first_pass(bug_a)\n",
    "        labels_bug_b = get_bug_labels_first_pass(bug_b)\n",
    "        if len(labels_bug_a) == len(labels_bug_b) and all([a[0] == b[0] for a, b in zip(labels_bug_a, labels_bug_b)]):\n",
    "            labels_a.extend([a[1] for a in labels_bug_a])\n",
    "            labels_b.extend([b[1] for b in labels_bug_b])\n",
    "        else:\n",
    "            # HACK: In some of Sen's files, the original evaluation did not include any exact_match or ast_match\n",
    "            # This means that the get_labels function_first_pass will return more patches than it should.\n",
    "            # To overcome this we only keep the patches that André looked at by removing the patches only Sen (potentially) looked at\n",
    "            labels_a.extend(labels_bug_a)\n",
    "            labels_b.extend([b[1] for b in labels_bug_b if b[0] in [a[0] for a in labels_bug_a]])\n",
    "\n",
    "    return labels_a, labels_b\n",
    "\n",
    "def compute_kappa(experiments):\n",
    "    # First look at the agreement between André and Sen\n",
    "    andre_labels = []\n",
    "    sen_labels = []\n",
    "\n",
    "    # We will only consider bugs that have been evaluated by both André and Sen\n",
    "    for experiment, andre_experiment, sen_experiment, _ in experiments:\n",
    "        # print(f\"Computing kappa for {experiment}\")\n",
    "        labels_a, labels_b = get_labels_first_pass(andre_experiment, sen_experiment)\n",
    "        andre_labels.extend(labels_a)\n",
    "        sen_labels.extend(labels_b)\n",
    "\n",
    "    assert len(andre_labels) == len(sen_labels), f\"Number of bugs evaluated by André and Sen do not match ({len(andre_labels)} vs {len(sen_labels)})\"\n",
    "\n",
    "    kappa = cohen_kappa_score(andre_labels, sen_labels)\n",
    "    print(f\"Number of patches evaluated by both André and Sen: {len(andre_labels)}\")\n",
    "    print(f\"Kappa between André and Sen: {kappa}\")\n",
    "    print(f\"Raw agreement between André and Sen: {sum(1 for a, b in zip(andre_labels, sen_labels) if a == b) / len(andre_labels)}\")\n",
    "\n",
    "compute_kappa(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of disagreeing patches evaluated by Martin: 244\n",
      "Kappa between André and Sen: -0.7970419559311801\n",
      "Kappa between André and Martin: 0.04116026575919618\n",
      "Kappa between Sen and Martin: -0.029151842075060275\n",
      "Raw agreement between André and Sen: 0.0\n",
      "Raw agreement between André and Martin: 0.6024590163934426\n",
      "Raw agreement between Sen and Martin: 0.3975409836065574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def get_bug_labels_martin_pass(bug):\n",
    "    labels = []\n",
    "    if \"evaluation\" in bug and bug[\"evaluation\"] != None:\n",
    "        # Skip bugs that were not evaluated\n",
    "        if any(x[\"exact_match\"] or x[\"ast_match\"] for x in bug[\"evaluation\"]) or not any(x[\"test\"] for x in bug[\"evaluation\"]):\n",
    "            return labels\n",
    "\n",
    "        for i, evaluation in enumerate(bug[\"evaluation\"]):\n",
    "            # Skip if the patch is not plausible\n",
    "            if not evaluation[\"test\"]:\n",
    "                continue\n",
    "\n",
    "            # Skip those that are still disagree, which means Martin did not evaluate\n",
    "            if evaluation[\"semantical_match\"] == \"Disagree\":\n",
    "                continue\n",
    "\n",
    "            if evaluation[\"semantical_match\"] == True:\n",
    "                labels.append((i, True))\n",
    "                break\n",
    "\n",
    "            labels.append((i, evaluation[\"semantical_match\"]))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def get_labels_second_pass(exp_a, exp_b, exp_c):\n",
    "    # Sort bugs by bug-id so we get the same order of labels for everyone\n",
    "    exp_a = sorted(exp_a, key=lambda x: x[\"identifier\"])\n",
    "    exp_b = sorted(exp_b, key=lambda x: x[\"identifier\"])\n",
    "    exp_c = sorted(exp_c, key=lambda x: x[\"identifier\"])\n",
    "\n",
    "    # Ensure they have the same bugs\n",
    "    assert len(exp_a) == len(exp_b) == len(exp_c), f\"Number of bugs do not match ({len(exp_a)} vs {len(exp_b)} vs {len(exp_c)})\"\n",
    "    assert all(a[\"identifier\"] == b[\"identifier\"] == c[\"identifier\"] for a, b, c in zip(exp_a, exp_b, exp_c))\n",
    "\n",
    "    labels_a = []\n",
    "    labels_b = []\n",
    "    labels_c = []\n",
    "    for bug_a, bug_b, bug_c in zip(exp_a, exp_b, exp_c):\n",
    "        labels_bug_a = get_bug_labels_first_pass(bug_a)\n",
    "        labels_bug_b = get_bug_labels_first_pass(bug_b)\n",
    "        labels_bug_c = get_bug_labels_martin_pass(bug_c)\n",
    "        # HACK: same as above, plus we only keep those which have different labels in André and Sen's eval round\n",
    "        labels_bug_b = [b for b in labels_bug_b if b[0] in [a[0] for a in labels_bug_a] and b[0] in [c[0] for c in labels_bug_c] and b[1] != {a[0]: a[1] for a in labels_bug_a}[b[0]]]\n",
    "        labels_bug_a = [a for a in labels_bug_a if a[0] in [b[0] for b in labels_bug_b]]\n",
    "        labels_bug_c = [c for c in labels_bug_c if c[0] in [a[0] for a in labels_bug_a]]\n",
    "\n",
    "        # Extend\n",
    "        labels_a.extend([a[1] for a in labels_bug_a])\n",
    "        labels_b.extend([b[1] for b in labels_bug_b])\n",
    "        labels_c.extend([c[1] for c in labels_bug_c])\n",
    "\n",
    "    return labels_a, labels_b, labels_c\n",
    "\n",
    "def compute_kappa(experiments):\n",
    "    andre_labels = []\n",
    "    sen_labels = []\n",
    "    martin_labels = []\n",
    "\n",
    "    for experiment, andre_experiment, sen_experiment, martin_experiment in experiments:\n",
    "        # print(f\"Computing kappa for {experiment}\")\n",
    "        labels_a, labels_b, labels_c = get_labels_second_pass(andre_experiment, sen_experiment, martin_experiment)\n",
    "        andre_labels.extend(labels_a)\n",
    "        sen_labels.extend(labels_b)\n",
    "        martin_labels.extend(labels_c)\n",
    "\n",
    "    assert len(andre_labels) == len(sen_labels), f\"Number of bugs evaluated by André and Sen do not match ({len(andre_labels)} vs {len(sen_labels)})\"\n",
    "    assert len(andre_labels) == len(martin_labels), f\"Number of bugs evaluated by André and Martin do not match ({len(andre_labels)} vs {len(martin_labels)})\"\n",
    "    assert len(sen_labels) == len(martin_labels), f\"Number of bugs evaluated by Sen and Martin do not match ({len(sen_labels)} vs {len(martin_labels)})\"\n",
    "\n",
    "    kappa_andre_sen = cohen_kappa_score(andre_labels, sen_labels, labels=[True, False])\n",
    "    kappa_andre_martin = cohen_kappa_score(andre_labels, martin_labels, labels=[True, False])\n",
    "    kappa_sen_martin = cohen_kappa_score(sen_labels, martin_labels, labels=[True, False])\n",
    "    print(f\"Number of disagreeing patches evaluated by Martin: {len(andre_labels)}\")\n",
    "    print(f\"Kappa between André and Sen: {kappa_andre_sen}\")\n",
    "    print(f\"Kappa between André and Martin: {kappa_andre_martin}\")\n",
    "    print(f\"Kappa between Sen and Martin: {kappa_sen_martin}\")\n",
    "\n",
    "    print(f\"Raw agreement between André and Sen: {sum(a == b for a, b in zip(andre_labels, sen_labels)) / len(andre_labels)}\")\n",
    "    print(f\"Raw agreement between André and Martin: {sum(a == b for a, b in zip(andre_labels, martin_labels)) / len(andre_labels)}\")\n",
    "    print(f\"Raw agreement between Sen and Martin: {sum(a == b for a, b in zip(sen_labels, martin_labels)) / len(andre_labels)}\")\n",
    "\n",
    "compute_kappa(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
